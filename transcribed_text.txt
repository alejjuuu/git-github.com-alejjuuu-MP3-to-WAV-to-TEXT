welcome to the first episode of machine learning guide or an algae which is a high caste structured as an audio course whose intent is to teach you the high level principles of machine learning in artificial on tall jumps in this pie cast i will provide you the bird's eye overview of the fundamental concepts a machine learning this includes things like models and algorithms both shall loring models and people earning models shallow learning machine learning models
include sings like linear logistic regression naive bazan decision trees which has a machine learning newbie you may not have heard of but i also cover people earning models which i'm sure you have heard of things like neural networks convolution ono met works and ricard neural networks all discuss the languages and frame works that you wanna use in machine where in all talk about python to answer flow psychic weren't hiked porch these types of topics all discuss that
allowable the math you need to know to succeed in machine learning this includes calculus statistics and when you're algebra and i will go into all these topics in as much data as ordeal all hours and then i will provide you with the resources needed to deep dive any of these topics offline to master the details that require a visual element wetherbee textbooks or videos this bypasses of course intended for anybody interested in machine
learning but it tends to be too common subscribers to the pod cast the first is managers and executives they're interested in knowing johnson off machine learning to be dangerous whether it's to assess what technologies are available to use in their projects were at their company or maybe they want to intelligently conversed with their machine learning and a science employees the second our people who want to learn machine morning maybe they're considering pivoting from a different
the old into the machine learning field machine learning or official told and sunday as science by myself calm from all wet and mobile development background and decided that i want to become a machine learning engineer and self hot myself machine learning and ended up getting work in the field i only have a bachelor's in computer science so that shows you that it is doable to self teach machine learning effectively enough to land work in
the industry but in order to do that you have to be very rigorous and diligent in yourself teaching jury and you're going to want a very structured police force guide and that is what i'm going to provide to you separate from the spot past it's as important component of look you're listening experienced machine and died they you visit the resources section of my web site on my website o. c. develop dot com o. c. d. e.
the p. l. dot com forward slash am algae there is a resource is tab click that hat and you'll get a fair prickly structured list of resources with filters so you can filter by format wetherbee audio book textbook video corset cetera price filters callie filters and so on and this tree structured resource guided will provide you the step by step in sequential order what re
sources you're going to want to consume separate from this pod cast to most efficiently manage your rule earning power through your journey of learning she learned i have spent a lot of time and effort on curing this list of resources on subscribe to many machine learning and is signed some bread it's an art says feeds by frequently proves the syllabus of various courses and the textbooks that they assigned to their students and their
are of course just very well known machine learning resources out there like andrew being corsair course and fast got a guy so this spot cast will provide you the overview of the models on the concepts but if you're serious about warning machine learning you're gonna wanna also spent time on the resources list where i have crafted for you a step by step guide to effectively learn machine learn now this resources list is not only for that
the divers there are also plenty of great resource is listed on that page including other pie casts similar to machine learning guide some of my favor pie casts that i listen to out their various pod castes are news an interview based upon casts were they interview experts in the field and those experts discussed latest technology is an invention some white papers some other pod casts an ordeal learning resources there similar to an algae whose goal
is to teach you machine learning in other fields tangential to machine learning such as math operations research as a more fond topics more inspirational concepts in machine wanting things like the scene to larry and consciousness can robots be conscious and what is consciousness so after us into this episode i recommend visiting proceeded belgacom forward slash m. l. cheap there you'll find the resources list which includes all other audio supplementary resources that you may want to enjoy it
alongside the spot cast as well as the deep dive materials that will need during your learning jury now as of october twentieth twenty twenty one get agency dee p. t. has acquired the machine and ipod cast and they're paying you work on it which is fantastic so on and be breathing new life into the old pipe cason in the dusting off old episodes fixing up summer rana subjects that may be considered missing in the traditional machine learning educational path things like decision to
he's a naive bays and they're bringing you machine learning applied for free previously machine applied was a separate pay pie past i've had which it's best to be applied things in machine learning things like technology stocks programming languages job hunting machine learning operations for camelot somali sing song and emerging in these episodes into the main feed banks to depths generosity the value of gets to you is listener is there anybody using their projects as talking points
the very subjects and you'd be discussing so for example from discussing to pure illusion or natural language processing might pull in a deft project in most subjects for their senses company does data science and machine learning if you're listening to this pod cast because you what happened the industry will apply to them apply to dad's by way of building sano citadel dot com or fury manager listening despite cash china decide what is feasible by way of machine learning and what types of technologies are used in the space near looking for talent
the high rise that is always looking for new clients and since i'm on the team hey you might have me is your engineer if i don't have a debt case benny project he indeed the talking points that episode on abuse in my own personal project know she she n. o. t. h. i know si se upon is ancient greek for no thy self know see is an online journal a personal journal that uses a dying to provide resources and insights things like book recommendations therapist suggestion
n.'s it generates beams of your journal entries commoner current patterns it allows you to ask questions of your journals allows you to generate summaries of your journal entry so for example give me one years worth of journal entries in one paragraph bill as you share your journal entries with therapists that they can use these tools as well and then the project being open-source no fuse open-source you can go see the code you can see how and some computer vision how
rhythm is programmed in python using to answer flow in the wild you can see how we would program the summer is asian model in the deep natural language processing episodes in the future note as i'm redoing the pod cast you may find some gaps in your pot capture it looks like maybe in episode is missing that's normal it's because i've taken out all the material and that's no longer relevant is important to bear in mind that by discuss resources in my old episodes at the end
the episode i discussed them in the pod cast episode when you get that point in the episodes to skip ahead because this resources page on osuna lot calm is where i'd jump the resources now this allows me keep things fresh and updated as resources change as newer better resources become available for some specific topic that i don't have to go back and and it's an episode so going forward as everybody sees all right so dole no longer be including the splash
n.'s all the research themselves instead you need to just go to the website to get the resources now with all that matter how to weigh less died and annex episode where i define machine running an artificial intelligence how they differ from each other compared to date is sciences so on will see you in the next episode


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt he hurt i'm also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. machine learning guide episode to this is what is artificial intelligence machine learning and davis sire
it's this is a complete redo view bridge all episode so few forty wasn't episode to recommend listening to this anyway because as good a deal lot of new information especially regarding jaded science which was sorely lacking in the original recording let's start with a bird's eye view davis science is the all encompassing umbrella term inside of which is artificial intelligence inside of which is machine learning so data science contains a k i panicked
ai contains an owl but before we go in today's science what's actually start with artificial intelligence from oxford dictionary am i a means of the theory and development of computer systems able to perform tasks that normally require human intelligence such as visual perception speech recognition decision making and translation between languages so basically ai is automated intelligence automation of things it
normally require human intelligence that the object of definition of a high in the same way that the industrial revolution was automating the body through bogs in machinery the ai revolution is automating the brain by way of intellect so the object of definition is pretty clear but the sense it may not be that helpful and so people have difficulty defining ai because they have in mind morbid subjective take on it and i said jack
the definition of artificial intelligence can be difficult because defining intelligence is difficult so we have the objective definition of ai we have the subject of definition of ai this is neither of those tapes for super hopeful in this context let's instead just break ai down into it so fields so you have a better understanding of ai as a discipline has broken down into multiple sub disciplines street firm wikipedia these are reasoning in problem solving knowledge representation
planning learning natural language processing perception motion and manipulation social intelligence and general intelligence let's start with natural language processing or an l. p. the reason i'm as starters american if paint the story line of the disciplines and ai and this is a good starting point and l. p. is this some discipline of ai dedicated to language anything that is simulated language is now
trolling which processing is to include machine translation for example we will translate from english to spanish it includes shot boss things like syria for example or let's say that you have a customer service chap bought on our website you chatting with that bought new beauty be speaking your utterances out loughran it will perform speech to texts that's within the domain of n. o. p. n. and once it has that text is going to perform something called name depends due recognition so the chaplain
says how can i help you today you say i need help with my credit card it'll pull out credit court has a name to entity that's named entity recognition work and your talk about these kinds of topics in an opium future episodes they can do question answering some realization that any other task associated with language next let's talk about the sub discipline knowledge representation long time ago by a b. m. created a jeopardy playing bach called deep blue what would avenge
a. b. calm i'd be on watson and deep blue hat inside of it the capacity to play jeopardy not just by way of natural language processing you need an old piece of that when the question is asked if they can take that question and transform it into something that can be used internally to answer the question but also that there has to be summoned carnal knowledge representation either trained on the way about war on wikipedia were on books some way of taking this knowledge about the whirl
all out there and transforming into an internal representation usually innocent discipline we deal with things called on to haul in jeans or knowledge graphs to that path you gonna question and transformed it into the type of representation you can traverse an autograph for to find the answer come back with the answer and then represent that answer using and i'll be back to the human soul these two fields or some more related especially these days in an l. p. we have a specific task called
question answering where you can hand the agent a corpus of documents for example a book you can ask the question and it will answer the question either extract of oui by pulling the most probable a sticky cancer out of that corpus or abstract of lead such it'll actually coarser question figure when you implied by at go through all the documents in the court is in order to paraphrase an answer to the best of its quote unquote understanding
and then relayed that answer back to you that's called abstracted question answering which to me is all a bit more magical been extracted question entering the sound a lot like knowledge representation but they're slightly different domains began knowledge representation isn't explicit we dealing in line which per se although whole certainly be using language is part of its told shane instead it gets to some discipline of representing knowledge in the ai the agent says that that knowledge can be true
first for the correct answer rather than just by pre trained text extraction language models next up we have reasoning and problem solving this is related to the prior some discipline of knowledge representation reasoning and carlos saul window or more typically dealing with probable listed models about the world in order to solve problems and then we have planning planning is actually the sub discipline of taking actions in an environment to buy our official agent
this is what you may have hurled with chess and go playing a box so long time ago ai researchers in developers created a body to play chess and played chess against the current reigning champion of time garry kasparov and beat him zachary lot of publicity around ai reason we go balls deep mind alf ago learns to play the game of golf and was pitted against the reigning champion lisa dole and won
again bring a lot of publicity to ai by way of much more massive old-school indal talk about midday than was previously used in the case of chess is a great movie about deep mine versus lisa dole which will link in the show notes i highly recommend watching and so planning is more about taking actions in an environment whether that be playing video games or driving a self driving a car with a car nice to build take turn by turn actions on the roads speaking of self driving cars take
action some arose as two other pieces that need to be present for quark the first is perception so in ai nice to be old perceive the world around it's eyes ears mouth and nose attach so itself driving cart nice to be old to see the road by way of cameras or light are in may also need to be older here the road here horns blaring work course screeching and so that would be using microphones so this is perception and also nice to be all too for
for my actions inside disbarment this is this some discipline called motion and manipulation which is basically just robotics so car had has seen the road through camera that pixel data gets transformed by ai into some internal representation to perform some planning steps in order to determine whether it should break excel rate her laughter term right and it has to be latrines form those actions steps into physical reality by way of robotics
to the some discipline motion and manipulation that particular we call this hard work the point at which the ai performs physical actions in an environment actuators sort actuator myself driving car might be the breaks the steering wheel the wheels whatever know you'll notice i'm talking about robotics here and all large part of robotics is simply hardware that's cameras microphones and the machinery used in robotics but we
you include robotics as a sub discipline of ai to the extent that you're facing with the hard work by way of intelligence whether that be planning in the case of taking actions were representing would perceive the way they can be internally utilized by the day i agent in the case of perception they've probably seen in the news all the cool stuff by boston dynamics those guys are killing it in the robotics field we have robots that are jumping on boxes and doing back flips
there's one thing they can do well defend themselves against hockey sticks irish job that if there ever was an evil killer robot take over all we know their weakness because boston dynamics is shown us that hockey sticks see my warner stock up on hockey sticks to defend yourself against ai revolution also was on wikipedia social intelligence in general intelligence social intelligence is the concept both ai a being thrown into human emotions so we may be trying guffaw
all o. athenians emotional flow through to find in the case of rowboat therapists by myself i'm actually building a pager all act that uses a aggie view resources and recommendations for your journal in germany you'll hear about it and few trapped so just called no fee and there are packed switcher robo therapists that you can converse with an easel attempts to pick up on your emotional state based on an l. p. tasks such as sentiment analysis whether the things you tie
beer positive or negative or neutral and they're also computer vision technologies which will try to pick up on the emotions on humans face and then finally general intelligence or artificial general intelligence the g. i. a. g. i. does the pot of gold at the end of the rainbow it's a pipe dream it's a pie in the sky it's the goal of creating an artificial want whore giants that is not only good that it's particular tie
asks was also general weisel to all other domains so that it is as effective at those as a human if not better okay so we say we keep ai if the guy is dedicated to a specific domain for example series is really relegated to natural language processing room by it's mostly robot experts also incorporating action and planning selected to for break their self
i'm in cars were good little bit closer to the holscher bang the grand or prize of combining all of these the eyes of disciplines into won we have perception mainly computer vision and hearing to some extent on the one hand may be listening to sounds on the road it can also listen to voice commands inside the car of course action and planning this turn by turn navigation acceleration braking and then motion and manipulation all through robotics it's okay
a tesla self driving cars giving us closer to the end goal of artificial general intelligence as you can tell it is not very general who were general in this context means that the robot or the id agent should be able to generally apply itself to any context you wear in which human confinement self and perform or act as intelligent we as a human if not better so let's say we take a boston dynamics robot which is good jumping on boxes into
back flips and calling luggage with a pack mule and and put in for a course of its handling legal cases it should be old look pick up the skills needed to handle legal cases as well or better than a human whose plopped down into the court and that's the generali of partial general intelligence week means is dedicated to specifics abdomen not necessarily this week is task but just that it can only handle specific domain and general means it can handle all domains and jenner
lies across contexts universally like a human put now rice a pie in the sky pipe dream idle mean it just isn't gonna happen i actually do believe we will achieve it age yeah i and i believe it'll happen in my lifetime between ghoul minds open ai an ollie other fang companies go one hog wild with ai these days looks like grown-up refocused path with major improvements week by week when mean by that is that the g. i.'s is more concept in the future
gold rather than the day the day dealings of summer you work smarter shawn told aides machine learning so basically the objectivity of nation about ai is automated intelligence or automating intellectual tasks which typically require human intelligence to perform that's the object of definition what is not very helpful because for example like to write a python scripted does some number crunching on a spreadsheet that is automation of an intellectual tie
ask the usual your course human intelligence and so that's a ways austen to be subjective definition of artificial want holden's which is really hard to pin down you'll find that when you ask somebody what is artificial intelligence would he can read the definition from the dictionary but strangely it can be easy to get into something of a heated debate of the about what the actual definition of ai is you'll find all of you will take the definition of okay i've been making a little personal and may
the subject of and the reason for this i think is because what they're really driving after rather than the definition of a diet is the definition of all i the definition of intelligence in general and indeed defining intelligence can be quite a subjective and dicey matter which say ways into the conversation what is consciousness for example i discuss intelligence unconscious isn't a future episode cholesterol little bit hand waving here to the weekend become older
more settled with the subject of take on artificial intelligence the way i see it we have manual hard coated scripts on the one hand on the far left and then we have true intelligence all the way to the right far on the horizon pass for the eye can see the reason i say this way is because i think that's intelligence is an analog concept the same way as some humans can be more intelligent than others and humans are more children
dogs and dogs and cats and cats and rats and so on this sort of an analog steel to intelligence and so too will be the case with artificial intelligence and what is the upper limit we don't know and therefore there is no sort of bends to the to the right side of the scale and analog soma left of the scale we have hand written scripts that can be considered ai buyer the object of definition on the far right we have the power in the sky concept of age
yeah i wish all discussed in the bed and anywhere in the middle is sort of what we're dealing with and trying to define a nice objectively the layers by which the software becalmed removed from the program or ads elements of magic to the concept of ai so it hand written script is not very magical a machine morning model which will discuss later the pool learns what save the rules of the game of chess or go or how to drive
self driving core drama wrote will at some number of layers removed from the original programmer and that removal adds an element of magic that magic is the sense by which the concept of that ai can be defined subjective way so where do we draw the wine by which we call something magically ai in the sense that we're all looking for willpower and hurrying had really did take your deterring test for the imitation
game alan turing is essentially the creator conceptually of the computer by way of the universal turing machine and he was fascinated by the fear radical concept of artificial intelligence of course technically achieving yeah i was well outside of the technological capabilities on his hind the u. s. really interested no concept to cover the charring test which basically says if your chatting with him or official agent and ai bee
the envision that you're on a terminal which had bought any other end of the chap are responding cure utterances you don't know is either a human or or officially intelligent agent if you chatting with this agent and you can't tell if it's a human or an ai then it is intelligent whether say human or they i if it ai can sufficiently convince you that is intelligence through jackpot where poking and prodding and sean fein holes negev
i'm ben is intelligent in the magical cents so he says the third day i'd can convince you that it's not not an ai this then it is intelligent so no way and if we say that there's a sliding scale from hard skirting on the left hand or official general intelligence way into the horizon to infinity the point at which we achieve intelligence in the magical sense that we're all trying to subjective weekend yeah
on is the point at which you can't deny it as simple as that it walks like a dark night hawks like a dock which is behavior isn't if you experience it ai agent and you were flabbergasted and can't deny that it is intelligence than it is an anything happen to that point is not anything beyond that point is the future so that's sort of the marker delineating this day oleg infinity scale as that
wanted which we subjective we define intelligence that is you'll know when you see it so that finally takes us to learning machine learning the last some discipline listed for artificial ones conscience and the most interesting and powerful some discipline of artificial intelligence as you'll see here the reason i say differ last is not only that it's the primary subject of this pike have series but also because it is machine learning this
some discipline of air eyes that makes this ambiguity in the terms ai and machine learning difficult and that is because in recent times machine learning has been sued soon in all of the other so disciplined support the short haul jets machine wearing what was previously relegated to its own silence some discipline is now breaking out of its cage and hostile taking over polio
you're so disciplines grabbing it should hold hold on those of disciplines and some of the self ilsa been completely turned blue there's nothing left but machine running so west of flying machine one first and l. s revisit the sub fields machine learning is learning on de have to make predictions it's pattern recognition over day that so that future productions can be made is learning andrea that's a machine learning as cicero assembled really easy to understand it is you feed a spreadsheet
as soon as spreadsheet the stock prices for a particular stock or time and all we're trying to do is protect tomorrow's price for that stock markets a typical use case of machine running all rhythmic trading box you take this spreadsheet this day down and the machine learning algorithms will look at all the data in the spreadsheet in order to try to find patterns in the day that is trying to learn the patterns of the day at what is it that makes the price
go up or go down given all other variables in that spreadsheet such as the time of day day of weak sentiment on twitter whatever the case learn those patterns and once we have those learned patterns saved we combine dole learn to patterns into the algorithm into what's called on model a machine learning model so models the combination of the algorithm with the computer programmer robes and the patterns learns what the machinery model itself to die
now we have a model and that model to make predictions for the future so tomorrow come out for dates based on today's context of circumstances what's the price can be for the stock to more on and then in may you just protect the price or may actually take action on that production such as the buyer saw on that stock so that's machine learning it is learning on data to make productions it's as simple as that had previously later said this was a dedicated some discipline of a either
was focused primarily maybe aunt had dealer de learning on spreadsheet or database is the patterns in that taddeo or day that by way of simple algorithms simple models things like guinier logistic regression with full discuss later and the symbol models made machining maybe less interesting to the other practitioners and the other side disciplines of ai lot after the deep learning revolution of about two thousand fifteen machine learning story to be calm
generally applicable in storage dominating the other side disciplines of artificial intelligence felt natural language processing previously an l. p. why is language theory basically go to university use any language theory you learn all about syntax trees no ways in which certain words combined with other words in grammatical constructs so that you could traverse these trees in order to pull out named entities or dancers to
questions these were hard coated scripts by domain and experts in an l. p. all long cons mushy learning with his work hard neural networks and transformers models and climbers these original expert crafted scripts we're each expert was a star but experts with an adult he may be some experts and their scripts were really good question answering and another for name density recognition and anna
or for machine translation well these transformers models are generally applicable generally pluck old to all of the tasks within an l. p. transformers can handle machine translation question entering the summer is asian and more all with the universal warning model and that this model can be trained on the language tasks generally rather than handcrafted was specific language task force sample
difference between legal documents versus news articles so machine learning takes us some steps away from a handcrafted script which add some magic to or some discipline of the day i can't some generality a step towards artificial general intelligence some generali in so far as the transformers model can be generally applied to various tasks hands domains within an hour p. computer vision previously keep your vision was
handcrafted scripts algorithms that had a hand designed pattern match in patches these sliding windows that would slide over an image from top to bottom left right where the patterns that is looking for in order to detect an object or classifying image work domain expert handcrafted pattern matching windows so an expert would design a series of manual convolution birds which were looking for
why isn't edges were looking for certain patterns for dog breeds for example some such algorithms were cold hard to georgie anne heart h. h. a r. which all discussed in the computer vision episode will long comes machine learning with this convolution own your own now works these c. n. n.'s learn the convolution cities sliding windows these pattern identifying patches and blows the old guard handcrafted solutions to smithereens again
these models are not only lear is removed from handcrafted solutions is so far as they can learn of the pixel pattern matching process but also or general wizened old across different domains so the hog in our people might have had to him craft defined convolution says for different domains whether we're dealing with animal managers were closing managers or outdoors objects managers anyone connell
show me all met work can handle all these contexts generally and finally reasoning problem solving planning motion manipulation and perception in these were all previously dedicated so disciplines and ai they still are to some extent in particular the reasoning some discipline dealt lot improbable listed models and the planning some discipline adult lot in sri needs search trees like the day
star search sri and these types of trees would've been used for example in the chess playing bach against garry kasparov will on calms deep reinforcement learning models with their deep kuna works an approximate policy optimization models and i don't know which specific reinforce some models are used in different context for example what reinforcement mop what reinforcement learning models used in golf ago zero breeze become
in very obvious that there is a massive amount of general eye's ability for use of deep reinforcement warning models in a situation that requires taking steps in an environment planning actions and taking those actions we previously had searched trees and more cross models in all these things which would have been custom tailored to specific environments for example pre programmed with the rules of chess along
if the how to solve playing chess well deep reinforce of learning models but not only learn how to play a game or drive on road by taking actions none of our men observing the point value consequences down stream at the end of the game an hour to modify its behavior but also learns how the game is played in general what game is even being played so many of these reinforcement learning models
all you have to do is show is screen through to find so take space in pixel values and to find the progression of the video game as it takes specific actions and it will learn the rules of the game the contacts and barman of the game and how to play it so now this point we're another layer removed from a hand crafted script even more radical than than the models used in an l. p. and keep your vision some machine learning is
i only more affective at tackling a lot of the sub disciplines and ai then the old guard of previously handcrafted scripts is also more general wizened old and frankly more magical and speaking of generalizing billy strides are being made on many fronts now to unify these machine learning models to tackle multiple sub domains within ai at once for example recently in the news who will announce
then called pathways which as i understand it is a machine learning molly can be applied to computer vision natural language processing and reinforcement learning also is ill were many an l. p. episodes and then knelt the model concept called transformers is now also being used for vision and other tasks so steps are being taken towards a grand unification machine learning model and those steps also take us towards john
dating or official intelligence at large by way of machine learning so that's what special about machine learning is that it seems to be the master algorithm for the short haul jets and that's also why does some big you waiting those two terms has been relatively difficult is this previously m. l. was simply a sub discipline of ai but in modern times it's the dominating driving force of ai in all its other side disciplines no
now let's talk about d. s science data science like i said is the umbrella term that encompasses artificial intelligence on machine learning more than that davis sciences the umbrella term that encompasses anything related to d. n. if you are professional and your daily job is dealing with d. n. then you are idiots scientists this can include things like database and ministrations the analytics business intelligence machine learn
in artificial intelligence even something as simple as maybe attacks person if you're dealing with spreadsheets day in day out to some extent your days scientists so roles that primarily deal india or rolls with india science but there are rules more understood to be davis science proper an asshole discussed here by way of the dia pipeline because the typical rolls dealing in data science fall somewhere in this
i climbed a pipeline from left to right beginning to aunt beginning of the day the pipeline gets the data from somewhere and pipes it into the machine and the output of the pipeline is either jay alix business intelligence or machine warning them as a whole budget machinery in the middle so let's go down this pipeline journey the first step of the pipeline is day that ingestion ingestion which is to get your data from somewhere this day yet
indiana spreadsheet like microsoft excel x. l. s. x. or c. s of the files they can be in a database like post grass or my eskew well those are relational database management systems there are also unknown relational database management systems or no west too well no siegel data bases like mondo d. b. and dynamo g. b. or it can be from a stream of data on the next train means that
the day is coming at you so much and so fast you wanna stored anywhere you this what process it we'll time en route to your receive your didn't do stuff to an probably for way it's own example the stream of data would beat quicker twitter is what's called a stream or a fire hose of data is comanche so fast as urso retreats in the world that you probably knock any storing this into a sequel to base or no single database and certainly not a spreadsheet the dataset typically me
eames filed for men ended up with a spreadsheet the data storage typically means database for granted a belt light on postscript rescue op and then we have streamer fire hose this comes into your pipeline and now you were a total idea somewhere you want aggregate all of the day that common to some use case altogether in what's called a deal lake dale lake is the abrogation of various dataset status stores ensue
on repository one umbrella and that is given me during its cannot be cleaned up yet it's coming straight from the injection phase and dale lake technologies all while you to apply a certain unifying aggregate inga features on top of your day up one of these might be permissions so for example is your collect spreadsheets and databases and tweaks all into one unified repository call the deal wait anything you do
wait technology to apply permission in on that deal lake says that various downstream day that users or consumers can access this daily call by way of a unified commissioning system another feature you my worldwide today allayed is a unifying scheme not get much of your data has a lot of structured in common even though they're coming from disparate d. s. forces you can create a devious chemo
that you generally applied to the data sources in this deal wait so that downstream consumers of the day that can expect a certain format and structure to the degas the next phase of the pipeline might be something called a future store now this can sometimes be wrapped up in the daily phase or they can be separate components of the pipeline a feature store is the face at which you take the data from the daily japan
clean up any story and it's cleaned up state so you're gonna have dated coming from twitter coming from databases and spinning the jury this could be missing entries missing columns for certain role is some fields are in text format court date format and as you'll find slayer the analytics and machine learning don't like dates for strings they want numbers machine running again analytics always wants numbers and so this feature engine
during phase they call it featured engineering is where we will perform some of these steps once that might be to fill in missing values we call us in beauty and you'll learn about amputations strategies later another feature engineering step might be taking your date calm and turning it into numbers suffer tabloid and take a date column and turn that date straying into time of day day week week of the year and so now we transformed the date featured in two three
their features we call this feature transformation as well as simply deciding which features to keep in which ones aren't worth keeping for example in the case of twitter user name might not be that valuable we really are concerned with the date the tweaks text and maybe the engagement with the number for pliers or number of retreats so we throw away the user name is called feature selection at this whole process is called feature engineering and then we see
for all these engineered features as something of a checkpoint in our pipeline called a feature stuart and feature stores also while you two version of your transformed features such that if a machine learning engineer has been working with david and such and such away be some awaited the engineer or had previously been feature engineering and the date engineer makes modifications f. a.'s new pipeline well the machine and engineer won't be side swiped because they will
using a specific version of the feature store okay so david injection takes thirty ephron dataset sunday's stores and streams and fire hoses these roll call dia sources stores them all into a deal lake which is jury on clean large degas or big idea you've probably heard that term big day at any time we're working with day care fit on a single server it's big enough we may apply a unified cooling on top of that we
it things like permission anal or a coleman d. s. ski not and then we call that a dead and awake we clean up the day that step called feature engineering those steps may be featured transformations that is altering one feature into a different future or set of features impute haitian that is filling in missing values feature selection that is deciding which features and portman which want and then now we're done with that third phase and we're ready
move on in the pipeline at this phase in the pipeline word of your left and right is a fork in the road just like there is with pipes and so to the left we have the analytics and business intelligence and to the right we have machine learning these are two to the police separate walls will discuss how these are considered several snub it let's go left down the pike towards did analytics the first step down the pike for state analytics is to store
we've already cleaned up in the day that is something called a deal warehouse deal warehouse the warehouse is separate from a day late it's little different typically when a data warehouse will do is take a slice of historical data from the recent past france for an ad david to be represented in the hall i'm a format rather than role formats and then maybe apply some additional transformations on the degas soulless
the step by step the way you would think about the data warehouse is that is something like a proxy this sits between the video wait and the deed analyst at proxy almost like a cash layer between the data source an analyst in the same way that if you've ever worked with web development is a thing called men cash deed the few stand up in front of your my seat pulsar work and have a proxy requests which are frequently access
the caches sequel queries sofas lot of really conseco cruiser get cold frequently and would put a lot of strain on the underlying data base than cash deacon said between the database and the server and cash those common queries so as to reduce strain on the database and speed up sequel queries will do warehouses similar to that we take a slice of dia over or less able last one year or two years we transform that did it to be represented in color
much talk sure format rather than role for math row form as we're used to when looking at dinner with a spreadsheet or database now imagine taking the rosen clones just floating around that's clone structure day at that structure of david is more desirable for analysts men for other roles in the data science pipeline like machine learning engineers machine running models dim light world structure date of this type were familiar with but the analyst sucker for
or call unstructured day that nasty cars running degradation queries specifically over columns is faster in now way which counts and so on an average running those cruises fracture on called structure dana and then we might catch specific queries which are frequently ram bite panelists and all these things combines tawny data warehouse into a powerhouse proxy between the day awake so that very fast cash to real-time queries came
ron again started out by the panelists so that's our data warehouse and finally a part in a pipes out of the day warehouse to the da analysts and the business intelligence professionals or business analysts b. i. b. i.'s for business intelligence these are two very similar rules very related rolls there is a new wants difference in these roles are more likely to be differentiated from each other
a larger companies were we have lots of rolston filled as opposed to smaller companies were one person wears many hats so both of these roles deal in analytics deal in skirts in graphs we take our gate at the data warehouse in we won't hear around in our hands we look at top we look at the bottom in order to make fuel minute decisions that's what makes these roles different than machine learning machine learning makes automated decisions in our state ale
lakes or business intelligence is for humans and machine learning it is for robots will get to that and at the data analyst is more of a technical rolled data analyst is actually will likely to be involved in the entire dia science pipe line up until this point everything we've already discussed such as injection into a deal wait feature engineering and working with the data warehouse is likely to also be tasks
formed by david analyst in addition to the actual analytics task of shorting n. grafting looking at the day that in order come up with human decisions where as a business intelligence person the b. b. i've professional is typically a less technical role there are less likely be involved in the entire data science pipeline up until this point and instead it is likely to be working with the data from the day warehouse
the consumer as a user and a giveaway difference between these two roles is the troll in the eye people will be using b. i. specific schools things like tout low ti eap ltd you and power b. i. by microsoft and quick site on amazon aided us whereas the day that analysts is probably get actually be doing their analytics themselves with cold by way of python in jupiter no books
with map lot lid and seaborne talk about those things in a future episode or during the deed engineering phase of the pipeline by way of things like amazon dino wrangler work amazon glue date abreu so analyst says more of a tactical rolled and baby i've personal less technical rule that state analytics david driven decision-making for schumann's so de analyst may take the tweaks out of our
data warehouse and then charting graf something's look at the optimal time to talk weeks during the day and what day of the week based on the engagement of those streets number retreats a number for plies an go running the bosses say boss boss we should between at noon on tuesday based on my research of the data that today analyst machine learning now we go down the right side of the fork in the road in the branch of our pipes machine learning is automated production
it's it's productions and decisions made by robots our essay robots of course i'm being clock will by machine learning models so rather than shorting and grafting degas as an analyst would do a machine learning engineer will design a model using sheen learning tooling things like cancer flow care ross hi torch psychic learned x g boost maybe all hosted in the clouds using sage maker says it will learn the patterns of the tweaks so that
can a cotton make projections or maybe even take actions sir rather than coming up with a conclusion then is optimal two sentries at noon on tuesday the machine learning engineer might build a model that literally sense streets at noon on tuesday they take this step further and maybe they might write in and l. p. model whereby the marketing team can feed it something they wanna say the machine learning model will construct the optimal way to save
that's using an elk he put that into which we send out at noon on tuesday so that's the day the science pipeline is the flow from left to right of the dead at injection put that into a lake apply some stuff upon our weight whether it's permission in war unified steam up pull out a lake in future engineer it clean up the day yes stored in the future store of your left towards the panelists first we hit the day
the warehouse which does some further transformation of our day is so they can be a real time proxy today analysts for fast clearing the panelists have more of a technical role in doing analytics charring in grafting on dinner for human decisions and the b. i. people have all less technical role for the same viewer writes to the machine learning engineers to build on the maid of the predictive models nobody's indeed as scientists to ban well that's all
that nebulous and the reason is the reason is it depends on the size of the company you work for if you work for mom and pop shop and you were there all media person that has indeed a scientist your role is everything everything that is listed you been handled each phase in the day the pipeline but if you apply to a larger organization like frank corporation facebook amazon apple mac flicks google then is likely the case that you will be applying
as a role for any one phase all this pipeline so for example goebel might have an entire day this science team and that team is broken down into the day engineering team the analytics team in the machine winning team and then you might apply to the machine learning to him or the feature engineering team in other words depending on the size of the company anyone phase in the hierarchical breakdown of the day the science that is it
candidate for role as it is scientists and so were you might find is that people may market themselves by either as specific roles which date analysts business intelligence machine learning engineering or deed engineering or they may market themselves as india scientists quote unquote either implying that they can handle everything in the sack or that they're looking to be more generally up little maybe they're targeting smaller companies morning where multiple hats and in a company might be look
and for the team of scientists typically meaning they want somebody who can handle any and all of the moving parts in the data science pipeline or they may specify which specific role it was the machine mining engineer or the data analyst were featured engineer and that's why can be confusing sometimes the difference between machine learning and davis science mouth which of these role should the u. s. abiding engineer in the space targets machine learning date analytics did it
engineering or the whole data science back well technology from web development there's fraud and server database and down lots as an example and some are you tackles the entire staff calls themselves awful stack engineer you can either specialize in fraud and and it really good at it there's a lot to know about just for an end for example similarly machine learning is enough to tackle all on its own keep you busy for a lifetime
there's an owl lobster machinery operations deploying your machine running models to the clark developing in training your models on various tool kit slight pipe scorch and to answer flow deploying optimize saying in monitoring your models all these things could keep you busy for a lifetime and you could take eager salt exclusively the machine learning don't be a coven of summary dedicating themselves exclusively to from an engineering where other roles in the day the science that are analogous to other roles know whether velma stack
so it depends on if you wanna tackle the entire stack at large or if anyone portion of this fact sounds more appealing than another one about machine learning versus ai now i did mention the machine learning is assuming the various some domains all day i've but that's not necessarily or completely the case at least not yet and so machine is more of an industry practice a professional feel that if you want independence tree i would choose machine learning
over artificial intelligence you might get a master's machine learning our data science where is targeting artificial intelligence is kind of that to be scientific chasing the rain both the pot of gold of the future that's more of an academic and ever so if you wanna stay in academia and push the scientific frontier then targeting ai conceptually would be immoral wind been targeting machine learning so that's davis science artificial intelligence and machine
learning now i'm going to take the history of ai that i had regionally recorded in twenty seven teens episode to an end just to make inserted here scott or re record that stuff to fury listen episode two you can stop listening now if this is your first listen through keep on listening okay so we've covered the definition of our consultants the definition of machine learning and now finally let's go into that
history of artificial intelligence and recommend you some really good resources that i've picked up a minute from around the web some common recommendations but people who are looking for good history information on our official tells it's so are the sorts of this goes way way back away back further than you imagine starting with greek mythology and then you're coming on jewish with all due of columns and this is actually very interesting point to me that's artificial intelligence has sort of been conceived of in the dawn of you
yeah maybe it's almost like it's part of it was kind of our our quest read it back to that actually in the next episode which is artificial intelligence inspiration will try to inspire you around our chortled of michigan for netscape than mind that we we didn't think about arthur shortage of sense at least greek mythology the first attempt at actually implementing cannot thomas han think this guy's name is raymond wall and thirteenth century leonardo da vinci was working on board made some walking animals saitama time to dig ha
and whiteness know they have a lot of her for philosophical musings and one consciousness and whiteness are co created calculus stuff that was used in the development of ai a algorithms mathematics and baltimore think about ai so this infuriate out ai has been around for very long time the real degree stuff 'cause are happening around seventeen hundreds in the eighteen hundred's the curious olive servant statistics and mathematical decision-making were first most important figures is thomas b.
he's worked on reasoning about the probability of events so thomas days reasoning about the probability of events remember the name he's become up again multiple times in the future the c. n. inference is very important phenomenal component of machine so he is a very big figure george poole who was involved in logical reasoning in by ariel throats got lot friday with proposition all logic so these were components in the development of computers in general but also in the velma machine running
charles babbage an idea byron slash love lovelace in eighteen thirty two design the analytical engine to the program bowl calculating machines and then he was in nineteen thirty six that we got the universal turing machine and if they don't want people consider that have kept up with computers now entering design that can stick concept of a programmable computer with his universal turn she is also just about as alan trying was very interested in ai he talked about but yeah as an art
caller computing machinery and intelligence or he was thinking about ai pre early on as well ninety four eight six john vaughan knowing and uses at countering to your full time machine the annulment of his universal computing machine if i'm not mistaken and we've universal turing machine was the theoretical and universal confusion estimated computer i think jamal none maybe architecture firm for scum of universally can programmable computer and allison research against the actual machine on except during an artificial intelligence in ninety four three warren look
like an walter pits the kind of built the computer or representation of the new on that later fine by frank rosenblatt to create the perception on an award percent on come up come up later this the first artificial new round of course it's that those an artificial neuron by way of called a motel where perception zyuganov showed no my work and that is the plumbing that's fun stuff so you call it and that's an rosenblatt three v.
important figures was until nineteen fifty six that the word artificial intelligence was coined by a john mccarthy that the darkness workshop so dark john mccarthy marvin minsky arthur samuel all over south ridgeway solomon off our nolte and herbert simon all came together at dartmouth into that at this workshop in nineteen thirty six and their goal was to simulate all aspects of intelligence and quotes him as the definition of artificial
thousands of beginning with our guest says guys to find ai they created a field of it and then they set off work and so at the workshop it was kind of what hath ahmed them open to act on the dis crack that mule inside increase your sticks to branch purist extends human incurred this thing called general problem solver did you receive the creation of computer vision natural language crossing shaky the robot says between the nineteen fifties and nineteen seventies so there were few of them on this at the actual dartmouth workshop
the mail truck to the projects home to their own universities they continue crack at these things we call the sort of the golden era of artificial intelligence is when was that an explosion of interest and in research and joy and light in the field fight fight embalmed creeps experts systems this is also sort of the creation above what we call good old-fashioned ai because later you'll see we can switch gears 'em away we do ai in today's generation good old-fashioned ai or go far
a g. o. p. f. b. i. uses an approach called symbolism these are logic based approach is knowledge based experts systems basically where the knowledge to end the logic is kind heart coded into the system and full well that it's little bit like that once and twice for moving now joe use back and play checkers little bit less magical maybe then the connection is an approach of nomo works by at some of the experts including marvin minsky during this time for the side of go five symbolism and they said
dead the connection is an approach a partial neural networks one hold that has some problems so this was the golden arrow to the fifties and set the fifties to the seventies a golden era of explosion research and ideas and discoveries and developments by some of the most important hundred influential mains in the history of artificial intelligence or you'll read any book on the doors also little bit of adversity between some of the players and like i said so so connection is um began to be
criticize due to something blows cold comet comets coral explosion basically y'all networks work too hard copy to show it to heart on the computer to be effective at generalizing solutions going forward on the heel that's in the nineties seventies came up this report by james white help call bill like phil report and listed that's damage to the field aboard the schwan totems so let us set this is between the fifties and seventies on this
golden era wasn't suing the artificial intelligence when the world was just vibrate and excited about ai and in that excitement darpa yom the military defense and lots of companies invested into these guys these major players and create companies around these technologies there's a lot of hype a lot of excitement and binder delivery he was almost like a silicon valley's bubble blast it was almost like they said we can do anything in the whole universe because for some u. n.
intelligence anything you could imagine no actually we can simulate that but in fact of course it's little bit more difficult than that we're getting closer every day by day under delivered in too long over time and so people started cutting fines and spreading grandstand having contracts with contractors and bowater that was due to this like hell report which was basically report on all these come up negative aspects saying yes indeed these guys are by not one issue and the
the whimpering so that create what we call the ai winter ai went underground and very few people were funny anymore the ai winter lasted for about the seventies until the nineties is starting to make a comeback in the reasoning come back why is that the ice friendly sword have some practical applications in terms of these diehards that we're heading in the cave they knew that you would that have real application industry is this violence and making to practical use of ai without some lofty promises
so for example advertising and recommended engines which are two of the most commonly used applications of our official tokens in modern era the other thing is that the computers got better so remember that one of the reasons for the ai winter honored to liberate was due to this common toward oaks lotion that these generalization generalizing algorithms could not perform on the computers of that modern time but they could perform on computers twenty years later thirty year
later the nineties into thousands that they are really certain pick up steam again and finally the last piece the puzzle was the one thing that you're gonna find the money to teach you in later episodes is that the accuracy of your machine learning algorithms improve the more to you have hands with the internet becoming so popular and to becoming so prevalent all over unit which scrape web pages and there's accel spreadsheets and databases ever aware of this and that's business you're in our rooms have basically
it's a goldmine of misha work with the smooth arrow was called the degas now you can work with all the data that you fingertips you can imagine big data it's okay i finally made a comeback if i may have had this day i spring after that ai winter another reason i think that hey i'm a combat which i know here are tara what was actually optimization study our rhythms though they say that our computers are really fast so the cop competition we've got more fish episode is that the algorithm so for example in two thousand six jeffrey henson
optimize the back propagation our rhythm which made it our official neural networks substantially more tractebel and put connection isn't back on the map in fact i believe it was that the sort of solidify connection as um as a really powerful commonly used technique going for an egg go find symbolism certain to go out of vogue finally we have the modern era two thousand two thousand seventeen currently bloomberg says that two thousand fifteen was for water for the ai industry and i don't really
no why i think maybe what happened is just it just that this graph this cause to be on the rise sense the nineties is finally hit keep that it's become really popular in the modern are now almost any technology companies adopting machine learning an artificial thompson's the hiring machine mining engineers new data scientists like never before so we're if spool ocean of ai factors little bit of concern this could become another balboa smothered at which are the law that people who say that that probably will happen that we're actually really do
while the our rooms in the data that algorithms a very precise to the amount of data we have the computing machines we can scale these algorithms horizontally across in aid of us closer all the sins sort of very very good time for machine one very interesting and good time to be alive see what's happening now is one company that will draw attention to an this'll be my final point on history of our official told shuns the company's name is he might indeed mine was acquired by tula boys and twin fourteen do you mind that first was kind of just
playing games to use artificial intelligence system using deep learning approaches like the cue networks in order to play an old classic inside pond in benchley old console games and more recently modern consul gains weight from playstation such but doable saw potential in the types of things that they were doing with deep learning in out reinforce atlantic specifically an acquired them and keep my husband putting out why they have just then demolishing the field of machine learning in artificial intelligence there
probably the most present figure in the news of our official told us today they put out papers all the time making big splashes in research and development something i mentioned previously color differential computer which brought the sub field of knowledge representation and reasoning into the domain of machine running for example deep mine is something to keep very close eye on they're doing some really interesting work these days okay so that's a history of artificial and
knowledge and so we covered what is a guy the definition was machine morning compared ai was mission learning compare today's science compared to statistics why is an owl the sort of essential some field of ai what is the history of ai can brief if you're interested in the history of ai and the definition of ai i'm going to provide some resources this one book it's commonly recommended an all points out i'm the show not so good to show notes o. c. d. e. feed the bell
oh see the bell dot com and this'll be episode two negatively suffrage than history of machinery so in the next episode on going to try to help and spire edu to be interested the field of chaotic and specifically in mushy learning think that this episode was probably rather boring the next episode of all that says i have plans so far the next episode i think will be the most fond episode it's been a deep inspiration around field of ai worth talk about the senior wearied huh
i just missed automate automation above the entire industry some really philosophical and crazy stuff so i'll see you then


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt he had time also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused and tips and tricks some which could prove beneficial in your machine learning education jury finds that at waseda val dot com forward slash l. l. h. february tenth two thousand and seventeen in this episode three inspiration in this third
so that i hope to inspire you to want to be involved in the space of machine learning in artificial intelligence or lease to keep the very close by elements ai i think is the most important thing happening in the world right now curious and i hope that by the end of the spike s. absurd you'll see weren't coming from this is completely a philosophical episode it's a lot of fun and i wanna fluff so west you lose credibility in music teacher of mushy learning just know that this is the last of its car
i ain't related to some gory details of course starring in the next episode this was just for fun like this said this is all philosophical there are four sort of philosophical conversations are happening around the space of a high right now and talk about for lesser with the most successful which is on a mission of the economy as i pointed out last episode the definition of or official told since being able to simulate any intellectual task you can imagine the types of jobs that are going to be displaced once the icons and full
swaying in the dawn of the i. we rock we are the kind of expected to be very menial digital data entry type jobs tax preparation kind of jobs to go away that's not you know we didn't really think of that as a guide coming in and taking our jobs we thought of that as the digit decision on nation of jobs of simple jobs but more and more very high skilled intellectual jobs are being replaced as well as guys hitting the medical industry with a vengeance medical diagnoses a huge space for him
and sitting in radiology looking at trees and coming up with a prognosis well the action do that very effectively by way of something called convolution you'll know works for surgery is now being revolutionized by ai very high precision surgery robots are coming into the hospitals and the right now they're convinced the need for surgeons we don't know how long that will last for replacement at least theoretically of course cars but we're seeing right now already on the streets autonomous vehicles google solved
i think heart is in many cities already is in the news of people are discussing the implications discussing the economic implications discussing the philosophical and safety implications but right away you can imagine that taxi drivers and freight truck drivers are going to be impacted by it's the immediately in fact programmers designers and logo creators programming web web site programming for example this website called the great dot io which is has the goal of generating the website my way out of
the short haul jeans if you give us some specifications web design weeks has a module that you can have it design your cage around your haunted by way of artificial intelligence and as aria alone go generator and ai logo generator rather care remember the name of it but he does he even creativity is not exempt from the eyes and in fact talking of creativity music and art is already being generated by day and people having trouble distinguishing between human generated piece of music or art
a machine learning generated peace to their to natural response is suspect intimate takeover by ai one is sphere of robot to take my job does aware that on b. b. c. dot com that'll put on a show notes were you to look up your job to determine whether it's safe and the i. takeover to wanna find i think god nursing ranks up there is something that safer now i think it's because it involves a lot of highly unpredictable body motions were robot on the road salt driving a car falls very predictable set of rules
will the nurse running around in character words and ideas in rushing to and fro in answering doctor's orders molly sings this is something you can vary widely predict with robotics artificial intelligence one general response of the fear to the ai economic takeover is that if you look at all the prior economic revolutions in human history we've then okay there was the agricultural revolution which switches from hunter gatherers to farmers and people adapted for the better the risky in dust
roll revolution that automated a lot of hard work physical labor and the result of it was stronger economy were people could focus on things that are west menial and that is the information age of the you know the invention of the internet and everybody got on board the internet in new jobs were created even as the others were displaced so for example previously you'd call it travel agents to put your flight to destination while cool flights can give you your flight find the best price no prob
those guys around the job but new jobs were created in the information age such as what developers mobile out developers who didn't exist before that our economy is stronger than it was before so every harm every economic revolution does have a small impact is time potentially negative truck consequence on people who have the jobs are being displaced that is new jobs are created people are able to adapt their economy comes out better on the flip side is entirely differ
take his economic take over which is in fear with excitement people in this camp points to exactly what we're trying to achieve with economic animation replacement of jobs so that we can live more comfortable lives in the same way that industrial revolution replaced very hard menial labor will think about the more comfortable and safe white supported you living with fox honasan beagles writhing of surround and delivering free the money saved by all this on me she has to go somewhere and it's not going to the pot
of these robots see yellows were conducting meetings including a graph that are going up to the sky is shaking hands with other robots and laughing and having beers after work go to rio de asta robots are built to serve us so that optimists say that it's not to destroy our economy is to make life a lot more livable for us the new jobs will be created that we can participate in but we don't know what those mighty yet if you look at the result of the prior economic revolution wheel
with more like fiends after each revolution than ever before and that whole flynn was exactly about to come really do more keenly lives the way party would if they'd about the way we live now in the information age with our couches or t. v.'s or video games that we can all those go out and eat and persons net worth being bathed in a bygone era that they lived on the streets persons net worth being negative today is all too common to student loans must just because the wilderness stage with so much back now
leniency is provided to us in one way which he optimists think that this might hang out something assorted ingrid we discuss the multiple government's such as in scandinavia called universal basic income you b. i. where the government basically just gives us a minimum wage so how about battle would look into that u. p. i. so there you have it at the moment automation world is about to change under our feet by way of artificial intelligence and i don't think that there's anybody who disagrees with that
that section hold maybe a little bit less consensus that economic revolution in this next section of bout the senior weary in this part is really furnaces were put on my tin foil hat that we start to go down that philosophical rabbit hole into the void salome define the scene you larry the singular he was a term coined by burger binge and champions in recent times by guy named ray cruz while you're in here his name all watts so just a familiar with that name way kurzweil come back to him at that what is the singularly in athletics
scott of what he called seniority maybe the point of no return if you've got an exponential graf coming from the left and it's going to the right getting bigger and bigger going at another not any goes up into the sky into infinity the point at which it is clearly to me a break for it up into the sky is called the singularly at an exponential function melfi zoom into the grass me try figure out where that point is that it made a break for iraq to build find it because it does look like any point is substantially greater than the next by comparison to the prior difference
points but if you can assume back out in usa take a toy car trying to push along the grass from left to right to rule come up quick work of the front bumper car seemed to hit wall and that's the point we call the scene you weary so some say the human technology falls along this type of craft if you look at our economic revolution some prior section the agricultural lucian didn't assure revolution in the information age each revolution was closer to two previous want then no one was to its predecessor and higher up on that for
aft and right now we're coming up on what's called the intelligence explosion where are all mental work is potentially on readable by artificial intelligence now you might be thinking well that doesn't sound like it's going to infinity matches sounds like the next big hot on the graph figures were things change artificial intelligence is taylor towards specific applications medical diagnoses autonomous vehicles stuff like this the notion of artificial general intelligence where are you have the intelligence agent which can perform
all of these tasks generally any any mental tasks generally cross the border school artificial general intelligence borscht rong ai if you give it the task of sort of wanting our economy make your life squishy fantastic if you give it the task of making a version of itself that is even better at performing that jaw and then that's new child robot has one goal of improving our lives and the other goalpost making another version of itself it's even better
and it you know this is sort of evolution robot evolution what's called siddiqi i see the artificial and haul jets and sent it doesn't take twenty five years for robot to grow old and cons mala job out world as to the way faster at metal processing underscoring to inherit the knowledge of its prior generations you're gonna get a spiral sort of out of control of technological progress and that is a shoot up into the sky and that my friends is
singular did something that is the next phase of evolution humans effectively stopped evolution when they invented such effective medical science well the exponential growth maybe doesn't start and stop with technology with the dust bowl way back into evolution starting at cambridge explosion and must necessarily continue all the way past humanity therefore is almost are inevitable duty to create artificial intelligence so that evolution can continue this is all off on this craziness ray
i am promise on a talk about real technical details of machine running a future pockets of his award you inspired first there's some really relief on philosophy happening around why you should care about artificial intelligence we better not you can take your side after you've heard the arguments are right for the next section are you ready for this consciousness this one is near and dear to my heart i love philosophy and i love the philosophy of consciousness an interesting thing is that consciousness might be the last bastion of open saw
scientific riddles we found the alphabet of life in the genome we explored outer space and discovered the black hole in all these physical mathematical phenomena that were previously magic everything was magic or religious so for example at one point sickness bill must uphold why is what is now 'cause you're simply bacteria was an infestation of evil spirits back and it was magic and it was religion wealthy think about it consciousness is still the domain of magic and really
none but surely our brains of physical substrate of the minds of our brains follow biochemical laws physical laws of the universe therefore can't our brains be considered to be machines and if that's the case are our minds the results of the machine the byproduct of the workings of the machine that brings up into question the notion of free will do we have free will or simply are we reactionary to an environment is the human mind is the byproduct of the machine
and we're creating a computer brain which itself is a machine will the computer brain creates as a byproduct of computer minds this is a very deep deep rat hole up to date in the space of cognitive sciences carton of science incorporates philosophy neuroscience psychology and artificial intelligence always people these philosophers coming together debating whether or not in fact a machine artificial intelligence can be conscious was talk
bob words intelligence and consciousness when i say that a machine is artificially intelligent is intelligent if it is affected that performing it's mental task at hand in fact media calculators and tortured by that definition of a lot of people missing yes global haven't told us like humans have intelligence the civil case of human intelligence is superior to machine intelligence at present but you can compare them they're not apples and oranges intelligence is intelligence what you're thinking about what you have a debate in ma
and this consciousness is that computer experiencing the phenomena that it's simulating war that seeing the world in the case was solved driving a car court computing in our rhythm is experiencing these things and experiences called claudia in the world philosophy of consciousness that experiencing the things if you're religious and i'm sure you're you have your own conclusions about consciousness were conscious is basically quits the soul is called toulouse and sold separate from the brain
physical substrate separate from the by product in a substantial way during different dimensions that's called dualism in one of the biggest opponents of the philosophy of dualism was rene descartes well we've moved past dualism you'll really find very many philosophers and cognitive scientist who bullied into was undoubtedly than most likely mona's i'm which says the brain is the mind and some important way if so how does the brain create a mind a very prominent some field of consciousness philosophy is called funk
wasn't that is accepted today and the boiled down to this money didn't trouble for this but if it walks like a doctor talks like a duck it internally as experienced as being a doctor so for human express intelligent action that yours to stem from consciousness the robot expresses similar intelligent action then question mark this segue is effectively right into alan terrains test for consciousness which is called deterring tax if you're talking to a robot that you're talking to human doubles
when curtains in you can see which ones which any can't tell the difference that i think we've done its best called the terrain test so this is so far on the bed discussion above can robots be conscious can artificial intelligence be conscious and it warrants a whole pie cast series of it's own i'm going to point out some good resources at the end of this episode my heller recommended he in the real fun and finally the last big philosophical throw down in the space of any i evil killer role
what's so the big scare will they id inevitable we rise against us what most prominent books in the space is called super intelligent by nick bostrom and an example of his thoughts goes like this is called the paper clip maximus asian algorithm if you start utilizing am i had a paper clip corporation to maximize the production of paper clips and you don't specified not to destroy the earth in the process what might happen is that ai it might
find a way to achieve consuming all the resources of the planet in or to maximize paper put production the general idea goes but be careful what you wish for because you may not be able to specify all the constraints big fears liked you on mosque stephen hawking and bill gates got on board with this at one time and it became really big depress me personally i don't think that there's work for such a scare but i believe that you'd come cheer on conclusions by diving into the resources so here the resources the book missing you larry
smeared by ray kurzweil describes from the seal whole process of the singularly and this is colonel must read for anybody in ai it's very fine it's very fluffy ahmed snow not rigorously scientific but it's very easy read it in the space of consciousness and whether or the machine can become conscious breakers while also wrote a book called how to create minds that's also very good book my fear resource on consciousness is a video series by a company called the great courses and
the series is called philosophy of mind brain consciousness and thinking machines in this goes over all the philosophy of consciousness from the beginning until now including rene descartes in toulouse up in including artificial intelligence is who really really good echelon we're recommend and then there's another book on curly reading by a daniel denning who's a prominent modern philosopher constance called consciousness explains and when it comes the fear stuff is a book of course i mentioned called super intelligence by nick posture
and all these resources are put on my show notes and those c. d. v. e. l. bossi duval dot com forward slash pod casts forward slash machine learning with a hyphen forward slash three in the next episode were finally begun to tactical details i'm going to describe machine learning from a high level review and how it's broken down into the sub fields of supervise on supervisor reinforcement learning hope to see you then


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. eight i'm also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. february twelfth two thousand and seventeen mrs episode for our rhythms intuition
in this for that so we're finally getting get into the details of machine learning so let's recall how machine morning fits into the overall artificial intelligence puzzle artificial intelligence is broken down into multiple some fields such as reasoning and knowledge representation search and planning and then this morning and learning has its roots dug into all the other sub fields in each side filled you might perform action and then you might make a mistake in that learning from your mistake part
is learning some she learning is broken down into three steps infer or project error or lost schrader war learn were sick or in these these are synonymous words so infer or protect there are lots showing or learn so let's do an example we're playing chess in chess i might make a move my moved upon them let's pretend it immediately aitken know how bad that decision was all this is not the case but let's pretend
you need a liang know how bad was that move to the move itself was at production may production to make the move them and they'd move and in determining how bad that wise is called the terror or loss function which is step to the machine learning process error or loss function that girl lost function tells me how badly i did and then by a charade myself i learned from my mistakes so if my terror function said that wasn't really really really bad move
then i'm going to update my model what's called a model or get back to the dead by substantially reducing some numbers these numbers are called weights but if i made a good move then the or function my tell me to reinforce my waits good job selling for project girl was function in showing or learn so like i said when i use the word or it means that one or the other of these words might be used when you're reading attacks barker watching some video series
i'm just gonna start using one or the other word myself going forwards why my only use the word error function and some of los function so here shouted process a machine are usually works let's use an example the we're trying to predict the price of a house on the real estate market of portland oregon this is kind funny him shimon eagerness you wanna of commonly used examples over and over i call used hello worlds of mushy learning where the reference of dataset like a professor
or or textbook or on video series war reference some very common dataset such as the iris flower dataset or the portland oregon real estate market dataset so let's use the real estate example while we're goin to try to do is be able to predict that cost of a house in portland oregon given the number of circumstances so for example where is the house located how close is it to downtown where the number of bedrooms number of
affirmed square footage etc no way that the machine learning process works is that you typically fried schering your model by top loading a giant spreadsheet let's say one million rose one million houses in portland oregon where we already know the price of the house we blow this short algorithm so in machine learning there are many are rhythms dedicates you specific tasks so for example the next episode you'll see linear rig
russian is one of the best pop rhythms use for this exact type of scenario you get for your spreadsheet into your linear regression our rhythm for example hands that barbarism shrink its model it free train sort of it takes all these rose rolf rolf rolf role in trains and trains in trains and trains until it thinks it has a very good sense of what it takes to determine the cost of the house and pour le know
or in so that initial step this kind of like free trading but it's actually that's training that's machine learning that part of our whole three step process that we're talking about and then going forward now we can do that you have a train model is that you can protect the price of a house that you've never seen before based on your model and then let's say they knew the real state aged you sold his house to some young couple they haggled you down to you gave them that
christ came out of this machine learning our rhythm the predicted price and the haggled you down to thirty thousand dollars less you go back to machinery younger than you say dude when you try to embarrass mate was thirty thousand dollars last and so you tell him that it goes back and read trains its models so every move example going forward is another piece of the training step so at first phase of all blowing a spreadsheet and training your model actress retraining phase we mess
school training and in every newsstand everyone single example going forward you read train your model is called online learning okay so we use a few words here on going to tackle these words one by one when i say algorithm that's a piece of software written in python intense afloat typically rick talk about languages and framework some of the track is so but that's a very popular starring brown and she learning these days python and sensor flow and algorithm is say a hundred lines of hi
some intensive flow code which for example spells out the linear regression mathematical algorithm now when you piping your spreadsheets a million examples of houses that came with their prices high to them you're algorithm runs over those rose and crunches the numbers to determine what it finds in common with all these houses that really determines the cost in it saves that information on discord in the database
for in memory what kind of high to the hour proven itself in the algorithm plus the saved a sort of that distort pattern the combination of those two things is called the model to the model is the algorithm plus the pattern that has warmed now the spreadsheet a huge import into your algorithm it hasn't rose and columns you rose r v houses themselves house one house to house three house ford center the
homes are called features features or pieces of the house to contribute your production so for example number of bedrooms number of bathrooms square footage and distance to downtown each one of those pieces on the house is called a feature to the rose or the houses and columns are the features of each of those houses and then typically the end of the spreadsheet alas call on the cost of the house in the machine learning how
rhythm what will do is it will take out our last call woman we move it put it aside and then consume the rest of the spread sheet which is each house and all of its features no feature sort of deserved a pie cast episode of its own features can be multiple all types of information so for example number of bedrooms number of square footage those are numbers sodium numeral features that can be nominal features which is short categorical so is this house down to
al gore does it not rather than using distance downtown yes or no that's a phenomenal binary or you can say it except what types of animals cats dogs were birds that phenomenal feature that allows three classes in its features that are the most important part of learning process the algorithm determines what coefficient to put next to what feature and what you get is an equation to bash important part to realize the alta
rhythm will look at your role a single roa time this house has four bedrooms two bathrooms eight hundred square footage i don't know about houses so i'm always getting strong as one mile to downtown those are numbers of your numbers associated with each feature in the power of them what war earns is what number two multiplying each of those features by this is called in mathematics of coefficients in machine learning are proven
it's called all wait so let's say that the power than goes over all of your houses row by row by row looks all the features and determines that the best way to determine the cost of the house is multiplying the number of bedrooms by point three the number of bathrooms by point seven per square footage by point five and then what you get is a number so the things that the algorithm learns his blue waits for the kosher
chance that puts next to the features and the features or your variables x. y. and z. in algebra in fact algebra is a primary math love machine learning it's actually linear algebra that were confusing machine learning what linear algebra does is it allows you to take up onto the clayton's and stack them on top of each other and then soulful for all of them together that's exactly what you're doing when you pipe your spreadsheet inch
or operate on each row is basically not algebraic equation were the machine running algorithms learning the coefficients next to each feature the whole combination of rose by columns your entire spreadsheet is called a matrix in mathematics so your machine learning our rhythm is working with the matrix of features in order to compute a matrix of coefficients have urges them altogether boils them down to a single formula they can then be used
make a prediction for the cost of a house in portland oregon so that's very jam packed and we're going to be piecing this apart overcoming episodes so don't worry if he didn't understand this time through let's take it from the top machine learning is three steps in for slash verdict and there are we're lost function in the train slash lawrence that while we do is we will import a giant spreadsheet of a million houses from portland oregon and ball
all of their features weathered the number of houses square footage for distance downtown and bowl last column of the spreadsheet is the actual price of each of those houses the machine running out for them will take it rolled by rodin actually make a prediction which shot in the dark part of the hour them will tell it how bad it did for that one row how far were you off in that single instance that seat error function the air functional telly
one number or a vector of numbers on how far off you were free of production the machine learning our brother more than an update its coefficients or waits that's the training process or the learning process is that their function said you were off by a huge margin then it will substantially change its numbers if it was off by a small amount they will change its numbers by a small mouth and then now going forward we can make projections on new houses on the market that don't already have a price assigned to that
the machine learning algorithm we use this call to break its equation that has constructed make a prediction the numeric production we can give that to the buyers of the house and if we're wrong about that production we can tell machine learning our rhythm there was wrong by what amount using the terror function and it will read sure ain't it small and that's that's called on one morning okay so that's the general approach the machine learning is actually broken down at three categories of algorithm
it's supervise learning unsupervised learning and reinforcement learning supervise learning is why just described to you everything i just described shoot up until this point is actually fall into the category of supervise learning to supervise learning does is you traded with a bunch of information that spread sheet that you upload you trained on dana and you can train on da going for the point is that your training it's usual or training gets so
it's like to have a newspaper in your hand in your state evaded our bad dog based on whether it got a production correct or incorrect examples of this type of algorithm or linear regression in logistic regression so those are two types began wanted will give you for example on number outputs of the price of the house as one type of supervise learning algorithm what's called a continuous function so based on the inputs you get a number and another type of supervise learning garber
and will predict for you a class so is what i'm looking at a cat or dog or a tree or what unsupervised learning unsupervised learning as little bit different the machine will take your examples that the new rose the u. give it an adult toss it into a pin for example this is called clustering assad branch of unsupervised learning to cart put it in a bin but that dan doesn't have a class is not like i'm putting this ball into the cats category and putting this ball into the dogs
i worry i'm looking at the small ended its blue line connor turning around in my hand it's blue with little bit misshapen i'm actually does come up put it in his spin over here and i picked up a new ball this one's raddatz got spikes and put it back in over there but when the human asks me carved want calling these bins or why i made my decision to get really explain it is to set these balls over here have some something in common lot more common with each other then compared to these balls over and it's been
so unsupervised learning is the machine learning algorithm can figures out what is in common between this and that and segments portions of the day of population so an example of where are unsupervised learning might be used isn't user segmentation in advertising so i want to just german based on user actions on amazon dot com what the clicker one on what has that to show them these types of users poll showed that won him the sex abusers are walsh
oh that to the mass market segmentation supervise and on supervise lerner obviously the little bit different than each other but as a machine learning in junior ortega scientists in a professional setting you probably can be using a mixture of both these are rhythms on the day to day basis for your job they're different but they generally use the same types of mathematical equations and don't be afraid that your nafta commit should make a giant mine shift from one to the next on the other hand reinforcement learning here
as a totally different beast reinforcement learning to knead is where most of the fallen in the action is happening in machine running right now i consider reinforcement learning to be sort of the real ai in machine learning the stuff that makes you think is this thing thinking if the real juice and meet it's the stuff that the mind is most heavily involved in it's also the stuff that sort of reaches its roots into the field all of planning and searching and we
reasoning in knowledge representation each other fields of artificial intelligence supervise learning is really interesting because it applies to fields like vision that in speech you can imagine for example where you are training machine running our brief them on my quarters of dialogue and it learns kind of what makes sentence structure common from one sense to the next so that they can actually construct sense on its bone can have a conversation with you that supervise learning or reinforce
that learning is involved in taking actions taking steps so for example robots taking us that in an environment or artificial intelligence bad guy in a video game trying to figure out how best to kill you the way reinforcement learning works is that you give that goal was pretender we've got a mouse in the game may is trying to find she's at the end of the maze the goal is that she's at the end of the maze and then you give it to us
system of reward and punishment that's the reinforcements that so let's say that if it gets that she's sick it's a hundred points and if it falls into a pit of doom they lose a hundred points the reinforcement machine running out for them will figure out on it's own what are the rules of the game where the action sick intake and the best strategy to achieving its goal soap on like supervise learning were you the programmer sure amy arboretum which it with
bunch of degas in it warns the equation for what all this data has in common in order to protect the numeric value or class the reinforcement our rhythm warren's from its own behavior what actions to take an environment so reinforcement learning is sort of action oriented and get trains itself c. can see why this is the really juicy cool stuff now unfortunately i'm not very savvy on the world of reinforce that learning by match
we'll learning a lot of those algorithms myself right now but also saw field it presents itself to people who were just getting started in the world machine learning algorithms early on reinforcement learning is an advanced topic in machine running soap or not to recover reinforcement learning for a long while many many many episodes down the pike but just the kind of give you a general idea of how they implement these algorithms common algorithm icy these days is called deep tissue networks soon
uses deep learning machine running our rooms in order to determine the best course of actions course of actions is called a policy in reinforcement learning towards achieving that goal you give it negative points for certain things in a fireman and positive points for certain things a farm and kind of the key to the puzzle for making and it reinforcement learning our bread and salt puzzle quickly and effectively is this thing called before is lava basically every step the mouse takes in the maze
it is minus one and so if wants to get to its goal as fast as possible for us lot being alive hurts so i need that she's asap so those are the general principles of machine learning so starting from the top again this is all very hierarchical artificial intelligence by some field of artificial intelligence is machine learning the sheer learning is broken down in a threesome fields supervise learning unsupervised learning in reinforcement learning an indian each sob field of mush
in learning dark multiple algorithms so for example and supervise loring there's linear regression logistic regression etc and we're going to be in the next episode talking about linear regression sort of the wa no one hello world machine learning all right i'm gonna go on a tiny little break maybe three weeks envoy to put this pod cast series out on read it and some other places i just wanna see how interested people are in the series if i see law
with interest expressed them of course i'm gonna actually continue and i will do so and post haste the final ceylon vicious right away that i will continue but maybe all too little bit slower i want some feedback if you guys to go to see the valves are calm thwart slash pod casts forward slash machine learning and click on any episode airing go to the comments section or you can just e-mail me at tyler run ellie t. white alley or are you any yellow he actually no lock on with the feedback or no or these pike assets
as of the two rushed towards a two basic again my goal is to teach you machine learning anybody who was no mushy learning he doesn't have to come from a programming background or a mathematics background just let me know overall kind of how this approach is working for you if there's any suggestions or improvements that you have this forest resources for this part cast episode i'm a post an article by machine learning mash three dot com which is very good series of articles on introductory topics an issue
learning disorder of what this pod cast games to be black in oracle format and then it on voyager recommend the master algorithm master algorithms really good books it's sort of like this episode and potentially the next four or five episodes we're going to be spelling out the basics of some very fundamental machinery are rhythms of the reason i like it is it you can actually it's it's very introductory very accessible for new bees and it's also an audio format and envoy richard you're
demanding as often as possible audiophile or matt machine learning and mathematics resources because like i said in a prior episode the best way to learn machine learning history books and especially courses like the corsair a new jack city courses and would recommend later but having an audio supplementary backup resource while you're writing or cleaning or commuting grizzle lot of time in the data want people could learn some really valuable information from all
your resources is wasted so as often as possible and would recommend audio resources so check out that book the master algorithm and i hope to see you next episode


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and algae it time also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode five linear regression in this episode were finally did they get into the nitty gritty you're
first machine morning algorithm linear regression malice recall about machine learning that it's a very hierarchical feel starting at the top something i called mathematical decision making on actually know what they call the super field to be broken down into artificial intelligence statistics operations research control theory and some other fields artificial intelligence is broken on a machine learning this were we are and an owl is broken down into supervise learning unsupervised learning and reinforcement learning three go down the supervise learning rattle
you'll find this supervise learning is your bread and butter professionally and even maybe academically and in machine learning of course to be dabbling with unsupervised learning doesn't come up quite as often unsupervised learning where reinforcement learning is looking more like local ninety nine territory and i can see that stuff until end game so here we are unsupervised learning unsupervised learning can be broken down into two more such categories classification and regression so class of fires and regress years ache
last of fire is supervised learning how rhythm that will tell you the class of a faint so far look in that picture my looking at a dog or cat or a tree the classify wrote tell me what i'm looking at what class of things i'm looking at the regress sir will give you a number a continuous variable so the outflow of your regress your function of her supervise learning regrets are algorithm will give you a number one two three four five were not talk about classification in the next episode on logistic regression button
sadness over and talk about linear regression which is a regress are supervised learning algorithm to give you a continuous burial it is the hello world of all machine learning this is work everybody starts a linear regression everyone starts here even before this that they go off and specialize in vision or robotics or controls the rear something that's kind of level fifty we want walk class system everyone starts beginning the matter what your field is linear regression an example the ruddy you
in the near aggression is the exact example for the last episode with portland housing costs estimation that the linear regression example because you're guessing the cost new numeric cost of the house given features so recall we have a three step process in machine running projects figure out how bad you did which is the cost a quarter function and then learn from your mistake which is the training or fits that scene of fighting machine learning that this like ten words for any given word it's
synonym hell so the project face to be per day to or hypothesis or objective or estimate the error can either be an error or lost in the trains that needed to call learning or training or fitting were a number of things are probably in a siege ramos offered so all use that myself all let's start at the first step which is to make a prediction the way you make the production is by way of function called a hypothesis function or objective function and usually look
like either th with parentheses and backs inside of it and h. stands for hypothesis which are hypothesis function that x. is going to be your example cigarette put into it a house the house has remembered has features such as number of bedroom square footage number of bathrooms distance to downtown so packs actually gets broken up into x. one x. two x. three acts for the multiple features inside of the function side of the hypothesis function we take that acts that one row would break out into its features
and in the body of the hypothesis function is multiplying each of those features by a coefficients so looks like in algebraic equations so were you have is age of tax no p. h. with parentheses and attacks inside the parentheses equals fail one times x one thing it two times axe to fade if three times extreme cetera buffet has on sometimes old cease your w. four wait oracle fisher
if you talking to a mathematician coefficient those are the things that are in the multiplying by your features in order that the summit of that equation gives you your estimation your prediction so for example let's say that we wants us to make the house cost of the house that has a hundred square feet one bedroom and one bathroom while we want to learn the whole process of learning is learning the cold fish and the parameters the faded
don't you stop you for wait fate of fry ono the thing that goes in front of the x. value so why do we multiply the square footage by white times square footage quite tense eight hundred was blank times one for one bedroom was blank times won for one bathroom for lauren those blanks in going forward is to call them fade out now what i'm gonna do in order to make of this episode little bitty
here to visualize on a lot off all those faces except for one we're only gonna be working with one x. x. one which is the square footage of the house so we're it pretends that the cost of the house depends only on the square footage of the house obviously that's been accurate any fact if you are using very few teachers in your hypothesis function that you about that certainly be incorrect generally speaking up into appoints the more features you how
of the more pack your your outlook is good to be but just for visual is is sacred pretend that the cost of a house depends only on the square footage of a house so page with parentheses an axis side of it the hypothesis function equals fail one times x one okay so we're on the predicts that the murders of three step process to the machine learning system one is to make production one is to figure out how off you were an end want is to lower
earn so warheads you were at work make a prediction with our hypothesis function we're going to import a spread sheet of houses in portland oregon and their actual prices on market so remember every rosenhaus in every call is a feature of each of those houses we've walked off all the features but one so right now we just have one row one call one roll on homeworld tuskegee house one has eight hundred square foot house too has nine hundred square foot
then the second call on is gonna be the actual cost of the house what's called a label and before we go any further last shred of visualize what we're looking at here to riyadh that exit value which is the square footage of each house panel why value which is the label which is the actual cost of the house to have an axe access panel why access the x. x. is being featured the wyeth says being the actual cost of the house and why
we're gonna get is a scatter plot a bunch of dots can in the shape of a football works kiddie clout and it's pouring op in right north east so that's all a party of those roar houses with all their square footage and their actual costs gives us a clout the northeast wallwork shrine to learn is a hypothesis function that old santa lying right through the center of baffle all sold that hypothesis function won one with horrible
they planners will maybe be pouring down and write southeast would be the opposite of what we're trying to achieve a very good hypothesis function will shoot northeast right through the center of that football nephew think about the function of this line why equal status times acts that look similar to something you may have seen in algebra why equals m. x. plus b. point slope formula which is exactly the formula for all lying sloping
which is exactly what we're doing here we're trying to come up with a hypothesis functions creating an angle line that goes for most of our d. n. seoul looks very similar fate x butters no be blood on the flip side is an x. plus b. one slow formula okay so we can replace them with fade out and let's talk about this be parameter white is be in that point slope formula if you do recall for outclassed would be is called a pious it kind of shifts law and left or
right before even start slow be sued where is your lying cross over the x. axis where is it the d. n. n. n. that annexed it is tells it it's lopes left poland for equation we have h. equals fail one x. plus fate is zero best and your clue of them x. plus b. sago one x. plus three to zero worth it is europa is or pious parameter why is a bias parameter up my ass parameter is kind of like an average
we're starting around if you don't have any other information so what happened if we got a house that didn't have any data on the square footage well then square foot would be zero which would zero out that fatal one parameter leaving us fully with the bias parameter in other words what it's saying is why is the cost of the house if we don't know jack the cost of a house if we don't know anything in the portland market is gonna be average co
hasta la house in portland area maybe two hundred fifty thousand dollars now what is with like somebody asked you how much would it cost to live in portland you're saying hey it's anzio depends on where you are the numbers for foot in as towers across lubricant leak knowledge plans on the square foot on are not how much does it cost to live in porn fined two hundred fifty thousand are on that bias parameter is where you start if you don't have any information it's like the average in its worries
we're moving fromm was the start why you rather parameters sore newly your aggression hypothesis function is h. you fools sago one times x one was fate at zero okay so now that we have of visualization in mind let's return to our production step step one of the machine learning process and later aggressions biscuit was to do is to put these labels he's why values on one side of a flash cards right and to look at the other side before face
inside the flash credit says the square footage of the house is gonna make a random shot the dark guess it seems to know where to go see why does the sun and that is to look at eight hundred square foot house and corliss was a wound ten dollars and you're like ten hours through this house you don't say i'm not on what the costs yet okay build picks up the next card so that's why your response nine hundred nine hundred square feet feet on the back actually says two hundred thousand dollars
you can see these values yet goes through it makes a production for every house the ten our spy dollars three hours to go or seven hours what tickets to the ends not allowed to collect all the cards flip 'em over and all my gosh it's classes said mrs though most way off sold what it does now now is the costs that step to figuring out how bad it is called the cost function or the terror function is very simple function very simple formula
it's basically just the average of all this mistakes so the distance between its estimation white hat or paychecks and the actual value the label or why so h. minus why would one average always sort some auto revived by the number of examples battle of the mortar given this is actually a twists the puzzle we're actually going to some mop the square differences of the actual and production so the big production minus actual
old quantity square in the square doubles as an absolute value because you want positive and negative differences be canceling each other on this average twenty square some whole thing out divided by two am so we have to to to have a square and we have two hundred dollar he's come to play in great essential sooner did the main reason there there is so that we can take a druid hill c. n. that doll in order to visualize the cost function we have our hypothesis function on a table and x. y. graf
okay imagine attacks on a table in kitchen and without that football cloud above docks scatter plot dora moved to new table and on top of this table is up all so this is now the three dimensional graf we have x. and y. play it and i'll pull on top of it going up which is busy axis in the way we're in visualize our costs function is the x. axis might be fade as zero with a one the wire
axis obedient if it solicit x. x. is they zero and why axes to stay one hand the error is into the air so that some value of fear is zero and fatal won we have an error of one hundred for example when we plugged in our random shot the dark guess in the initial pass was coming on to flash cards gone through all the houses lunar gresham algorithms going to row houses and taking a gas and taking a guest he
c. was way off it had assumed some random values for fate is your own fate one just to start with so they can know how bad it could be on how bad a don is of value returned by the cost function and it is on that easy access that up and down axis it's in the bowl somewhere so that dots inside a global now we move on to the learning face what we want to do is we want to take that got to move it down again
to the very bottom of the ball with it here are two zero we wants right now that there is way high in the bowl the dot that we have with our faded parameters they wanted a zero the way they're set right now gives us a result way out here on the removable we want to take that dot we want to move it down the bull the way we do that is true process called greedy and descents and at last you learned your first
learning algorithm greedy and descent create descent uses calculus k. calculus to take to do with this of the cost function in order to take a step one step at a time take that dot and move it down the ball and so we found a place on the ex wife playing a buffet is your affair warm where this the value the cost value is minimize the bottom of the ball so how greedy and descent works is lie
this you take the jury evidence of your costs function where you are in the graph to your dots on the ball high up on the bolt you take the two rivers of that function with respect to that point is your thing about calculus would you read it tells you is the slope of your point in that function said he'd imagined taking a piece of paper and pushing against the doctor cotton that the polls really really thin pig
thins the dock to be either on the inside or the outside of the dust matter if you push a piece of paper against that dot the piece of paper has a certain slope like exploiting kind of downwards right really steep that is the tianjin line or the drone residents of your cost function at out what's the what it tells you is that you're doing really bad if you're slope is really steep it means you're very far from zero so what radiant as santa
is is this is says okay guys your needs take a big step on this a. twelve inches southwest twelve inches southwest so south let's say is they want and west to stay too so those guys change their number to change their number so you just ought to be due for a favor parameters that still learning process is changing your favorite foreigners learning your waits to that your function is now
more accurate and you can imagine what just happened on our hypothesis functioned as we actually just changed the ritual theta parameters there inside or hypothesis function imagine we'd member that football graf to scatter plot that cloud we have an x. and y. axes the other tables were left the week when we made our first initial production why old yes shot and dark it was really bad saying houses cost a hundred ten dollars and twenty dollars or wall
i mean maybe was pointing south east right the exact opposite of what we wanted to be which is northeast right down center of all this stout that greedy descent just took by taking it to riveted of the cost function in order to reduce to to minimize the value that cost function outputs by changing the face of parameters it's like grabbing are lying and rotating it counterclockwise one big step so great is set
now steps back and cycle okay i work in closer been close over the goods that he scratches this shannon says okay on that are to take another to read it here so we are nu theta planners and ways are new hypothesis function in our new cost function amin why don't you say in order to minimize your cost another step southwest last two and nineteen inches south and forges west so that's another generation
of the greedy and descends process so take sense that it changes they want any changes faded to smaller this time now or closer to the bottom of the ball in our cost function and are lying in our hypothesis function just rotated even a little bit more counterclockwise now why is actually touching some of the docks in our scatter plot from our original spreadsheet in grenada senses okay okay we're really close that really close now why you take just one more babysat i took
derivative have terminated such take one more babysat southwest's or faded parameters are outdated fail one gets as alteration fate to get some alteration our hypothesis function is changed and we are lying going right down the center of the football so great a descent has learned for us with the parameters that will give us the smallest error they call it minimizing the cost for minimizing the air traversing the ball in our paragraph all way to the bottom
the ball so that our hypothesis function is most accurate so that's kind of cool you actually see math in real life you're using the term rhythms of the function to murder figure route how big of a step to take in which direction to change your coefficients so that you now have a more accurate function this process of learning is also called function estimation because you're estimating the parameters of the functional give you
more accurate out what no one even note you never really get a cost function of zero in order for the cause function be zero all of our examples have to be exactly on our hypothesis function lying soul is slight wind your taxes one wise one axis tune wise to x's three wise to ask you know you never see than real world remember we elliot cloud it looks like a football and wine goes right down the center the air or does not see
we're out the error is b. squared difference of the errors of all the points to the line that we created which is someone or some positive number but what is right down the center it looks like it's just it's fitting the graft just sell them that there is minimized and that's the ideal place for or lying to be by the way mirror function in this case where it's the square difference at all
all summed up in the bye bye to him this is called the means squared error now real quick i'm going to tell you the equation for glory and descents can remember that gradient ascend is taking the jewelry is that is of the cost function carried the two ridges of the cost function and cost function one more time the cost function is your hypothesis your production minus the actual value so why hackman as wyatt's quantity squared all
all of those the sum of all those in your spreadsheet divided by two times am divide by two and and and is the number of examples in your training set now to take the jewelry evidence of a function you don't need to know calculus right now you can know some of the very basics there's some rules there some rules we don't actually have to go through this grieving process and it feels there's a power in front of a function you can just take the output in fines attracted by juanita the lift shrieks of
trade and that's the reason now we have these two zen there it to the bottom you too and by by two men added to it talks where well when you take a derivative of the cost function those that two comes out of five in the car cancel itself out while we get in the hands is this is that this is the great sex algorithm on say is your own equals what was before my yes alpha times one
over am times the sum of the differences why half months why so you to this that you took you had faded zero by its premiere tickets that in some direction by some amount that way at all flood variable is called the learning rate and i'm not to talk about bungling you'd spoil this in the details from the resources section fail one is fatal one so what was before minus all the time is one over am time
the samba of white hat minus y. times acts on at the feature in edo needs and remember that the cooking when you're not the other this is more for people who are just got macklin klein curious you're gonna learn all the details of this in the resources section on a talk about later okay so that was a whole lot of information lot of information i'm going to basically star from the top like to do this lawyer i'm sure you've noticed by now like to start to talk and do it all over again now
so we have all the pieces in place bless you all over again machine learning is broken down and supervise on supervisor reinforcement supervises the case in which you are training your algorithm with a bunch of david soul portland oregon housing costs sounds like supervise learning our room to me you supervise we have classification and regression regression is coming up with a number classification is coming out with a class like cat on a tree well portland oregon were trying to figure out a casa houses sounds
regression to me linear regression is the most fundamental machine learning our them whatsoever but also of course is regression algorithm for estimating a number stories linear regression we have three steps we make a prediction we figure how bad we did with our cost function and then we trained we learn how to meet fix that mistake in the protections that we have our hypothesis function every machine learning our them how
the difference hypothesis function the hypothesis functional change from our them to heart rhythm as well as the cost function that cost functional change because the hypothesis function is different but remember that the cost functions based on my boxes function and of course the training step for every machine running our remote change because taking the jury is another cost function will be different depending on the function in the linear regression model your hypothesis function looks like this
sure that's the polls fate is zero which you're biased forever with which tells you the average that you're working with it he didn't have any other information work with clients say no one times acts fail wine is your weight or your coefficient bigger trying to learn eternal learned by astronomers will you try to learn the fate of parameters and x. is the future that scanner commanded this function for every road we're lookin' that we make a prediction
the hypothesis function will call that production why hats and the initial patch of things will go through our spreadsheet one road time make a prediction make the production make a production random shot the dark reuse say one of fate is your new random values your year random results step to our cost function will tell us how bad we did howl far from the truth we were on average it's called a mean squared error
or in the case of linear regression and it is the average of the differences between or estimations and their actual values the equation looks like this one divided by two am times the psalm all of all the differences squared so page of x. minus why squared for every roll some ought to buy went to overrun cost function that tells us how bad
did on average as a function which puts us as i've got all bull if we we use greedy and descends to move that got down step by step by step to the bottom of the bowl to minimize the error in the air is a function of our fate parameters so we find that point in free space where our dot is at the bottom of the bolt we find what the values of faith is zero and feel want ma
or on the table such that that there are his minimize and now we have learned our hypothesis function buffet wanted fate is your parameters we now have those handy and we can make productions in the future in the way we visualize greed the senate is as moving down that cost function that old course the bond of bolt which is the same as changing the parameters of our hypothesis function so that we're like grabbing ahold
goal line which was a bad random shot dark initially and chino rotating it clockwise or counterclockwise and tell it fits that they sat most effectively that point at which the error function is minimize the lever be perfect but there is definitely a point where that error is minimal our rights the savvy amongst you will recall that something is missing here something is missing is that i have worm
of all the features but juan i have reduced our situation to something called you leave the area it's linear regression one variable one feature which is the square footage of the house but that's not how linear regression works mo while of course yet many features the number of bedrooms a number of bathrooms the distance to downtown whether it's in a safe word dangerous neighborhood etc will determine the casa house all things considered and then of course the bias parameter
which is basically the start cellmark average that we're working within this in this housing market with multiple variables to multiple features we're dealing with something called mole most liberia linear regression and it's basically the same as ye very linear aggression but it's more difficult to visualize your mind i'm describing it's you and audio som actually not to go into the tactical details of multi very linear regression it's so similar to ye very at that making the transit
when you see the details on line you'll you'll you'll understand right we'll be alone a fascist basically our hypothesis function as did the age of vexed people's faces europe was fatal onetime sex want was fated to time sex to cetera we're learning all of favor parameters through the greedy and descent process but not to describe the mold siberia to linear regression model t. u. instead i'm going to point you to do resource though reaser
says section of this pod cast boils down to one resource one resourceful on it is the andrew being corsair of course andrew angie being corsair as c. o. u. r. s. p. r. k. and if you've been around the machine learning community for longer than a month that you will have heard of this recommendation a million times over stat overflow and cora and read it and wherever you may hang out this is
most recommended resource for getting started machine mourning period is of course so soon online courses beside a twelve week program or something you can go really fast self case i finished my name three weeks but more than recommending it to you i require all be used to take this course this is the most essential starting around for picking up and she learned if you start trying to learn she learned from any other resources you're not enough
have enough the fundamentals in place i read very many books i started reading textbooks are washoe wanted you to video series of listens to audio books and pod casts were ours for starting to learn machine learning i had heard this corsair course towered over and over and over and i avoided it because i'm not because i'm typically like to learn from luke's m. l. o. c. 's all what stands for these online courses i'd rather learn from a book so i avoided for awhile what he's a big mistake because my finally
hunkered down and took the course it's just hold the kearns and i saw everything for what wise and i slapped my forehead and i said why deny it wait so long to take this course enjoying it is the best teacher on the planet for machine running back course has you doing programming exercises in our lab and quizzes and tests again itself paste and there's great visualization spin the videos if the course equivalent of what i'm trying to achieve with this pod caseloads copley
one oh one is gonna be using a lot of mass that he teaches you the math along the way like essay in the introduction of the spot cast with audio being inferior medium injured beings course is far away that sip your your medium school learning she learning so i'm not recommending it's you i am assigning it's you and requiring that you start there if you haven't already taken and looting corsair of course don't fool yourself and thinking you don't have enough time for that you'll do it when you do have more time a new law
it's just to twenty minutes a day even ten minutes a day itself case puts it super facets pretty easy and you're gonna wanna search it away at it now sooner than later so that you prepared for the good stuff what search for all along like people earning in return on our stomachs are talking about all acted stuff additionally in cases like this episode i'm not going to be posting show notes with the algorithms and actually kennedy posting andrew beings show notes i'm going to direct you to the core sara page
she was the notes from his courses billing sergei did their authentic into you'd have to create new count on corsair first before you can access is links the better today and tomorrow anyway it back outside outside of the class in get started so that's it for this episode the next episode is gonna be on logistic work ration which is a class of fire version of supervise learning if you haven't already yet pleased to give me your reading on itunes you're stitch were brutal play or think everybody from the right key beauty
you came over and subscribe so we got some subscribers and i'm definitely going to be moving forward with this pod cast i hope to do an episode every saturday or every other saturday without gonna get special down on p. b. s. and loop see you next episode


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt he had time also starting a new contacts which can use your support and it's called last year's like tax and he just conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that at waseda val dot com forward slash l. l. h. this is episode six certificates ten degrees so despite guess episode is to be at all why
fourth year of them alaska realize that the last episode was really heavy really really have the mathematics in our room staff don't wanna give you a break from that episode before move on to logistic regression i may do this from time to time in between very technical episodes trod upon showing them with fluff see gilligan of a breather this episode i wanna explore what are the options for you as summary pursuing machine learning whether you want to get to the industry professionally or you want to
ooh go down the deep rabbit hole as a scientist and cracked the mysteries of artificial intelligence and consciousness or whether you're just doing this is a hobbyists use curious about the whole thing working nights and weekends maybe just on side projects one reason that i want to address us right now is in the previous episode in the resources section i recommended that you will start on the intruding corsair of course in wallace not a very difficult course is a bit of the time to men minutes something like twelve week course i guess said itself tasty could probably finish it
read for weeks if you dedicate urso for the some people may be wondering if i may want to go back for a master's degree in the machine learning or to woody's you das began a degrees should i take that tangerine corsair of course now or do you wasted time sunday learning accessed from scratch anyway so let's approach the types of people learning machinery so we can answer this question the first type of person would just be doing this for self codification they just are playing around with machine learning as a hobby for side projects and for fine i think using my typecast
as aids curriculum for syllabus to help guide you in your own self motivated education would be plenty sufficient for that kind of person in the resources section every episode i'm going to be providing you see quite chilly the best resources for learning the details so early on in the series like i said i'd recommend intruding corsair course as long as the new g. busy for the next for six weeks highly highly recommend resource and enforce the end of my tie cassie resented be recommending things like artificial intelligence amar approached
x. book and indeed learning book i think for the self learners coming up with your own curriculum or letting me guide your curriculum albee play sufficient then there's people what's happened to the industry professionally and one actually speak to right now is on middle ground between those two types of people on the one hand we have people who are just doing this for fine and then the next wrong we have people who won it's happened in the industry and they wanna have eight and one have very strong resume some more right in the middle we enter this world all of what's called me
books and volvo c x it stands for massive open up online courses in there some major contenders in this field one is corsair another's an ex con academy in you dasa be so let's talk about these one by one con academy is for actually for high schoolers and cut a piece duties were very early college so the courses are offered on con academy of things like calculus statistics leader algebra would actually be very useful for machine learning education but i don't think
they're gonna have anything specifically for machine running so it's all the prerequisite stuff in a future episode gonna have on math there's branch the mathematics their recommended prerequisites for learning machine learning i'm going to make shouts con academy they're not useful for the purposes of this discussions we move on to the next one and acts doll i see a lot of discussions on read it and hacker newsom people are talking about whether these nukes are worth their salt and unfortunately that x's never won a kind of makes the cops never really a contender in these conversely
it's hands whether or not this is a good approach given that day that and we use that as a telling it evidence that may be bad acts is not one of the better platforms for learning to a science or machine learning so we're just going up to start at x. ers come up with the matter the equation the two contenders that i see in luke's most commonly are corsair at n. yu dasa the course sarah tends to be more recommended for one off course is that you offered these nano degree competitors sir
to check it's called specialization it's corsair specialization such as specialization indeed is science and machine learning etc but i find that people don't gravitate words arose or recommend them alive on social media course sarah is fantastic for one off course is like the man who being course intercourse or you'll see corsair courses recommend it from time to time but they tend to be worn off courses i think in my own experience looking at corsair it's a little bit less professional model little bit less in
history backing and post production by comparison to you daft city it's a princess you jack city new jack city has one off course is as well free course the u. take very good ones like the deep learning course that was put out by google they also have this thing called nano degrees their certificates the concur on line you pay two hundred dollars a month i think they tend to be about wanted to your programs these men are degrees the sensors stipulation waikiki finish it and utah
i mean you get half these men are degrees of very professionally put together a video series with mentors assignments programming assignments in class projects and solid pure it here interaction i absolutely think the huge assay is the future of online education in education in general i really think that moves or we're going to be revolutionary in education space i really hope that moves eventually kind of overtake the requirement for
are accredited university degrees because these are scale bowl professionally made manner degrees certificates put together by the best the best industry like intervene in sebastian to ron wyden however that presents these nana degrees or not widely accepted recognize or respected by employers now this is hearsay this is my own opinion you may wanna get a second opinion about this night you plan to revisit this pie cast episode feet
or what i've learned a little bit more about the space i plan to come back and redo this episode but from my own findings from talking to recruiters personal aide for reading hosts by recruiters are hiring managers on various social media websites it seems to me that you jessie anna degrees are not respected in the industry yet it master's degrees or of course accredited degrees there respected by corporations but you jessie entered
griese or not what they do do for the candidates whose applying to job is to provide you with your portfolio of projects that you have built for that class and they prove that you have the motivation and self drive to take an online course and it's a nasty to finish on line course i have seen hiring managers recruiter say that they very much respect the initiative shown by people who take these courses in the side projects at dave to
oh it's during the course of these men and agrees with that banana degree itself is not respected in fact it would cost that portfolios mentions y. c. hiring managers recruiter say time time again it is that portfolio portfolio portfolio portfolio is the most essential ingredient to getting a job and in industry having a strong portfolios projects you've built nights and weekends not just little dinky things they can recognize a cash from an image some
in the scales something the u. deployed to the dead u. s. steel horizontally and shows that you have skin in the game in developing large-scale machine learning projects to less likely you das eternity in between somebody who's a hobby us is somebody who wants to get a job because it feels the hot is because you'd be learning all the good stuff you wanna learn from these mannered agrees to this day helps you get a job in the sense that you are the shows that yourself motivated
and spew have a strong portfolio their degrees self doesn't help so what i would recommend for somebody who doesn't have the money or for and for people actually listening my pike s. security was in my part as shows do you have self motivation now if that you could follow up with the resources that idea of the end of each episode sort of a homemade curriculum merciless for you to go by you can teach yourself everything you need to know anything work on your own side projects on earlier that you whether or not you want to pursue
banana degree by way of the gas city or few just wants are working on some top side projects on your own in learning of the need to know from these books and videos me personally and i don't plan on getting a new jack city nana degree okay so now we're now we're in that mule categories people who are doing this because they wanted job like i said at the portfolios most important piece for getting a job and machine running for talking to recruiters are hiring managers this is sort of the way that wet development is
today's generation him in wet developments you'll see a bachelor's of computer science was is a minimum requirement on the job description that apple cans with a strong portfolio can kind of bypass that's that i often kind of you that bachelor's degree requirement on wet development jobs corruptions as a suggestion i do have a bachelor's in computer science so i don't know how much that helps public education is never talked about in an interview only might have passed projects similarly i've heard from people that
the master's degree requirement on a job description for and she learning job can sort of be bypassed if he have a very strong portfolio to focus on that portfolio but if you do want that peace of mind and black shirt edged the master's degree is in that actually accredited degree by comparison to death c. n. agree which is simply a certificate and will get you in many doors that you wouldn't otherwise get into now master's degree to be very expensive forty to sixty thousand dollar
some universities that there is one master's degree out there that is on line three on line is by george attack is called a pole and asked c. s online masters of science in computer science in eighty seven thousand dollars slowed substantially cheaper than you typical master's degree in the way either stand works i'm not entirely certain i believe they use you das city courses in lieu of certain classes it may just be even the u. death
c. d. n. a degree disguised as an accredited master's degree i'm not sure but i do know that your partner with you cassidy and you're using at least some new jack city courses in their program so i think they're able to drive down the cost of their degree because of because they're benefiting from the infrastructure the new jack city provides me personally when i think i'm going to do is start working on some personal projects nights and weekends to deliver shrunk portfolio so they can put in applications to companies if i find that
not going very well then i will call that pledge look like this poll and masius around the arterial secret now guys have worked professionally in machine learning for time but i was sort of grandfathered into the position so i didn't confirm my way there that's why i say this episode as little bit opinion based based on things that i've read it from conversations rama wedding conversations i've had with recruiters but you may wanna get a second opinion on some of the stuff okay p. h. d. why would you want a phd
don't you stay is not necessary for getting a job machine learning is not necessary masters it's twenty sufficient and in some cases you can bypass that masters requirement by way of very strong portfolio so why on earth would you want a p. h. d. l. phd well obviously get you into more doors like a machine mining engineer role at google or facebook some of the more hardcore basements artificial intelligence development rolls by find that most people would you go further phd that's not why they're doing it there now
doing it to crack in the industry they're doing it for the sake of the phd itself the thing about industry is it your job is going to be prescribing to you the types of projects you were gone and the nature of work in the industry tends to be a little bit boring i'm talking about right commander systems like amazon's rector manner system or anomaly detection programs for some very simple charts and graphs sort of d. n. a. analysis kind of gigs you working in machine learning yes we don't
really have the freedom to explore all the greatest need be greedy rabbit hole science it's going on in the realm of artificial intelligence and discussions are happening around consciousness and the future all itchy of modern research in ai going for your p. h. d. in mushy learning or artificial intelligence affords you that time to die a deep into the stuff that makes am i fine if you see yourself as a mad scientist who wants to solve these real
is that a phd is free okay now teach the program he's you on one of you know this your master's degree you pay for your master's degree seven thousand dollars for the onus c. s. program was to get into a phd program they pay you now they pay you about thirty thousand dollars that by comparison to a hundred and sixty thousand dollars that you might be making in industry in machine learned so it's kind of a compromise your jews on one hand to like get to work in all the really
cutting-edge research in all this fun stuff and a thirty thousand dollars in the d. working eighty hours a week or your work an industry where have the decent work life balance the very high pay scale but the kinds of projects i'm working on are less likely to be very entertaining on average assets compromising your behalf to come to terms with me personally i'm going to try to crack in the industry first by way of a strong portfolio if that doesn't work i'm going to get my man
stirs to greet them voyager reevaluated be three years now no-one whether i wanna go for my phd they definitely see value in the phd program really those hard questions being tackled by the scientists is very appealing to me some relieve the decision up to you if you have any experience or any questions or round this topic please calm in a shot at you if you get some discussion going back and forth maybe some more concrete evidence one way or another i'm going to pose some other conversations
i've seen from around the web in the resources section and let you conjure wrong conclusions as it for this episode and the next episode will get back to the technical details with logistic regression


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash n. algae to get to i'm also starting a new contacts which can use your support and it's called last year's like taxes and he just conductivity focused and tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode seven logistic regression in this episode work and talk about classify
years namely the logistic regression crossfire algorithm to remember where we are in the artificial intelligence treat we've gone down to machine learning down to supervise learning unsupervised learning is broken down into two sub fields classification and regression we studied regression last episode with linear regression which will give you a continuous variable outlet on number so the cost of a house in portland oregon an hour we'll talk about class
haitian which will give you the class of a thing in my looking at a cat and dog a tree house or the case of binary classification is this a dog yes or no zero or won that you may have noticed right off the bat the word logistic regression that is really confusing kiwi i said that supervise learns broken into aggression classifications of those are two separate categories now are talking about logistic and linear regression of both sound like regression to me how was logistical gresham classes
occasion actually the term which is ration it is historical it was a mistake i believe more effort schweitzer the nordic you can effect it has regression in the word logistic regression i think what's going on there is that associate it weep high or linear regression all over them into our logistic function so the classifications that is a function of our linear regression algorithms i think that's why regression isn't it
i know it's like we're doing logistic thingy too linear regression logistic regression and what is logistic mean well what cons out of our classification function is what's called a large it l. o. g. i. t. swelling to imagine this way linear regression is for guessing numbers with the casa house morgan and logistical gresham is for guessing classes this that or the other thing and why you do imagine logistic regression is like this machine with this car
tuned machine with conveyor belt going into a nightmare ball coming out of it and so incomes are linear regression our rhythm and it goes down a conveyor belt was inside and of our logistic regression machine mikado that cartoon like whoa ball being bang boom it looks like the fight going inside logistic russian machine and our palms a number which tells us how confident the augur that is that thing we're looking at his house or how confident is that the thing we're looking at is a tree so our common these now
or is associated with the class in these numbers are called lodge it's so point seven percent probability of this in-house point five percent hitting a tree point three percent in the dark and then we take this thing with the highest lodge it with the highest probability and we take that class that function of taking the class of two with highs lodge it is called order max you'll see this in various machine and libraries or max be or g. m. t. x. says fine thing with the highest number take that
class now all the random side you'll notice a set point seven percent likelihood of house point five per century point three percent on where are those all add up to one right to know at two hundred percent of us are real probability distribution it's not a proper probability if your architecture needs a proper probability distribution venue titles lodge it's so we're going to another machine the type of lodge it's been to something called soft max soft max best go past he
at apex it takes your lodge it's an it transforms them into a proper probability distribution where all of your logic to add up to one will cover soft maximus up so cover that the lawyer up some so logistically regrets and takes into its linear regression and then it does bing bang boom in the machinery in and out calms lodge it's one two three four large it's maybe we have before class system either this picture can be of the house the tree and dark or a cat and each lodge
it's associated with each class comes out of the machinery and we picked the one with the highest value in this case house with point seven by way of a function called apart max know like that in the last episode were made the episode simpler to visualize by working with you leave very it's linear regression rather than malt side very linear regression and i'm just assuming that you're going to take that into being corsair course where you'll learn the details of malted very linear regression and make
this episode symbol or by working with binary classification so is this picture a picture of a house or not yes or no zero or wind so incomes a picture bing bang boom outcomes one lodge it's been stated the valley between zero and one where's your represents no and when you represent yes so unlike the point seven which is the logistic regression our them telling us that it is seventy percent confident that this is a picture of a house is not a hundred percent confident said
the present hofmann the warrant do with logistic russians say anything over point five is yes anything on your point five is known so this is a yes this is a picture of a house which the guest of the picture my house okay well the example that we're gonna be using actually for this episode is the same example for the last episode we're putting in a spreadsheet clubhouses in portland oregon the rose or the houses themselves each call mom is off
it sure so square footage number bedrooms number of bathrooms distance downtown and center on the last label from a previous episode was the cost of the house two hundred thousand dollars three hundred thousand dollars that's why or labels that last call numbers called labels will why values the actual cost of the house and renewed using the spreadsheet to train our model reuse the spreadsheet to train the pattern that we recognize
so that in the future we can make productions will again logistic regression is non linear regression are guessing in number were guessing of class is so in this example reduce instead of saying the cost of a house which is a continuous there was now we wanna work with let's say and do we consider this house expensive or not yes or no expensive or not expensive says euro will be not expensive and one will be expensive as sole goal to spread she ourselves manually anything less say over free
a thousand dollars will consider that expensive anything under three hundred thousand dollars for considerate not expensive for him on fire spread sheet we're not open up in microsoft excel one road time and say zero one one one zero zero zero one one one zero zero one one one zero just replacing always actual numbers with whether or not we consider it expensive so we're working with classes here in this case binary classification it could be one of two things now remember how the machine learning system works we have a freeze
that process for takes to warrant for sept one step two is our bureau in los function and step three is train or learn soaring up high tenor spreadsheet inch were logistic regression function as to go through all rose row by row by row and to make all winter productions a bunch random shots in the dark that step one predicts phase in then steps you remember we're in use in error was function under function in order to that
erman how bad we did help off were weak and then we're gonna do step three which is to train our hypothesis function we're going to train these fade out parameters the coefficients in our function for that update our values and so we have a function that fits ortega accurately all wine autograph that's it started accurately now it's not to be aligned in the case of logistic regression so let's die even let's open up that machine
cartoon machine and zoom in and let's look at these three steps in detail so the hypothesis function in lyric ration we remember we had to scatter plot cloud of dots looking like a football pointing north east we wanted to shoot all wind straight through the center baffle all its quarter or progression line and linear regression or not to have numeric values in our case in logistically gresham were not an america is renowned ones and zeroes so on
one side or things that are expensive based on some combination of the features of houses and on the other side are things that are not expensive so we need a function that somehow gives us zeroes or winds were somewhere in between and our linear function of the tal y. one on the football cloud northeast that does not give us one or zero it gives us a number two hundred thousand three hundred thousand so the function we're going to use is a mathematical function and statistics it's called loses
function logistic regression the logistic function or received more a function in the recent callie signori function as an alternative to logistic function is it looks like an ass imagine if you take an ass you draw an s. and eat with your fingertips you grab that hot right hand of the s. and the lower left hand and you stretch it out you stretch it out so that on that x. equals zero on the x. axis coming from native city commission left you come from left from the left hand
and once you get towards the wire access to start for irving up really fast you cross over the wire axis at x. people's zero point five and one half and then when x. is positive you start leveling out and then you get to x. equals one up and you go to the right towards infinity so to guess on a graph the bottom of the ass is on x. equals zero the top is on x. equals one shoots off to the right word infinity towards the west towards infinity
crosses over the wire axes and zero point five two we wanna fitness s. curve this signal we function or logistics function we won a fifth part did in the grass on how to that function well we want to do is create what's called a decision boundary that puts all the data on one side if it's yes and all the data on and on the other side it's no more learn what that decision boundaries where we
crossover the yes no access door to train our fate of parameters remember that's from berlin linear regression episode we have these fayette parameters there are numbers inside the function that we're gonna learn what traced a tremor so we get this good decision boundary so that's our hypothesis function or our objective function it is the sick boy or logistic function so remember hypothesis or objective function is the name for the function they were you
the meeting the predicts that step one and depending on the machine learning algorithm you're using for the task at hand that function will be a specific function in math so in this case and logistic regression it is the signal you or logistic function in linear regression i don't i guess so when your function i guess i saw you call it what you call in sicily your function now let me just give you the formula for this function the formula for this ignored function it is one that bowl
we're one plus you to the native linear regression scum weird right so why not over one plus speed to the native of and we see see where c. is your linear regression function or specifically faded transpose acts when faye that is the vector of parameters that were no learn were weights and x. is the matrix of examples your spreadsheet if that transpose word her
you off that's the technical details of the multi very it's linear regression step that ice get in the last episode we're gonna learn that indeed andrew being corsair course so you'll learn this whole staff with factor is asian and matrix algebra and all that stuff and andrew being course so don't worry about that right now one more time logistic regression function to give you that s. curve on a graph is won over one plus speed to the native faded transpose acts
so linear regression is inside of that logistic regression function okay so step one is we have our hypothesis function and we're not tie dinner spreadsheet and ramat all our graf we're going to make up one to random guesses remember that step one is to predict predict randomly soul regulate yes no no yes no no no no yes yes yes yes yes with the actual values are no no yes as a no no no yes as i am warner do is it now rigorous step
to which is figure out how off we were how bad word that's our error or lost function and just like is that one where are our hypothesis for objective function is the d. a specific function depending on the machine learning our rhythm you're using our cases logistic work signori function in this that or error function will be a specific function as well ours is called the law that likelihood function because it uses all log rhythm in the function and here's how it works
we can't use or linear regression error function because we're not working with numbers were working with binary classifications zero or one when the actual value is one of my guess was zero how bad did i do or vice versa my guess was zero the actual values one habit of you or if the actual value was one and guesses one how bad idea fast case that there should be zero if i'd guessed correctly the error should be zero and remember that we're using lodge it's a steal things you're us one way
or anything below zero point five is known anything about zero point five is yes we may have guessed and our projects that points to as in i am twenty percent confident that the answer to this particular case is no where the actual answer was yes in that case were last wrong then if i would've guessed zero in the actual answer is one so our terror function is what it is if this law the function it started zero and those chords infinity why
people's infinity backpacks eagles won so those open up another nothin' happened to infinity cell or look and act in our error function is it a graphic starts its zero and it goes right ports one and off into infinity often to why equals infinity before never hits x. equals one the closer my guess is too wide equals zero which is the correct value to close my guess is to zero the closer to zero in
as the error and the closer my guess is to wine even though the actual answer is your own the closer to infinity is my care or ok this is very confusing and don't dwell on the details you'd been warned us on the corsair of course i'm just describe needs you now for fairness now let's take the other example photograph in the cases where the house is considered expensive been here so that their function works the closer my guest go score
juan in other words i guess that house is expensive i guessed correctly the closer i go towards won the graph pecans zero in other words the error is europe and the closer my guest goes towards euro where i'm guessing that house is not expensive but you know is the closer or my terror on the wire act says goes to infinity so this one's like a sloping grass in the other direction so it's like you're coming down from a lamp on the y. axis
c. n. you hit zero where x. equals one again i know this doesn't come out well in audio format so just dive into the details in the danger being course by just wants that you through the process okay so that's the visual representation of the cost functional were constructing for our objective function course ignored function the cost function looks like two separate cases of all locker room organic combine those two separate cases into one for
action and what happens is that's one of these gets canceled out depending on whether we're dealing with a yes or no hard to describe what the function looks like is why it's crime scene log rhythm of your gas was one minus y. times larger than a loved one minus your gas okay that's the error for one roll of your spreadsheet one guess how bad did you do guessing for one particular wrote some all those out into
bite them by the number of examples so it's the average all the errors in this case is so will the complicated what we're working with water rooms but just go to the journeying course notes for week three of a clue how was crazy set too was to figure how bad we did with all our yes no yes yes no no no guesses how bout were we ought to remember the point of our cost function is to tell us how bad we did so that we can srinagar hypothesis function we can shrine
in the fate of parameters to get up there graph that more accurately depicts the way things are with all of ortega and that's that's three step three is to train our hypothesis function using the park error function okay so our hypothesis function goes into art error function or you or is it the function at all of our hypothesis and then apart error function goes in shoe or train function name
ingredients decent remember gradient decent the function of gradient descent is to take to do with that love your loss function the two revenues tells you which direction you need to step with all of your faded planners which direction it each day the parameter needs to change maybe some native value or some positive value down laughter right and by how much surge rivers says how much you
should your favorite parameters in your hypothesis function needs to change in order to reduce your error function and we're going to keep doing that one ingredients that the time keep taking the derivative and changing your fate of parameters until our error function is at a minimum at the smallest point that they can be so he hypothesis function goes into your error function and your pair
function goes into the directive function remember that the derivative itself is a function and you repeat that to resist that one step at a time until your error function gives you what the small value the smallest value that they can give you which means your hypothesis function going back once that now is ideal in our case it means that our signal it function has a good decision boundary they can separate all the yeses on one side and all windows on either so
and and then in the future when you make a gas with the new house you've never seen him you don't have the label is this house considered expensive by our relative definition of expensive it will throw around my craft and if our function gives us anything it greater than point five than the answer is yes and it gives us a lesson for fighting answer is no okay so gradient ascent trains your fate of parameters by taking the drool riveted of your loss function which tells c.
you how big of the steps taken which direction over and over and over until your error is small and the gradient descends formula is for each of your fate for amateurs you have your fate brown or what was before minus also over am times the sum of all both your guests minus the actual value times that feature in that position so they j. people stage a myself over am
the song from my equals want them all of the hypothesis for that role minus the actual value for that road times feature j. for that role again you'll learn this unit and routine corsair course all man that was wild and so let's run through this one more time remember we have supervised learning broken down into classification we're we're trying to guess the class of a thing is it kept our country and regression which is where we're trying to guess the value of a fee
in the continuous variable or numerical value within an end in sight of classification you have any number of our rooms such as the decision tree or appeasing crossfire we're focusing in this episode on the one oh one class of fire which is called logistic regression logistic regression takes a spread sheet of dia whose values were labels hawaii call 'em is gases and those zero one zero zero one one zero zero one in the case of binary caught
the vacation in the case of love malt psych class classification he will be any number of classes were not to talk about that mess up so we hike that spreadsheet into our logistic regression algorithm logistic regression algorithm goes all over that spreadsheet and makes a whole bunch of gases that step one is predicts that to his determine how bad he did with those gases and step three is to take your ear function from set to an op client repeated applications of
the door resident of that function to tell you how much to change your hypothesis function faded parameters so that you can get more accurate and more accurate overtime and sell your error function finally reaches a minimum value now you have a hypothesis function that is trained on your p. l. o. and spreadsheet or your matrix and now when you get new samples in the future you can pipe it into your hypothesis function and it will give you a guest now
castle be more accurate details of each step is that the hypothesis function in our logistic regression our rhythm is called a signori function or you logistic function inside of that signori function is actually linear regression so logistic regression is a function of all of linear regression autograph or logistic function looks like it that's a stretch out s. and we're trying to find a decision boundary that put slowly yeses on one side no one knows all
one side or there are functioning step two is called paul lot likelihood function and it tells us how often we work with or gas from the actual value and the function is actually quite complex so i'll just refer you to the intruding course to look at the equation into watches videos to understand the equation but in summary just tells you how daddy did and of course the trading status supplies repute applications of the derivative of los funk
and in that luke doing repeat applications it's called greedy and descents were descending that paragraph to the bottom of the graft were that error is below its that's what's called the censored sending now let's sort of take the ferry big step back and remember what we're trying to accomplish image artificial intelligence the very general sense of the term remember that artificial intelligence is being able to simulate any mental tasks now we'd go
down the details rabbit hole of linear logistic aggression talking about mathematical equations and graphs and charts and the training process ruble learning process was like taking these law names or these ask curves in all three men in some way that says you probably feel like you're very far removed from artificial intelligence by now so let's take a big step back and let's remember the goal simulating a mental task remember the artificial intelligence is broken down to multiples
fills one of which is machine learning and i said machine learning is sort of them the most interesting in essential in my opinion sobbed field of artificial intelligence in that it affects all the other fields it's almost like any mental tasks could be boiled down to learning boiled down to storing up pattern about how the world works city you can make a protection in the future and inference eleanor
samples were restoring a pattern or a model of the costs of houses and portland oregon that doesn't feel lot like artificial intelligence yet or whether house is expensive or not yes or no logistic were linear regression at the pattern and then we can make a prediction was that pattern in the future but if you step back a bit and think about other more high level sorts of machine learning tasks such as let's say you're on the african savannah in you'll looking to look
a new for you sort of like taking a picture visual picture of what's in front you oh they're so happens to be all lion values classification in order to determine what class of objects is in front of you this is so why in the tree house or food if it's food i wanted to get if it so why am i want to ron okay so my classification algorithm has determined by way of my stuart model that this is indeed a lion now we go to another learning algorithm what action should i take
given the circumstances you may have learned you know and shimon you may have learned that lions will you be there if thoroughly from your parents or maybe want to go by that your shoulder one day when you're on the heights so you have learned that lions will keep you in that the predicted course of action now given that there is a lion in front of you is to run away so here we have vision turning into action and if we wanted translate this into a machine learning situation we might use accomplish
no neural network in the case of classifying what you're looking at k. with vision that we might use a deep que network in order to determine what course of action or policy or plan to take given our determination sold everything in machine learning sort of boils down to this learned and predicts cycle but we have to start at the very bottom with linear logistic regression the building blocks the way goes in order to work our way out to the more advanced high level topics of things like how to take
shuns the native environment given your state or advanced algorithms in vision in classification now i wanna go on a little teach were i said that's lyric and logistic regression are like legg o's are building blocks in the grand scheme and the u. learning the latest building blocks right now nasa whites important machine learning you will find it easy theory come home was a bold branch of engineering compose a bolt if you come from the software engineering background or maybe went on
and or even mathematics you might be familiar with this thing called functional programming functional programming is style of programming and it's use languages like haskell or less where you have a function function again and it takes as its arguments other functions functions b. and c. and let's say that function be takes as its arguments d. n. the functional programming is like russian dolls where you messed up all these functions incited the child
or and eventually the very bottom you have to sort of given a number or a stirring up some constant k. and then you can like start the process conflict opening these russian dolls where time you open the russian dalton what's inside another russian on the open at what's inside another in the open that was set this is called compose ability compose ability your functions or your equations are composed all of the other functions are equations which are composed of other functions and so on so everything's nest
inside each other you already saw this in machine learning by way of logistic regression being come home is that although linear regression it is a function of linear regression to to berliner aggression on rhythm and we put it inside of logistic regression we also saw this in the steps one two and three process of machine learning we have our hypothesis function and we put that into art error function sort error function is a
function of our hypothesis function quarter function is composed of our hypothesis function and then we put our terror function we put that into the jewelry is a function that's the greatest sense that that step three training so in the case of logistic regression here's how roll on wraps we have our russian dolls are very power russian dole accused or derivative top that open inside is our los pop that open inside is our logistic function pop that open
inside this linear regression then you'll find that everything in machinery is this way now let's have a thing in mathematics it's kind of the mathematical major all of machine on her memory that machine running is kind of like up quite statistics really and calculus machine running is highly mathematical in mathematics is highly compose a ball so it's like this by nature but this is also a very useful and necessary after beauty in order to scale machine learning what's your actually dip
when these architectures you cold putting it on amazon web services at each of us and stealing them horizontally if you know anything about functional programming as a software engineer or architect you know that up proper horizontally steel ball system needs to be functional by nature and you'll find a machine running it needs to scale indeed lot of these algorithms especially once again to deep learning are faring very computational expensive very heavy algorithms in the war
virtue deployed a service that will be used by a any number of people you'd need to be able to steal horizontally in order to do that the nature of the architecture must be functional okay there was a long winded digression one of the reasons i wanted to point out this compose ability aspect of machine learning is the following you're probably shopping at the bit to learn about deep warning that's all the rage in machine learning and if he came to this pike pass because you're excited you see
ali's articles in discussions on hacker news about artificial neural networks and deep learning all the stuff that's happening in that space well patience my friends because we will get there'll be other sooner than you think will get to keep learning blacks in war to understand deep learning you have to understand logistic regression and linear regression because logistic regression is eighteen you're on a new ron in the neural network so that compose ability paradigm is that
wait here a neural network in deep learning is a function at all of logistic regression which itself is a function of love linear regression so everything's composed and nested in sight of each other so before we can get to deep learning and neural networks were really need to learn all these little basics these linear units and logistic units because they're gonna be calm neurons inside of our neural network so that faculty learning is a function of love shallow learning recall
it's shallow learning these simple algorithms like when you're logistically question okay so that was a very technical long winded episode i believe don't call me honestly believe that the next few episodes won't be nearly as technical the next episode specifically already talking about mathematics not to go into math run talk about the branches of math that you need to know nor to succeed in machine learning and how much of these types of method you need to know whether the resources that you can learn these
things center of the cassette to common question that comes up what type of nafta and you to know how much of it do i need to know can i go into machining without knowing any math etc to review episode on that sometime soon i'm into episode on languages and frame works so python versus or or versus matt lab to answer for overseas piano versus porch and then will do a high-level overview of deep learning and all these things before we finally get back into the technical details so do not fear my entire pica series will not be like this and that's lynn
regression episode which are super super technical okay where the resources for this episode no new resources on going up for you once again to the intruding coarser course so like i said in the linear regression up so that course is not optional is required you need to start on iconic keep recommending it until we start getting into new territory but the white star working on that course that's it for this episode and all see that next time


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash n. algae that and i'm also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode eight math this time we're not talk about math mathematics not that
sure clayton's the various branches of mathematics that you need to know to succeed in machine learning run away those branches are linear algebra statistics and help us now before we go into the details i don't want to scare you into thinking that you have to learn these things first in fact what i am going to recommend to you is not to learn the math first analyst in ruffle some feathers especially the mathematicians come in this pod cast in machine learning
especially we're taking these introductory courses like intruding course for these won the one text books they have in the appendix for the first or second chapter or the first or second lesson of the tangerine course up primer for all the matthew need to know is succeed for that level of machine running this introductory love old machine learning and i'm a big believer in the top down educational croce learn how to build something first of your own two hands and then you can learn the theory behind why you did what you did so for example
and whether mobile out development you can go to these boot camps were you can learn how to create a web site a high level essential to overreact or a mueller jost acacia mounsey assess the bill got your portfolio enough that you can start going to jobs in doing this today they'll start with the fundamentals teaching you prick it calculus discreet mathematics an assembly language some would say that you don't even need that staff at that high a level others would say that maybe are unique to be made that in order to truly become a match
your craft you want to learn those building blocks i might agree with the latter but i would say gopac to it later you'll be better equipped to appreciate why you're learning the fundamentals and the fundamentals will then snap into place your mind inside your machine learning our rooms where you're lacking knowledge so that's my personal recommendation about how we should approach math and machinery learn machine running first you know we're in the essentials all math for room machine learning the first jack
for the first course then either choose to learn that the mathematics fundamentals after you've learned the basics a machine learning or maybe the same time you're learning machine running some small percentage of your learning time allocated specifically to learn math say twenty percent of the learning time or one day we were the rest of the days are dedicated to learn english human okay celeste i've read into the math one f tissues where was start with linear algebra linear algebra it is the easiest of these branches of mathematics
it's also sort of the most commonly used branch in machine learning every step of your panel process uses linear algebra let's try to understand what we're outward as on what to make an analogy to cooking throughout this episode cooking some machine learning is like cooking i think linear algebra this kind of like shopping your vegetables so it's essential step at every point in your machine learning process but it's sort of an easy step you learned three quickly binge eating will give you a primer on fi
medals linear algebra and you'll be off to the race is no time was linear algebra do you remember that we didn't order the spreadsheet of rosen columns okay we call that a matrix in mathematics rose or your houses every rosenhaus an example and she knowingly nope and every column is some aspect of that house like the square foot is the number of bedrooms a number of bathrooms we call those features in machine running up and the last column is called the label is the basic actual price a
a house that we know so we can use that to compare to our own estimations and figure out how that we're going so there you have a matrix we call those taxes every row is a lower case x. the whole matrix of rosen kong's the capital tax every column of one individual role is accessible by i gained a column number two that x. three x. two x. one x. is your oath that is square footage number bedrooms number bathrooms and then will we want to do is malta
why of those values those features suspects features i fade as they get parameters it's faded that we're learning in the machine learning process we're coming up with these parameters these coefficients these waits that were multiplying through each of our rose each of the x. values in order to get one final number hypothesis age or why hacked wherever you wanna use that will be our predicted cost of the house okay so we're multiplying fate of parameters
as we have faded three fated to say one thing is your x. three x. two x. one x. your faces of the things that we're learning taxes are the actual features of the specific house so famous three times x. three faded to time sex to say one has exxon and then fade as zero which are members are biased parameter times x. zero which were always been basically make won because they zero stand alone remember an x. plus b. y. equals m. x. plus b.
bias parameters is the starting point if you don't have any data about the house you turning the production don't want any information about the square footage number badge and the senate then you just be used the average cost of houses in portland oregon s. are biased parameter are multiplying the whole list of fate as by a whole list all features are multiplying our fate of actor by our example a vector times a vector what vector back to the west they freeze its youth they want a desert bathtub after all was the
numbers are multiplying one vector by another factor now our ultimate goal is to multiply our fate of vector by the whole spreadsheet for every road in the spreadsheet we want to multiplier fate of actor into that row so the spread she's called the matrix now let's think about how we might do this and python well would have to do is a triple nested for luke for every rohner spreadsheet for every column in there
row for everything that in our favor that factor multiply and then outside that luke is to add those together and then outside that luke is to reduce them altogether a triple message for lou okay so that's fine with you do that but why this are spread she had a million rose and fifty columns last monday barry slow hands as you'll see once we get into deep learning you're gonna be multiplying major seas and vectors that
every new on and you may have a hundred or thousand your arms so using for luke's is simply not an option that's where linear algebra comes in linear algebra it is basically doing that exact same thing but with a single operation linear algebra is the study of matrix algebra it is how can you multiply a matrix your spread sheet of rosen kong's your houses by a vector of fate as all at once bambi just multiple
the process is executing this matrix algebra on your seat you or your jeep you as a final winner episode is done by way of something called a c. n. d. s. i m. t. single instruction multiple day at the allows you to multiply matrix by matrix or a matrix by a factor with one fell swoop so honestly that saves you to cons and times of time and that my friends is linear algebra is simply matrix multiplication okay let me introduce you to waugh
new word it is tense or tense or tiki and escobar that is the general word for any dimension list of things okay so we had a vector that's our fate at parameters thea three two one zero that's a factor swords a one dimensional to answer a matrix which is our spreadsheet rosen columns x-ray to x. one x. euro okay in the next road is apt to exert longer that's the two dimensional to answer
fourteen matrix you might have a cube in the case of images you have rosen columns of pixels and any individual pixel has orgy be values red green and blue values anywhere between zero and two fifty five so you have a new newest kind of death wine is like looking forward all three items so that's a three dimensional to answer campbell discord cuba i think it's called three d. tense or so testers the general word for any dimensional with the things and if that
ten sir with dimensions euro is just an number so the number one or two is easier to mention tense or unreasonably met this keep an eye out for that because you can see that in the namesake of cancer flow the most popular machine learning framework put out by google there were no discussion those languages and framework to episode so linear algebra is simply to answer math which you could do with or without linear algebra by linear algebra helps to make it fast and vector rise to any keys
it's reason about so i like to think of in my cookie malice yeah like to think of linear algebra is chopping your vegetables it's essentially you gonna do it it's easy really so we'll sit down tuesday had shot vessels again it does to a necessary step he had to take it every point in the machine learning process the next step is statistics statistics is the horde parked the very hard part the hardest mouth of the machine learning triumvirate and statistics in our cooking analogy is good
cookbook it is the recipes all the algorithms that we use in machine learning calm from statistics and flex it to six is saying hey i came up with this first linear regression that the statistics of formula logistic regression statistics those are in our protections that our hypothesis functions for hypothesis functions or statistics equations straight at us that textbook now we go to war care or functions are los functions mean squared error stats lot
likelihood function stats tsui grab our recipe book you we slide down a table we open its page one mrs rats you we insist chop some vegetables so the recipe itself is a statistics equation sure enough these equations are nothing to shake us to get me if you look at that log likelihood error function for logistic regression it was something like one over to wham times the psalm of kites am all of my times lot of our hypothesis minus one minus why times lot of one minus
our hypothesis and you add in regular is a shoe which we haven't talked about yet it's just audibly in harry if you're looking at an equation in machine learning that looks wild and crazy if statistics statistics is the heart burst the sixties what makes cooking cooking what makes a good chef a good shaft is having good recipes having a good cook book knowing how to put the right ingredients with the right other ingredients that seat essential piece of cooking if the essential piece of mud
she learned so this is why i said basically machine money could be considered a white statistics and finally we have are learning stat train or fit or learn what everyone call it step three in the machine learning process is to help us calculus calculus takes the delivery of our loss function in order to know how big of a step each faded foreigner needs to take to fix itself and it's all part of the loophole
all greedy and descent so our los function which is the statistics equation our los function could be graft in three d. space for forty space or whatever in the linear regression episode we talked about the loss function looking like a bowl in three d. but the last one she can be any number of things sometimes it looks like a mountain range when you're trying to do is traversed the space walk around in your graf until you find the smallest valley or sometimes
highest peak depending on your algorithm so it's like you're a hiker and there snow and you've got your boots you're hiding stick to snow everywhere in the visibility really before but you're trying to get to the bottom of the valley that's where that yerkes lowest as where your hypothesis function is optimal what calculus dies by taking the drug residue of your function with respect to your little guy in the functions of taking the derivatives on your mountain range the huge rid of all of somber
after with respect to your little guy in the mountain ranges like to play video games tory old he can't see well enough in front of him do to pour visibility and bitter relative creates this sort of semi trance period yellow arrow pointing down the mountain slope is really long installing a new walk always the end of this really long daryl gates that up big radiance de cents remembered this process is called read the sock were descending the mountains once again to
the end of this yellow arrow you can stop cannot take another derivative with respect to where you are now cannot make in new yellow arrow pointing you down but we're not your ego left little bit this time so great in the senate is the learning stat of our equation using to live lives that calculus in the cool thing about this is their calculus is pretty easy conceptually i mean if you understood where discredit you very good you understand intuition taking the drug rid of a function proved actual be
quite easy in machine learning at least as far as you're concerned as far as implementing these our grievances concerned lot of times when it comes to taking the derivative of all los function you can do it pretty easily by way of some trick of calculus these rules like that how or rule or the chain will be just memorize these sort of sparks notes calculus shirts off with the rest in order to get rid of all lost function so you transform your loss function into a new function that's good too
riveted and that tells you how much of a step to take in which direction now all of this fits into our cooking analogy very effectively actually it's like putting the trade into the oven setting the heated for sixty five and pressing start okay if the final step it's like putting your fate of parameters all you say parameters in your hypothesis function in the initial steps that one project they're all set to zero were sometimes are all set to some random small number
really i've said initially take a random shot a dark so that the learning phase can tell you how bad you were step by step by step through great said until all your fate of parameters are just right so this that is like cooking your fate of parameters the all star our raju k. your hypothesis function yet it's roxy put in the oven and now they all start to cook and smell delicious and brown and finally your daytime or beings and it's ready to come out the other than all your thinner parameters are just right in
she in learning we have this final step of learned by way of calculus and that is the name's sake of machine learning learn machine learn so away this bill lynch pin of our machine learning puzzle was says what differentiates machine learning from other fields like statistics in our cooking analogy we compare this step to cooking cooking a dish is done namesake for the field of cookie if your cook them is a lot of stuff that goes and cheer
crafts your chopping vegetables you're putting together arrested the than the final step is putting a tray in the oven hitting start in that final step is actually cooking the dish so that's that is the namesake for cooking the field of cooking just like the learned step of calculus by way of gray in the sand machine learning is the namesake of machine learning and by the way we've been talking about calculus as a means for descending the error graf by way of agreed to send this apple cage
of calculus towards minimizing some function is a branch of mathematics called optimization borges convex optimization convex means that the graph sort of looks like a cop for twenty to the bottom of that caught convex is caught in the shape of a graph we're trying to traverse the graph by way of calculus to the bottom of the cop this is a spinoff of calculus called convex optimization in the same way that physics is a spinoff of calculus optimization is
captain so you may see when you ask somebody what nafta or need to learn for machinery know say when around raw statistics calculus and optimization what kind of optimization toward goes hand in hand with calculus stuff and you'll learn them together the specific application of calculus you're learning in the case of machine earnings called optimization is not something you need to go off somewhere and independently in the beginning this is something to be aware of peacekeeping aisle for that worked for wanna go over one more time sort of what each branch of mathematics is
for independent the machine learning so linear algebra is all about matrix math for cancer math to use that in machine learning at every step where your combining faces and exits or any other sort of ten sir matthew might be doing a machine which is very frequent statistics is the mouth of the populations of things were looking in the portland housing market we have a whole bunch of deity in our hands we wanna come up with some sort of probability
strube you shouldn't of the portland housing market okay it's such an sob branch of statistics call probability you can be learning two branches of stat probability and then inference inference is the step of making a prediction about a new house has it fits into the market so statistics is all about the death and then calculus the field of calculus is all about motion of objects invisible world sort of physics calms directly from calculus and fizz
dixie might put no video game dole out objects dropping inbound saying in your running into walls and you're walking up and down mountains of like that's a basket of how in machine running full earnings that is our little dot descending victor are slow to the bottom of the valley if the motion of an object in the physical world that physical world is a graphic thereby statistics this the distribution of our day that we're least the distribution of our errors given the faded
renders using a hypothesis function so we're out for breast cancer math stats is degas probabilities that says distributions of day that inference stats is making predictions on dana and calculus is motion in the physical world namely motion of our little paradox to the bottom of the valley now whitehead said in the beginning of the episode jolt learn math first learn math to rue machine learning learn the essential math
the need for machine learning for room machine learning it's like learning the essentials of cooking in general by cooking some dishes you don't go off and take a course on cooking and then start making some dishes you the other way around you start cooking dishes and you learn the principles of cooking through the act of cocaine now let's say you end up at some steak house and everything sir great cook them all in to some money but you wanna steal ladder you wanna be the best shot
if there ever was you were working some fancy rash or maybe open your own in paris will now we're gonna go back and you're going to find some books on how to chop vegetables perfect how often does everything needed be for certain dishes why do certain ingredients pair affectively with the fury of ingredient matching without temperature with the optimal temperature for specific ingredients and how do we find that to be the case and why is that the case the similarly with machine learning you can go back now can you
start taking out the details of linear algebra start learning old from some of these more esoteric concepts like like i gain of actors and coefficient major sees i understand why the statisticians chose these hypothesis and lost functions for specific models how did they come to these equations so when you learn math after you learn math through machine learning you got an eye for these things if you were to learn math first your eyes would glaze over because you were
have an appreciation of where these equations are being applied so you don't know what you're looking at him when i see so calm as a result of that is that people burning out on math before they get back to machine morning and then they go with their legs tucked between their tails six plus we're back to their prior feel like wet development to learn math after the start to pay attention to the details of the equations and it's sort of like gandalf nice poring over all those books he's looking for something and he catches something that people didn't catch that
for okay so enough of my opinions on now going to be resources for learning math year after machine learning or some small percentage while you're learning machine earning more hey if you wanna just ignore me in just learned for machine running references resources section and break it down into a few categories one is milks by way of a company called chronic academy you never milks like you gatsby corsair out all those things online courses videos and lessons quizzes all that stuff connick yeah
i mean is sort of high school level or katie lowell courses white u. s. history includes calculus one two and three statistics in linear algebra c. can learn all of your math basics from con academy will close links to that show that's the next category is textbooks so if you prefer to learn by text books instead of milks all posts what i've seen to be commonly recommended from course curricula or from recommendations encores
overflow cetera textbooks for statistically around who brought and calculus all also post t. f.'s for primers on the basics of these mathematics branches that you need to succeed in machine learning so maybe three or four page key yes they're primers on just the essentials maybe in countless just taking derivatives of functions in statistics it would be some basic probability theory and inference so those are three
categories different roaches we take to math and on any give you one final category and this is a little bit hard core but i really personally like this it's the course series on line called the great courses we used to be called the teaching company they put out these thirty video series on every topic on us on history cooking arts and they do indeed have math calculus one two and three and statistics to series on statistics
i don't think they have anything on linear algebra but if they do all posted there is one course i wanna drop particular attention to what's called a mathematical decision making is actually very similar to this part cassie reason honestly much more professional league on babylon mine it covers all of the machine learning topics with a special focus on the mathematics is actually a study of the field called operations research which is very similar to machine learning more official told since it's like math for managers trying to decide scheduling
employees or train departure times or factory settings it turns out the math is there some work for your new learning and achievements of course is a mathematical decision making on post and show notes now here's the catch with the series there are video series but i wouldn't necessarily recommend them as a primary source of education for these topics go tom academy or go to one of those textbooks instead but what i use the series forties i convert them to audio and i'll listen to it on my ipod one our next
sizing or cooking or cleaning commuting et cetera so obviously you appreciate audio supplementary education because you're listening to the spot fast so that's how i used the series as audio supplementary education video converted to audio now that sounds really harcourt because obviously we're talking matthew these professors in the referencing graphs and charts and equations burden of the points of things in asking you to look at things as they explain them the one nice thing about these course series is that the instructor
are fairly good at near reading their passions for bully talking you threw everything they're doing status of okay i'm drawing all wine horizontally this is x. axis i'm trying every kwanzaa why axes and making us quickly line now goes up then it goes down the niggers up and as a dot on the valley and so they're very good about near reading the whole process is definitely bit of the brain exercise your gilmore the well caffeinated if you read you what i do and convert these video series to audio use put them on your guy pa
i as video and simply listen to the audio or actually have a script that iran bash scripts that converts videos to compete three files all post and the shone out spike can recommend agree course is enough not even just for mass on june the calculus one two three courses the statistics and this is the sixth to have the whole thing about philosophy of mind whether ai ek she's consciousness the father of our minds as machines to have a whole series on neuroscience so lots of supplementary so
off fringe lee related to machine learning by the way degree courses can be quite expensive maybe a hundred dollars per course if something has an audio format option like the philosophy of mine one dies you can get it from kabul by way of amazon for cheaper but the math ones i'm going to reference in the show notes or video only so you have to buy them as video unencumbered and audio okay so that's it for this episode on math the next episode will be about people earning of the
reid basic overview of neural networks please don't forget to give me the reading on itunes stitch her or to go play whenever you use if you have any friends china learn machine learning please put them to this pod cast as always you can find the resources on koci develop dot com forward slash broadcasts forward slash machine hyphen learning beds eaux c d e v e l o dot com thanks for listening and i'll see you in the next episode


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. eight and time also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode nine deep learning hello and welcome back to the machine running died
this is the episode that you've all been waiting for even if you didn't know you've been waiting for it deep learning and artificial neural networks this is the very exciting stuff that's happening in the world of machine learning in twenty seventeen when you see on hacker news or just the actual news artificial intelligence and machine learning being applied to new and innovative spaces is almost always keep learning they're talking about neural networks if you have any sort of bakker
and interest in machine learning prior to him barking on this pod cast been you've already had your ears perked to deep learning and may be wondering what neural networks are all about if this pod cast is maybe your first introduction to machine learning well then just keep your ears perked because this is the stuff this the most interesting in the space right now no i keep saying keep learning and neural networks their slightly different first off remember everything in machine learning as hierarchical we start with ai
that's broken down into machine learning and some other sub fields machine is broken down is supervise on supervise and reinforcement learning supervised is broken down into various categories one of which is called deep learning the other which is shallow learning shallow learning is the stuff that we've been learning so far linear regression would just aggression a handful of other our rhythms that we will be talking about in subsequent episodes were to skip over them now and jump right in the deep learning the reason i'm doing that because i wanna whet your appetite
i also want to appease the bitch don't know what you have to work very curious about the blurring the fire ago the standard education or route you would take me many many many episodes until we landed on deep learning so i just wanna give you a hint of what it is now so you can at least understand what news articles are talking about before we dive back into the details of showing our rhythms so we have to keep learning in shallow learning and then keep learning is broken down into various other space is one which is called neural net
works artificial neural networks for eight and anne's as far as you're concerned deep learning it he is neural networks the other models with indeed learning i haven't really seen them in the wild whether it's professional your academically really essential deep learning can be boiled down as neural networks and the neural networks themselves are broken down into different types of neural network models which will talk about him a bit for example most allaire perception ons work for neural networks combo whoosh
no no my works etc to this episode is all about the subspace of supervise learning called deep learning emirate talk about what it's branches called neural networks the only branches for your concerns and some of it's some branches wire neural networks so interesting and exciting on a good three reasons why make is that they may or may not school get to this later represents the human brain which of course takes us a lot closer artificial intelligence
that's our goal there little bit magical way they work internally is like a black box we don't know what's going on in the little brain of these neural networks unlike the shallow learning our britain's war we know jacquie what's happening in deep learning that a lot of it's learning be inside the box and we can peek inside it's very magical another reason is that it is fast said sue mean the other spaces of artificial intelligence remember how i said in the past we have a natural language processing and vision and all
these other subspace is which have been consumed by machine running now vision is almost entirely the domain of convolution ono met works language modeling is almost the entire domain above work for neural network caesar all deep learning machine learning algorithms so is specifically by way of deep warning within the machine in world that machine learning has calmed to sub sue me other spaces of artificial untold gents and in that way it's almost like deep warning is the master all
rhythm of intelligence service lot of magic behind deep learning a lot of automation as well as diminutive spaces of artificial intelligence is speaking of black box the third reason all give that makes deep learning special is that it removes machine learning one step all way from the programmer remember that i said that the closer it your program is to the program or bullets it feels like artificial intelligence artificial intelligence remember is simply defined
as automating any mental task if you could build a perfect role based system like the symbol lists were trying to do in the early days of our official told us that would still be artificial intelligence budget if else roles that doesn't feel very much like our official told us and furthermore it feels like impossible task how could you possibly enumerate all the if the old rules of the universe so we came up with statistical approaches to machine learning like we've seen with when you're in logistic regression
we have the state of brammer so we've learned we boil down aspects of the universe into the east say the parameters and then we can use them to make estimations so that's one step removed the machine learns these parameters on its own well deep learning does that even the extra step which were now discussing a bit it's called feature learning it has to do with the features that we've been looking at a linear regression for example you might have x. three x. two x. one x. zero square
good to the house member bedrooms number of bathrooms and this is to downtown the thrall of features will if the model is impossible to represent linear lee as is the case in many things in the real world than that x. three x. two x. one x. zero breakdown does not work you can represent your date as all wynona graf maybe it's a squid or mcgrath so you have to learn how to combine features in the specific way maybe squaring the square footage or combining the square footage with the distance the downtown
in some way to create a new graf knowing how to do that how to combine specific features is a programmers task in shallow learning but indeed learning the neural network learns how to combine parameters away that is effective so it's another layer removed from the programmer which makes it a little bit more magical little bit closer to the angle of artificial intelligence okay let's take a little step back and differentiate shallow learning from to
learning shallow learning is everything that we've been learning so far linear and logistic regression i don't really have a concrete way to represent shallow learning i think of it as the pistol of machine learning really small algorithms or mathematical equations you had been at it and put it does a quickie inside a box and out comes in output by comparison to deep learning which i think of like a bazooka you pipe in an input to a factory castle now comes in output let's use that castle analogy what makes deep learning deep
is that your stacking shallow learning algorithms deeply okay so we already been talking about when you're logistical aggression logistic regression is a ole go a single way go in the late though castle that is a deep neural network sewing neural network is composed of little way goes and those lay goes can be shallow learning our rhythms specifically in a mall tyler percent drawn the vanilla form of a artificial no met worker to talk about a bit you have
religious sticker gresham unit logistic regression lego is the primary piece of your nomad work so take logistic are rational we've are you one for prayer episode combine a bunch of those together whether and now you have a neural network so shallow learning is your basic algorithms indeed learning is taking those things and combining them deeply into a network an artificial neural network why we call it a neural network is because the units of this network that were constructing a
a little way goes the units themselves are called neurons us talk a little bit about the history of this unit we learned as logistic regression to classify or thing for classifying dana is his house expensive or isn't it is this a picture of a dog or isn't it it actually is a little bit deeper than that there's a history involving characters by the name of war in mcculloch and walter cuts proposed the mathematical rep
resent haitian upon the human biological mural on these were both computer people but mccollough closing euro physiologist and hits with the computational neuroscientist to these guys are no new bees to the space of the actual human brain was frank rosenblatt the ritual we came up with the idea of a perception on the news these two guys is sort of formalized it as an artificial mule on a perception on is a step was function it's similar to adjust
progression that little bit different were ignore that fact for now think of the perception and as logistic work russian wrap your logistic regression into a unit will call it the unit and now you have what we call an artificial neural on according to mcculloch and pits the mathematical representation of the human neural on this very interesting so i think a lot of misconception about neural network says that they were inspired by the brain in a very fuzzy way by computer scientists who don't know one thing rather
about the brain absolutely not was inspired by the brain in a very real way that each of these units is believed to be the mathematical representation awful biological knew ron selected that from the top one more time we have mcculloch and cuts in europe physiologist it computational neuroscientist cool looking into the human brain and formalizing what they believe to be a mathematical representation of the biological knew ron they call the senate or official knew ron which
string of bunch of these together and we have what's called a neural network as far as we're concerned for this episode eighteen year on it he is logistic regression to wrap up your logistic regression into a unit the string almost together and you have a neural network lane you're on doesn't have to be logistic regression in the case with frank rosenblatt he used something called in hirsh said tron which is a step was function very similar swords can continue calling it would just aggression in logistic or signal with
on june unit and the first artificial mule network to come to be is called a mole tie layer perception on so it's a lot of verbiage hear lots of words so let's start the hot again we have people earning the field of connecting shallow learning algorithms together in a deep ways stacking them with deportees broken down into neural networks artificial neural networks and an artificial neural networks are broken down into various types of artificial mailman works why
scold accomplish all neural network and others call the records neural network these are used for various different spins off of deep learning applications such as language modeling image recognition et cetera but the vanilla neural network sort of the poster child of neural networks business thing that we call it a malt high layer percent tron word feed ford network him altay layer perception or the earliest versions of a neural network created before they started specializing neural networks in various ways for different applications so
that's already be sought him out in this episode in malta jai alai your percent tron what does the neural network look like let me try to paint a picture in your head and then we're gonna talk about an example of why you're using your own at work the way you might invasion that the neural network is coming from the left i'm going to the right on the left you have your input that comes in so for example or spreadsheet every roll one roa time calms again from the left we
take our role and reflected on its side so it's so looks like a column feature feature feature feature features all the features of stacked square footage number bedrooms number bathrooms etc so as the first layer it's called the input layer imagine he's a little circles so each of our taxes as a circle all of those then feed it to the right into new circles each pinpoint feature each input feature feeds into
a layer of neurons let's say that there's five neons five foot logistic regression units each feature feeds into each meal on in this new layer sir we're going from left to right to hear it puts in feeding them into each of the neurons and then let's go right one more layer and we have our output function our objective functional hypothesis function is our final logistic regression unit is
wintertime less whether or not a house is expensive for example in the case of a classification neural network so the architecture goes from left to right imagine a column of circles over your gun puts one row of your day down with each circle is one feature of that row they all feed into a new layer of circles those are your neurons the corner activation functions or activation units they're all just logistic regression
and they all piped into your fine old single new on the third layer and that's your final class of fire your objective or hypothesis function so we have an input layer on the far left that's your data it hikes into a hidden layer it's called will see why it's called in layered abed those are your neurons your activation functions and those pipe into a third layer called your output layer so bad
the architecture of the neural network as sounds all well that why old and hard to understand what's use an example to help you understand this would icons to health care they are many features about an individual which might determine that the medical costs of all that individual would say that an insurance agency is interested in how much to charge the person based on various aspects about their life things like gauge whether they're a smoker
whether there are obese etc well your first impression is to use linear regression those are all features the high command linear regression an outcome hawaii which is the cost per individual of medical bills but that doesn't work that way it turns out with health these features don't apply in the linear fashion it turns out that age gets more and more and more important the older you get the snow
linear it's all a bit more like texts squared so it's more likely the case that you're sixteen years old you had the doctor maybe once a year or four years older than the doctor maybe once every six months ago when she started in the nineties you seeing the doctor may be weekly sottile little bit more like a pauline o'neill function x. squared so we already cannot use linear regression here but there may be other combinations of features in our example for day
all we know with the smoking and obesity combine to be greater than the sum of its parts it turns out that an obese person and a smoker separately have their own health issues but if all those are combined it causes even worse issues than those separately to combine nonlinear lee now if we knew little bit about this already as i'm explain to you right now all we do know we could construct a holy know me old linear
aggression algorithm out of combinations of features like each square obesity times smoking etc manually but again we do things mentally that's not very machine learning is it and most the time we don't know enough about the puzzle to construct combinations of features like that ourselves as we won the machine money now grown to be able to do figure out how features combined in a specific way most effectively to deal to determine the output
so let's see how a neural network might handle this situation let's use that malts high layer perceptual and architecture that i just described will start from the left wheel and put one role at a time for mark sri nina is set grammar from the portland housing market we import a spread sheet of training at the data for which we already know the answer what's the cost of the house so every rohner spreadsheet is a house may recall on it is that it's the
cheers on that house such a square footage number bedrooms number bathrooms in the final column is the label or the gnome cost for the house on a case of logistic regression were we were just trying classify whether a house was expensive or not dennis is gonna be a zero whirl wind yes or no in the case of regression are labels will be numbers and a case of classification or labels will be a yes or no so far neural network we're going to the same thing remote upload spreadsheet it's gonna have rows of people in the columns of features on those
bull such as age b. m. i. a. whether or not they smoke and various other things like location do they exercise do they have a healthy diet et cetera and the final call on is in the case of regression maybe the yearly medical costs of the individual or fu wanted you classification maybe will say is this an expensive person ought not we're gonna do a regression example miss case sore neural network will take one roll roll by row had been as the first way or that is
first layer of our neural network each circle of that layer in a column is a feature of the first row and each of those features heights to each of the neurons on the second layer we called each of these neurons a unit word activation function in our case of the mole tyler percent on we're using logistic regression as the activation function so part activation function is to say to more aid or blood
you stick function was pretend we have five neurons in that second layer they each feature of the first row of data will be fed into each new lawn of that layer remember this layers called the thin layer so each input feeds into each of the neurons of the hidden layer and in each of those neurons feeds into the last meal on which is the objective or hypothesis function neurons of the hidden layers the layers
between the input layer and out where are called activation functions and activation function depends on what function you used in the near on which shall learning arbor than you used in our case it's logistic regression for using the sycamore it function or logistic function so the activation functions of our hidden layer is the sycamore eight or logistics function at all the logistic regression and those all feed into the final muir on which is our hypothesis
or objective function white is going on in that he then later here is what's going on every feature of our role the feeding into the neural network is being combined with every new ron of the hidden layer with a hidden layer is doing is learning how to combine the features optimal e. and then it sends all those alps to the final
all neuron full tell you the final result function in our case may be linear regression if we wanna estimate the projected cost of the individual deity or what may be logistic regression is to try and classify something the point is that the purpose of that hidden layer is to try it every which way a combination of features from our data in order to learn the best way to combine features so let's say that my figure out that age can
mine is best with itself in other words page squared like we said before or it might figure wrote that smoking and wait combined to the purpose of that hidden layer is to find a optimal combinations of features in order that the neural network can properly protected values and by the way i apologize if your reign in the background i have to do this episode outside unfortunately so long story c. may hear some background nature noises so remember the freeze
that's the machine learning far project case a remake of butter production football for rose or spreadsheet figaro how bad we did that's the loss function and then shot rain schrade up on that lost function in its this training step or the learns that this final step we're using an algorithm called back propagation it's the staff to application of gradient descent sosa way we would make greeted descents deep
talk about that a bit is his final training step that has each of the notes of the hidden layer warren how often morey to combine features okay and so there you see a single layer or neural network learning how to best combined parameters for a situation which is nonlinear and of course our final i'll put some sure objective function in this case is linear regression because we want a number and a case across the k.
shoot is logistic regression okay now we learned one super power above neural networks feature learning learning how to combine features no way to construct on nonlinear function but neural networks have another secret power hierarchical representation of the breaking your david down hair prickly by way of this feature learning paradigm deeply stacked deep
we don't care so let's switch to a new example removal away from this whole situation and to face recognition in the images so we're to do is worry about load a bunch of images of faces and non face it's so skinny a classification example non faces might be a picture of a dog or a picture of a house no real quick i'm not to get too deep into this but we've been applauding spreadsheets of rosen columns rose or or individual examples and homes or their feet
ers the way we would turn an image into a spreadsheet like that is we would take its pixels pretend that it's a five by five image flatten it so it's now the twenty five by one roll call vote call arms where each column work each feature is a pixel and then we'll take those pixels which are currently or g. b. values in color and transform it into gray scale so that each pixel could be represented as a single number
and we take all our images and they're now rose and we put them in for spreadsheets a very very distant a bunch of pictures and you turn him into a spreadsheet bill sealed immoral that we get the details like andrew being coarser course of course our final call on that is whether or not the thing it is a picture of a face so your ego we hiked are spreadsheet into or neural network or neural network takes it one road time we're going to train on each row
first we're going to do the feed for the past feet forward that's the prediction step of our one two three machine learning step was a process feet forward pass will give us a prediction as to whether or not is a picture of the face then we will use or loss function to figure out how bad the neural network dead and then we will send them back through the network in a backwards pass called back propagation which will use gradient
santa at each year on to update the weights of fate as in each new ron so that the whole neural network will get more accurate by the end of the spreadsheet after it has seen all the examples but here's the twist to this neural network or have to hit in layers so there's four layers now to hold all the first layers called the book where it is simply all the features of an individual row and that will happen once
for every role of our spreadsheets of the neural networks input layer has the size of twenty five twenty five pixels free five by five pixel image and its neural network will be used over and over and over for each row of the spreadsheet the next layer is our first hidden layer so or pixels of an individual row from the in the lair connects to each new ron of the first hidden layer so what is that he and layer
trying to do now we said that the purpose of that the lady are in the neural network is to combine that all the features of the input no way that the whole is different than the sum of its parts so what are the things we're combining here well or combining all the pixels of the damage work shrine or hand act of one should love combinations of pixels in order to give us something new so that first layer might be combining
all the pixels to figure out if there's lines or edges or concourse in the image they will combine all those pixels into winds and those lines are now all new features those are now the features all the first hit in the layer those features get cents to the second hidden layer which tries a bunch of combinations of those features in order to figure out
to combinations are important so the second layer might be combining lines and edges and contour is into eyes and ears and mouth and nose and then it will take all those combine them into one of which is our final output the opal way or the object of function the hypothesis function a logistic regression unit that holds us whether or not we're looking at a face so he'd neural network does two things one
it breaks thing down into chunks and to it combines things at that level in or to find important combinations to make a determination so for our image the first layer is the input layer the second layer is the first hidden layer of neurons teaching your honor is called an activation function in our case the activation function is the signal we do or logistic function from logistic regression in
other words he's your honor is logistic regression and all the neurons of the first hidden away or are trying to combine all the pixels of the image in every which way possible in order to find important combinations of pixels well in the learning process of backup propagation the final step we're going to be training these neurons we're going to be telling these yawns whether or not some combination of pixels
it found was important or was not this the training set up so this first hidden layer is finding combinations of pixels that makes important things those things in our case are likely to be lions edges and cons wars well it's not up to boston euro met work itself will figure out you'll figure out what combinations of pixels combine in a certain way that it finds important in order to increase its production accuracy now what that first hit in layer is now
acting as the input layer into the second hidden layer all the neurons of the first and layer all accommodations of features from the picture are now in new features and those features go into the second hidden layer it is the purpose of the second had nowhere to combine those new features into again new features to the things that the second handle arab might learn our eyes and ears and other things objects of
a face that it will combine all those together into the wall asked knew ron at the objective function of the logistic regression hypothesis function which will tell us the gate or name is this a face so bye think that's really cool we have logistically gresham which we learn a prior episode for classification of a very simple pattern well off base in a picture is not a simple pattern it is not a linear function you can't possibly the goal line on some
scatter plot of pictures of faces this method does in make sense that the united with visualize and your brain doesn't make sense what you can do is look at face and come up and parts cut out the eyes of the face cut out the no's couple mouth then cut those up into further parts until we have winds work backwards from there really is what we're doing a neural networks in order to combine things into higher he were the final loops of the higher key is a yay or nay so by like to
you give it like him or charge of a company let's say that we have the box the head honcho the company that ceo he is the object of function these are hypothesis function our last muir on or i'll put a layer under him he has supervisors they are the second henley are the supervisors each have subordinates employees who work honor them now on like a typical organisation shark which is purely hierarchical
where supervisor has their own employees each of our supervisors is sharing the employees of the company so's not purely hierarchical so what happens well this organization that ball resides inside the building that building is called a neural network we don't personally know what's going on inside the company manages itself will we knock on the door in the door opens and we hands an employee a picture an employee caps on his nose give you up and says okay off come right
after all tell you whether or not is a face egos aside and all the employees huddle together around the picture and are all pointing at various pixels and some other employees point as other pixel this one's blackmail yeah but this one's white leather all try to combine all the pixels of the picture in their minds is ten employees huddled around the picture and i've got magnifying glasses out well ten employees means tenure on its meaning of some sort of combinations of pixels it to be boiled down into ten principal components maybe
a dark think line and i short skinny line and stuff like this okay they all nod their head may think they have their solutions they walk down the hall to the room with their supervisors and each one of them reports to each of the supervisors each supervisor is looking for specific object we've got left by a supervisor right by a supervisor knows supervisor mouth supervisor last year and write your eyes ears mouth nose so let's six supervise
years six neurons in the second in later mean it is six specific objects that were trying to be attacked at this layer of the hierarchy soul left eye supervisors listening to the report from all of his subordinates all ten employees were all clamoring over themselves basis hush hush hush spoke everybody raise your hand if you saw the line or an edge and all raise their hands and sorry got the senses yes okay we definitely have left by so they all run over
the nose supervisor and he says okay everybody who has a thick vertical edge raise your hands and three razor hani's okay now i need us circle ish wine anybody in one raises their hand he says okay how about how bout a shadow kind of figure and no we raise their hands these kind of nodding themselves i'm knees as clipboard miss penny he's checked off seven out of the ten features he's looking for his kind scratching his chin is what you gonna call it it's uno was missing three of the features but it's in
ups seven's enough for me waited some remember that logistic regression works on awaited some such that if we get a seventy percent of one hundred percent and say yes if anything under fifty percent it's an know all the hidden layer to supervisor in your audience now have all their cancers whether or not they have the object they're looking for eyes ears mouth and that's the mail or rushed down the hall with their quick boards to the boxes room the ceo is at the top floor with these windows overlooking the city he turned around
in his chair is a big fat man he's got a cigar is now off the fancy suit because while boys do with the eyes ears now the nose and bull's eyes supervisors raise their hand yes we both saw eyes nose supervisor raises his hand i saw knows what mouth doesn't raise his hand soap boxes scratching his chin is looking at this club or shifting his cigar from left to right of his mouth and some maybe there's a beer or maybe something was obstructing the mouth who knows ancona face yes we now face seventy percent probability that we're lookin' interface silly
comes down the stairs and he opens the door were waiting patiently outside the company's building praises tyler on an essay to face seventy percent probability i'll give him with a sad look on my face 'cause i knew from the spreadsheet i a job to him i gave him a picture we're already knew the answer is not a face it's a dodd the answer is zero and he gave me point seven so i use my walks function remember it's called cross entropy in the case of neural network simple little bit diff
i've been a typical logistic regression cross and chirpy los function but is fundamentally the same in you'll learn the details may intervene course so i use my little function i calculate how off he was and i tell them i say you were off by this amount and he slices forehead and he is mad so he went back to his supervisors and says guys you are easily yelling at them and they're all shifting on comparable in looking down at the ground and in his mind he's changing some numbers something a parameters remember each new law
including the hypothesis function has a set of fate of parameters just like in logistic regression so he's a digesting all the fate of parameters in his head as he's barking worse to his supervisors now each of the supervisors adjust their fate of parameters in their own heads as their rundown all up to their midway from the start yelling at their planes and each of the employees starts adjusting fate of parameters in their heads as they're looking miserable in the ground and taking no lashing and they turn
around a look at the picture and they open their mouth steel of the picture but but the picture doesn't have faded parameters the picture can't fix itself the pictures the picture so of course the input layer doesn't change and that last learning step of yelling down the tree is called back propagation it is running greedy and descents down the war chart of the neural network so there you have it the neural network or keep learning what it does is it's just like any other supervise learning
system except that it learns how to combine features in important ways and if necessary hierarchical e. how to break down your day get into a hierarchy of features and i hark ye of combinations on features so when we say deep we mean the number of layers the number of hierarchical layers in the break down and we say why did we mean that the number of neurons in each layer
a white layer has a lot of neurons a deep network has a lot of layers neural networks are sort of a silver bullets they can handle any lean your situation just like a linear algorithm good can't handle stuff that's nonlinear which linear algorithms cannot they can handle housing market estimations linear where they can handle face productions in images nonlinear so there is silver bullet by what
you take that statement with a grain of salt because you should not treat them like a silver bullets can you use neural networks did everything yes should you use neural networks in everything mel wall i why shouldn't you use no met works for everything well it turns out that many problems or linear and many problems don't need to combine features baby is not some wine autograph like when you're aggression but you could use an algorithm like
the scene again for its which doesn't depend on the combination of its features nor to succeed there are many other shallow algorithms order to be going over in future pod cast episodes be seen in france decision trees support vector machines k. nearest neighbor k. means algorithms all these things are shallow there quick and they can handle and many real world problems will if people earning can handle most if not all real world problems and shallow learning can handle
some real world problems why shouldn't i use deep learning for everything the reason is that people earning is expensive very very expensive on your computer to run a typical shallow learning our rhythm like linear regression or the scene in france you could probably do it on the laptop for people earning you're going to want to rent space on a job you what's cluster of titan x. g. p. you machines i mean the difference in performances scale ability of deep learning versa
shallow learning is significant and can cost you an arm adelaide if you're running an online service i like to compare it to our board chart analogy if your situation is linear the result would happen you would knock on the door of the company a new hand or role of your data to the employees they will take that role in all look at a scratch their heads may determine that there is no sort of combination of features that needs to happen so they all look each other and they shake their head may pass it on that we're just gets passed on as
is to the second and lay or your supervisors they'll do the same thing this crash ahead a look at the data and determine that there's no sort of combinations of features as necessary nor hierarchical breakdown of that data bill look at each other they shake their head and a past that day on to the boss and this time the bosses and some fat cigar smoker he does all the work he's linear regression at the end of your chain and he starts cameron chiseling that this thing and outcomes of solution so you just
they eat a whole company of employees to do the work of one percent linear regression that's why you don't want to use deep learning for everything because you don't have to and it costs more money and time to compute the deep learning solution than a shallow learning solution so it's still is in your interests to learn these various shall learning algorithms and where they apply the ruddy going over various our present and future episodes okay so that was a mural
i'll work my friends it's called a malt high earlier perception on a feed forward most highly your perception on and a perception on because the type of neuron we're dealing with is in most examples that you'll see a what's called a perception or step wise function but in our case we use logistic regression is basically the same so it still counts mole tyler because we have one or two hidden layers and feed forward you see this word feet forward as the feet forward pass that you do in the initial productions
at the window safety for now work like a thief forward a mall failure perception but they're referring to is bayou comparison to other types of architecture for example a thing called a word current neural network it doesn't exactly feed it gets degas forward if he didn't like that into itself we're gonna go over with colonel networks in future episode most neural network architectures or feet forward but there are some sheffield twists of an architecture that can do occur
persian and other types of feeding one final technical side that is that in the back propagation step of neural networks you may or may not be using gradient descent it's height of things that is used for training is called an optimize or so great in dissent that we've been talking about all these episodes for the learning step of machine learning is in a optimize are in their various other types of optimize years that you can use
called the grad one called adam but i just wanted to be aware that if you just jump into the deep than any star working with your own iwerks you start seeing these optimize or is used the hell we're names like that at a grad adam what they're doing there is they're replacing gradient descend with maybe some more specialized or optimize optimize or for that particular architecture okay i wanna compare keep learning to the brain we did this little then the beginning of the abyss of an hour and come back to it being you're on
in a human brain looks like this we've got a cell body lope along any into it comic inputs by way of the structure called ben fright so has the full lines coming into it and i'll concede output of an you're on that alkaline books i tailed it's called an x. on so physically a human you're on looks a little bit like an artificial miron an artificial miron takes inputs from all of the dado or the way you're before that it does some competition the mill
that's collect the sole body or the soma of the new ron and out of it comes the output there's a lot of debate as to whether the artificial muir on created by mcculloch in it's really does represent a biological human you're on i think the argument might be missing the point from a functional us perspective they do the same thing is a common point of comparison you'll see made between birds and planes in order to achieve flight in or kira we didn't have to create jai
in flopping feathered wings instead we use the laws of thurman dynamics to create a giant metal box with stationary wings the point is to achieve the same effect will are official neurons may or may not represent the human your honor enough on away and but functionally they achieve the same effect and in our way i think is safe to say that neural networks are our big chance towards solving intelligence and bring up the brain again for another reason in the human for
name we have different centers dedicates court solving different tasks speech image recognition planning etc each of those centers of the brain is what's called a nucleus it's not like to sell body in biology it's the same word in neuroscience they call the center of the brain annals of specific task a nucleus and a nucleus is nothing more than as far as we're concerned the neural network so a neural network in order for
told no my work land of machine learning is like a nucleus of the brain now as differ nuclei of the human brain are taylor towards handling different tasks they have slightly different physical architectures let's say that primary visual within the human brain has maybe shorter asks aunts or some different combinations of neurons in a specific way that makes it very good at handling vision where broke is area for speech might be physically structured in a different way
human brain doesn't use the exact same nucleus architecture all throat the brain instead it's specializes in various nuclei to be better at particular tasks is still uses the master algorithm of the new ron and combinations of the new on but it does so with a twist deferred for plants we're still dealing with blueprints within a house but just different floor plans for different specialization it's you'll find this to be the case in our fish
neural networks as well within deep learning you'll rarely see the most highly air perceptual used in the wild the most highly air perception on is sort of the trainers and neural network neural network one oh one thing that i taught you in this episode is like neural network wanna want this not really what you're gonna be using most of the time in vision for example recognizing images what we did actually in this episode the most common neural network you'll see here is co
the convolution old neural network and has additional types of layers inserted in the neural network different types of neurons so you may not be using logistic regression units for example for language modeling or anything in the domain of natural language processing your money you something called the work current neural network and bark n. n. and s. has a special tweak of the architecture like i mentioned earlier that neurons can feed back into themselves its pre clever and for planning
all you something called a deep que network henri de que en su variations of the general neural network architecture with a twist to make architectures specifically suitable for specific applications now like i said each of these different architectures might use a different type of new on or activation function or hypothesis function remember in your on inside the thin layers the neurons in the black box or called activation functions
and the final meal on or final neurons of the output layer is called the hypothesis function or objective function activation functions and in layers hypothesis function it be in the alcove where you may use logistic regression or soft max were linear regression in the final i'll put you on or any number of things in the heat in layers you're more likely to see a thing called raya lu or he held you rectified linear unit
as for the more common types of activation functions were more common neurons in neural now works another type you might see is to hand at each and there are quite similar to signori functions they get pretty much look the same to all of them look like an ass to say to defer way in a case of rae lewis flack for x. equals zero all as stuff isn't so important it's it's easy understand the neural network as stacked sig moret functions since you've already learned this ignored function but you may be dealing with different types of activation functions different types of neurons and you
lead to determine earl why you would use a different your honor murders different circumstances is based on the way the math works out for that particular architecture that particular neural network you're using it may learn better using calculus you need backup propagation stab degree in the sense that the calculus may work better for certain neurons under certain architectures me personally i just take whatever your honor we're architecture is popular in the space and i just rolls
marineau in question and so that's it my friends as steep learning math neural networks like i said we're going to come back to shallow learning algorithms like support back to machines decision trees k. nearest neighbors came means the scene in france et cetera and eventually will get back into deep learning and talk about record neural networks convolution all male met works de que networks and all those things for the resources of this episode i'm going to recommend a mini series on you
to which will give you a label lands of people earning architectures really short videos providing the visuals representing various people earning models like r. n. n.'s c. n. n.'s etc you can plow through that one pretty fast then as usual i want you finished and trooping corsair course he has a whole week or two dedicated to neural networks were gonna be learning the mall tyler percent on and then i'm going to recommend you the book the book is by eight yen good fellow
joshua banjo an earring core filth and it is simply called people earning is probably the most popular resource for learning deep learning it's a textbook has been long time in the works it's rather newly published but the great resource for learning keep learning now again you don't want to start this book until after you finished andrew king corsair course once you finish that course it's safe to begin right away renewed t. blurring book because deep learning is the immediate next step after shallow learn
so finished intruding course and then start on this textbook that'll put new show notes again as usual a shot threat bossi develop dot com forward slash pod casts forward slash machine learning this episode nine began lewis c. d. e. v. e. l. dot com there's also a contact but in the top right of that web page where you can get my contact information my email address twitter are linked and et cetera if your reach out to me the next
episode is gonna be about languages and fling works we're gonna talk about python versus or versus java burnet talk cancer flow versus piano versus porch all those things see you then


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. eight and time also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks the sum which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode ten languages and frame works in this episode we're gonna talk about languages and frame
works languages like c. and c. plus plus math languages like or matt lab oct cave and juliet the g. b. m. like joplin scholar of course python framework slick c. n. l. torch and ten sir flo i'm going to give you little bit of apelike hast spoiler and that is that at the end this episode i'm going to recommend you python and spencer flow python intensive low without knowing anything about you if you're looking into machine running down my guest for you is the language python
the framework to answer float your member in a previous episode on linear regression woodhouse talking about a bias parameter athena zero without bias for energize is it says if i don't know anything about your situation i can still make a protection so if you give me a house in portland oregon and i don't have any information on its square footage number bedrooms number bathrooms etc i can still give you at least the average cost of houses in portland oregon to that
ice parameter is it's like an educated guess if you don't have any information and that's what i'm doing here my bias parameter is cancer flow and python not knowing anything else about your situation of course the details of the situation in a change that sword again into the details of why you might use one language or framework over another in this contest episode let's start right away with c. and c. plus plus c. n. c. plus plus or as close to the metal as you can get in computer programming before you go to assembly language
and below that binary language there's no way you can be riding in binary assembly so to milk the most performance that you possibly can in computer programming you'll end up using c. or c. plus plus what just so happens that machine running really needs performance machine learning is heavy heavy heavy on the computer aidid neural network costs all lots of competition resources and time and so in order to get the best performance out of your neural network c. n. c. plus plus
are the best options for you though often see this to be the case in large corporations that specialize in very computational expensive things like video games and crypt harder fi you also see it in machine learning company is very large scale or his asians like google and facebook they might use c. n. c. cost plus for their very hard for a very highly optimize machine learning algorithms there's one more new ones about c. and c. plus plus is that machine earning power grid
as our best performed on the g. p. you the graphical processing unit the reason for this is at the g. p. was specialized mac mac cooperations floating point arithmetic especially matrix algebra out remember linear algebra or really any sort of mathematical operations whether statistics calculus or linear algebra or best suited to the jeep u. n. not to seek you and c. n. c. plus plus the direct access to running your operations on
the g. p. you belie you manipulate the math directly on the jeep you which makes c. and c. plus plus the best bet for highly performance machine learning program i'm gonna throw curve ball you and say i do not recommend c. and c. plus plots as you'll see a bit we get to the python fran works these things called computational graphs or symbolic crafts when you write your code in high-level python intense or flow and it will actually be converted into c. and run directly on the g. p. you
they get the best of both worlds your coat will run again see but he gets right in the high level when was like python and you'll appreciate that because c. n. c. plus plus for very difficult programming languages very difficult to write very verbal many many lines of code to accomplish the same thing that much fewer lines of code higher level programming language which she would say a hundred lines of seagulls pasco becomes boil down to fifty lines of python coat and get a handle things like memory management stuff that you would
after normally handle in a high-level language so highly do not recommend c. and c. plus plus for machine learning programming unless you are a noble languages me feel confident in which case power to you you've actually got a lot of runway knowing those languages for a job applications or if you're working role little bit closer the metal on some really screaming performance characteristics of machine learning models basically my recommendation is that you will know when and why you need c. or c. plus plus the field
feel like you need it right now skip that malice dive into a category of languages and called the math languages mathematics languages are now lab octavia in julia these languages are very different fromm typical procedural programming languages object oriented languages or the white like python dole looks of all our is substantially different than the looks of python code are for example is highly optimize for mathematics operations
linear algebra and matrix operations are directly support in the language itself is hard to describe it you'll see things like coal ends and commas between braces of malts like to mention or raise what's called a frame for me to seize the bar the looks older we're coming from a traditional procedural programming backgrounds and what these ally you do is slice and dice major c.'s directly perform matrix algebra directly and then statistics functions like to stay
your deviation and correlation of factors or steelers are major seas are directly built into the standard library of our programming language so math is a first class citizen that really shines in the syntax of the language are also supports machine learning their robustly there are third party libraries around your alma works again you'll lot of leverage something you're taking many lines of code in its initial programming language that has do with math would really be slims down in a ball
are so are is a fantastic language for the mathematicians and as we know machine learning is nothing more than math which makes all are also a fantastic language for machine learning then there's two other languages matt loud and blockades these are direct competitors matt lau that is a commercial licensed programming language it's proprietary so if you want to use it you actually have to buy license is also actually the language of choice used in the intruding corsair
horses on fortunately but we'll have to pay for license to take that course andrew being has the code you could use for temporary license for the duration of the corsair of course will mal abascal criterion commercial octavia is a direct response to matt lab it's everything melody is but open source and fritos kind of a middle finger by freeing open-source community to the mat lab product it's kind of like what good is the photo shop if you're familiar
from what i understand it's got all little bit of the same thing going on the game has a photo shop sure isn't open-source competitor plaid it seems like matt lab stills little bit more polished now lads shines a little bit more on the linear algebra side of things that on the statistics thing from what i understand i don't really think that you would be seen matt lab in a professional capacity it's much more commonly used in academia university courses or maybe in research saw
i'll bring some simple puzzles in research before you my great you are models to a more robust solution on job a python et cetera so i wouldn't recommend taking up matt lab orbach tape on your own for your own professional purposes instead just take him out loud as it comes as necessary in your learning curriculum like that said and earrings corsair course uses now lads so you will be too don't worry the supreme minimal language dupree quick to learn and he teaches you all the basics and his course and
there's a programming language called juliann now are like i said specializes in linear algebra and statistics and calculus it has very strong mathematics foundations juliet is very similar to our but newer and slicker and sexier that has pros and cons pros being seen tactic sugar and more powerful machine learning models cons being that you really want ceylon people using it in industry yet so it's kind of a johnny come lately and pop in common
in language that i would maybe keep an eye on what hold off for now on trying to take thou language up so as far as the math languages are concerns my preferred language of this whole set is all our organs you lot of power flexibility it's open sores and it's actually quite common in industry so knowing our would be professionally advantageous now i see are used pretty commonly on the d. l. linux side of of the us
science remember from a previous episode that i said that davis science is sort of an umbrella term for various professions are fields of study one which is data mining one which is dated analysis or analytics another which is machine learning so while our supports machine learning a very robust way is so while last comment i find when looking at job postings or conversation long line then python which is a language much more common machinery space where car is more common on the day
that analysis side of things will get back to that shortly those are the math languages are now lad and octavia and julia next we enter the g. a. t. m. job of virtual machine languages job of course being the namesake for the job of virtual machine so job was the most popular of the g. b. m. languages scala being a recent contender on the space which is picking up steam in a very rapid clip style is kind of the dynamic function
old version of java with c. tactics sugar so i like to think of that is the kind of hit new start up contender on the g. b. m. java is one of those fifty year old mill manager or fancy suits languages and stolid is the big bearded coffee drinking started tipster style is a very fine programming language to program in especially its attention to functional programming functional programming is a very powerful tool in the program
school kid if you're not familiar with functional programming you will be eventually trust me it's becoming more and more of a necessity in modern programming paradigms but it is especially essential for distributed computing so discriminated feuding is the name of the game for the g. b. m. b. j. v. m. languages job and scala are meant for very very highly skilled all the high points or dado architectures by way of distributed computing of
they sell this to framework say you'll use if you're using job let them the framework of choice is how do they stay depot opie if you're using scala then the framework of choice is spark so what do the suffering works do you take in a bunch of data from the internet any pipe it into these distributed computing frame works huh due course spark an hour your insight what's called a dia pipeline or dina architecture but say that you're trying to piss
and every tweak on twitter so if you could use some sort of sentiment analysis on what is popular what is making people happy was making people in rage on twitter well consuming all that sweets from twitter would be done by way of the day architecture or indeed a pipeline through distribute computing so these would be your guys the j. v. m. languages and framework spud you pour spark you typed all the tweaked and to spark or had you and you write various nodes in this distribution
graf for processing the data so this is the mining this is the data mining piece of data science let's think of a mining analogy just so if you're mine are you going around your chisel away and you've got a bunch of or you bring into a truck and that truck brings it to a factory and factory don't sit at the receiving bay in all those onto a conveyor belt and or go through some sort of processing mechanism it gets dumped into some call john and mix of some liquid an outcome some shiny odds
that's on the other side the conveyor belt and that goes for some packaging system work it's wrapped up into a gift box and the gift box gets delivered to your machine learning our room so the data mining side or the day the pipeline of the architecture that is what the j. v. m. is all about and that is one of the three main components of data science data mining you need your data for machine learning your sheer machine learning how rhythms need to get tough launch
and when you don't want rod day that you need pre process data and if you're working with lots and lots and lots of data as an example i just used then you're gonna need to use a steel ball to shoot computing system like the duke or spark so this data mining step is all about wheeling and all your day head down into just the basics cleaning up slicing and dicing packaging up for either you're gay analysts but
are people or your machine learning people in python so that so these framework stu they take in raw data base slices and said chunks send us some soft down different conveyor belts those conveyor belts run the data through multiple processing steps in order to clean up your day and package it all up and now you have the best of the best clean day that for your date and analysts were your machine learning experts for resolve feels the d. s. science data mining dale
it's an machine learning now just to help you understand that differentiation let's go through one more example on gunny used the googled ecosystem we have google analytics and google pad sends analytics is charts and graphs it's for helping your growth hackers or you're does that make business decisions it's called business intelligence you're looking at charts and graphs to determine what sort of advertising strategy for bringing in new customers made
we need to aid the test our website in order to determine whether the button should go on the top right or the top left i think that search engine optimization is not working for the web site under this regime may we need to change the wording under certain pages to drive in more traffic just the budget charts and graphs in order for your business people to make better business decisions that speed data analysis or date analytics side of the data science package it's jobs like those that you're mostly guinness eli
which is like a bar or specializes in d. a. analysis now or can be used for machine learning and arcana also be used for data mining as you'll see in a bed every language can be used for all three prongs of the day is science umbrella what if you've determined that you wanna specialize in one capacity or another it in your best interest to choose the popular language of that ecosystem says the analytics on the machine learning silas talk about global assets and said
it determines what types of ads to show which users' based on users browsing history there prior search queries things they've clicked on in the past maybe they've sat on some amazon product page for like ten minutes really weighing the pros and cons of purchasing the arduous rest will go bill does is it tracks as much of this is a possibly can in order to determine whether the best adds to show the user in the future that's what machine learning die
is it learned some patterns by way of degas in order to make productions predicted courses of actions predicted ads classification numbers et cetera so cool analytics is like david analytics within days science go pat sense and serving piece of google is like machine learning within days science and finally we need all this day of bull fahmy analytics and the machine running side to work with and that's what data mining guys to mining polls data from some so
first slices and ices cleans up and enhance it off as a beautiful packaged your analytics or your machine learning so on these google analogy there someday a pipeline it google it's collecting every click every action a user takes lonnie user is sitting on some web page every search query all that stuff is being collected in terabytes and terabytes through the steel bull the uptight blinded the architecture by way of very likely the duke or spark now if your dinner
collection phase is very minimal if you're not collecting megabytes or terabytes of data that then you don't need a day a pipeline maybe you're just collecting data on the one time pass or maybe you're collecting cliques in actions but doesn't happen really that frequently so you don't need this major steel architecture you can just take the day as it comes slice and dice and package it all in the language that you're using weathered the python for machine learning or or for de ella next c. don't need a date architecture eat
oh needed a pipeline will know if you need it because you're trying to process watson was a degas did not turn a process got today that you don't really need this that data mining is a field of its own people specialize in data mining they know all the tricks of the trade for wet scraping breaking down day at cleaning up normalizing numerical employed and all less stuff okay so best datum ins that's what we're talking about on the g. a. t. m. so these languages nice frame works the language java
it is an older language it's more enterprising it's more robust and powerful but also very verbal oaks it's very chatty it takes a lot to do a little so stolid comes in and clean the water that up with sin tactic sugar scholar is a fresher take on java hut you is a videotape wine framework that that would go on top of java if you're using java and spark is that a pipeline framework you would use on top of scala now i believe that you can you
is how do with scala and sparked with java seeking big snatch him but i also believe it's probably a new passengers to go with the linguistics chosen as a first class citizen so you're be my recommendation is this is a greenfield project for you to call the shots in other words you get to choose the technology than to spark on scala it's simply a cleaner slicker faster sexier and you on job as sorrel lot of the mind share is and future growth in this space but you may not have a choice
maybe a point to a company may be large organization world of an older company or even new company whose developers are very well versed in the duke and java so had you on java spark on scala now one more note i said that any of these languages and airframe works can be used for any of the prongs of the of science are specializes on analytics side of things but it can certainly be used for machine learning it's actually very good for machine learning and shirt
can be used for data mining maybe not large-scale this genetic data mining secure working with terabytes or might not be the best solution but our can be used for any of these three pieces of data science similarly job i can be used of course for data mining of course on the analytic side and in machine learning by way of some modules that you would plug into your pipe line so if you're using high do on java the legacy stack then you would use the module called ma hout and a
eight h. o. we you'd see macaques that is the machine learning framework built into had to do so if you are mining data from the lab was save scraping some information off web pages an eap type against your factory in it gets caught up in the pieces and it gets massage been cleaned up normalized packaged up and sent to the last module or note of your day a pipeline which is the machine learning piece you would use macaques to use java how do
and my count on the scala stack you you scala spark spark for the day the pipeline framework scholars the programming language and then your final note of the pipeline for machine learning is called spark am at all and all for machine running so spark an owl so if your specialty is data mining with maybe a little bit of machine running sprinkled in the meeting go this route but if your specialty is machine learning with maybe some analytics and data mining
sprinkled and then that brings us to python python python is the most highly recommended it and most popular language of choice for machine learning engineers if you know anything about python then you may be thinking why the hell is python the most popular language for machine running as a way back in cac plus plus we need to the metal screaming performance for machine learning models we it's ron on the jeep you we wanna steal
data mining pipeline distributed across multiple computers python's as seem like a good fit for any of this python is a dynamic language that is if i dare say rather slow it's not functional sodas and distribute your lays low scala hears what python has going for it python can handle any of these tasks we v.'s by way of either a domain specific language or a framework okay so let's start with the math languages are 'em out loud and oct
aid and julia has python stack up to those languages for supporting mathematics as a first class field within the language of language doesn't support mathematics out well at the box instead you use these libraries called number high end pandas and you and p. why and pandas intellect or a panda nom de hi is sort of python's the answer to matt lab so numb pie gives you at the box including the syntax the coal ends and
the commons the way you slice across major seats all the operations that now lads supports so blown using python we just crossed out one entire language can as gives us all the same functionality that are gives us not not whole way not completely of course or isolde more robust to unfold community backing behind it but most of the way there as far as we're concerned isn't she in learning engineers all that we could possibly need so don't we just cross out are not
python my give us lots of other things that we might need in our didn't hire solution of a star up or some sort of wet service such as the server weary use flask or jane go and conductivity cure database to see pulled out to me so it fits into read the rest of the architecture of your company without having to no other languages and framework to just fits right in how about a pipeline a messing with something that was very well suited to java well guess what sparked supports python
don't need scala for spark it has a python a. p. i.'s you can use python for sparks a bomb which is cross out the whole g. b. m. section we have data quite gliding through pie spark we have mathematical operations through numb high in pandas okay and is one thing left performance we're talking about how it compares to c. n. c. plus plus you need your operations to ramona jeep you cannot see view that's where framework comment fiato torch and spencer flo what they hinge
reduced to python is a paradigm called a computational graf weren't symbolic graf why you do is you write your neural networks in the framework like to answer flow in python anticancer flow packages up what you just wrote it walks down the stairs to the basement and gives it to see this as your c. what your real rate this in optimal way for you to execute him as c. environment
see look so what you wrote intensive flow and python any kind of turns around and it's hands and have titles to itself mrs newbie their real writes it because it knows a better way to run things and it turns around the open sea out and pops in the oven that's your jeep you successor flow takes a python code converted to see and runs it directly on the g. p. you and when the avon's finished it pulls out see gives it back to python and python gives it back to you so there you have it don't we just blew see out of the equation
python handles everything that's why python is so popular and she nineteen eighty is not because the language itself stacks up to any of these other languages in fact it doesn't however libraries and framework swore it in the pipeline ecosystem can replace all these other paradigms very effectively so python gives you your full day of science stack from data mining today analytics to machine learning know will say that if you apply to the job
the data analyst or if you apply to a job as a day as india minor you will still find the g. b. m. or maybe are used for donnelly in those particular burke calls but for a company which supports the whole process a tizzy in which you might be spanning the full stack of jaded science python is certainly the most popular in common language and ecosystem that you'll see in the space most of the jobs i see when scanning the job market machine learning are certainly python jobs and again it's nothing
the language itself it's all in the ecosystem the libraries and modules they're available the python ecosystem that make davis science completely span of all from a to z. by way of a single language so let's talk about these frame works for soft like us said we have not high in pandas non pie gives you a little bit of the quick and dirty matrix operations it's sort of the answer to matt lab and pandas it gives you all sorts of statistics operations summaries analytics watson graf sobered
ada a bunch of operations on a day shocker called a deed of frame silesia to libraries are your commonly see in the python tool kit another libraries called psychic learned as c. i. a. take aidid latest scientific kids and this is a bunch of scholl learning our rhythms available for python showing our rhythms like we're aggression logistic regression support vector machines and be seen in france driving cars anything you could possibly name in the machine running you know
first i think you'll find that the majority of your machine learning more professionally in academically will tends towards people earning is not in the beginning of definitely by the end so most of the work begun be doing a machine learning will be done indeed learning neural networks from prior episode so once we graduated deep learning you can all longer use psychic learn instead of you move on to one of the suffering works these framework cy mentioned previously democratic categorize under the umbrella of con
a. t. show graphs or symbolic graphs one more time let me explain what the strain works do you write your code in python within the framework to architecture sees the framework speak yeah i let's say to build out the neural network let's say the u. creating new objects that sent in sense and you're all my work and you call a few methods off of that object adding new layers and noon your arms and you add the final operation which is that propagation which will perform create a sense and then
you feed a bunch of degas to train them at work upon and then you say objects dot go or session dot ryan or some other incarnation like that and what happens now is python by way of ten sir flow or whatever framework to use will head off what you've written to see see will assess what you've given it and rewrite it if necessary if it finds it can optimize it's execute the saying on the g. p. you and then give you your results
and assess that reseda c. may decide to optimize what you have written this whole set may sound familiar to you if you've ever worked with reacts natives as a mobile that developer reacts native lets you write your mobile happen in java script okay the language being shot a script and the framework being react and the framework on compilation will then pass what you've written down to the android by way of java and iphone by way of that
it's the worst left and those respective languages will make optimization says necessary and then compile your code down to native mobile code okay so that's on the mobile space if you ever made a video game with the unity is very similar you write your game in a high level framework and it gets compiled down with you ever use and objects relational mapping system and a bowl r m for databases may be using ruby on rails or jane go when you do is you write your sequel
queried by way of the framework you're not actually writing any eskew well you said you're running a bunch of functions passing in parameters and any hit go and all our am will do some analysis of what you were in no optimize it and you'll get out some stuff that son of cereal added some new stuff that might make things slow the faster and then they will turn it into the eskew well language so transform your python or you're just a coded into sequel and and sequel will accede to the codes is exactly what how
and in with these computational raskin python you write your python with any of these frame works piano torch were cancer flow the frame works optimize it and compiler down see executed on the jeep you and therefore you get screaming performance with a high level expressive language very slick indeed highly recommended approach to machine learning is also by way of these fireworks that most of the mind share and research and progress is happening especially to answer full up but
for week in the tens of flow let's cut it take a historical approached these friend works will start with piano t. h. e. a n. o. as far as i know fiato is the oldest of these computational graf when works was built out at the university of montreal were a lot of mine share was going on especially in the early days of deep learning making a comeback with jeffrey and sinatra badge all those guys so that was sort of the hotbed physically in the universe of machine learning research was in march
we all suffering work came out of it's called fianna and it supported sending mathematical operations down to see and from c. to the jeep you and all the way back got that's the competition on graft and wallet was used almost exclusively for machine learning it didn't initially support machine learning directly with more on the math side of things so the community bill third party modules once called blocks and another one's called lasagna supplied another layer on top of piano for coal
the machine learning tasks like linear regression and logistic regression so massive was built on the shah learning side we still lay that a high level expressive language for the deep learning side of things and so another layer on top was built called khabarovsk haiti or kate acts so fiato is more of an ecosystem that that individual framework if you use the fiato ecosystem you will be using the fiato framework indeed and any number of other modules on top five
find personally and this might get me into trouble that fianna looks to be going out of vogue to its competition namely torch anticancer flux i think the enters a little bit more history to hold or well over time it became more common to use very research heavy experimental deep learning architecture is like a calm the net seal and the heat heat for c. n. n. or convolution all neural network you've already seen this month prior episode about vision
c. n. n.'s are very good at classifying images cons net will turn out that piano was in great maybe at handling something a little bit more obscure like that so facebook create a competitor symbolic graf based framework called torture and torch particularly specialize in the image recognition by way of consonants and of course it handled any other sort of incarnation above neural networks and other machine in paradigms and the math at the lower level etc you see wife is that
be so interested in continents image recognition is their specialty please have your friends in this photo that's what face books all about so facebook is the king of the machine learning universally coms two image recognition specifically to something that courtroom from the get go was very very good at accomplishing incidentally the framework torch was written on lula tells you okay i've never heard of that language up until torch of never see
it anywhere else and i'm sure that they experience that kind of reaction in their community as well because they recently released a python the p. i'm so now you can do torch in python starting to see a trend here he wanna play with the big dogs in machine learning you have to support python as so we saw was sparked spark released a python rapper because when they were stuck in the j. v. m. they realized they were really missing out on a major markets of developers sophie eno and mentor
shh and then finally a new player has answered tense or flo tv and escobar best though or dub you tend to flow by google it facebook has an active interest in it image recognition in classification global has an active interest in machine learning period in fact if you haven't made this realization yet you will google is the god of machine learning of the universe everything they could possibly be
machine learning problem global needs to solve its language modeling in natural language processing for processing your search crews knowledge representation for coming up with a response your search crews image recognition by way of google images voice recognition with all their voice services on mobile reinforcement learning in their self driving cars anything you could possibly name in the machine running universe and especially the nation's only around her but statistics in calculus
in a middle layer you can work and denominator amongst their various teams trying to solve these problems the framework they use and built to answer flow on top of python i don't know if it was that people realize who google kazan the machine learning equation or if it's it's answer flow itself is a superior framework bowen says airflow is released people flocked into role in droves tend to float is barn on the most
killer or machine running framework on the face of the planet the most talked about most commonly used in courses in get how projects boilerplate stuff new and interesting research and projects tense airflow is little the similar to this piano ecosystem inez supports various layers on the lowest larry you have basic math operations and we're out of statistics in calculus in the middle layer you can work with wrong machine learning algorithms and constructs an anaconda heil
i told you can write neural networks with various architectures in a very expressive manner there's another option to go even higher than that with care rocks remembered the top level of the fiato stack you can put that tax on cancer flow as well you get a little bit even more high level expressive neural network capabilities to cancer flow has a little bit of a history it actually had some major performance issues in the early days
if you look up to answer flow performance and you find any articles from two years ago or one year ago even you'll find performance comparisons between tester flow torch and fiato and you'll see that sensor flawless loses however because tester flow uses that computational graf system the code that you writes is not the code is executed and so the ten to fourteen was able to optimize begin between the python goes down to see step and of course the jeep you stuff and a milk doubt that performance and that's answer flow is
one of the most if not the most performance machine running framework on the market to answer flow also has built into it's the capability for running distribute computation in other words you wouldn't even have to use spark if you use tend to focus it's built into cancer flow by way of something called cancer flow serving and finally a tense airflow goes beyond just compiling down from python to see into the jeep you're on your computer you can war on cancer flow on mold
all on an android or python device so the building amal bled to death machine learning and as some way we can do some of the medium to a minimal level compute resources models on that uses device directly let's say for example image detection are taking image and it will maybe highlights persons silhouette separating them from the background or something like that can be run for requiem all of ice making kick off your more caught the facial expensive operation struck with a tense or flow server successor fool
oh really is powerful arise everywhere he runs any sort of machine learning model honor this on including worker neural networks which is something a lot of these other framework struggled with for awhile and reinforcement learning which is really getting us into the artificial intelligence territory reinforcement remember i said is that gave way it takes you from sheen learning and didn't you pay either reinforcement like and sensor flow supports reinforcement learning models directly okay so if you decide on sensor flowing python which i
suggest you do you'll have the option to compile sensor flow to run only on your seek you or on your jeep you so few doing to answer flow on your laptop for work and you don't have a very powerful g. p. you you can just come pilot to run on your c. p. one whale it's slower of course ever very very heavy architectures it's been a very slow but for your server on an eight of us cluster of taking x. jeep use etc you've course will compile its ron on the g. p. a tense airflow supports out of the box and the biaggi
he used and it is sort of kings and the jeep you space of handling machine learning and deep learning computations on g. p. a date on it they know that machine running framework c. u. c. p. you and so they've actually written drivers for their jeep use so that machine learning models can run more seamlessly on their devices they do this by way of something called couldn't see you da and i don't really know what asked as far better than if you look at who dies says parallel distributed computing platform applegate
and anyway this for running a heavy math on your jeep you and then an additional layer called coup de n. n. c. u. d. n. n. n. n. is specifically for running neural mel works on a jeep you so they haven't interface the driver for machine learning framework psyched to answer flow to interface with directly there g. p. use soap incidentally if you actually are looking for hardware you may want to target and the act instead of that amd because they have these interfaces okay so that so
lay of the land of the python machine learning frame irks we have fiato torch and cancer flow now there are plenty of other machining frame works out there are many many machine and fireworks on going to give you a handful of some of the more common we mentioned was on wine was called cafe it's raining cac plus plus and it is interface is c. and c. plus plus so you're not to be using cafe in the pipeline ecosystem as far as i know it's a little bit old and dying sonoma their recommend cafe
for anyone really i may be wrong there take that with a grain of salt do some research c. n. t. k. is a framework put out by microsoft annex nets put out by amazon k. i'm haven't really seen c. n. t. k. or pemex nets in the wilds on job postings on line tutorials or any of that cell and then there's deep warning for jay if you're using job of using the j. p. m. stack particularly if you're coming from a high duke or spark stacked in yuma
you want to use java for your machine learning framework as well and so there's a framework called deep learning for jake this is actually a very popular framework is very powerful and the word for it being in job i was put into the framers comparisons it's a bit is on par as far as popularity goes to maybe the likes of the hour torch but it's in the job and ecosystems are very par for popular framer called deepening for j. and then there's another forever proud that you'll see a lot called open c. v. c. b. stands for computer
jim and it this is really just for computer vision so you'll know if you need this framework don't seek it out for its own rights computer vision can be handled with convolution all male mao works which are architecture that come along with cancer fluffy anno torches cetera so that's languages and free works at my recommendation to use python and spencer flow ma'am i'm going to post a handful of particles that you can see people comparing languages and framework for machine running purposes
where got a lot of this information i've also got a lot of my information from job hunting and from talking to call aches and for the resources section okay you need to know python team into machine learning your unfortunate we learn math lab first if you starting with the tangerine corsair course adult kind of be a throwaway programming language but it's okay because that language isn't very difficult to record at around python the whole bit more difficult because it's all the more robust as a lot more to know when the python ecosystem to veto power
you know python and i would recommend picking up a python book called too little research myself on amazon in all pop one into the show notes to learn python first and then were intense sir flow now there's books and is online courses and is video series on cancer flow and i've consumed a handful of various media for tessa for myself but the thing that i actually found to be the best resource for learning center flow was actually just the documentation on the website dictatorial support there
a thorough of course the roy's opted which is something that one of these other media can achieve that they have very thorough tutorials to go along with cody thin and examples folder of the tense or flow project itself that you can reference so i highly recommend to the tester flow tutorials and that is it for this episode friends see you next time


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and algae and it time also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. you're listening to me she learning applied in this episode or talk about practical clustering tools
for the usual difference between machine and died and zhang applied will be talking body theory about clustering or how these clustering our burdens work and justin be talking about some of psychic weren't packages and some of the tips and tricks that i've found useful in actually applying clustering techniques in no sea and hopefully in a future machining guide episode will talk about the theory behind some of the schools and clustering in general etc now the first
cold i'm going to talk about is psychic learned cade means everyone knows k. means k. means is just the most popular clustering arbor them f. or just ever it's everyone uses it ninety nine percent time for clustering they're using k. means an effect by just recommends trying k. means first if you need to close your vectors try k. means that if it works great if not then you move on to basically the other twelve volt
talk about in this episode has came means worked on it generally in practice not to talk about theory idle what talk about how the sand troy to move around and in the points shift in all the things more talk about in practice you'll be using this psychic learned clustering decaying means package and you'll specified upfront a number of clusters so let's say you have a thousand doctor satcher matrix thousand by ten ten being the number two
mentions her vector in your matrix well you would need to know in advance how many clusters you're going to be clustering you're matrix into that will happen is you'll say k. means parentheses and underscore clusters equals number of questions and the case of a thousand factors medulla say can attend clusters essence brigham and forensics or other hyper planners in the k. means package that will be discussing and they'll say don't fit or die
they predict your michio procedure matrix and they will return to you the labels the kloster assignment which cluster each robot assigned to sophie out of thousands rose you'll get a list of a thousand numbers between one and ten now i mention reason she lying that absurd the euclid in distance does not perform well in high dimensions and k. means is effectively using
the euclid ian distance metric on rutledge in order to move its ports around or good fine of central it's been no see the journal entries and that it's the same sentence transformers and bedding tool or seven hundred and sixty eight dimension factors that is too high for k. means to work effectively it is too hot as far as i understand it from from poking around on one when we say too high of dimensions for came
means to kloster effectively for euclid ian distances to work well we're talking like a hundred hundred dimensions is like push and things and we really want something like ten ted fifty dimensions that's really are sweet spot so he seven hundred and sixty eight dimension vectors is just right out to wait to high for k. means to work fantastically now i have used k. means in no fee in practice
as for okay it's not terrible an effective ego to know the ai dock on now sign up by korea few journal entries in and go to the incites page or you can generate themes all over your entrance of the journal entries themes in no feet or clusters the clusters by their clusters on your journal entries i'm in other words i talk about relationships law and i talk about politics a lot what may start out as a thousand journal entries be coms two clusters in
and the fiends section of no see they will show you add the key words the common keywords it up here or in kloster won in this case it would be politics tromp collections blah blah blah as well as a summary of the core of that feed using their own party face transformers summers asian pipeline and then it seemed to aka a cluster to 'cause we're using clustering baseball teams are purchased the clustering feature in practice theme
two might have the words relationship bands marriage in ali's things and then a summer's a shove the centuries you can on the website click the advanced into your icon and select k. means to be your clustering algorithm behind the scenes uses a bomb or a classroom which will talk about a bit and you can see the results i have found the k. means for this purpose for natural language processing and bedded documents now seven hundred sixty eight to mention vector very high dimension
actor clustering those does not work fantastically with came into works okay it's not terrible but i have personally found that the blomberg of clustering works better and will talk about that nugget but before we talk about that no k. means is the most popular clustering our rhythm on the planet's celeste while you're alone but more there are other implementation zaki means you don't have to use the psychic learned package a very popular other implementation of came into this the fights have a bypass s. is a facebook me
she learning package and it has it's own implementation of k. means which is actually faster it's as more performance and it actually is more accurate evidently after you train it's on very large gate and so the psychic learn k. means package works effectively for low to medium data and talk about numbers of rose not dimensions and the fights k. means implementation by facebook works better for reward the number of rose bowls computational a performance why
and accuracy performance was what this package feist die is not i will talk about this concept in another episode because it is highly related to no fee on actually using fights were any of these related packages but i will eventually and i think it will be of interest you listeners feist they call this approximate nearest neighbor search algorithm for packages and as a handful powder one is thrice won it is annoying eight and anatole why yeah exactly annoying and
and another one is h. m. s. dub you live to ann's you'll find if you wanted flooded into these now 'cause i'm will be talking to much about them currently is your final of the more bobby's actually go to the u. k. p. lab sense transformers package which is good but school i use a nosy four and getting your documents and in the examples directory there's code that uses these packages in discusses what they do and why you would use them calm so if you wanna get clicks
and identity sings i recommend going to the u. k. p. lives and transformers examples directory doing a little poking around but in essence what these packages do these approximate nearest neighbors algorithms is they are they're used for semantic search for looking things up just like you use co sign similarity and i can using co signed some larry you know she bought it for very very very large date set six to ring large days such millions of rose what the what these packages do
ooh that was just take fights for example fights creates an index basically your database all of document and denning says an index of these documents so you instead she an index of python code index equals five stock index princes and then you see in all of your doctor and petting zoos raul seven hundred sixty dimension factors into the index an index chris databases base of the effectively with a postscript database firm icicle database is a data base all of the factors that can
and being swamped in and out of disk and memory very computation we effectively and which indexes the the location in vector space all these documents to each other where you get to specify what is that distance mack truck by which these are compared to each other for look upon me and acts so if you're using doctor and eddings you care about close as alert as we discussed in the distance not accept some dark him bearings care about co sign similarity twitchell
urso you specify enough ice index cure my documents million of them and the similarity magic on what is co sign similarity now you have an index that is used for extremely efficient look up and search and annex anyhow the document so for devil by half a million books being my fight spandex and now i have a new journal entry that there are no thing i could say finding the top ten books matching his journal entry he just thrown into the spice index and it returns to
you can results which using the similarity magic a specified in this case because signed is able to just project your journal entry into vector space and these odd points these books are represented competition we officially can be just hold out really fast easy unawares relates to k. means is that if he does this fight a distance metric this could be using euclid indices metric everything noises you put in by default guinier's just like the dutiful distance master come everything and the d. fault
lost or is bikinis are written about fleiss's implementation of the king's opera i'm so the fleiss package primary utility is this approximate nearest neighbor search which allows you to do semantics turks between vectors you can vary computational efficient we pulled up similar vectors to wanna you passes for very large data sets but one side benefits of feist is that it uses its own implementation of the king is our them on the hood in order to perform this process it also exposes that
he means implementation to use a developer which you can then use for your own just normal clustering application for very large data sets so that's k. means the psychic learned package for small to medium data sets the fights package for large data sets and came in search of uses euclid in distances is part of its algorithm under the hood but like i said you quit in distance doesn't bode well in high dimensions and indeed with document eddings as we're using no see we're dealing with higher dimensions so what you do
yeah a couple of options one thing you could do is to mention aladeen read duke's your lord vectors and i will be a very nearer upcoming episode in machine on applies to men johnny reduction there's all sorts of dimension ellie reduction our burdens out there one is p. c. a consultant on analysis another is best vp another day is you map and cease me now to see is low but more commonly used for visualization purposes but you map and t. c. balls
or what's called manifold him bettors and are they can be used for to mention audi reducing your lord factors to very small to mention vectors are taking the case of t. c. two to three components down from seven hundred sixty eight and a case of you map you get specify however many components we won't talk about demand shelly reduction here that'll be in the upcoming episode and i'm not using them as shelly reduction know see instead i'm using a different class ring our remote
which supports different similarity metrics for determining the clusters of your roads and mainly as a mention in the similarity magic's episode in an l. p. co sign is almost always prefer over you putting distance anyway us not just that high dimensions don't perform as well as not the only reason we're not using k. means here it's that in the case of an l. p. euclid in distance which sort of driving force behind k. means doesn't really make sense
the distance metric in semantics search and co signers preferred presence comforts okay means is pre much kicks to the curb in this particular case of an opaque high dimensions check you clay and check when lawn and the place that diana planning on is what the law murder of cluster of blomberg of clustering tanaka spell it gets along word ago on more active you look at the shots of glory of posturing is a tape of hierarchical clustering it's a different set for later for style of clustering arbor them
no way k. means works these hierarchical clustering of rooms there's all sorts of different prayer book of clustering of rhythms and there are a lot of other question now grooms better or the hierarchical necessarily is wop bop a few out there that i've seen commonly used our best spectral clustering mean shift affinity propagation and some other question algorithms i will discuss ms episode don't discuss the three bears mentioned by the coup hair below cost
during works turrell a difference in the way k. means works and her pull clustering our rhythms are more friendly to using different similarity metrics honor the hood for clustering your day yet in our case for they say we want co signed k. means doesn't use co sign infected s. ne win support co signed and so in no see we're using a type of hair couple clustering algorithm called on kulon word of caution i won't talk about how cool
armored of clustering works mostly to actually don't understand it now while it is so happens to really work well for me in practice and i've tried a handful of their hair go posturing our burdens on guam report the best for me so so wanna go with but i will say technically that the way allows you to use the course i'm metric is that allows you to pass in a man's shirt by name so's you'd say secular not clustering topical armory of parentheses that number is
foster's yours as a number of clusters in the second learned that clustering packages so we have a thousand entries maybe ten cluster sounds good comma and then met church the polls and then you can specify work one which i believe supported is the word co sign like i may or may not be wrong about that are not actually using that amusing at a different one the metric stirring that i'm using is called pre computed pre computed el what that does is that they
specs you not only to pass in the day that the roads that you're going to be clustering but also a day another matrix and that matrix is the distance of every row to every other rob yeah because we're comparing every row to every other ro what we end up having is what we call it square matrix a square matrix is the same number of rose by columns all of it babe
i call of the sodas lot of repeated dia obviously but that's what these hierarchical algorithms which allow you to pass in the pre computed distance matrix expect is a matrix nautilus or a vector emma like emma matrix am being the number of rose in your date set so you pray compute the co sign similarity in all that every entry come begging by every other country i'm betting m. when you get back is
a square matrix of all 'cause i'm similarities and you pass an end to the aboard of clustering constructor function and then you called top fit transform on your country combat inks and when you get back is again ellis dove labels though less length being the length of your list of entries and ole ole values being any number in the range of the number of clusters the specified be ten in this example are going to puke the square matrix of the co signed distances from every and
three i'm betting to every other country and adding does have the sienna code i'm using pie torch to normalize although vectors first and then computed doc products right immense man in the similarity metrics episode the co signed similarity function can be approximated as the normalized doc products of two factors in the resided without way as high torching norm then doc product is that is actually really fast you can do this on the jeep you
ooh normalization and doc product or balls to extremely fast very first class implementation audience linear algebra on a jeep you can itself or as if you're to use one of a psychic learn co signed distance functions like that he'd jest or seed just methods i would be performed on the c. p. you annie would be slower okay so that's the k. means algorithm which uses euclid insistence on rid of that compared to the o. glorious clustering algorithm which
the type of hierarchical clustering algorithms which allows you to specify whenever by distance metric he wants and our case we're using the cosa somewhere in magic we don't just how what he used the co sign some learning magic for technical reasons using the string magic the polls close side is that we pre computer co sign similarity into it a twenty square matrix of babe i'd be co scientists insist that we do that by a calculating the norm of every vector and there
narcotics against each other norm to dock products is effectively co sign next sort of talk about deed the stand and h. t. b. stand up before we do on what to talk about finding the number of clusters to use in your clustering algorithm because like i said both k. means and aguilar of clustering expect you to pass an independent underscore clusters parameter being the number of questions your class jury roar the large data set down in
to buy honey no advance you might know you might actually know that there are three topics on this website three news topics politics sports and beauty read those of the three top of the only three topics we talked about on this website so no matter how many news oracles we publish it for abusing a clustering our brother milner cluster down to three and crushers go through this very rarely the case that you know it as the number of clusters you need for this clustering application so there are on a mad
ways for finding the number of clusters the first way sir do what we call that elbow approach that can show you this weren't contests format is a very visual think what happens is there's a point at which you start getting diminishing returns in in what we call in nerve shot will talk about that a bit and you can graft that autograph and you'll see that point is very crystal clear what happens is you try every number of clusters up until you start getting diminishing returns soap which write one caught up
i was in rose was strike clustering this into one okay we get back was called in their ship which plot that point on a graph and then we say how much you clusters we clustered those thousand rose into two postures and then we'd be back the inertia and we put that on a graph we keep on three classes for questions so what were the graph end up looking like is this witness we lease steep fall and then like that elbow of us a point where this is again angle it really stocks following an integral well
some goes to the late much much more local still decreasing but must much less rapidly we call at the elbow and that's the point at which she huge aside venture optimal number of clusters now you could do that by a actually creating a graph embassy tutorials teach you that approach literally your rate through wanted to forty or half the number of rosen your date set and jerry clusters for that number of clusters
get the inertia back corner autograph make the graph look at the graph eyeball the although what seems to be a good elbow and then manually you the programmer choose that to be your pen clusters i don't like that approach awake i know what the humans are being in the next year in that way i eyeball even visualization of the data sets and distributions in making judgment calls on how to normalize you day and stuff like this but i don't believe that humans to be in the mix in this particular way i initially automated and there is a pair
the job they're called need to take any he'd he'd be like need instead of elbow because that is that if if the king can graft is pouring concave up with common and although as his concave down we call in the so they decided to go with me and said although maybe it'll blow was our uses a package need with the d. a n. p. d. what they need a package does allows you to just automatically finding the elbow or the knee deep i have full piper
amateurs and which direction are we supposed to be pouring concave up convex down blah blah you specify those things and and you give it the graph that would have otherwise been generated in visual output you just get those points and it finds the need for you automatically very handy to late so need to talk to me locator parentheses your points and some fiber parameters edible find that elbow for you but those points on the graph outside inertia inertia less discusses
little bit the inner shot of the k. means model is the intro kloster distance between clusters it says the distance between clusters between clouds of dots okay so we'd use you say she became means model with with a number of course jersey specified to for example and then you fit transform it on your rose any remembers is a it's a variable now on the quay means model it's a public variable you can access
hold inertia that it is the distance between the two clusters the two clouds of dots the demean square distance between these two dots or does distance be euclid intestines okay will trivia from our distance metrics epps are so as to mean distance between clusters you add a third closer now it's distances between all three classes for each other so that inertia and is stored on the instead she became means model which is a very warner python could eat his access as mottled by inertia and
just a number can you just thrown into a snake pop that into your into your knee locator code which will and find elbow for you but there is an alternative method of fine amy optimal number of questions is not inertia is called the silhouette score silhouette score it takes into consideration both into her candidate in shrine clustered distances so it takes into consideration the distance of points in a cluster to their cluster sanford
and the desist from kloster to cluster so it's all little bit more informed to to what they have loaded more information packed into what provides you back and in addition it does something extra which is that the maximum silhouette score is the winner is the optimal number of clusters to joe's which is better than using this new locator approach because finding the elbow of a graph
can be a little bit fuzzy and are hyper parameters that you have to pass into this new locator algorithms of that you find the right elbow given the types of graphs that you're getting back to the particular is one hibor primer called s. which is the sensitivity howell sensitive or you to the spell those if he ever to smooth over graf we need to up the sensitivity so that it's a little bit more hardcore and if if it's a very jagger aphid sarah
so finding the elbowed graft is usually not all very cotton try to be subjective into the physique and it might be looted automated techniques like the new locator where's the silhouette score has to benefits overusing inertia for finding the optimal number of cluster fuck came means or glamour clustering the first benefit is that the information that is packed into this process namely in turk can't inge frock cluster distances
is more informative when it comes to deciding optimal number clusters you have more information to help you make a decision on the optimal number of questions and the second benefits is that you can't simply select the silhouette score there is a maximum as that represents your papa mama murph clusters and is no complicated physique new locator logic you have to do to find the elbow so nine time that can you see people prefer to use this
so what score and as a psychic learned silhouette score function it just makes whole process super easy for you and there's one third huge benefit is that allows each you pass again a pre computed square matrix of distances so you can use the silhouette score with co signed distances for deciding the number of often low-cost years in a blomberg of clustering so let's take a little step back k. means model you
it is you claim descent center that that has problems of high dimension as probably because it can use goes on so in an l. p. applications k. means isn't the best arbor them to use here in most other applications k. mean is that the fall and is a great crossing aren't used the specifically in this case documented eddings and cosa similarities high dimension and weep for crossfire over euclid in seoul can really use k. means and is decaying means model that generates the spender show
score when you're on the model on some number of clusters so we don't have the inertia score we can even use inertia scores are available to us we're using the ago on word of clustering our room which does not come with inertia score sell what we do we run the egg lot more of quest for for every number of clusters from some minimum to a maximum and then we actually compute the sole wet score for this the same square make
it's a co signed distances between the documents that we use as the da to train you go on earth clustering our burden on we use as the input for the pre computed distances for the silhouette score dose of silhouette score method takes the labels that are al public fromm do cloth for hand the distances themselves being in this case the pre computer matrix cousins and words so so as far as fantastic phenomenal is preferred in most
this is for stands for finding the optimal number of clusters either for k. means or for a lot of questions article clustering wherever have you and so for most applications where recommend is you go k. means and you go solo escort and dictum axel s. were that's the number of clusters of tiny fine minimal the optimal number of clusters jamie's and psychic learned if your day is small to medium to amy's from fights if you did his lord's hands you'll use the silhouette score
to find the optimal number of clusters for your kamins that's most applications born on talking about was that occasions you were talking about natural language processing so our case we're using the uncooperative clustering our rhythm with which works well with pre computed square distance nature c.'s whatever that may be in our case is co sign and then you can use the silhouette score on at the idea of glory of clustering i'll put in combination with the pre computer koh samet jerks so k. means and
ron rapoport us only our burdens out there and there are different popular ones used by different circles but in my opinion i like those two cars remember and sebastian eileen out talk about the beast gan and h. g. b. scan aidid be scan is another type of higher poll posturing our brim similar to the bomber clustering being a type of particle clustering our burdens of didi skin is another article plus for an effect it seems to be bombed and this is just my observe
haitian allow more popular on the internet just use a lot more commonly in paris applications ulysses to be the case that the law the community is moving away from k. means and sports guy de be scanned it seems that the big whenever didi scan as part of this special sauce under the hood offers a lot of versatility that lends itself to multiple applications and sort of auto learns some aspects as per the process one aspect being the number of clusters
sell big benefit if you use didi scan you don't need to specify the number of clusters it learns it defines the optimal number of clusters for you on it is on that he is being norman sets in a huge resented tuesday be scan over either buckle armor of clustering for k. means now dna scan is often algorithm it said concepts and so there's a psychic or implementation
called psych you learned our clustering not the bees can probably well there's a bomb more popular implementation are called h. d. b. scan that is just as his son on psychic or implementation and is suspicious more populous more powerful has lot of bells and whistles that does lot more for you and the box lot of great argumentation a huge community behind it tom so if you're gonna used to be skinner recommend using the h. d. b. stand package instead it is not
or the psychic learn package and yeah so let us send out one of the benefits of the staircase to be stan's a automatically learns a number of clusters off the mall for your clustering situation and another big benefit is that it's the sort of a bill on automatic next to it it is sort of does some stuff on rue the day o'connor said i haven't spent much time with it that it lends itself generally to various specific use cases where you would specifically be
using specialized arbor and so me personally been no see i am specifically using a gourd of clustering because it specifically line as well to the pre compute co signed somewhere in the matrix being passed and and and working off that can find it to work very well whereas detainees are brittle abuse over here for some other stuff stuff that you put in an ox language it should well kgb stand for what i understand what his great strengths is that it's
alluded more general and mall to play up little boy you can use it across the board honor various circumstances and it can handle multiple of these various circumstances very well i would recommend exploring the stevie skin and if you're doing clustering in your application and seeing how well works for you it's it's a very popular and people swear by people law they stevie scan me personally i'd try it in no fee for the themes feature and for clustering your journal entries the buffer
running goes through that semantic similarities surfed books even worth all for a fight i've filled with it for hours and hours they never found it any clusters was always native one which means it didn't didn't find any that the only cluster found was native one which means it was now liar i don't know if it's just that the da oh i'm using yielded good document and betting speed seven hundred and sixty eight dimension factors are all know if i need to normalize as an advance or
or confessed to ohio's dimensions of factors as far as i understand another claim of tasty be scanners is it doesn't matter the dimension of vectors using her to be wrong about that one but is supposed to be what is a catch all clustering our rhythms it can basically handle anything but it was not able and all my dana without bail and always document and bangs whatsoever neither didi skin from secular nor gauge the beast in the stand on package with all the filling of hyper parameters and
not to mention ali reducing my vectors not always sings i can get working with subsides move ahead conglomerate question was working just fine for us are stuck to my parents saw not using it but i do recommend you try it because people swear by it so cheney is confident that the faults beast the beast ghana's to catch all and conglomerate clustering is nice because it it allows you to go sour and any other pre computed metric you might be using this not you plug in base that's it for today
some will talk about doctor actually who will change and pace from the types of staff wouldn't talk about really see you then


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and algae it time also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twelve showering algorithms part one in this episode onwards viscous
or is shallow learning algorithms shallow learning to remember a previous out so we talked about people earning we talked about using neural networks which is kind of the quintessential people earning concepts as sort of a silver bullet approach you can use neural networks for classification for aggression you can use of earlier situations with features don't need to combine any news them for non their situations were features may need to combine in some special way maybe this is smaller act
three squared plus x. to transact swan says feature learning any other piece of deep learning is the hierarchical representation of the day it's learned some breaking down face into its apart spies becomes lines and angles and those become pixels by also said that while we may treat people earning is sort of a silver bullet isn't necessarily so in fact if you talk to the machine expert feel very much aggravate and to see the level to which new machine mining engineers are treating
learning as silver bullet is to promise was thinking of the pudding is a silver bullet one is that it's certainly is not a silver bullet you can't use no met works for everything there are still many situations for which people earning cannot be applied and two if you have a problem that fits the bill up shall earning power grid and just sell been your point to save immensely on time and resources deep learning is extraordinarily computation only expensive so always using your own mel works saw
all everything means time in mind when something to the news new problem for much cheaper to having a basic knowledge of the fundamental scholl learning machine running our rhythms most commonly used in the space is very essential when you embark on your machine learning journey learning machine learning you're going to be learning these shallow learning approaches first to understand a baseline of how machine and works before you dive into the deep end of deep learning i like it said in a prior up
so it all lot of scholl learning our problems can be used as a team you're on in a deep learning architecture column but some of them can be so we are going to discuss the high level overview of various scholl learning algorithms summer the most common ones that i personally see werner do so because having an understanding of all these different approaches is essential for your development is a machine learning engineer balls and being able to apply the right
too old to the right job especially as situations were deep learning cannot be used and also to the u. be equipped with more computational eat fish and our rooms for situations where a particular algorithm fits the bill just so now i want to warn you there are hundreds of machine learning algorithms out there who hundreds machine learning for the longest time was this space exploration of mathematical loquacious statistical formulaic trying to discover which
algorithm would fit what situation if your problem was probable estate he might you something like the scene inference be fewer data mining association rules on the market basket approach you might use something like pop reorder worried it's always a bit for all sorts of different strokes for different folks different heart rhythms for different situations and so much so that almost every particular type this situation warrants in a particular type of algorithm is created the quest for the master algorithm this is like dick
asked for the grand unification fury in physics and mention the mass for our them book in a prior episode resources is all about that cuesta quest to find an algorithm they can handle any situation that son or least most situations on us on an ass or if the golan heights running deep learning is these days because neural networks can handle a surprising amount of situations but for while they're a particular algorithm was used for a particular situation i think over time all of these organs became a little bit more say
in maybe one algorithm to handle multiple approaches elise within the domain and became increasingly obvious which situations were most common so for example in history you're gonna see recommend ursus and all the place amazon recommending the next book or product freedom by method for many the next movie for you to watch google recommending an ad based on your search crews clicks and other things recommend ursus obsessive very common situation another common situation something as mentioned cola market baskets
it's determining if you buy this and that what might you also by its to suggest for you and other products based on things are currently in a shopping cart so that's another common situation so we've learned over the years and industry which situations are the most common and which are runs in the pipes the situations so of the hundreds and hundreds of machine learning our rhythms out there there are a handful of insignia see most commonly and me personally what i've seen most commonly i have
boil down into this and the next episode and splitting this episode in half because it would be so long otherwise into the outer insight personally favor the most fundamental free to learn so in this episode that we're gonna talk about came nearest neighbors came means op reorient principal component announces indecision trees and the next episode work and talk about support actor machines naive days and a handful of other miscellaneous algorithms like anomaly detection
manor systems and markoff chains now understanding these basic algorithms like to settle the fundamental freer success machine and then you'll encounter so many other machine learning algorithms out there in the wild real quickly get feel for which algorithms are most common ideals also needs determined for whatever project you working on whether it's professionally your hobby which algorithm is algorithm to use for your specific projects as can be a key point is you are going to get a job machinery of the work
one person projects a machine learning there are hundreds i'm assuming our runs out there we can boil down to the most use our rhythms and so you may use the term in which our rhythm that suit you were specific projects so the end of the next episode in the show no sign of postal ain't it's basically a decision tree for deciding which are written to use for your project based on various attributes about your project and then you can dive into the details of that specific learning algorithm now as it is the nature of my pod past i'm not going
go too deep into any of these machine learning our rhythms in do very high level approach to how they work conceptually sell apologies for that only to dive into the details later on your own by way of the resources there recommends you per usual okay let's start from the top let's remember how old machine learning this haircut we'd broken down ai becomes machine learning machine learning splits into supervised unsupervised in reinforcement learning
three seven categories of machinery there is a force category called semi supervises kind of the middle of sneaky the this like three point five categories a machine learning i'm not there really talk much about some i've supervised learning but let's understand these three categories we've talked about supervise learning in the past with linear and logistic regression actually everything we've talked about it passes supervise learning new images to progression and neural network says ross supervise the old hours can be used actually for unsupervised and
reinforcements that's another reason why you know my work sir so popular is they can be applied in a space machine running supervised it's broken down into classification regression remember classification is categorizing of thing as this that area thing kept our trade and regression is coming up with a continuous a value output a number so either classification or a number so i'd think we all understand supervise learning just dandy by now supervise learning is giving your system
a spreadsheet with all the labels in place and the learns the state of parameters by looking at each row one at a time making a gas and figuring out how bad did and correcting that there are the algorithm is sort of supervising itself i mean you feed in the spreadsheet the matrix of data with the labels and models for consumes it degrades over and trains itself will think of it that we really we think of as we are supervising the training of the model
think of the latest think of it like training at dawn to sits or to do some trick and you've got your hands behind your back in one hand you have a treat and the other hand you haven't newspaper and you say sits in the dog makes a gas so you try something a stanza and you spent on the nose of the newspapers that was the terror function telling it had made a mistake so the dogs thinks it's head and try something else that tries rolling over in new spain to the newspaper is a bad job and so the dog stand
back up and i think savings and it sits and then you give it a treaty say good job so supervise learning is like supervising the training of your algorithm just like training a dog with treats or a newspaper good are bad got reinforcement learning skipping over unsupervised for now reinforcement learning is an interesting twist or not as c. reinforce the learning our rhythms until much later in the series in reverse the learning sort of the gateway to artificial intelligence it's the way by which
you exits machine learning as a professional field and enter the world of artificial intelligence proper way reinforcement learning works is you give your model de gaulle okay see you give the dog a goal to other dogs run through the woods and retrieve the pheasants that you just shot down but more than that i like to think of reinforce the learning is even deeper than that it's like putting up backpack on her dog giving him a sword and shield and saying you're gonna find treasure at the
and of your quest if you find yourself in a cave of sorrow that is that leave the cave if you find yourself amongst the blue people they're going stay with them though give you points so you give your model the system both positive and negative reinforcement and a purpose which is maybe to find the treasure the end of the maids and the learns how to navigate the system the words the rules of the game what actions it can take it's kind of coming out of that age
algorithm you stand on this journey sago now for time is of the essence and you pushed the doorway status backpack and it's sort this and he'll fight right now it doesn't know what firm right but eventually at the end of this journey it strong enough local ninety nine minutes figure everything out figured out the rules of the game how to play the game cuts are bad dog food is good and finally learns how to get to the treasure at the ends as reinforcement learn refer to my name is sending 'em all on its way giving it's some positive and negative
enforcement guidelines and over the mourns the rest of the system on its own okay so supervise and supervising reinforcements we skipped unsupervised unsupervised i think of as sort of the redheaded stepchild to everything in between unsupervised and it has a very simple definition it's that you are not surprising it's learning process whether by reinforcement roles in the case of reforms are learning nor bike free labeled d. n. a. in the case of surprise learning the algorithm is left with
own devices to figure things out and you're not telling you whether did good or bad it is saying okay all take what you gave me so for example you give a bunch of on labeled the west pretend that you have a bunch of humans tax and fish and you had them all to your unsupervised are rhythm and you say hey keep him as sort these out for me you as the human don't know let's pretend you don't know the difference between human affection of cat and the machine doesn't know either you didn't give it labels
to come with the day just give a bunch of stuff to me like you made me sit these in the three piles volume you figured out i'm sure you do just fine and so the algorithm of scratches it's channel and looks at this high all of objects skater not sword and not in three piles and needs to sift them out into three piles so does its best and it sure enough you know it's good algorithms says these things out so so all human your mom paula catherine one pile all fisher another pot at unsupervised learning because you didn't super
why is the algorithms training you learn on its own what characteristics separate things and the output doesn't come with any labels okay so supervise and to rise in reinforcement learning so okay so i think you understand supervise learning baby reinforcement learning sam lebudde mysterious interesting or not we get into that maybe unsupervised little bit fuzzy for you will you i think you'll understand a little bit more was made into an example buffers let's start with a supervise learning our rhythm k.
so again we're going to be covering many algorithms in this and the next episode just ole of the land of some of the most popular algorithms be used in the machine learning world we've already covered linear and logistic regression and neural networks three supervise learning algorithms to recover a handful of other algorithms when you're an logistically gretchen are examples of shallow learning algorithms neural networks of course are deep learning our rhythms and we're focusing on shallow an hour
and in this episode so your next shallow learning algorithm is called k. nearest neighbors katie and then came nearest neighbors is an interesting algorithm in that it doesn't learn city may be thinking okay why harry putting this in an episode about machine learning to visit our than a dozen are well it's commonly used by machine mining engineers it is commonly applied to machine learning situations imagine it
basically a brainless machine learning algorithm that's all it doesn't want it doesn't out the thing that parameters there are no theta planners in the system than the reason though we're starting with this algorithm this episode is the easiest understand in fact is the easiest machine learning our them to understand all the whole the machine learning our rooms in my opinion much more so than when you're logistic regression to this'll be a breath of fresh air came nearest neighbors how does it work imagine our example of hugh
men's cats and fiction many use an example for this episode mentioned taking a giant pile of humans cats and fish and dropping them again to the facts into an aquarium okay hands all humans are kind of position in one cloud one cluster over here and in all the fish for one cluster over here okay so we have x. y. and z. axes these are going to be features of course when you're working in space in machine
learning one deed or two d. or three the three dimensional space forty five the bestseller out the orton anshan is your features so the position of the human in this aquarium would be maybe three features here let's say their features here are a number of legs lives in the c. n. and it is a mammal so based on those three features were warned to position our camels unfair
it's locations in our queer him now are fish wall cluster around one area or humans will all cluster around another area and our tax will all cluster around another area as a very simple example with three features three dimensions in more complex examples we may be dealing with fifty features were even more having any range of numerical values whose values maybe you will be less obvious the wait hey nearest neighbors works is you drop in
you object into the aquarium claw you drop an attack an attack gravitate automatically to the cluster of cats simple as that came nearest neighbor means when you add a new objects into the vat and yet in new objects into your model it figures out which group of things it has the most in common with soul of all my neighbors which ones in my nearest to base on my features my number of legs m. imam
all enjoy live in the c. k. in cane years neighbors means the number of classes you're actually and see this all watson machine learning k. representing the number of classes think of it like saying class with a k. i don't know why it's actually k. but so what algorithm does you specify top flight how many classes are in the system in our case three we have cats humans and fish so three classes you put all your day then into place pre labeled from this
reggie sobel last column of your spreadsheet is the label whether it's cat human or fish all of them calm pre labels and so everybody's been placed in three dimensional space and then you add a new input to the system and a figures out what class or cluster that input has the most in common with based on his features who of all its neighbors is it nearest to source said this is not the learning algorithm this is what's called an instance basic or grew them too
we're works is you how full all of you were rose in memory and when you can put a new role in khumalo it compares that inputs to every single role that exists in your training did it in memory so we look over or spreadsheet every time we added a new route is slipping over every roll call a truce instances of living over every instance okay so that that's the nearest neighbor basically ally to think of it
magnets suspended in a vat is you have three clusters court kate clusters all center around a magnet can you drop the new objects into the back and stick it automatically pulled to the right magnet based on its features so that they supervise learning how rhythm the dead giveaway first whether something is unsupervised learning our rhythm is does it calm with the labels did you give that the algorithm of spreadsheet with pool labels as the last
on and indeed we did with the nearest neighbors so that's cayenne and that's a very simple supervise learning algorithm but it's going to segue s. very effectively into your first unsupervised learning algorithm call came means k. means k. again being the number of classes so first off unsupervised learning white guy said is that you have your day yet but it is not labeled in your algorithms can try to figure something out about the data
it's been and work with the dado bit but it's him or that on his own is a natural label davis nocturnal learned to protect the value or categorization is said it may we structure dale aurora nice them in some way get so three types of unsupervised learning our rooms are called clustering association and dimension ellie reduction let's start with clustering so clustering looks very similar to cain years neighbors we have the facts the aquarium
with clusters of objects k. we've got our humans are catch enough fish and it has separated from each other in space based on their features in knin we basically had magnets put into place in the middle of those clusters we had a human magnate have put the posture of the humans a fish magnet in between all the fish in a cat's magnate in vitriolic cats it doesn't really work that way under the hood thewlis be our revenue may not understand what i'm trying to get out there that kind of conceptually that
where works you kind of creating a magnet out of your cluster of objects the cat meg kincaid means you'll earning the magnets you or learning is machine learning your updating some parameters you're learning where these magnets ago now here's the thing like this said the unsupervised learning the dead giveaway is always whether or not comes with labels degas in clustering you don't have labeled day that instead you just have a high old stuff you just don't get a box
humans cats and fish into the aquarium we don't know if they're humans castor fish the machine doesn't know theory and scatter fish is stuff just a bunch of data points it's stocks that pay as far as you and i are concerned and the model in the computer is just a bunch of dots in the aquarium and what the models trying to do is figure out what or some sensible clusters of all those dots how can we partition and in a certain way it seems
that there is a senior of dots over here the sphere of dots over here and another one over there seemed like the space between now and i'm not touching each other there's not overlap or if there is overlap it still seems like this is the sphere and that's the sphere can avoid a van diagram that overlapping in the middle it's still seems to me that there's something this way the difference between this cloud in that cloud i don't know what i'm an organic call them the mongol learn the line that safir
it's none of that clustering does that sound category in unsupervised learning and k. means is one of the most popular specific algorithms you used in clustering so k. means k. means is interesting is very similar to cayenne and figures there is on an already you're probably turn figure out what could possibly be the difference the way i'm describing at the difference is that it learns were these maggots go these magnets are called central way it's central way
it's an once again you specified upfront how many you need so i have a bunch of docks and i know that i need three classes that as many specify three k. equals three you don't lawyer doc since the vats and so here's conceptually how i think of the arboretum maggi replace some sports game with a bunch of kids you drop all the kids off the field and then he dropped three team leaders on to the field randomly distraught them from the air a new lands on smithfield be free
we look around and they try to figure out who their teams are you tell them up right there are three teams and i'm pretty sure that i mean there's an answer to this equation i don't really know as a human but i'm quite sure that there's three can of real concrete teams here you've gotta figure them out to your team leaders look around and they'll scatter they weren't in some direction they running in the direction that it looks like to them is a cluster of people be separate
cluster of players and so they stopped in all players realize ok this is my team captain so the sign themselves that team captain players don't move these dots do not move in the fact that the team captains do and in the team captains all can look around again the reassess nineties samurai n. n. no doubt they'll scatter gan iran little bit closer than they do this over over over every time they do it all the team members sort of keep in their mind okay he and that's why captain that's my captain until eventually
all the team leaders have figured out what really is the natural clustering of the steeds i didn't really explain a very well i don't have a very good way of explaining this but that's how works basically you drop always examples into a back gate each have features but they don't have a label so that last coleman for spreadsheet is nonexistent the features or the dimensions those dimensions determine where in space these objects are and then
you drop three magnets into that and those magnets figure out on their own hope to position themselves such that there are in a position that separates the three clouds of dots most effectively and of course the purpose of any machine running our them does for making future productions to those magnets are now in place so that when you drop the future example into that it automatically gets assigns to the right cluster k. it mean
escaping the number of classes know you'll note if if we don't all whole bunch of d. n. a. into a back with fried natural classes of humans fish and cats and we then said that there are two classes k. equals two then it might not learned to you very affective delineation or who said kate equals four there may be sort of unnatural segregation of day that in the population that is not a parent
us nor is apparent machine in giving in the wrong number of classes mean sort of dooming the algorithm so which came means and clustering in general you sort of have to have an intuition about how many classes my kind of exists in the system is a good example the book i'm going to recommend to the end of this episode called machine learning with all our were the author or takes a bunch of both social network dana for a bunch of high school student
okay so things they might say on their facebook wall were things that they like or dislike various interests and in actually categorizes the students into five categories now as is the case in unsupervised learning it doesn't give you new labels live if you look in the way it categorizes these things you can poorly tell what is doing it has been jocks the princess's the nerds the criminals and the basket cases so these are have five
natural high school stereotypes that we kind of think of is natural our minds and it turns out the machine learning our room discovers with these is actually the case belongs you tell out that there are five stereotypes if you made it for them in my removed last stereotype of ennobled of the fuzzy won anyway the basket case the iowa bad as if you made six that might come up with the new class of people maybe that might still loops are stereotypes law that spread them thin
it can make a little bit less sense so happening in natural sort of understanding of the problems that you're working with him or fifth term in the number for classes mrs some number k. in the system is an important but there are actually algorithms they can actually learn the best k. for you there are somehow rooms out there that will help you to determine with which k. is the best for situation saw no leave it to of course as usual the ensuing corsair course to teach you the deeds
ills of k. means and clustering okay so we covered cayenne and they supervise learning algorithm for us or finding what object fits wear it with the label tree label day that makes unsupervised learning our them bands and then putting a new logic in getting a label back is also part of that supervised peace of the day unsupervised learning our them now we delve into the world of open supervise learning the seconds of the triumvirate of machine running out rhythms we broke that down into claude
strain association and dementia or the reduction and our other sub fields of unsupervised learning about clustering i think is probably what you'll end up saying the most common in the world unsupervised learning and we discussed a specific algorithm of clustering called a means very similar to pay and an okay so let's enter a new sub category of unsupervised learning called association or association rules again you don't give the machine learning model
free labeled dana so that last called spreadsheet with labels does not exist association rule learning as interesting type of machine running algorithms another name is sometimes about market basket analysis the idea of the physician role learning is you're going to learn if somebody buys this and that's what other things might they buy so this obviously has massive value in the commerce in regular commerce okay so new
sample is if you are designing a store lay out some of them by marshmallows and graham crackers or what might you also put in your of those two things obviously chocolate hershey's chocolate bars so association learning it is all about put more stuff in the customers basket it's learning wyatt's commonly goes with what other things if you have a set of some stuff baby and see what often goes with those deed be with
only have a the speed go with a select association rule learning market basket analysis now here's little bit it's interesting info about association role learning is often associated with data mining remember we're talking about davis science being broken down into various is some fields so one field day is science is the analysis and visualization charts and graphs and stuff another field as machine learning of course acts
another feel was a dimension apart so does just database science being really good at writing database queries being a mice equal or host grass expert case that's a sub field of data science and of course another sub field is science is called the mining it is mining information from the web or from some other days source case of scraping the web may be crawling the web put it into your database putting him through the day the pipeline by way of spark
or had you remember those two technology is spark and had to those are your data pipeline frame works for very large amounts of d. n. the dna call a big day that and then finally bring in all of your database and may be coming up with some sort of conclusion and that conclusion maybe by way of association role learning now says issue we're all learning is clearly machine learning we're learning to predict something based on a pattern no
recognize glad it's kind of also data mining so this is where you start to see all the sub fields of these signs were really they really are the molds together it's really tough to tell them apart sometimes a lot of overlap association role learning is clearly a machine learning type of algorithm but it's a lot times two hundred rise within the field of data mining and i think it's because the mining sort of predates machine learning as a very practical for
you'll get industry where people were using the association role earning eighty commerce really early on and so they just kind of categorize anything else machine learning that one along with the data that was super practical matter early days as simply part of the data mining process itself market basket analysis or association role learning is learning what typically goes with what other things in a very common in our revenues here is just call walked free ori the acri or
we are rhythm if you're that word means the public are for what the hell so operandi is like it comes before we're you know something already sold in philosophy if you have some proper yuri knowledge before embarking on an argument that piece of information is naturally part of the argument so offer your knowledge is that you already knew it the way our crew or works in the market basket is like so if i know that day is common
and i know that b. is common and i know the sea is common in case of marshmallows graham crackers and hershey's chocolate but i know that deed is super on common nail polish who knows how i wanna know what goes with a and b. all the natural conclusion as c. not because i know that see those within the but because i know that seed is simply a common and he is not so it's kind of using these like you you
a lot they small database of common things and then you group of those together in tunis and figure out which of those is common and an eager demont freezing figure out which of those is common they saw wrist shot things off along the way it does sort of reduced the problems that so association roller in really works with big day that most effectively and so the more that you can reduce your dated down in this process the better says the op for our room and finally won lasts
a category of unsupervised learning i'm going to mention is called dimension alley reduction to mention l. u. reduction dementia reduction is very easy to understand conceptually the idea is that if you have a lot of features many many many many many features for all of your rose in your data for your shimmering algorithms it is in your best interest to weed duke's the amount of features you have and you can do that actually you can you can remove feature
but what what dimaggio you reduction dozens dozens of bill week features it figures out what sort of combinations of features can make new features how can you boil two into wine and keep doing that over and over until you've really slims down all your features to the minimum amount of features so the best example i've heard is students g. p. a. b. g. p. a. is one feature that sort of represents howl
all the students did academically in college knows lots of features that you could consider even consider their test scores their homework assignments their attendants if you're trying to decide if you wanted to hire a student for example you could have been sent you there transcripts with all the information that he could possibly gather from the university the test scores are tenants all these things or you could just ask further g. p. a n. n. g. p. a. is basically all those things boiled
down into one number so that swept demand shelly reduction dies the menzione reduction algorithms figure route how to assess all the features that you've given it in words of boil lower was down into a smaller amount of features and one of most common algorithms years call principal components analysis and the name of the algorithm is basically an innate sense is everything i just described were figuring out all the components of the hundred compound
it's wishful you are the principal wants which of these are the most important or if all of them are important how can we boil them all down in to say five components siege features components equals features the principal component analysis and not to be given to the degree of their algorithm of course it's an intriguing corsair of course it looks a whole lot like linear regression in my mind so this is another case where knowing the fundamentals of
these basics and statistics linear regression logistic regression is that the common to play elsewhere and to machine learning so as lived by those first okay so that unsupervised learning unsupervised learning i kind of think of unsupervised learning as your miscellaneous juror that you might have in the kitchen okay you have your silverware drawer and you have your towel drawer and you have a drawer of o. bunch of stuff that doesn't really fit any work beckon it tends to be the unsupervised learning category
of machine learning they're all very useful are rhythms don't get me wrong i'm not calling i'm john they just kind of i don't see what keeps them all together it's this stuff that's not reinforcement status not supervised it's come all the rest there it's just a bunch of white utility algorithms unsupervised learning so unsupervised learning is clustering association roller rink dimension ellie reduction and full of seven mentioned k. means is a clustering our them opry henri businesses
the issue we're all learning our rhythm and principal component analysis or p. c. a. is a dimension audi reduction algorithm your lobbies from injuring course and for the final algorithm of this episode we're going to talk about decision trees decision trees are a very very important algorithms you know very important as i mentioned previously machine learning engineer's who've been in space for very long time are extremely skeptical and korea
all of deep learning they think that people are jumping a shark with steep learning taking it too far using is a silver bullet word should be uses a silver bullet a lot of times its decision trees but they'll use as an example of something that performs just as well though say see decision trees which of them round blocked since the fifties work just as well as your neural now works why say work just as well i'm talking about accuracy performance things over and talk about a future episode about performance evaluation
all those things but what's special about decision trees something that decision trees can do that deep learning can not do is explain it to the viewer what was learned was learned okay so when you use a decision shreve model for erna your date at the result is something you can read it's on your computer visually you can read the steps taken to come up with the decision so that's a mix decision trees particularly special
we're also very high performance there'd be worked very effectively to what is the decision tree decision tree is exactly what sounds like it's just like when you come up with the decision tree in real life you say okay if my mom rights of free then maybe you'll go out and do this but if she writes of five will cokie okay if she's not hungry then will go drinks first but she is hungry then we've got three restaurants to choose from it's raining or none of this russia will walk so basically it's a whole
what ship bethel statements the hare propulsion sure no recall one decision trees but they are to draw them from the top so imagine if you will be circled the top that branches downward left and right into two circles and each of those branch downward left and right into two circles now the decision tree learning our rubin will learn these these nodes and their branches it will learn what things to check first and what things the checks
and what things to check third cetera it will learn what was the very top white is the first thing that i should try or talking about our fish humans and cats scenario what is the first thing you can check to rule out the majority of the remember we have three features number of ways is a mammal and lives in the sea well humans in cassar mammals supper with no checked his mammal that we would have to break that down even
or based on the number of legs for example but we first chaps number of legs or lives in the sea either of those features would tell us right away that it's either a fish or something else and then if it's something else we could break down by a member of legs so the top note of our decision tree of oral learned that decision tree would be lives in the sea question mark if yes it's a fish
if no then you go down new branch and in that pledge asks the question number of legs if to that the human it for them it's a cat so that's a decision tree very simple our model will earn the optimal placement of questions and number of branches in history and then like this said before the nice part is when it's done it will presents t. u. v. show we a tree
and you can actually look at it and read it as the engineer which is not something you do with almost any other our room under the sun so if a customer comes in to u. n. you have a linear regression logistic regression learning our rhythm that is meant to learn whether or not somebody should be eligible for a bank loan okay you're a banker a financier and customer comes and asks can i please have a bank loan they thought an application dollar questions of the application or the features
they go into learning model things like have you ever de false alone have you ever been bankrupt which are age would you do professionally etc will to fight them into logistic regression or neural network dental spit out the answer now all ineligible for bank loan and go through around in your soul which ariel say i'm sorry sir you were knowledgeable for the bank loan mostly whites why in your turn around you look in your computer will be a whole bunch of green ones and zeroes trickling down the black screen like the major
stay and you're like love me turn off your monitor charger customers say i i don't know but you are knowledgeable in the storm out a paragraph you cannot visually interprets most machine learning algorithm suggest they're just numbers but if it was a decision tree the customer economists say can i have a bank loan an eap type in the application pungent your decision tree algorithm and it spits out a visual sri for you to you take a finger you pointed that hopman cup killed
c. so said known you'd go down to the right at last the good of the ride is o'clock this is where you failed sir it's because you've defaulted on previous long i'm so sorry this is all candor stanley walks out it just so happens that in the financial industry you are required to tell your customers why american eligible for a bank or so in this particular case that i used a decision tree is legally required so for many applications in
shimon indecision trees are very good beer redouble their obviously very easy to implement by way of any sort of machine learning library out there like psychic learn or any of our packages you can actually use decision trees not only for classification like i'd just use miss example but also for regression for actual numerical outlets floats have to explain but you call a classifying decision tree a classification tree and a numerical output tree you call the regressions
the natural now all the algorithms i'm not going to explain how they work the actual earning our rhythms the process of learning the tree is figuring out what questions to ask first second third and so on the call for them for learning those things calm there is one by the name of that i eighty three one is c. forgot five another is c. five thou go and others court case of a goddess funky names don't think you really need to understand how layout rooms work even when you're
learning machine learning detectives had them in with your machine learning libraries are packages in our psychic learn whatever using and thrown a buzzword set us on what prepare you for your received when you're learning the details so that you're not overwhelmed with a bunch of towards the sea in the wild k. when you start to explore the world machine learning you see millions and millions of words or just tell you what they are even without getting into the details just a you know come wearer thing falls into place along those loy
and there's two kind of popular spins off of decision tree is the vanilla decision tree is just like i described to you it has a problem with the cold over fading which is something ari get into a few drops are over fitting in the way you alleviate this problem is by way of making many many many many trees what's called a forest were you when i'm forrest sometimes you figure route of the average best amongst the
trees so who did that's w. w. trees who are the best trees amongst you and figure out what they all have in common in order to construct the ultimate sri case that's our way and i'm forrest approach make a bunch of trees boiled down to one and something very similar is called agree in boosting so i know but i don't really understand how brave boosting works at the gate hot off the tree is certain points at an early stage in the trees developments
then they can and do the same thing as rain forests don't call me on at the base that's wanted a heads-up you to those two words you're gonna see you when i'm forrest ingredient boosting very commonly thrown round all over the internet to basically optimization spawn decision trees okay how about that whirlwind to work this is only part one of a two part series on all holtz were always algorithms park who is going to cover support vector machines
naive days anomaly detection recommend ursa stems and markoff chains and again this may seem very overwhelming i just want to give you a lay of the land of many of the popular mushy learning our runs out there there's hundreds and hundreds of machine learning algorithms every power than can be applied to a different setting we're getting closer to the master algorithm potentially by way of deep learning and neural networks but it is in your message
stuart least have an appreciation and understanding of the most popular show learning algorithms so that you can both save time and money what comes to competition or resources and for situations for which neural networks are not suited at present and there are many such situations are many situations where shall learning algorithms or your best that's what i mean by that is we're talking apples to oranges it's not a performance comparison is just something in your mouth works may or may not be able to handle so
is it your interest to get at least a lay of the land of the shallow learning our rhythms and i would recommend trying to understand the details of them offline by way of the resources so let's talk about the resources on what the post an article called a tour of machine learning our rhythms that basically just like this episode on the spy machine running master dot com which is a website that i've recommended before he puts out very good stuff very very similar to this pot casbah in artful format will be a visual representation of ever
hey man i'm discussing in this episode so do something to visually latch onto says i know the audio congealed it too much there is an image online put out by psychic learn to remember sai kit learned is the python library of shallow learning algorithms basically everything that you're in here about in this and the next episode are gonna be things you can use by way of psychic learned these are not our rooms that are present in texture flow to answer flow is built primarily for deep
aren't you can build your own shallow learning our rhythms and cancer flows if you understand how the algorithms work you just use the low-level tends to flow math wipers by one recommended i'd recommend sticking to sai kit learn for showing our rhythms and then moving on to tester flow for the deep learning algorithms so psyched to learn put out an image that is a decision sri for which algorithm to use where how bout that decision trees very mad at so if this is
tree for deciding which algorithm of opal world of machine learning our rhythms you she used given your predicament okay them are working with tax to pay go this way how many training samples to have to go that way very very handy so look at that look of that before we get the next episode cousin accept sorghum do couple more on rooms and then i'm i'm busy lives in a cop in case the resources section in the next episode is the same resources battered be useful for the next episode the next resource on
when recommend it is called machine learning with our this is a tyler recommendation most of my recommendations are sort of averages for round here nothing's they see people reckoning olver and over and over can of the de facto resources this is a resource probably slipped out somewhere so summary recommended some work but i've never seen recommended otherwise and i a pool of this book machine learning with bart is a fantastic explanation of the machine in concepts in a way that i thought was
i think sometimes in other courses or books to read as a very good job thoroughly explaining the details machine in concept additional eight he commerce lot of algorithms that intervened does not cover all which are essential so for example naive day's market basket analysis in our pre henri sensation roland and kane years neighbors in gaza cars decision trees which is not covered by intervening decision trees are on the things that you're gonna see in almost any other machine learning resource you read so dole
to teach you are all long way which python is the main line was ignored be using your mission on the jury ally said in a prior episode that are in java are two very strong ron rocks that might be handy having your tool belt okay now i'm going to mention to text books they're very heavy and they're very commonly recommended they're there to help understand how machine learning works at a fundamental level in the copper lot of these algorithms that that a mention in the sun
accepts and remember my analogy of the machine learning process to cooking i said that when around rose kind of like talking things out and statistics is your cookbook your recipe book and and put on a gavin is calculus best learning process while most of the resources that i've been giving you up to this point are basically cookbooks light up a list of recipes to use your machine learning endeavors these textbooks and one recommends you are for lowering the theory of food
it's basically a learning why in the recipes or what they are if you have some understanding of statistics how can you blew a bunch of concepts together in order to form a machine learning recipe and be so these textbooks on what is called elements of statistical learning and another is called pattern recognition and machine learning so i recommend these textbooks eventually you should eventually read these textbooks they're very very commonly were
demanded an essential for you to have a thorough understanding of how machine learning works at a fundamental level i don't think i would necessarily recommend them yet now i try to tie my resource recommendations to the top at the episode topic even if you're not necessarily ready for those resources at that point in time so these are kind of these attacks bookseller recommend you come back to when you really wanna have a more thorough understand
how how machine and works of fundamental level and it will cover a lot these machine in our rooms i've mentioned but don't go even deeper though the help you understand not just what algorithms to use honor which circumstances and why but how do we even come to that conclusion how did we come up with the new machine learning our rhythm for the situation that we come up with these recipes learn how to invent recipes to the knock confounded by the recipe books is not
just members nation so again these are books though recommend you we turn to in the future once you have a more solid understanding and foundation in machine learning elements of statistical earning and pattern recognition and mushy learning to text books okay that's it for this episode i'll see you in part two


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and l. g. had time also starting a new contacts which can use your support and it's called black mayors like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode thirteen shallow learning algorithms park to support factor machines and naive
aids in this episode and be talking about support back to machines and naive days class of fire these are two very powerful machine learning techniques scholl learning algorithms these are kind of those power machine learning our rhythms power tools i consider a decision trees support factor machines and naive basic consider them all three to be sort of power tools lot of the other shallow learning our rooms and you'll learn were sort of dedicated to particular tasks or even if they're more like purpose they shine under specific
circumstances but decision trees support vector machines a naive bays are sort of these particles would be quiet across a very wide with spectrum of machine learning applications they're primarily bill for classification all three of these are rhythms are primarily bill for classification that can be used for regression before reaching far this episode are wanna talk about the fact that we're talking now so many machine learning algorithms morris up so i dropped a bombshell on you missteps already talked about two more in the next episode and actually decided to stretch bitch
you pour into a three part or the next episode and excited and even maureen shimmering our rhythms that so many machinery our rooms haris was aside what use when welders amoral type heart approach to deciding which machine learning algorithm to use giving of circumstances which talk to the passes sort of deep learning can be seen as the silver bowl they can be used across a wide spectrum of a machine learning problems right now are taking a diversion were caught in a shallow learning our rhythms that was selling showering our rooms at the nose
secondly which algorithms are supposed to be used and which circumstances described as small plates your approach to deciding which algorithm to use on reassurances that hot global cern machine learning our rhythms only handle specific tasks so it for example the next episode on a talk about an anomaly detection algorithm well you only use one or a handful of algorithms for not only detection you don't use things like linear regression or logistic regression or maybe our rooms are really talking on this set
so and so there is a level of domain knowledge that will filter down the types of algorithms that you can be using freer specific purposes it's apples to oranges this situation only calls for a handful of machine learning our rooms that can be applied whatsoever and i just takes familiarity with one machine learning our rooms and whether specifically built for i think listening to this broadcast series and taking a changing course and some other follow-up material wheels
did you feel for what algorithms are built for what purposes but there we go down all level we decided to work and be working in supervise learning classification and so that only includes a handful specific algorithms we excluded all hold on to our rhythms like all unsupervised learning our rhythms any regression algorithms us r. use linear pressure for this robberies k. means we're going to go classification but now we have a whole bunch of classification our rooms to work with some wasn't so it's not my decision treason is up senator gossip
vector machines and naive days all of these are classify robin's wilson of logistic regression we have neural networks how the heck am i going to choose from angst always class of fires will this point will want people machine into tuesday look at their degas and a look at their environment of system computer how much ram does it have and how flexible are we with high in the store is running our rooms making the inferences and productions certain algorithms work better with certain types of d. n. the baby logistic regression worked really well with
numerical input is also very sensitive to missing features such like this naive days works better with categorical input is not sensitive at all to missing dana naive days class of fires are very fast ryan in very memory efficient the middle of it less precise than something like a neural network and yeoman work takes a lot longer to run and train was very precise in king represent very complex malls so once we know what category of machine learning algorithms we're gonna be using force
the purposes then we considered the situation in hand memory restrictions time restrictions what is our data look like how many samples do we have certain machine lowering our rooms were really well with only a handful samples wall of the machine learning algorithms require townsend's kinds of examples your training dss neural networks for example are vexed by the fact that they need lots and lots and lots of data were isn't naive baze crossfire for example doesn't need that much
they get to get up and running so you look at your data to plot a chart graf it you decide if you're missing any information is a numerical is accounted for cole polish memory by hello my machine works time don't have to work with how many examples the white house and then you decide so that parcel little bit hottest have to explain and high caste format envoy to link in the resources section to the table of pros and cons for specific algorithms given situations like memory constrains time constraints is sarah there's also it
susan sri put out by sight yet learned a python library for scholl earning machine running out rhythms they have a decision sri up a picture like a flow chart for helping you decide which are written to use given the circumstances so i asked you some yes no question it's things like why have greater than fifty thousand examples in my training dana yes go this way if not go that way okay is this text based go this way my missing a gate ago that way and it'll help narrow down when the shimmering algorithm you're supposed to use ended up on the reason i mix
when all is that is skin seemed so overwhelming at first lady's role million she learning your rhythms that you anything i have all these organs how restless know what used when and where under the system is assistance deciding what to use in the final approach finally once we've decided that a handful of our rooms and we can use given the circumstances is actually just try them all the losses seal on the machine boeing engineers will do is built import from sight yet learned portents are flow just all the algorithms that can possibly be applied to their circumstance though
clean up the data will visualize the date of abuse and stuff with training they have both split into training donations has settled into that stuff in another episode bananas grow all are gay death room ten machine learning our rooms rome all parallel or cereal whatever and in the very end the file you'll see bill evaluate the performance of all the machinery now runs all the rights and code that determines how well he childbirth and did it compare them all to reach other and find a champion or champions for when the losers in may
keep the top three on hand as a continuing their programming eventually will sort come to a conclusion that one is clearly the winner this is the right now with a job or her role with that does not necessarily very clear what our room to use when it's like a three part approach we start the top where we decide what algorithms reason grossly applicable okay as supervisor or unsupervised learning situation why have the labels okay sit supervises it regression or classic
asian opiates classification now we have maybe two thirty algorithms that we can choose from bullets plot ortega let's look at the situation was look good environment work of constraints are we up against it at this point slow the difficult to memorize which our rooms work best given constraints and degas we usually do then is you look at this reference table or that psychic learned flow chart to help you pick a handful algorithms to try and they just throw them all against the wall you just shotgun approach all the zagreb
it's an evaluation metric the end of your script will tell you which one did best singular lot machine learning algorithms and use this approach to determine which aren't you getting a circumstance with that out of the way let's jump into the first of these to our rooms called support vector machines s. b. m. is a very we're we're i'll tell you why it's called an a bit less just try to understand intuition what does so like i said these these power tool machine lowering our rhythms like this is
sri support actor machines and naive days they can all use for both classification and regression so they're supervise learning shimmering our rhythms it can be used bowl for classification and regression also neural networks can be used for class issue regressions well you'll find that the primary use case of all of these are rhythms is classification animals this is true or not but it seems to me from my experience that classification is kind of the majority use case of machine learning that you'll see
in the wild hello this is true don't call me on that but you'll see that these machine running our loses power tools are paralegal for prosecution can be used for aggression because their primary use case is classification you'll see it the examples or the tutorials will all be showing you how to use them for classification and i thought i will be doing this episode and then you can know how to look at how to use them for regression on your own so support factor machines can be used for classification and regression when you use a support beckerman
gene for classification it's called a support vector classify your best b. c. and if you use it for aggression is calling support vector regrets are asked the war and bill broad category of these is called support vector machines so we're gonna go with the classification examples like i mentioned how works is it's determines a decision boundary decision mounted between your things over here in your things over there that sounds a lot like logistic regression is very similar to logistically correction
but gus and it's got some kirk's over logistic regression obituary minute but let's remember what a decision boundaries listed ahab all attacks on the left an old dogs on the right you graf of cats and dogs they're just dots on the graph right but imagine blue dots and red dots these are your date appointees are you training examples we have all the cattle ethanol dogs on the right we want to do is come of all lying in draw line in the stands between the katzman dogs so no dogs allowed over your left say the cats
so that line the separates you katherine dogs court decision boundary and now if you hadn't you can award to the next based on some features about the animal weather has whiskers doesn't bark how many lives does it have etc peas role of features will be used to determine where location only the object it's placed in three d. or forty space and if it's on the right side of the lying then it's a dog benefits on the left side of the wind and the cat that's your decision boundary and that looks a lot like him
logistic regression situation know what makes a supporter crushing differed from logistic regression in categorizing things over here over there he is this decision boundary specifically they support vector machine destinies align it doesn't drop in here finn lying between the two sets like logistic russian guys instead it tries to make that lying as fast as it possibly can it makes all wall doesn't use a one point why in the use
is the sixteen points brush stroke how fact is this wall well the borders on the wall on top against the innermost cats and dogs so the right most edge of this decision boundary is gonna bump up against the left most dogs and you'll left most edge of the decision boundary wall bump up against the right most cats okay so that makes sense is that you just heard phil river or make a wall
between the two things these over here in those over there as white as you can before you catch them to do that with your training set of examples know why we do that why was logistic russian insufficient what was an hour winds official why do we know that line of the reason is because of a of a problem that we're beginning to if you trapped so called over fifteen overfishing is basically if i were to draw the line between cats and dogs i could try wrong actual
let's say that i had a cat's closer to the middle wolf i wasn't smart i might draw the line to accommodate backpack skolnick outlier something that doesn't really fit the bill of the majority of the day that i might skew the lying maybe i'll tell blind counterclockwise or clockwise little bit to accommodate for that one out lying packed in other words i didn't make the most ideal line possible you and me
we're humans we look in a cluster of doctor weir cluster bombs over there and in our minds we can drop of vertical line way down the center even if there is an outlying die we still have an intuition where that wine goes where's the best line that separates the two classes of that in the future fire at a new objects into the mix or go on the correct side logistic regression little bit sensitive to our wires and things like this in the skin cause aligned against him properly drawn and is
is called over fitting in extreme example of overseeing the magill line that goes right out of her cool and then it like squiggles outs like half circle to include that i'll wind happening keeps going imagine adweek created some wild function of paula no meals that allowed for that little squiggle out of the wild example overstating but in our particular situation where we're using logistic regression which is away your function will make him work with all i'm not porno mill
function i just tilting the line maybe counterclockwise or clockwise might cause some overfishing soul wide support vector machines do different than logistic regression in the case of coming up with a decision boundary between the cars on the left in the class on the right is it makes that decision boundary as fast as possible so that we can deal with these al wires no problem now the thickness of our line it's called a margin we want this fat
the line as possible we call this a larger margin classify are large margin seth ward for the thickness of alliance margin and in the word for the dots that are being bombed out against by this wine that we're trying to call support vectors that's why this thing is called a support vector machine support vectors it's gonna we're we're know why we'll call this i think we should call this algorithm bill but fat why in our rhythm and we should call
these dots that death that wine bumps up against we should call them bumping dots but no we call the algorithm of support that the machine or large margin crossfire and that these dots the dots that wine bump up against this they're called support vectors a vectors so it dawned on the u. clean graf that's why a player was what we're looking at is a bunch of doctor mcgrath you can think of them is a dot org point or you can think of them as an arrow pointing from the porch in two
that got you can be you can graft that arrow with functioned mathematically seeking represent these docks in another way and we call it a vector the vector is the line that points from the origin to be got that's why they're called support vectors there the factors that support drawing off that line okay all finding good sport actor machines seemed pretty simple looks like logistic regression with a foul wine and so skinny linus the only difference for
a well it's got one more little twist and this is where things start to get wild really we are if you ask me support vector machines only handle linear classification so does logistic regression bacon for some porno mills into the function in a situation nonlinear the battle of the less than ideal typically we typically if our situation is not only a year we move away from the when your class of fires into something more complex like a neural network for example so bold logistic regression
support vector machines are all in your class fires but here's the trick at schreck they can transform a a support vector machine into a non linear class afire in this truck is called the colonel's trick colonel katie bar any e. l. you'll see colonel's views quite commonly machine learning to vary we're dead they're very hard to understand i still haven't quite wrapped my head around them what i think it was a colonel
it's hello ports you into another dimension or this rose colored goggles that you put on an eight change the way everything looks case as a very strange to me give you an example the example who used from the um she learning with our book that all post inner resources section is that if you're looking at a graph of dots summer blue is some red okay this is what looks like you have updates blue circle of dots in the center and surrounding that is a
red circle of dots it's like a blue circle with a red border patrol dots k. it's not drawn on to the graphs to bunch of dots all that is clearly the nonlinear situation this isn't a bunch of cattle laughed and dogs on the right which is lee nearly etc bull by our decision boundary no this is a circle and a circle those are not separate all by lions however if those dots represented something can such
all week for example if we were looking at latitude and longitude and those dots represented say snow on peaks of mountains versus non snow latitude and longitude well that would really make sense as a way of looking at this would it we really care about is altitude how high up the mountain keep his hands latitude how far north and south we are those of the two characteristics that are more important rather than lou
longitude longitude doesn't help us all sofa we think about the problem different we get actually transform our situation into the new graf where dots or indeed lee nearly severed all all blue dot suddenly have become sort of rectangle on the left or right or top or bottom some sort of situation where we can actually jaw line between a bit shoe classes of dots okay so that's little bit we're let me think of another way of representing us
if you have two circles blue and red dots maybe you can think of them instead of any u. clay in space of exim why you can think of them in radiant way so this is what colonel dies of colonel what does it takes your degas this after looking at right now which is non linear league soccer ball and a transforms it into a new set of dimensions so you're looking at the latitude and longitude representation
of mountains in the world and we're trying to decide whether or not to have snow on the peaks in your scratching your chin eli collagen separate despite grab your hand might not come order come over here and hold your ground so you're looking at it from a different angle in baja okay look here from this angle things seem all that different so i'm colonel is just something that you multiply it your day nobody in or to transform it into a new dimension so i think of it is
this looking at the problem from a different angle i think it is like in the cell that all linked to the past i can refer you blow you blow on a fluke or you do samir trick and you move honey are now on the dark world to the jury get him home mahogany that you're in the white world you're the same place the whole world is really the same place everything is the same but you'll looking at different than it helps you to salt different hostels so you can transform of the circle world into
all lying world is old ones of criminals out there there's like regular basis function colonel pollen no mule colonel signori colonel to this whole bunch of colonels a whole bunch of of collared goggles that you can put on imagine a drawer full of collared goggles that you could put on a time but you have to know little about your situation get to know whether the date of the you're dealing with is sort of could be transformed into a different world so that it's easier to war
with so that is now linear least operable sell support vector machines very strange machine learning over the mind i really i need to be allowed to wrap my head around and i still don't know exactly when it's preferred t. be used under certain circumstances are not celeste listed for the top or more time with reference prior our rooms that we've used remember linear regression is a u. we gretchen algorithm for coming up with a number i'll put it okay if we want to classify something in the past we take linear regression into a
new function call logistic regression logistic regression is like using linear regression to classify things that cat or dog is ago on the left of the line or the right of the line not essentially logistic regression draws the line down the middle we call this the decision boundary this line that separates the cats from the dogs now with logistic regression unfortunately this line of maybe prone to overfishing based on i'll wire is there's a lot of day
yeah it's bad bad day that were just norway's or anything like this it could come escrow car line in my tool to its tilted down and counterclockwise or clockwise may not be the best fits that ideal loin street young center separating the katzman dogs cell we have this new algorithm for classifying things callie support vector machine and it uses a decision boundary is well that makes that decision boundary as fast as
of sulking larger margin it bumps up against the innermost thoughts on the left and right classes we call those innermost thoughts support vectors and that large margin helps us prevent overfishing future examples that step one of support vector machine is simply maybe a little bit more accurate little bit more efficient version of logistic regression you might consider it step two is this strange trip to the trade called the colonel
track at colonel trick lets you take your date at which maybe it represented nonlinear lead if you look at it like this transform its by putting on some goggles into a new dimension and now suddenly it is linear so you take a nonlinear dataset look at a different way and now it's linear we separate will also support actor machine it is a class of fire or can be used for aggression and it has the ability to represent non
when your circumstances the problem is like i said you have is this drawer of colonels goggles that help you look at situations from different angles was only so many of these is the alike circle world war real basis world is only so many ways to represent a nonlinear dataset you know linear fashion and you have to know which one to use it given the circumstances on like a neural network which is able to represent nonlinear situations completely on it so
on it will learn of the way to represent them nonlinear lady can represent any number of complex situations so support back to machines and neural networks are often compared to each other because herbal teas black box methods and they tumbled handle monitor situations the difference is that a neural network indeed learning is more powerful it can represent more nonlinear circumstances and you as the developer don't have to know in
that way is this situation nonlinear the neural network will learned that mapping for you where is the support vector machine you have to know sort of in what way does this circumstance nonlinear you don't necessarily have to know in advance you can just try throwing at all the colonel's in your drawer but you don't have that sort of upfront information that might be better to use on yeoman work anyway so why wouldn't you use a neural network well if you're sick joyce
you can be handled with a linear support back to machine okay vanilla support that the machine or you do know about the situation any can pop in what was colonels in two years we're back to machine then support after machines are all walk faster than neural networks there faster and they take up last memory and in fact you're gonna see that this is a very common recurring theme in machine learning like this said previously machine mining engineers they look at people who use steep learning as a silver bullet for
every situation they say you could do this faster with a dedicated shall learning algorithm for specific situations that call for the show learning our becomes so if your situation supports using his support actor machine then you'll be a lot worse speed and memory savings using it that's over a neural network will have to know little bit about your data said henri circumstances in advance to help you determine whether using support back to machine is for you or not so i kind of like to think
of the shimmering algorithms is you have this backpack whitman role playing game you have this backpack of tools you can use you have a grappling hook for certain circumstances you have your sword and shield yellow magic wands and so if you are presented with a puzzle okay so you need to kill a bad guy you'll use the sword and shield he needs you opened the entrance to a cave use the bomb well the neural networks and deep learning that come with a bazooka beetle sold any situation with the bazooka
but maybe it's overkill and expensive and can cause collateral damage so kill baghdad bazooka or treasure chest bazooka over cave entrance bazooka but why not use that sheep bombing case of the cave entrance when i just use your hands and see what comes over you treasure chest the way i think of support vector machines is like a gun and apollo soyuz space guy with a plastic reagan is coming for me and told it has to know when you use this bank and you try to figure out what's the best
over opening the door that's locked you can use a key you can use the bomb you can use your dues you got worried uses we're plastic reagan and a proper approach is to try at all of them try all of them and evaluate the performance of all of them at the end of your scripts determine which he did the best which took the least amount of memory the least amount of time was the most accurate model etc edges so happens that it turns out at sea in this particular case open the door the best logistic regret
and handled situation a the best by our state of support activities is we're reagan you cornet good door you shoot away sir comes out and nothing happens and how to turn around and some reba jaime soldier not using the regular basis colonels course that's why it's an open to his youthful module and you look at is the real basis connell buick guide you put it into your reagan and you should put it the word and you shoot an outcome the sonar circles woo woo woo the door opens any such
seeing that it was obvious now that wasn't support vector machines now let's move on to naive that baze class of fires naive at baze the scenic inference is a very interesting and important component of machine running in general that's the scene and friends really is a wrong that the latter of statistics like metal your previous sepsis statistics is the god mouth of machine running statistics is everything in machine learning there
basic principles of statistics with probability joy probability conditional probabilities cetera are used everywhere in machine running even if you don't know what many of the machine running our burdens that we've been discussing so far there are algorithms that come straight out of the statistics textbook we're logistic regression that statistics statistics is really essentially boil down into probability probability an inference that inference is based off of
probability and probability is sort of raul statistics so that our burdens that we've been learning so far are probability and ross is to six deep down inside the down under the hood they're just statistics but at the high level way we've been looking at them because look like machine learning our rooms accountable quite computer our rooms are complex mathematical equations yes they are they are indeed but there are truly fundamentally based on probability
now on aka teach you statistics in this high caste and i'm not a teacher probability but i am going to really quickly when you threw the basics of probability here in order help you understand how naive base crossfire's work because to understand how basie an inference that is naive days crossfire's how they worked you have to understand very basics of statistics like it said all the opposite we've been using thus far they use the test
the use them under the hood these them conceptually in principle will be easy an inference which is a class of fire supervise learning our room but also can be used for regression the scene inference is like raw statistics it's like using statistics in the wrong to handle machine learning circumstances statistics in the rock so be seen inference is really just broad true pure statistics in order to make an inference for estimate
about whether something is classified is this that or the other thing so let's try to understand probability a little bit probability probability is very simple it's the chances of something the likelihood of something of an event we call it an event what is the probability of getting it heads for a flip a coin well fifty percent fifty percent fifty fifty right it's one half the time it is heads and one half the time to just hales
so the probability of this event of flipping a coin is fifty percent okay so that's that one basic probability step two joints probability why are the chances of me getting eggheads first that flipping a coin again that banging heads up again with the chances what's the probability of a hand to be well it is simply the probability of a
winds the probability of the fifty percent times fifty percent that is points twenty five to the probability of heads and then heads again is the multiplication of the two witches points to five that is called the joint probability the probability of these things do we'd set to join the probability step three conditional probability and now we get into right proper statistics the good stuff the meat can
additional probability what is the probability that my second foot disney heads the f. the first flip was heads bombed that that's an interesting question that i don't see how the first flood has anything to the seventh what exactly as i flip a coin once heads tails with a fifty percent i flipped the coin again and get heads or tails that second foot has nothing to do
with the first foot the results of the first flip does not affect the results of the second flat those are what's called conditionally indeed pendant events independent because they do not depend on each other they do not affect each other independent events well there are some situations out there which are not in the pendant they are deep pendant so for example what is the likelihood of it's raining did
a given is cloudy outside ah now there is an interesting question is this cloudy outside then is let's say forty percent likely to rein in the probability of it raining depends on the probability of it being cloudy we call these conditionally de pendant events and is all called conditional probability conditional probability conditional probability
isn't it interesting thing is very useful and widely applicable machine learning as a mathematical formula okay probability rockabilly step one was just probability joins probability steps u. is probably times probability just multiply to conditional probability step three is this mathematical equation the probability of being given a bad is the possibility of rain given that it is cloudy outside
is the probability is a and b. the joint probability eight times be all over the probability of a okay so the probability that it is rainy given that this cloudy is equal to the probability that is rainy and cloudy over the probability that it is cloudy very strange very strange this seems kind of non intuitive amenable for some of mathematical formal
a little bit taft is 'cause he's won everything is in this puzzle was toppled the more papa called believes general probability the matter we have a big giant circle that represents whether it inside that circle or a bunch of little circles we have cloudy we have seventy we have rainy these are built based on observations on the past the number of times that the day's rainy is the number of times that we've seen it wayne last five years for example over
or the total amount of times we've seen whether at all okay so the way that we build up probabilities away we go look like we're the chances of getting cloudy at all this is that we look today's day after day after day and how the number of cloudy days and we divide that by tour lumber days with observed so let them make sense don't don't over think that it's been a rough times we've observed something over the total number of observations that cloudy days we have rabies we're sunny days now the joints probability of two events is
the number of times the overlap there is in the case of non independent events that are times over lots of an item we have cloudy days in rainy days in sunny days but say that it's cloudy forty percent of time and it's rainy thirty percent time the little sliver of overlap between jackson and little sliver very large chunk of the time they kind of ball fall move same day we have both a cloudy and a rainy day the same time that this joint probability that states cranes be george probability the number
the amount of overlap between the two weeks of the nine gramm and then conditional probability is a very interesting formula is very non into it doesn't make a whole lot says wartime visualize this as a bunch of circles and then diagrams begin to show probable you'd remember the question that we're asking is what are the chances that it's going to rain today if i know this cloudy today and formula says that the answer is the joint probability the amount of overlap between the two
and the number of times it rains and is clouding over the probability of being cloudy at all so the probability of a and b. over the probability of a soviet we have thirty three steps so far we have basic probability and we build that just by observing things over time they quince looks familiar times new ability to base of the fifty fifty we have joint probability which is the probability of two things
cole occurring and and we have come to shun all probability and that is the probability of something if we know something else and that my friends sounds a lot like fundamental machine learning right was the probability that raining given the biggest cloudy well it is cloudy days of feature a feature in our spreadsheet x. x. one and we're trying to determine is why whether or not it will rain today that looks a lot like glue
just aggression earlier aggression or any other algorithm the basic fundamental machine running out and we see this kind of the skeleton form of machine running so conditional probability is really cool war machine learning science carnival lot statistical formulation of the machine learning algorithm dish in all probability now the next and final step is called baze theorem days the day why indeed that's that is the namesake
for our algorithm here called a naive baze class of fire door the man long time ago named reverend thomas baze who's a statistician any learn a little trick of the trade when it comes to conditional probability of specifically if you know the other thing then the thing you want to know you can do a little river see on our conditional probability formula that's it that's always fear and it is it is using some statistics
you brought some public algebra and flipping staff to the other side the equation so if what we want to know is is it's cloudy and we do know that it is way meeks of the opposite the opposite of what what we're asking for well they're not the same thing very obviously they're not the same thing how likely is it for wayne if i know that it is cloudy well it is very likely to rain okay maybe less a forty percent maybe not that likely to forty percent for a fifty percent white
tour way that if it is cloudy outside well how likely is it to be cloudy this is raining all old whole really differ number now we're talkin' like many ninety five percent have you seen rained on a sunny day yes so i on a blue moon is substantially more likely to be cloudy if it is raining then it is to be raining if it's cloudy so conditional probabilities don't reverse been on the same thing that they're reversible there is a way to weave
first of them and that's called these theorem and base their looks like this the probability of a given be the case so i want to know the opposite we're equals the probability of being given eight times the probability is a tough all over the probability of the what the heck i'm not in explain where this comes from you'd have to learn based here in an era where all this and statistic say wait a spirit is a very fundamental component of statistics proper
you'll learn base there in one of us earlier chapters of your statistics textbook or the comic had recourse does not specific to machine learning it's a very wrong final court component of statistics in general and all it does is it gives you the ability to ask the question of the other way around so why is it so fundamental that sounds like step three was so without our regular probably set one joint probably step to the joint depends on where the no
look i believe an conditional probability wishes step three that depends on a two and one so they they all what you learned in sequence 'cause they depend on each other thing that conditional probability is the crux of what we need to use statistics in the rock salt probable a stick machine learning situations yes that's true but very often the question isn't passed the way you wanted it to be asked the question is the other way around so
obeys the urine is using a conditional probability and you're reverse see on the equation so you can ask the right question okay so little of the crazy let's talk about an example using e-mail spam this weather and spam are the two most commonly used examples in understanding the scene and friends and in fact weather and span classification are two of the most common applications of naive days in the wild at least up until now and to recent times
when i think deep learning principles or use little bit more commonly in the space is naive days was the champion of whether production and spam classification for e-mails the way worse for e-mails is you break that your e-mails into all of the words of an e-mail was it we build up a dictionary of english words only for while all buried downwards and basic were cited by izzy and police stop words they are of course important in grammar and understanding sentences but they may not be as import
in just classifying e-mail us spam to withdraw the stop words and we keep the essential words we start to learn that that's certain words are commonly call occurring with stan e-mails versus non spam e-mail so for example the word viag rock is very often seen in spain and e-mails unless i get ahead of ourselves first off what we wanna do is just build up a database of cop hominid every word is again in the me
nail in general and how calm in that span is in general to wiggle out a day probability of the word niagara we don't look at probability were friend saturday's weekend every word and it's on ended a probability although whether or not and e-mail us spam with say it's highlights it's like sixty percent you know spam blessed that okay so we have a bunch of probabilities that step one real role probabilities step to risk it
because we used joy pyle believe in the equation of conditional probability we'll release it to rest the exocet three is conditional probability if i've got all these probabilities words and spam what is the probability of an e-mail that i'm looking at right now all being spans to straight out okay what sixty percent with marty said that all what's the probability of that e-mail being spam given it has the following words because it does in this circumstance
it has the following words by agra free act now etc okay well we will use the conditional probability formula it'll give us a number of us the probability of the thing being spam and we have to ask the question of differ way based on information we provided which is usually the case you'll use based here until the river see on the conditional probability formula analogous our answer these theorem now the specific algorithm for class again
she is actually called naive that baby's made these classes are wiser called naive well this'll level to which all the probabilities in our formal actually depend on each other because they give it as like a mexican standoff it's like what is the probability of this given this guy this guy the other guy all day all depend on each other's excite figure like three guys with guns each other in all of nature also probability of this given that guy wall probably of this guy depends on the problem that i know yet while the probability of again
and on this guy this back to the wrong kind of mutually code dependents they may even the parts of naive days cuts off the dependents of events from each other it makes things not penetrated and this makes the algorithm tractebel able to be computed within reasonable amount of time without that naive eighty parts of the recall the naive that assumption the algorithm would be to computational a difficult for modern machines to perform and sonar to use basis
you're him and conditional probability in the wilds for mushy learning applications that introduces naive idiot sunshine which severs the dependents of events from on each other that assumes that they're all independent events now as we did was superb actor machines let's compare are naive days too deep learning made days is commonly used in text based applications like i said spam classification of e-mails study based off of words in the e-mail we call this a
bag of words approach is called bag awards because you're not assessing grammar or how words relate to each other you cannot swift maggie's days away words relates to each other remember that would be dependent events that would not be independent therefore we would not be using it to be naive assumption if we're is related to each other in the grammatical structure they would depend on each other and our approach could not be naive sort
owing to assume that they don't depend on each other instead we're just about the whole out all the words of the e-mail and we're just a necktie keep our eye on trigger words which by agra what is the probability of an e-mail this spam given that the existence of the word viag rot so that's what's called a bag of words just all works just ramona bag handbags the naive days they were currents neural network which is an algorithm the walk into the few trap sowed
is it it's height of neural network so it's a type of deep learning algorithm that is very good at handling tax base applications as well so naive days and ricardo networks are commonly pitted against each other but unlike naive days which is a bag of words and the naive assumption that there are no relations between the words were correct neural networks which really read the email from left to rights top to bottom and they keep grammar in mines negating were
as modify the words a negate mean i think i'm never colonel not work as taking an e-mail printing it out and said it's a classy english gentleman who sits in his leather sofa and he has uptight there are well the city's this is a viable us not be too hasty because they also some amount of anson in this in this particular structural hit that you find that they use abbreviations more often than real worthwhile these abbreviations these below that on educated guess investing for more grout will of the tribes safe precious stay
so that and you know what edgewise i believe through full analysis of the doctors and well indeed dealing with spam but that the guy almost a day to come to this conclusion but comparisons naive basis in their full lose arms is that cigar is mouths agape awe is as niagara the only other information for colonel network looks up from the papers as though yes of course that back room maybe because the probability beings that of course because obviously he's always causes the naive base snatches the paper out of the colonel networks hands we
that says that the goddamn big spending has biographical need to know painting else so it's kind and memory or crucial ter application is things need to be fast and not consume a lot of memory the naive days is a preferable chilling algorithm to add more complex outgrew mike rucker know my work but if you need more accuracy and complexity in the analysis of the situation then or colonel mill workers more likely to be your guide this time it in memory or less of an issue for your particular situation and you
other have higher accuracy in a more complex modeling of the situation and then keep learning it is preferable to naive base blasting about e-mail spam classification you don't have all day to determine if an e-mail is stamped when somebody sends an e-mail the recipient expects to receive the e-mail in very short order or less a knoll more than one minutes all very powerful record neural networks on very powerful machines could probably do that i'm in it but i'm not so sure where's
naive these class of fire could snap his fingers and make a judgment blink of an eye so the case of e-mail span classification is there like that case indeed been a few days crossfire is preferred to or colonel network and is a prime example where we see a shallow learning algorithm maybe better or for a particular purpose then a deep learning you know deep learning is more accurate and complex and magical effect this particular situation above using the record no man or
forty mil classification they called us feel natural language processing been using or colonel now works with what's called work vectors were beginning to another episode the way that it represents these documents is as a point in vector space they can be compared to other documents it's actually very magical summer so that the spin of natural language application using in this type of technology is called natural language in understanding which indicates if you my structure mind so far that the machine may be on
standing in a fundamental way the meaning behind what classify as a document es amor not spam very interesting indeed so there you have that support actor machines and naive days and i do want to admit i don't understand these are rooms as much as the algorithms that i have presented to you thus far so this is probably one of my worse episodes would encourage you to go learn these are britain's offline which brings us to the resources section of course the andrew being caught
sarah course he has a weak bond support vector machines will lead to that show notes into being that does not cover naive baze class fires i found that very interesting actually because naive base casa fires that's what the fundamental algorithms a machine learning b. c. brought up over over over compared to more complex models like like neural networks and used in the wild today with great success will see it in most introductory machine and textbooks noise things
so i can issuing cover in a daze i actually found the video by intruding on youtube unos trying to learn eight days later on naive basically clearly came from his course he took it out at some point i think that he didn't want to bog down newcomers to machine learning with statistics because like i said to understand i am days classifies it understand base theorem to understand is fairly have to understand initial probability donors
initial pablo you have to understand statistics so the whole world of these ian methods if the world of statistics raw statistics and stats is a hard stuff my friends so it's important it's essential but i have a hunch that intruding decided bill get to that later i don't want to scare them away from the field yet if you don't need it to succeed right away you can start to lanier logistic regression and indeed die right into deep learning annual my work's skip
asked all the statistics staff buddy it is essential for you to know so i would encourage you to learn naive days crossfire's the machine learning with our book that'll put nissho notes is it has a great chapter on naive days crossfire's dawson of the great chapter on support actor missions as well in the mathematical decision making great courses series that i've reference from time to time also has a whole episode audio episode decay
it's naive dates so all put spend the show notes that would encourage you to try to learn the basics of these two algorithms offline 'cause like as seminal figure did a very good job of presenting them unfortunately i prepared met prepared by little bit out of my helmet for this episode and file an issue before had you choose which algorithm to use when you know your situation whether calls for supervise learning a classification and regression is sarah will be alone narrowed down gross
which algorithms to throw out and now you have in your hands twenty our rooms if you could possibly use for classification and in order decide which of these algorithms you she used you assess your day yes so for example naive days works well with categorical dana and missing dana something many other mushy learning algorithms do not work well with this missing a gate at the day's work stay okay with missing and county examples the avenue training dataset we're working with tax or numbers cetera using these types of questions will help you narrow down even for
they're in order to do that i'm going to link to the table of pros and cons of various algorithms honors various situations and the decision tree put out by the psychic learned project for choosing an algorithm giving various circumstances in your problem and from there once you've got five algorithms to use skinny hands and you still don't know which of these five is best use you just try 'em all the trial all you see which one which one has the highest performance based on some of our huish
metrics in the next episode i'm going to be talking about some of more miscellaneous mushy learning algorithms things that are very dedicated so the last three organs i thought about decision tree is support back to machines and naive days crossfire's these are all very general purpose power tool machine learning our ribbons all three of these to basically be swapped with each other in and knowing which one goes where little bit difficult by the next episode the machine running out of line would be presenting to you are very specifically to high
eight two very specific use cases so it'll be a little bit easier logos apple sources that stomach easy for you to decide that yes you should uses are ruined because situations a or b. i'm going to be doing these episodes now every other weekend of become quite busy recently i apologize to run every week elder other rid himself so i'll see you two weekends from now


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. d. but i'm also starting a new contacts which could use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com for words slash l. l. h. this is episode fourteen shall earning algorithms periphery this is the third and final of the show
oh learning algorithms episode in this episode will actually be quite easy by comparison to the last two gallons of recovering this episode of our recommend nurse systems anomaly detection systems and marcus chains that commander systems anomaly detection systems are extremely easy to understand sol star with those they're also very specifically applied so you knocking you seize our rhythms may be all over the place like we did with last few algorithms namely support back to machines now
these days and decision trees have a lot of applications in this difficult to know when to use which where these algorithms recommend her systems anomaly attention are used specifically in those applications recommending things and detecting anomalies but actually start with anomaly detection excess even the easiest understands server software which you use an anomaly detection for for example fraud detection in credit card spacewalk figaro when something went far
awry with your credit card they wanna find some sort of transaction that didn't look typical it looked and it's it's not your typical spending habits or its overseas when you made most of your purchases today in america the way credit card fraud programs work or by using an anomaly detection systems they may indeed use the algorithm the envoy to be explaining in this episode war they've probably in my opinion moved on to deep warning think a lot of big corporations
has moved on to the blurring for much of their machine learning process another application both anomaly detection is fearing affordable wanna server is acting out visit spiking in seek you and ran usage swapped slop space now working etc it could be the case that maybe it got a very big loaded got asked to perform a task that is very heavy indeed mrs not anomalous and so implementing an anomaly detection system can see through that and figuring out when something is in d.
the comments of that a systems administrator could pay attention to this anomalous server activity so anomaly detection to have white application and real quick pak unexplained you how they work without getting into the details and that is simply to find an al wire on all bell curve that it is so simple you have a bell curve of typical behavior whether it's credit card transactions or server activity in a cluster is something in acts
and i'll wire something where were the right over here on the bell curve or wayward left over there then you have an anomaly that said let's break this down a bit first off let's describe what a bell curve that it's even if you haven't begun learning statistics on your quest still learning machine learning and you will as some point because statistics is the god math of machine learning i'm still certain that you have seen the bell curve it is the most fundamental component of statistics probably one of the most fundamental it
lesions on the universe defiant soul bull's eye may you're awful lot of mathematicians feathers here but in my humble opinion i think that the bell curve should be the symbol of mathematics is very likely already the symbol of statistics but i think it should be the symbol of mathematics proper so for example fewer represent various branches of science with different pictures for chemistry you might use of beaker little potion symbol filled with green liquid in bottles
or physics you use the adam with all the electrons swirling around the easiest image used for like adam the taxpayer for example for reacts the white development framework you know that someone talking about it's that adamson bullets used to represent physics inmate in many places maybe biology might be a cell but math we think emasculate ok what kind of picture by you you probably think of the eagles and c. squared my friends i propose that the symbol of mathematics be the bell curve the bell
curve or also known as the galaxy in the distribution or a normal distribution i believe it's called normal because it's the most common distribution of all and let me explain what a distribution is and distribution is how we were diego falls onto a graph so we've been working with david innes by cast series for example we've been talking about housing costs in portland oregon if you're throw your spreadsheet both houses just throw them all intergraph
by cost you a good bell curve you see that there is sort of an average cost of how was it swing the center and and things start to move away from a year on the high or rommel whoa soul you have fewer very cheap houses and fewer very expensive houses and most houses of some more in the middle soloway that this falls on a graph is you have your average the average cost of all your houses is the center of this bell is called bell curve this is shaped like a bell coming in from the left
it's slopes up and it keeps at the top rounds out comes back down slopes to the right thing goes off to infinity looks like a bell standing up so the center is your average this is called the mena your average of all the prices of houses in portland oregon and the reason is soaked whole reason is that he of your grasp on why access that is the number of samples taken from this distribution that's the number of houses that war of the average price
so so naturally it's the highest as you go off towards the left the slope down you start to reach the very cheap out wires houses that costs all lot less than the average and if you slow to the right you start to reach the very expensive al wires house is a cost a lot more than meets the meanest determines the central location of this bell curve and what's called the standard deviation determined the width of the bell curve so if your date it is why
legally disbursed if there is a huge range in housing costs a lot of very wide bell curve why it in short will have a very large standard deviation if you have a small range of costs so let's say the average is two hundred thousand dollars and nothing goes about to fifteen or plunder one fifty that'll be as skinny bell curve that's paul an uneasy of a small standard deviation so that's understanding the bell curve it's a very simple distribution
again so distribution means how we were gay it all falls on to the graph that soda distribution is a taking a sample of your distribution is taking one little house out of your day yet been looking at it and looking at the price okay so i sampled the d. n. a. in this house is a hundred and seventy two thousand dollars says taking a sample of your day at the house that i took its core random variable you are always words when you're working with statistics be they have always terms first off we are
working with distributions so probability distribution as all sorts of probability distributions out there i said the most common it is the galaxy and distribution within normal distribution aka the bell curve there is also things like the burn newly distribution or the multi new redistribution within normal distribution etc different graphs they look different depending on how the day that is formed and like they said i think this is one of the most magic
old and comment functions of the universe that you'll see is actually a mathematical function you can actually say like f. of x. equals and in this big mathematical equation on the right i'm not analyst now for you it's actually quite complex you would've isabel korea like okay the most common distribution the universe is gonna have a very simple formal it's actually pretty complex formula will see it in your dealings with statistics but anything that falls outside of the norm k. that's why we have this word the norm falls outside of our normal
distribution over here on the left her over here on the right very very very expensive houses on sock a million two million our house or you're on the left and the house it costs one dollar okay let her out wires big al wires really wants to get pasture standard deviation you typical waits on the left and right you didn't al wire territory but when it comes to the anomaly detection algorithm were more concerned with mate al liars things that fall really far on the right or left side is the system worked
oh you just build up a graph you are normal distribution representing your day that sold all how to servers typically accuracy few years common network usage years common disk usage and build up a normal distribution now the normal distribution but explaining so far is in two dimensional you got an x. and y. access by u. typically gonna be working with multi dimensional degas and it's hard to envision a multi dimensional normal distribution was a three d. looks little bit like a hill
once again the fortieth little bit you can really envision that bye birdie working with multi dimensional dana hands you just simply cannot find our wires now people earning bit of anomaly detection it is this little variable you learn called epsilon at epsilon basically is the fresh hold the you're going to learn what he had a small threshold earl large special would basically determines how sensitive you or or system is one working with al liars
and why he'll do what is your train your algorithm all i'm a normal stuff stuff it's not anomaly an annual tested against anomalies so for example they say that your training gail consist almost entirely of your native examples and then you're positive examples will be split mostly between your validation your tests at the reason for this is we want a system to be able to learn what's typical of your dana and then you're gonna basically be using your pal
wires to fine tune the hyper parameter of the anomaly detection algorithm which is called your capsule one variable and that's a it's so simple honestly excellent i've talked to long's i don't worry over think this it's so simple all is is coming up with a bell curve in one two three dimensions are many in figuring out what is mate al leiter it's a learning process is learning where is your fresh cold how sensitive or you to al liars so for example let's think about
the classroom setting where you have a bell curve of grades were the average grady is a b. b. is so baby is you are mean and so that's the number where on your graf on the x. axis the peak of your bell curve it is so very top of the bell curve you cover cool i'm right down the center and that number is the big off to the right you got a bunch of a b. plus a minus they ate was so people were getting a plus or on the far to the right for
apple in this smooth curve connecting to armenia and coming down to the left i'm going towards people who are getting b. minus c. plus c. c. minus indeed now let's say that that's basic where bell curve and we have one student who got an f. that's two falls very far outside the typical population of students therefore that student is more than just an hour wire the student is an anomaly as far as the teacher is concerned and so the teacher needs to go talk
festooned see you know what's going wrong and how committee to help the students that are so something that falls basically doesn't look anything like the rest of our population of pulled up outside that bell curve in a significant way that's the anomaly detection system is simple statistics gas in distribution so let's move on to recommend her systems recommend assistance or one of the most common applications machine running in the industry and i think they were one of the earliest applications of machine learning this
specially indeed commerce an online companies direct manner systems are exactly we think they are so for example amazon pandora spot five catholics at all recommends to you a product or a song or a movie based on things that you've liked in the past that basically ads if you think about it this isn't an system we call them rectum enter systems and there's two very common flavors of recommended systems one is called contents filtering
and another is called collaborative filtering so too common recommend ursus stomach algorithms and they're very very related to these the same underlying principles which will get into an addict in order to show you how all of these things are different and similar i'm going to use to popular internet radio cox one is called pandora and others called spawn a fly now by the way i believe this spot by his gotten a little bit more complex with the way they
work so what i made the misrepresenting the way the spawn of fireworks this is just how i understand spot fight to work so pretend that it works this way bear with me let's start with pandora pandora internet radio pandora he is a web sites where you can listen to some music and even like or dislike some songs i will play songs that are similar to things that you've liked and dissimilar to things that you don't like when you
go on to pandora and we first started says see the song or or it's like what gimme something mr west would you like and i say i like ryan adams he's my personal favorite musician is is okay i'll give you a few right out of songs listen to that three minutes another three minutes man with the set the second or third song comes on it is a new song says you're based on fact you like mine and i think you'll like orion or even brothers took place a new song for me make a juicy either thoms opted for thoms day
on it's hands as you found your way through the songs pandora learns your musical provinces it's got this thing called the music genome project's the idea being that all these workers at pandora have determined to for every single song it in the pandora database theories attributes about these songs things like male versus female vocals is instrumentals there some bases air guitar with genre of music is is what sub genre of music is
as an eighth tag all the songs the database with various tax when you like or dislike of song it's sort of frozen these tags to the dislike pool and these tags into the light pole it comes up with the sort of them diagram conceptually of the types of songs eli to listen to this now works underneath the hood the way it functionally works is by way of this thing called content filtering in content filtering your gun all of this is very easy understand because you're you know the algorithm the beauty use for content filtering
one of the algorithms we learned in the very early days of this series let me give you a hint what we're trying to do here is determined us or for a song in the whole day basis songs based upon your personal user preferences score is a numbered usually wearied working with rather manner systems like an athlete you might give a movie on anywhere between one and a five star rating solace for between one and five of the number it's numerical it is we grew ration
the case of an or it's a thumbs up or thumbs down to look look more like classification but i imagine on her leg pain or probably does some for conversion of your thumb to a score for example so we're working with the numbers were trying to protect the number that would go along with this song in the database given your use your preferences that you have built up over time given your fade out parameters this my friends is linear regression it's just like china per day
it cost of the house we've looked at hundreds of houses and we've come up with a bunch of faded parameters for protecting the numerical out what it's about when you throw in a new house admits it's ever seen before but it can look at the number bedrooms number back in the square foot is this is downtown etc on moines to guess i hundred seventy five thousand dollars using my fate parameters of this linear aggression a model so content filtering recommend her systems work exactly the same as linear rig rash
we've thrown in new song to the mix and i've already learned all these theta parameters by a thumbing my way through the songs this new song gets thrown into the linear regression algorithm an owl calms a score on a scale from one to five or zero to one or whenever i guess as to whether the usual like this song or not so it is linear regression there are some very seldom princes a you'll see we take a tangerine course between eared russian and content filtering but it's
so why are so they have at the first algorithm you ever learn for machine running when you're regression is used for recommend ursus sons also content filtering now there's a problem here the problem is this thing called the music genome project the music genome project is this process where workers that pandora worked hacking at every single song in the database with various tags hundreds of tanks are thinking of thousands of tags tiger represent various features remember features were you were
in one form or part of a machine running on the songs that is very time consuming and it is often very difficult to do you if you have the time on your hands you have to listen to each song one at a time when you have to have a lot of musical knowledge we have to know how to apply these features songs what you're looking now for usually we don't have this kind of time nor the expertise for tagging every piece of content in the database and especially as new content
yeah it's a baby's cry silly constantly in fact i do not think that this music genome project that pandora is being applied manually anymore at all i believe that pandora's unit radio is using a deep learning and audio analysis machine learning algorithm to determine what the jurors are associated with new songs as they come in and although tag them with these features i'll think there's any human listeners there anymore and leave their robot listeners new
saying it deep learning algorithms less crittenden you can't even do that or this too difficult for you to the new circumstances so for example for example cameras on his new products are being corsa we added to the database you relate to hide them with features why you're interested in or what a user might be interested in is what other users have looked at were white or purchase so this is to my understanding how spot of fireworks be competitor to
and your internet radio like acid don't call me on it they may have changed their algorithm sense by the way i understand spotlight to work is what's called a call labyrinth of filtering it is users are also liked so not similar songs based on various features about songs but users are also liked an amazon and that flakes and in fact alice say the majority of wright commander systems out there use call aberdeen filter
and rather than content filtering users also liked rather than a similar items have a call abusers also liked he is to determine similarity between a products or songs or movies it is indeed trying to figure out similarity between products to recommend to you but it is not by way of attributes of the product itself but rather by finding users who were similar to your purchasing preferences and
earring out what they also liked to you there's also liked is the call labyrinth filtering algorithm and i'm gonna cut can waive my way through this hour room you'll learn the details may intruding course but the way it works is really kind of nifty is linear regression and yet again bites you don't have features of the products to work with in building not be worth a vector for each user you don't have the features you don't have those numbers and
so we you're going to build up those features yourself to honest which examples to movies math works for example to make this little bit clearer to describe let's say that user k. is intercity dan fantasy movies maybe some side by side he doesn't like romance he loves action in user be blows all romance in the finger is that like action hates fantasy inside fact they so we get the screech you opposite users and in the database for revues be content filtering recommend or how
over them all the data base would be pre populated with how much fantasy and how much romance and how much thriller is associated with each movie but since we're moving on to call operative filtering the rectum enters systems we'll have those features every movie is a blank slate is an empty vector of features now we don't even know what to call these features we're we don't know that we're necessarily can be learning the amount of sight fire the amount of thriller were just in al okay let's eight
and features for every movie and when you sergei rates movie k. and user aways movie b. and we see the movie d. we start to figure out the common theme of what this user likes so we can start to build up a numerical representation for each movie factor is actually linear regression in the reverse we're using basically the movies are learning their fate of planners from users scoring system then we're going to use those learned features two
and guess what users will like so then we switch again the other direction what we have is a linear regression flipping sides over and over users great movies from that movies learn their features from that you will recommend movies users break those recommendations from that movie learned his features et cetera so we have this infinity loop of linear regression between the movie
use and the users is really pretty magical it's really cool as an incentive loop of linear regression so you might imagine just cause so we'll be in code you have this running loop of two linear regression models that's also actually what you can do is you can put both linear regression miles into the same equation little trick or you can put two linear regression son to wander into one mathematical formula and now you have one
that model and that is your call operative filtering recommend her system and there you have africa manner systems so true spins of wright commander systems one is called content filtering and that's the preferred one to use when you can and do how of features hide your contact the database do you have a bunch of information associate with each book or movie or product yes use content filtering it he'll have that information anyone all learned that information
by way of your users rating were purchasing worry ninjas viewing products you can use this twist sis system called clobbered of filtering which is basically like an infinite luke linear regression between the contents and the users as best thought of in my opinion it has user is also liked so content filtering is similar items clobbered filtering is users also liked out all the above ball from these are intended to be sim
we're items they just work off of different principles beloved about the name the reasoning court content filtering is because you have features on the contents itself the content itself has features in using that to filter down things that users will like content filtering worth call labyrinth filtering is your content is call lab worrying with the users and users are cooperating with each other to help make recommendations recommend ursus and very simple now i wanna make a
distinction clear we talked about an algorithm of prior episode called a market basket algorithm or an op for yuri oliver them for determining what items gold typically with what bothered by it and so for example of the earnest or anew by marshmallows and graham crackers what she you also by with that well the answer is hershey's chocolate this is different fromm recommend her systems or recommend nurse system finds similar items in other words if i like
hershey's chocolate and i'd also like a snickers bar or kit kat bar it's basically something to replace what you have in your hands i ate what i had in my hand i liked what i have my hand give me another one of those something similar to that we're as market basket or opry worry algorithms are for finding things to go with what you have i bought diapers i also want beer and finally i'm going to explain an algorithm called markoff chain
it's an update arkady kol the and this maze seem whole ill that we're in and out of place if you are a machine learning engineer and you're familiar with all the education omits your religious cult listening to this fond including markoff chains here may seem all that we're one is not one of those while one machine learning algorithms that you typically get exposed to it actually will seldom see exposed to lay her down the pike especially when you're starting to dive into reinforcement wow
ming territory doesn't necessarily so there you can use marcus chains in various aspects of mathematical decision making babies little bit more common like operations research for example or or control theory but it is a machine learning algorithms you may see it from time to time but you're not very likely to see it as a newcomer to machine learning is not the wa no one algorithm so why am i explaining it now the reason i'm explaining is because it is brought up all
that's from time to time you're going to see markoff chains and monte carlo war monte carlo marcus chains and c. n. c. you can see them these words thrown around all over the place stands for me personally went for started getting into machine learning and i never saw these are rhythms described in my books or my videos or intervene or anything i fell like alice missing something was very curious what marcus chains or at least in principle so i'm explaining marcus change religious to kind of ease your mind explain some
the arena c. from time to time but tell you now that you're not going to be using marcus chains anytime soon you're going to probably see this stuff once you get to reinforce that learning is the court all room of artificial intelligence it's the base the foundation of reinforcement learning upon which you'll use deepening principles of stuff to go deeper and deeper but markoff chains or the linear regression of reinforcement learning so let's explain how
they work on real dramatic here bear with me imagine you're playing a video game and you're a barbarian and you have just stumbled on the last box in his way or the dark lord this flames and darkness everywhere the dark lord turns to face you with his flaming demonic eyes he says calmly bureau and you grin darkly you buckle down any star running at heathrow us fireballs and dark spells it you when you got lofton like casting gas leak
shadows in your wake he made one giant launch a new gown off the ground leaping into the air when she seemed a great soared from your back twirling in your hands locking in a place coming down for a blow the dark lord golf roddy spring up his upon for defense and fleas i'm the narrator of this game and i stepped in and is seen as a barbarian dark lord come with me maples turn to maine and follow me into the game room the game room where i have them both sit down a dark lord
apparently used to sit down and i use my laser pointer to points to the scene that they were just in the barbieri it is suspended in the air with his great sort coming down for a blow the dark lord is bringing his on again from the left to block deflect the blow as a dark colored waiting is to happen here and says o. r. don't know what is complicated yes yes yes but what's the probability of the barbarian landing noble whilst i mean you have your arm coming into molested a block he's over here with his project
reed and his velocity all this the dark lord scratches his chin he looks the barbieri the barbieri mugs and shrubs and he looks back at the scene and he's decisive assess the history of this seed well he was well he jumped through the air after you is dodging left and right from my magic spells i come off as a dark lord dark or the magic spells don't matter that's all bygones at this point he's in the air with his sword common down a u. does it matter that he dodged your fireballs and your dark spells
in fact dark lord does anything in history matter op and to this point the dark wood scratching his chin and he's looking at the picture and as a slate blue trajectory showing the path that the barbarian had taken up and to this point it barks bruni error in his jump trajectory comes off the ground zigzags worries dodge those spells and stops where he'd entered the room surely that trajectory is important the dark lord's
is that if our look at this blue line like a few days common as a no no no dark colored nevermind blue line history doesn't matter up into the sport all that matters is and now all that matters is now this is called the markoff principal while male explained the players history up until this point that has made clear barbarian is absolutely essential for this battle the player has will not gain in strength and agility fought various bad guy
gained experience both as a player and it has a role played a barbarian he learned some spells he's earned some weapons and armor so the players history certainly should make a difference in the outcome of this battle but we can boil all black history down into a handful of numbers will level strength agility experience et cetera in other words we keep you race history if we know the presents or rather we can
boil all of history down into the present this is the markoff principle of play the mark of principal says history is irrelevant for determining the future and in certain models in certain circumstances and this may be just one now that all seems fine then you let the players history is condensed down into their level strength agility what about this trajectory that is sending the player down onto the dark lord to deal is strong blow does
that depend on this sort of blue line sri sing them through this dark lord slayer leaping through the air and on to the dark lord told the steps in history matter yes they do but they can all be boiled down into a handful of variables all the presents as well so maginnis blew my mind hoechst get sucked into the user flashing blue lights and now you have an arrow pointing downward quarter of the dark lord coming out this or wrappers
named trajectory and velocity of course we also have those variables strength agility local etc so you can boil all of the history in this video game including the wax fuel milliseconds down into the present all lay the presents trajectory and velocity acceleration strength agility center a handful variables that represents the present state we call it states the dark lord has some state funds
off he has the trajectory his own arm and how strong is he can he make a good defense those other things going on in the environment may be the dark lord has some minions are currently shooting arrows at hero so the history only matters is so far it can as it can be boiled down into the present that is the mark of principal at play the market principal says that the next the market principal says that the next state only depends on the current state so the next day depends on the current state
and in that state in a mark up chain is a series of probabilities a series of probabilities chaining their way through an action sequence self words apple in our current state the state being comprise the love the users lovell where he is acceleration velocity trajectory etc combine that with the dark lord's position his arms trajectory and velocity the configuration of minions all those things that's the current state the next state can be too determined by the
current states and some probabilities let's say that given up all that information the probability that he delivered a blow is eighty percent probability that he is blocked by a the dark lord is ten percent and probability that he misses five percent in the probability it's club to buy a minions arrow is another five percent so we can use these probabilities these percentages to determine how likely is for each possible next state we're in the current state imagine trying circle around
our current state and trying to forty arose out of that current state of the number next to the arrow is the probability of it happening and the circle in that next stay with your points is the state itself swell onex state being delivered a blow another next state being cooked by a narrow another being blocked by the dark lord another being myths are marked off chain then it is a sequence of actions in time you start in the beginning of the video game yeah some probabilities
of maybe one left arm right going forward improbably few and far into going forward again we're waiting there if you go forward again i'm probably picking up the sore or you can ignore the sort of sequence of actions all pointing to each other and leading eventually to the end these actions can look back on each other what's called a cycle which makes the markoff chain of a graph the grass and computer science you can see these alot of graff is a sequence of things that can look back on them so
folks if there is no such luke is called a tree decided decision tree from a prior episode remember i explained decision tree similarly you have a circle and out of that calmed somewhat arrows point to next circles this decision tree goes like this is my mother's in town we go over here okay if she's hungry we'd go to this restaurant if she's nonchalance and drink to these bars okay i've she's in the mood for this to that bar few you this'll about arthur decision tree looks very similar to a market chain is some difference is a marked off che
as a graph c. to have cycles and accordingly a mark of chain is basically a case sequence of probably stick actions in time it's a process the system are running loop of action sequences if you well the decision sri looks like a process or a system but it's not really you input some input to malcolm's an outlet it's a regression algorithm or classification our remaining comes you're employed snap your fingers do the decision tree out comes a number
reclassification where the markoff chain or a markoff decision process as you'll see in reinforcement learning is a sort of running loop sequence of actions is something that may determine the actions taken by an artificially intelligent agent for example in our video game analogy or may describe the action sequence taken by customers or components in the system so it's more of like a time based system or process sequence of actions
we're the decision tree is a symbol algorithm for computing it out but somehow i asked the dark lord again so what happens next uses are now why y'all given this current state with a trajectory and velocity of this or he's going to deliver the book with probability of eighty percent that's this morning for an old answer i say yes dark lord good job now that the market's changed when they do one more example this example comes out of the mathematical decision making great course
a series that i recommend over over that goes like this let's say that you restaurant is mauled by a markoff chain the group or an individual or whatever can enter the restaurant case a tougher states the next state is they are waiting for table you have a fifty percent probability of moving on from there to being seated by the hostess or a fifty percent probability of holding back on to yourself in other words you're still waiting okay so we enter this markoff chain and
we may be going back and back and back to back into our current state which is simply waiting then eventually we'll leave that state with probability fifty percent to basically is cut like that crime it takes to leave the state so like the time it takes to stop waiting in to get seated cotton is based on the probability is if let's say ninety nine percent probability staying in the waiting period in your way for very long time when one percent chance of acceding mending see it is fifty fifty then you will wait for very long
it's the air table you're in a new states the three you can order or are you still need time besides so you have a loop back to your current state you back to back you look back to try to decide and finally decided so you call over the waiter and you wore that brings into new state of waiting for food etc so you've got this graph of circles representing states that you could possibly be in at the restaurant with probabilities also at saving your state into a new state that's all market chain the market
channel of the different than the other machine running out of them zewe seen so far in that it represents the sequence of actions that's what marcus chained guys now we've seen near aggression logistic regression the selleck snapping your fingers and coming up with an estimate summary says what's this house can cost any step two fingers and you say eight hundred seven five thousand dollars next those are different than a mark of chain which is new and turkey system the system or a sequence of actions you take
areas actions depending on the probability of taking that action in a case system and you navigate your way through the system from our craft chain now marcus chaining can be it represented a dna matrix form a markoff matrix and that's just the mathematical convenience for representing the chain the chain that i was describing what circles and arrows that's for visualization purposes it makes sense to look at on a piece of paper for us the way you actually were
isn't this mathematically order computers with a matrix just like almost everything with machine learning the way we represent most things in machinery mathematically isn't matrix form that's why linear algebra so important now you might use the markoff chain him the operations researcher mathematical decision-making sort of to come up with an assessment of how long it takes for user to maybe leave the rash on or where is the bottleneck in your restaurant system where it
a fine users staying the most well maybe we want them to leave or where we find a state where are they haven't yet paid for their meal and the probability of leaving so the next eighteen weaver is higher than we want we can use them are cough chained to sort of model assistant incredulous acid figuring out what's going wrong figure out where bottlenecks are not we come up with these probabilities not first place while we do that just through
observing degas we have years and years and years of users coming in and doing various things alice pretend over there in the chorus and statistician and he's riding on a clipboard every time a user leave the table or every time i use orders food and so eventually can come up with some sort of probability of a thing happening next given the current state again the thing that makes us mark who vienna thing that makes a market system is that no current state depends on
prior states we have all prior state information is encapsulated in the current state the fact that i'm sitting at table must necessarily mean that i waited at some point for table and i can't hurt the restaurant all prior states all the history can be boiled down into the present so that some argus chain dallas go back to the game room with the dark lord we asked the dark lord is this player going to win the dark lord looks at you will to the barbarian
you look back you sir i don't know that's to load of a question as to what incredible reporters you cannot analytically use this market chained to come up with a simple numerical i'll put probability of the user winning when there's too many variables that play in a more coughed chain is said and using some mathematical formula to snap our fingers and come up with the judgment we use what's called a monte carlo simulation monte carlo marcus chain ought to carlos basically just
earning a series of simulations all were no word over until we get sort of a it's what's called an expectation unexpected valued seal lot of us were needed to reinforce that learning learning simulations over over over into you get what you think to be the most probable cancer because in mathematics way you are solving an equation worth system of equations that we're fingers coming up with a judgment because saw an equation analytically analytically so when you can basically calm up with a mathematical formula
that takes an input now comes out when you can just add two fingers and bone you're doing something analytically will when something is too complex to do an olympic lay it then we might try different trudges through my trial like durations for loops for example in this case in the case law monte carlo simulation we're going to basically run simulations of this clay are playing a video game over and over and over and over and over until we basically start to confer
urge converge they call it when you start to see a pattern on to what we think is going to be expected value the expected value being the final score what is the average final score that i'm starting to see it over time when i'm simulating a running the user through this video game okay so monte carlo markets chain let's take it for the top here is how it happens the user enters the dark lord's way or the dark lord times around and said
call me hero and a barbarian bottles down with a dark when when she says great sort whirled around his hand walks in the place and then five hundred thousand ghost images of the barbarian ryan in different directions when those lifelong goes right on before another is already dodging left dodging right dodging left gets hit by a fireball and diets dissipates into the air the other one is jumping into the air yet another was blocking the fireball with us
she'll call five hundred thousand simulations ghostly heroes are running through this scene killing the dark lord being killed by the dark lord missing getting caught by a narrow kill kill by guy killed him to that moment and finally we succeed a score of seven nine three to six eight that seems to be the confer urged expected value of this monte carlo marcus chain simulation so the chain the markoff changes in five minutes
eat what happens next all probability of point nine this standpoint one that monte carlo part is running for a bunch of simulations the oslo the complex the way basically a one of these ghost heroes ones to stay two and our guest this i'd see him run to state three four five well we take this thing with the highest probability with that probability and sometimes we take the things of lower probabilities this is kind that monte carlo process this whole system to this there's this algorithm cold
topless hastings algorithm random walk etc but basically that's and c. and c. monte carlo mark of chains in a nutshell is that you have the chain or a graph of states and neck states are all determined probable was thickly given your current state are running through that whole series with simulations is the monte carlo bit now like i said you're going to be seeing this mark hoffer chain in a reinforcement learning and or official intelligence
when i see that they're all walks there is called a markoff besiege and process them that the key and this might be for example the algorithm that sits in the core of the brain of the dark lord the dark lord is the artificial intelligence that is fighting you you were the player the dark lord is that ai this fighting you it's not again if i knew if the dark lord big game is basically the physics system the rules hands of the markoff chain
the thing that says given state wanna what is the probability of being in what is the probability transitioning to two three or four and the dark lord needs to act upon this decision given probabilities groomed for men what is the best next action to take to the area and m. c. and see him and the resources section of this episode it's just gonna be all linked to week nine of entertains corsair course on anomaly detection and recommend nurses sounds very simple and at kansas
theories on shallow learning algorithms the next episode is going to be the most boring episode of all is going to be basically the parts of machine learning algorithms things like performance evaluation underfunding and over fitting regular is asian and all that really boring stuff it's all on fortune only essential and machine learning your alertness of actually really early on elsewhere like in asia
being a series i've decided that point on it for awhile because it's just so boring way yes they have dessert first and vegetables later by olga that's over with animal move on to the fun stuff all seen as that


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and algae and get the time also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks the sum which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode fifteen performance this episode is going to be the last of the
the six machine learning my friends at this point you have now gone through all the very fundamental introductory components of machine learning a shallow learning the basics of deep learning a little bit of the math and technology and finally this episode we're not talk about performance evaluation and performance improvements e-mail heard about things like hide bias and hide variants or fitting in under fitting regular is asian men accuracy fusion major sees all these things
this is going to be a very boring episode unfortunately i apologize your wanted coffee for this episode let's just get it over with and the deep end of this episode you will be an official machine learning power want my friends you have come far applied yourself so let's do this episode see you can call yourself a machine manner in this episode we're gonna be talking about performance performance evaluation and performance improvements no performance is basically greeting at your mushy
learning algorithms how well did your machine learning to the task he was given so for example classifying spam guessing housing costs of portland house is calling things cats dogs and humans all of the machinery applications that we've talked about fire episodes we want a measure to determine how well machine learning algorithms or do we need at performing their job now you may be thinking wait a minute we already have the performance measure don't
we we had this three part machine learning sequence we have predict error or functions okay and then learning now we will learn based on these error functions nigger functions tell us how poorly we're predicting so that we can improve upon those productions by using the learning sat by using calculus that's true this error function or this loss function that exists in every machine learning algorithm can be used to measure the performance of the opera
so they can prove itself however the loss or terror function of a machine learning how rhythm differs from machine learning algorithm to algorithm each error function is different for these different algorithms that because these algorithms function differently the learning step of the jogger the metropolis components is different depending on the structure of the algorithm or when we're dealing with neural networks we have this back propagation process that uses greedy and to say
that each new on in a backwards pass through the network which is totally different than decision trees which is a thing called entropy howl chaotic is all the information at any given level the tree which also the phone logistic herb russian with which uses cross and chirpy so every hour that has a different their function annie's error functions or like subjective personal error functions is basically a personal gauge on how well you're doing al
as a algorithm and by which you can improve yourself where is the performance metrics that we're gonna be talking about in this episode or more of a universal greed like a baby c. d. f. grade that you can compare machine learning how rhythms against each other so this matter to really talk about school accuracy will talk about that later burnett get into the analogy first this magic accuracy is very similar to what the final grade the end of a semester
or by taking a final test and getting back eight percent score from zero to one hundred percent and you can use that grade to compare our runs against each other for particular task or you can use that great to basically informing the single she learning our grimaced how well it's doing overall so let's use this analogy taking a class let's pretend that this class discourse at university is the spam of detection course again remember how we said that there is machine lowering our rooms
and be used for various purposes som are very specific such as anomaly detection algorithms and recommend her systems but some are all other more universal such as naive days decision trees neural networks and support vector machines these power algorithms so we may have a situation such as spam classification for example where we can use a number i'll rhythms well we're gonna put all these algorithms into a classroom class that is the detecting spam claw
asked and university and we're going to give them textbooks and assignments and quizzes and tests at the end of the course of their gone all get the final grade would soften how well they didn't know morgan use that great ourselves to determine which of these machine lowering our rooms we will select for the purpose who got the highest grade class the learning phase in this class is these students these individual machine learning algorithms trying to learn what it means to be spam when we're looking at you
ales what makes an e-mail ham that is not spam versus spam so they're going home a take on these assignments and our reading the text book an or crunching incontinent crunching this is the training phase of machine learning and each of these algorithms has it's own metric that uses to figure out how well it thinks it's doing in the class so we've got one learning algorithm naive days and he's the father of three kids may have that really taxing job it comes home
reunite he studies any studies and he gets a little sleepy listens to lectures on his ipod on the way to work and he studies in the break room and he just beating himself octopi don't have enough times daddy i'm not that passes class speeding himself up and he takes a test needed to be so surprise oh my gosh i'm doing pre well bye-bye to improve it looks like i may be to focus on some of these and not focus on some of these other things too is that this matter that he kind of compares himself against that's his loss function is error function
basically his own subjective personal care or function for how well you do in class that we have another algorithm over here over them be logistic regression an easy game or and he's playing games played out in this class i'm fine one day he actually studies an hour and a textbook and he's congratulating himself oh my gosh i'm doing so much better than i usually do i am going to nail this class studies got his own performance magic that he's had comparing itself against well the end of the class rolls around to take a final test they get
their scores and yes you did better the father of three kids so they each have their own loesser air function of their own personal subjective performance metric that they used to tune their learning process and then there is a final grade which is this performance evaluation magic to work and talk about things like accuracy and asked to score noise things still basically inform objectively war universally how our rooms are performing against each other or how well the algorithm did it in general
this class at this particular tasks which in this case is spam classification this performance measure the subject of performance measure that measures algorithms against each other we call this accuracy is simply how many questions did you get right over all over all the questions available very simple accuracy the case of classification spam class occasion how many e-mails did you correctly classified as spam or correctly classified as him over how many total accurate
a very simple it's not always as simple sometimes the task is a little bit more complicated for example let's say the task is classifying cancer not spam cancer well cancer is let's say one percent of the population were spam it may be well it's not actually fifty fifty blessed pretend it's fifty fifty well classifying spam versus not spam takes live know how if it's gonna be fifty fifty breakdown but classifying cancer
says non cancer can be done with a very simple learning technique learning to always classified not cancer why it because of cancer is only one person of the population then you will be ninety nine percent accurate if you always classify it a patient as not having cancer so the accuracy measure of number of corrects over a number total will not do in this particular case so for this amplification course and you never
city there are a machine learning students were all taking in and we met at the mall against each other the keys to the final test that is the use is going to be a different key then in the oncology class we're learning to classify cancer is going to be a different key to the professor uses to grade the final performance evaluation of the machine running our rhythms so we don't use accuracy in these ads cases so what we do varies on a case by case basis based on the machine learning task
that hands the more we have or are these two measures that we can balance against each other given the circumstances again and these two measures are called precision and recall precision and recall precision is when you're making a gas do you always make the perfect gas and then recall is kind of like how why do you cast your nets so i think of the definitions of precision recall by way of an analogy to a video game i play called robo
we called for recalls in the very name of the game and in fact you do have these little measures precision and recall at the end of your game play precision is like how precise you are that's the definition of precision how precise you are how much of a straight shooter you are do you mean never miss a shot you take he is at every shot you take a perfect head shots you never miss a shot then you have perfect precision the higher the precision of the better you
or shots that you take you never missed a shot you take high precision you are a precise shooter he were a straight shooter the purpose of the game the story is that is always robots and there is a virus now turned evil and have told robots is called robo recall because you work for the company that made these robots and so it's your job to recall the defective products right you're trying to recall the products which means you're going on field you're killing all robots sold the amount of robots you
he'll use the amount of robots you we called the amount of defective products you recall back to the factory so your precision is how sharp assure you were you always make the head shots and the recall is how many robots you killed in one level and their balanced against each other you i don't care how good a sure you are you can possibly have perfect precision never miss schott and perfect recall kill all robots so there's some sort of balance you want maybe on
a decent amount of precision were you gonna don't miss too many committees in our recall where balance against the precision you're killing a lot robots some assets or get back here precision and recall is your beard trying to balance some level of how precise you are when you make your shots versus how many things you get right over all know let's bring us back to stand in cancer analogy in the case of cancer you want i recall you want to throw a wide net why is
that because you are okay it may be falsely classifying some people with cancer because you never want to miss the people who do have cancer you would rather make false positives then miss any shots so you wanna hire recon the case of cancer you want to cast a wider net you're okay accidentally classifying people don't have cancer the long as you always classify the people with cancer in the case with spam you walk every shot you take to
hit the spam e-mail and you'll let some spam come into the in box at the expense of casting a wide net why because of the past two white men that you might ask billy send hamlet in an e-mail to the spam box that would be very unacceptable you would rather the user get a few spam e-mails per day let some through lower we call rather than acts that way it sends a non spam to the spam box sell every shot me take you wanna
head shot your spam e-mails high precision so different edge case machine learning situations called for different performance measures the general kind of catchall measure we use is called accuracy and if your situation is look at edge case then now we have this sliding scale let's say we have a slider with the mob we can always left with precision a war always to the right with recall and there's some maybe it's sweet spot for this thirty two
are our room where you slide that knob until just so and it's it's really good freer situation unless they met that standard performance measure that we call accuracy is whether slater is right in the middle so accuracy precision and we call them is is another matter called and asked for and there's different types of test scores f one f two etc dot com and once you just have to soak up and asked to score which sort of measures the balance of precision versus recall what kind of a mathematical clayton
gives you the location of that knob on the slider and i'm talking about what it's like left ago precision sweater right you'll recall that f. to score kind of tell us where that knob is on the slider and so you use that for different situations other things you'll see in that this space of performance evaluation when you're reading around one thing is called a confusion matrix okay when you classify stamina and ham you can make true and false negatives and pas
that's good for options you can make it true positive a false positive it true negative a false negative for you're when you're making your guess is new classifying spam so eight sure positive that is when you correctly classified spam a false positives as we said it was spam but it wasn't a true negative that is when you said it was ham and it was him and a false negative as we said it was ham but it was spam so for options and put them together on the grid
a two by two matrix you recall this little matrix a confusion matrix doesn't have to be two by two of their multiple classes that we're trying to identify said he kept on human endeavor three by three et cetera and this is kind of like the key that you were professor of the spam class will use at the end of the class to too great your final test this confusion matrix is how old your professor will sort of visualize the things that you are wrong nothing's eighty
got right now you don't use this performance evaluation metric whether the accuracy your nest to score or not you don't use it to train your algorithm you use it to grade york algorithm you use your algorithms error function to train the algorithm sell your algorithms taken a class and he's adjusting himself based on that because i'm a grades and the quiz grades in always things in his life decision
isn't this union nor sleep etc improves and self improvement self pity and the class we get a final grade and that tells us how well or algorithm did john salvi performance metric is not for training is for greedy now how does this work what we'll do is we have data we have our spreadsheets both houses in portland where we're trying to estimate a cost of a house given its features we have a spreadsheet must say it's a hundred thousand rose will do
what is will take a need to put aside and that will be used for training your rhythm in other words eighty percent of this spreadsheet is our algorithms steady material it's it's textbook it's it's homework assignments eighty percent is called the training day that the training set so we just took our spreadsheet we caught it exhibits the student and twenty percent goes to offset the professor we call this the tests that an assist has said that were used to grade the fine
performance of the algorithm this is the final test the final that you give to the end of the semester is art hast set so we take our tests that of features and we have the hawaii values the actual prices of these houses but we hold them behind our backs sort of this is the test he so we put all those rose as questions on the final test and we hand that's to the machine learning algorithm which sago answered all questions and gives us the test
can we look at we compared against the key that we have and we agree it's now there is little something called a hyper parameters when a machine learning algorithm learns remember linear regression and logistic regression episodes we talked about these things called parameters fade out parameters specifically a parameter is a coefficient the machine running over them learns it's the number that goes in front of acts that this is multiplication
keith's that the machine learning our room learns in order to construct an equation that fits all lying to the date parameters that is the stuff the machine of rumors but there are other components to a machine learning algorithm stuff that it does not to learn that can be tweaked to improve and algorithms performance so for example we were talking about linear regression we said that the d. m. may not optimal eat beef it will line it could be just sold that may be lost
squiggles or earth it says exponential functional look for the op or something like that so the amount of holan no meals in a linear regression algorithm that's called a hyper parameter it's not a parameter the machine learning all the more and it's something that we'd squeak as a human hyper parameter is a thing that the human tweaks subs can avoid allowable above the algorithm the controls the algorithm
then the arbor them learns it's faded parameters and do the restriction of the hyper parameters that we impose upon the algorithm i think the best way to visualize this is with the hyper parameters of the neural network piper parameters of the neural network are the number of new ron's in any layer and the number of way or it's in the network so how wide and how deep is our network you might think well it seems like the optimal neural network would just be
the biggest brain ever let's just have a brazilian layers deep and a brazilian neurons why at each layer when the baby's brain ever be the best neural network not necessarily the size of the network is particular to this circumstance if the situation is simple been having too many layers or too many neurons actually causes something called over fitting the real be getting into towards the end this episode another problem with two complex
of the neural network is that build more complex than network the more training day yes than no work needs to eat two understands the situation if you've got a spread sheet of hundred thousand rose by yellow the zillion neurons that's not enough information and you could probably never get enough information with that complex of a network so you slam your network down to accommodate the amount of d. day you have available furthermore the more complex your neural network
the more computational resources required to many neurons means your computer just can't handle it so you actually want to have that as slim down in your network as possible as small a brain as possible that can optimal we handle the situation at hands so the number of neurons in the number of lawyers or your hyper parameters in you as a human shoes the hyper parameters that best fit your circumstances based on
your understanding of the situation so it takes actually understanding on the human part in order to optimize is hyper parameters now that takes away a little bit of the magic doesn't it we have machine learning algorithms that learned the information that you're giving it but there's some stuff about the machine going on than a dozen wherein you have to tweak yeah so so viper parameters live there are on their our libraries out there that automate is process is not actually machine learning it's it's more lie
you're reading through a handful of different combinations of hyper parameters given these restrictions you get it and finding the thing that she is the best performance metric using that evaluations that we talked about an earlier part of this episode was now machine learning proper basis of their libraries out there that can cycle through paper parameters run your machine learning our rhythm against it against the performance evaluation and squeak the hyper parameters until it gets the best score possible so these hyper parameters are thing
that week sweetie and so what we do is we added a petition all step to this train and test process okay so training machine learning algorithm has the algorithm tweaking it's parameters of theta parameters nikkei so guinier images aggression and in the final test just told us the overall score of the machine on in our room at performed we had this third piece called of allegations that allows us to kyle i'd stop look at how leo
that was performing before we get our final grade and if we sort of have a hunch on how we can make some changes to these hibor parameters we can tweak them the making quick go again on the training phase and your brother can we trained given that be adjusted hyper parameters to call us that the validation step so now we have and i think of this in our university course analogy as the mid term the midterm so it's just seconds tax we insert in the middle of the semester that
measures the performance of the machine learning our room about allows us to kind of step in and change the hyper parameters an asset backed out and lego dentist alike we have our machine learning algorithms are taking this course and they've got their internal measure of their own performance so i think i'm doing that i think i'm doing good they're making some adjustments the way they steady okay they start to realize that they fully understand concept day that they may be needs to focus little bit more concept be so they have their loss function cross
and chirpy for example and eight adjust their fate of parameters adjust their fate of climbers they learn learn learn the coarse material and we give them a mid term and they get this great back and elect will see a fire was duro well and so we pull him aside as a human k. we turn them off as if off switch on their back 'cause these are learning machines after all they're robots right and we open up their skull and how little robot brain inside we take our read to me take her screwdriver him
to tweak turn turn turn we make some adjustments to the hyper parameters whether it be number pollen o'neill's no linear regression algorithm or number of layers or neurons in a neural network and we close lynam ahead we put back into its class and it starts to learn learn learn that we can actually made retake the midterm panic is a this time great is actually improved so i think we're ready to take that final test and she moaning over the text has gets the cell trainee is learning
we have a mid term which lets us peek into how well we're doing and make any necessary changes store hyper parameters is called a validation step and we have our final tests which tells us are greedy for the semester so we split or spreadsheet into three chunks let's say it's sixty percent training degas that's the stuff that the machine in our rooms in all learn from and then twenty percent validation sets that's the stuff that we're going to test the machine
earning power over them on a midterm to see if there's any sort of hyper parameter adjustments we should make to improve the algorithms overall performance in there we have our final test with the tests that and that tells us the overall score says is called the process is called across a validation we split our spreadsheet are dana into three chunks training sets validation sets antacids and we use that day yet to train validated see if we need to make the header primer judgments and tax
final evaluation okay so we have a way of measuring algorithm a performance bulls gets itself with the loss function against each other with a performance magic like accuracy then we can use this grade to decide which algorithm to use for or particular purposes but what could go wrong in an algorithm that could mock up its performance what could cause an algorithm to be less performance than another algorithm it could be that one out of mischief
the better fit to give him the day yet why it could be a handful of other things things that can improve weary talked about hyper plan or adjustments those are things that we get it but just to improve our algorithms performance sama algorithms require more d. n. to learn from then on their algorithms so one thing we can do to improve performance is just collect more data so for example we split up r. j. get into training of validation and tests that we just dill
looted the amount of d. n. a. that an algorithm has to work with when it's steady wanted learning some algorithms work fantastic with very little d. n. a naive days in particular works is very well even with little day that whereas neural networks need lots and lots of data to train with so one way to improve performance is to collect more data another way to improve performance is sometimes the day that actually needs massaging this amazing fields some
times maybe we just need to fill in those missing slot swiss maybe maybe an average across the drina or something else basically filling in missing fields in your dataset is this whole science to let it depends on how the rest of your day is structured and what that particular field ideas but choosing how to fill in that missing slots is a very advanced topic i'm not to talk about there's this other thing you can do call
normalizing day yes sometimes numbers are wildly different let's say that one feature for example is number of bedrooms were talking about housing costs can be two or three or maybe four where is distance to downtown measured in feet is the new while we hire well those two numbers being point wholly different scales can actually hurt the performance of certain machine learning algorithms say you want to bring them down to the same kind of
scale recall at normalizing see you normalize your features so these are all things that you can do to improve your performance but there is one thing that drastically improved the performance of machine learning our rhythms and that is called regular realization and you're going to learn about regular is asian probably in the first kabul weeks of andrew being were the first couple chapters of any other book i apologize it took me so long to get to
in my pod cassis just so boring and so technical and detail it's like i just wanted to kind of cover the high level stuff before gotten to the nitty gritty some hard understand it's just it's just a technical so it's super super important to to machine learning understanding you can go on without it and so i would estimate you probably know little bit about rivers issue already and if not you'll learn about it when you start diving into the details machine like the irritation it is a step you take to reduce or fitting
and under fading overfishing and ponder fitting or sometimes call high variance and hide by its respectively in machine learning we have variance and by its a machine learning algorithm has any level of variance and by it's the level of variance determines the amount of over fitting a new level of bias determines the amount of ponder fitting and this is what these things mean the best way to
think of this is judging a book by its cover now your machine learning algorithm uribe support back to machinery on iwerks and you're trying to learn whether or not you will enjoy a book by its cover well you read hundred thousand books and you see all their covers and you know which books you've liked and which ones you haven't liked and i think you have a hunch as to what books would be good just looking at its cover now a naive machine lerner will cost you say you can't judge a book by its cover itself
pottery buddy no you can't judge a book by its cover and you tap your nose new say kansai the other allred says no love course you can't and re single book is unique in it's own special way every single book cover represents a specifically different book okay every book has its cover and it's unique there's nothing that connects these things together that's algorithm suffers off rama high variance old worth
fading a case that's a problem that you can have an mushy morning you're on the other hand let's say that you have gotten a little bit over confident you take a look at you look at your fantasy and camillo this book and hyperion seldom looks at it and spit has a bare chested man sitting on a horse and a girl behind him swarming with her hand on her head is a cop come on that's romance with which this fantasy well maybe you were tipped off by the horse and a strong man sitting on it maybe looks like he's right
the battle i don't know the fact of the matter is you guess too fast to fuel the area bulls tipped you off you didn't think hard enough to take up another bout you like fantasy that another guy facie case against anything fancy you my friend suffer from a high of bias that is a wonder fitting under fitting or height bias is when you don't use enough data to make the decision usually this is the result of not having enough data to begin with in the training phase overfed
ng or high a variance means you're too specific you're not generalizing enough you noble even generalization you think it's harmful so if we're looking at linear regression for example linear regression remember was fading a lying to a cloud of data points docks in the shape of a football let's say that the day it is not shaped like a football that's a little bit curvy and we need ideally to come up with a pollen o'neill function maniacs
where are some sort of kirby goes out now not again that so we're hoping for that's an ideal fit good general was a high bias would not have any polio new law issue is bill wine goes right to the center is not using complex enough features by using words not using enough features in general to make his productions is too simple it's productions or two simple high variants or over fitting might fit the line with too many holland o'neill's
might have to meet x. squared simex cubes and exco forth into the fifth what you get is all lying they go through every single point on the graph that's not right either we want something that general lyons's something that reduces the overall error but is as samples we can make it without being biased i think this is all comes razor i'm sure most of you know what bottoms racer is i'm not to define it really empty stomach
summarize it as simplest solution winds now you wanna go to assemble otherwise you have high bias under fitting we will go to complex because any have overfishing higher variants what you want is the simplest equation possible that will give you a generally good algorithm but generally good graf something that's squiggles up and down not really cleanly and subway doesn't touch on all the dots scott i go through the mill mall than the reason for that is
you adding new docks gianni de points it's more likely to get back rights or at least close right then and over fifth graf to the problem with bias and variance the problem with honour fitting in overfishing is that a male while you to make somewhat accurate predictions in the training phase but when you come to the ballet shoes that were the tests that your way to be wrong to leisure both evils variants unbiased and we want to reduce both of these if
possible you wanna come up with a good generalization strategy is neither too complex nor too simple and that is this thing called a regular realization regular edition it is a little chunk they you add to the end of any machine learning how we're them that modifies the equation and by reducing counterfeiting and overfed and it's interesting what would it does for example linear regression is it might reduce the effect of pollen no meals at heart
explain m. leave it to the intruding stuff for you to learn the details of regulars nation but it's basically a little pat on the u. act any new machine learning algorithms that will reduce bias in variants so once again under fitting is jumping the gun basically you have enough data to come up with a good generalizations prodigy and so you come up with an insufficient generalizations strategy video line when what should be used as a whole no mail you pick up a book you make that judgment call
based on true few features of the book in your hands now i do believe that a book can be judged by its cover at least the genre of the book could be judged by its cover but over fading algorithm over here comes along says no book could be judged by its cover every single book is independent this algorithm has does not come up with any sort of generalization strategy they wouldn't be able to tell what genre even the public is looking at because as to specific so the optimal strategy is
i'm sort of middle ground where you can judge some characteristics of a book by its cover not too specific not too simplistic okay so that is performance we talked about performance evaluation and performance improvement things that can work performance thing second her performance would be i'd buy us a hyperion said is under fitting an old were fading respectively non of normalized feature is missing degas too little dana orally to whom to hyper parameters such as number
a new ron's or number of layers and you're all my work and dark and number of ways to improve on any of these issues you to win your hyper parameters or you would feel in your missing degas your normalize york numeric data and you would regular rise you algorithm by adding a little extra algorithm child to the ends realization that would decrease overfishing and under fading away we measure our algorithms performance well the job i'm has its error or loss functional used to measure
to own performance while it's learning don't subjective personal performance metric and in the end of any session we will find retest are algorithm with the tests that this performance evaluation we might use something called accuracy which is a very general simple catchall for measuring performance but in certain edge case in areas we might try to balance precision versus recall precision is how much a straight shooter you are you may all have
we estimate you make verses recall which is how why'd you pasture net do not let any estimates escape you can use m. f. to score to measure the balance of your precision and we call and finally we'll just break our day is set into training and tests that's no we add a third set we break into furry training set validation set and tests that we use our training set to train to learn the fate of parameters
we use our validation set to determine how well or algorithm it is doing it is not doing too well that we might add just hyper parameters human that adjustable parameters of you algorithm and finally the very end we will measure the algorithms overall score by way of the performance evaluation boring i was not one but my friends as i said maybe episode you are now look down at with the basics of machine morning of course per usual encourage you to finish the engine being caught
worse and then from there move on to the deep learning book start putting with python and tanzer flow there are no resources for this episode performance evaluation an improvement is kind of high aid to learning the machine learning algorithms you're going to be exposed to the south but i mention this episode not as an independent module in any learning series but alongside any individual algorithm as you go and starting now all i'm
owing to be moving on into people earning i might break this pie kass now popped into seasons with next season's going to be deep learning which means that i may end up causing a creating new episodes for this pot cast in the short term just to catch up on the deep learning essential so like him so i have in my mind good enough to teach to you guys and and create new season on people earning but that's organized be covering again
next sequence of episodes as far as the eye can see we're going to be moving completely into deep learning sorts kennedy find words are never cardio met works convolution o'neill networks and more eventually ones i've covered all the deep learning material i will move to reinforce no learning i hope to sort of easy to artificial intelligence proper with this pod casts serious bits and take some time actually don't know
enforcement learning myself yet fanatic learn mastiff no background for to teach it in order to pave the way a four deep learning and we inspire you to learning this stuff because it it is beautiful it is magical fascinating algorithms i'm going to do my next episode on consciousness i'm going to talk about what people are talking about thinking about in philosophy incarnate science in neuroscience
along the lines of consciousness and how it may relate to neural networks artificial intelligence and all those things it's going to be a little bit pseudoscience seeing definitively subjective and i know it could ruffle some feathers but i do encourage my rigorously incurable listeners to listen to the episode it'll just be some fine it'll just be a little bit of inspiration i'm not going to try to make any definitive claims one way or another i'm just going to introduce the topic and spiral of
sinners because there is certainly some potential correlates between our official new owner works and biological know mel works if not entirely in an architectural way a potentially in a functional way i think this will be a very fine episode at the end of that episode i will get back you guys as to whether i'm going to break this up and seasons one and two or if i'm just going to continue from their right into deep learning all let you now see you guys next time


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt chief get time also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused and tips and tricks some which could prove beneficial in your machine learning education jury finds that at waseda val dot com forward slash l. l. h. this is episode sixteen consciousness this is gonna be my favorite episode to make the
said topic near and dear to my heart that concept of artificial consciousness cannot and will artificial intelligence when downright be conscious an extremely controversial topic to be sure and one which will require the definition of consciousness will get into that nugget but before we get into consciousness let's talk about the inspirations that drive people into the field of artificial intelligence we talked about this before in a very early episode
the various inspiration to bring people move field in the past people who were developing in artificial intelligence or probably more specifically machine learning gone to the space because they just want to solve interesting and challenging problems maybe they had at statistics or theoretical computer science p. h. d. n. a. quickly found out what were the best applications of their skills to make big money and to solve challenging problems these are people like that wants to use machine learning on wall street for high frequency trading an ally
things have changed recently the graduating class of twenty seventeen machine mining engineers are brought to the field not so much to saul new and interesting problems for its own right but by way of inspiration we see major things happening in the world right now itself driving cars music poetry and court synthesizing things that we considered impossible by way of artificial told him snug prior generation and we're seeing it happen that day by day in new king crab
all breakthroughs on a monthly basis usually by google also by facebook by two idea i'm open ai always other companies but big stuff is happening so that for the main inspirational drivers are bringing people into the field machine in there seeing this incredible amount of economic automation there were cheating by way of this technology itself driving cars even scientific discoveries drug discovery in creation medical robots x. ray analysis legal proceedings
don by machine moaning like robot judges all the stuff so people are saying that the new wave of jobs is by way of automation and a smart move as a professional is to be at the top and automation engineer one they'll wanna get left in the dust so i wanna make smart special move but to these problems that we're solving by way of ai are incredibly inspiring and powerful things celeste the first inspiration in my mind it's bringing people into the fields trying to stay on top of the economy and being inspired by
things are being created inspiration number two the singularly many have come to this pot cast after being inspired from media books like the singular using year in high korea mine by ray kurzweil tv shows like west for older than black nearer than just the overalls id eyes of this concept called the singularly the scene you larry is this idea that technology is progressing at an exponential released at a poem gnome you
oh case not you win your case as should be expected of technological growth the idea was proposed long ago but champions by a guy named ray kurzweil who wrote a seminal book called the singularly is near mount mind fifteen percent of the expert machine mining engineer or or phd listeners are rolling their eyes and growing at the mere mention of the name breakers while and my seventy five percent of bright eyed and bushy tailed
inspired listeners are sitting up in their chairs in anticipation for both of you i'm actually not going to take a stance one way or another on the credibility of the singular d. i'm just going to describe it my purpose in this park s. episode is consciousness to the idea of the singular eerie is that if we look at the rate at which technology has progressed in human history it here is to fall on something of an exponential or maybe pollen o'neill graf rather than allying we've got an awkward of facing slocum
all the way back to tools then to the agricultural revolution then the industry revolution then at the information age which we presently live in and possibly what comes next being the intelligence explosion by way of artificial intelligence each one of those technological revolutions was more substantial than the prior and closer to the prior then the prior to its prior thus looking like a net exponential or at least pollen o'neill graph the idea goes that's that's
some point on an exponential graf they're curious to be a hard elbow the corner which the graft sort of rockets into space trying it in fig leaf to reach what's called an axe and talked a lot of the graph prior to that point it appears to be increasingly nearly a very normal pace but some point there is an elbow horror shoot up into the sky and we call this the singularly both our technology appears to show the trend of the pollen on your exponential graf and we huh
and hit that elbow yet but we do appear to be increasing in rapid clip what might cause such an elbow well artificial intelligence matters of thinking ai it is the automation above our technology curious all of our technology can be automated potentially by artificial intelligence which is simply defined as automating any mental tasks that's what artificial intelligence bits of course as robotics so according to the singular in some way we may not even be participants in the middle
as technology will the automated well what is an ai not only cool it candle automation of a particular mental and physical task mental by way of ai physical by way of robotics but also influenced its successor by improving upon its own algorithm took either influences celtic either update its own our room to improve the outgrown that we gave it in order to achieve some asian or improve the next generation of such algorithms so that they're better to task
ever is then maybe their task we have at the self improving machine learning algorithm of sultan proving a high and that's what's called seed ai ai they can prove itself once that happens we'll even know what comes next for the sky's the limit so that's a possible candidate for what kicks off the singularly what is said to see larry is highly controversial it's debatable flocks of fun though you can read all about and recruit wiles book the singular he is near and most of the episodes all black near t. v. series
are based on concepts of the singularly so that's another inspirational driver bringing people into the field of artificial intelligence and by the way i keep talking about a high but this is a high caste series about machine learning now my listeners remember from a prior episode that i made the distinction between ai machine running on one make this distinction one more time because we may have new listeners years specifically for this episode on consciousness so let me just define a guy with freddie said it is our male bull mental processing above it
any sort anything that is an automated mental processes theoretically considered artificial intelligence then there's this concept of artificial general intelligence and artificially intelligent agent which can perform all mental tasks or least all mental tasks that humans are capable of and to the level which humans are capable of performing those tests in other words in the k. g. i. a. agent artificial general intelligence is an agent which can do everything humans can do mentally at least as good as humans if not better when it's better we call a super
am i worried as good as humans equality gianni a little less than humans we call a week ai with the specific task in artificial intelligence such as image recognition speech sentences etc it is a specific artificial intelligence task not intended to be applied to everything across the board we call a week and i and if we can apply across the board because strong ai or cagey i know what is machine learning fit to this picture machine learning is a sub field within ai
are many filled with amanda's robotics there's perception is planning knowledge representation all these things and the reason that i'm focusing on machine learning is for two reasons machine learning it is the most accessible field within ai if you wanna crack into the industry professional wait a whole lot of machine learning jobs are opening up all over the place is becoming wildly popular a lot of professional application know why you crack in the industry things like fraud detection image recognition speech said
sisters and see a lot of chap ought to papa not left and right any sort of actionable in sight space on a data collected by companies so so machine learning is the accessible part of the iron it's the way you cracked in today i professionally and also it is increasingly a quart component if not of the court component of that day i was above dedicated spaces with an ai are quickly becoming sub soon the war at least major league contribute to bah
i'm learning we're finding that learning is more than just an aspect of intelligence it may be one of the most important if not the most important aspect of intelligence so if you're interested in ai anyone involved star with this potash series is very introductory star from the very beginning worked his way out okay so to inspirations all three driving people into the space of artificial intelligence by way of machine learning the first being economic automation
is in their best interest professionally to be at the top when this major economic revolution at lansdowne horgan near future i will find myself that this position i do think being and automation engineers say why is career choice the second inspirational little bit more controversial being missing hilarity if you believe in the singular he then you can be a participant in this exponential explosion of technological advancements by being a machine mining engineer i'm not going to stay
whether or not i believe in the scene you larry because i want to save my credibility destruction before the next inspiration the third inspiration driving people to the space marshall intelligence and that is consciousness artificial consciousness can robots be conscious this is a very important question because of robots can be conscious if we can say that their conscience and we say they have a mind if they have a mind they have a full eight
soul all three things are synonymous consciousness soul and mind and that is a very very important assertion indeed that has major implications with religion with carter science and everything if we can be convinced that a robot's is conscious then my friends everything changes life as we know it changes this is indeed the inspirational driving force that led me to studying machine learning
is what brought me into the fields out in this contest that so i'm going to do my best to not make an opinion and to not a fine myself with any theories that are presented in the space of consciousness i just want to personal able and i want you to explore making your own decision not to be able to make an opinion just from this broadcast episode but in the resources that i'd give you you'll be able to continue exploring representing ms episode and eventually come up with your own conclusion one thing i'd do you wanna say
is a lot of times when this topic is brought up in the space these from mckinley jerks come around and they say we don't know waving about consciousness we can even talk about it nobody in the space can even agree on a definition hate when that happens is just not true honestly when people come at me with that report it shows that they lack understanding about consciousness and therefore there is no understanding about consciousness see me wanna be careful bring up the topic some of the stuff you learn intercepts
no because you will find people get really upset really angry that is for shoot down the conversation and say stop talking about it shut up shut out the closer years may just do not wanna talk about consciousness there's two reasons that you'll see people respond this way one is that people leave that the topic of consciousness as all this government did call the heart problem of consciousness is not something you can even talk about because it is definitively subjective another reason people get upset is that they think that we as a machine mining engineers artificial intelligence people center
or not neuroscientist score not neural physiologist says cetera and therefore we cannot participate in such a controversial scientific topic you'll see that this is not true artificial intelligence is a subspace of cognitive science the machine learning is a subspace of artificial intelligence and therefore we are indeed curtis up into the table of the conversation consciousness we are board members of this company additionally this stuff is inspirational if we gained called conversations of consciousness science fiction
we're pseudoscience which i believe they're not i think they're legitimate conversations that doesn't affect my desire to explore the topic at all it's science fiction and pseudoscience inspires some of the greatest minds like you on mosque to explore and achieve the impossible we as humans are capable of achieving so much greatness if we believe in the impossible and we apply ourselves a lot times that inspiration comes from the science fiction a society topics like stay
it's exploration sending people to mars and yes artificial consciousness so don't listen to these nay sayers for one we can prove them wrong by achieving the impossible ever to they're simply wrong about statements like nobody knows anything about consciousness know in the space to agree upon the definition of buffeting so how can we ever hope to talk about such that is wrong that is pure wrong there's a lot about consciousness about us definition that is disagree upon
from one expert to the next but there's a lot it is agreed upon a lot of agreements in cognitive finds solace talk about some definitions let's talk about cognitive sciences first card of science clogs i'd is an umbrella term for all of these sort of ballerina sciences that includes psychology neuro physiology neuroscience computational neuroscience neuro biology so those are all those hard sciences about the brain it also includes full
sophie at least the branch of philosophy concerned with the minds now doesn't quite concerns of the wane as such because that stuff is covered by the brain science is that our bridges of philosophy which concern themselves with a mind and as i mentioned already also includes artificial intelligence very importantly it includes artificial intelligence because my friends artificial intelligence was not computer science first we didn't start by making a bunch of algorithms
i'm saying holy cow this time it looks like a brain what's called the neural network that would be disingenuous indeed if that was how we went about it no major names in the region nation of things in neural networks such as the creation of the perception on our frank rosenblatt warren mcculloch and walter pits these guys are respectively were neural biologist neuro physiologist and computational neuroscientist so these were playing guys and they wanted
come up with a mathematical representation of what they were seeing it in the brain and they came up with the perception on later to be calm the artificial neural network the day and and came fromm bilby n. n. at the biological neo my work to artificial intelligence was a spinoff of brain science therefore we're allowed a seats in the cognitive science board of directors within coggin of science these various fields there is a agreements and disagreements about what constitutes consciousness
so what kinds of things to be agree with and disagree with well for one we know approaches definition of consciousness a consciousness by a comparison to the intelligence so we have two things that come out of the brain intelligence which is simply the capacity to compute to perform a mental task every human is prospectively intelligent maybe intelligence comes in scale some humans are smarter than other humans humans generally are more intelligence than the lower speed
he's down to a fish all the way down to a snail to intelligence seems to come in scale sales are intelligent just less intelligent than humans intelligence is the capacity to compute information processing capacity maybe will call it as such we have no problem calling artificial intelligence artificial intelligence is replicated intelligence and computer form and it is indeed intelligent it is simply computing information processing information doing now thinking it's head then
separate from intelligence is the only other thing which is consciousness mind the soul so we agree upon that distinction amongst the various fields and cognitive sciences know one of this consciousness well that's where things beloved dicey consciousness is many things to many people must go through some of the aspects of consciousness which may or may not be controversial within the field of god of science perception that appears to be an essential component of consciousness will start there just to work our way up to figure what
receptionists not consciousness well it's maybe an aspect of consciousness baby a prerequisite for consciousness will see perception is your ability to see things you're things touch things are a five senses in humans other biological species half your senses maybe others have more maybe a robot could have more than humans there is thoughts of that perception is a requirement for thinking okay some people say no we don't need perception to think you can live without perception bring in a
act they call it you can think math we have the architecture in our brains to de dukes to come up with mathematical equations for philosophical sludge isms all these in general or coal production by contrast to induction which is learning by experience warning that touching the hot countertop burns your hands some philosophers think that wall we have at the architecture for deduction built and for brains naturally we cannot take it into process without fer
inducing something without first experiencing something by way of inaction on which to de duke's so the idea goes that if you see one cop another cop and another cop you can come up with the concept three you can come up with the concept two plus one and two minus one and all those things but if you've never seen items in the world in some cannibal capacity before would you ever be able to perform arithmetic in your mind as controversial some say yes some say no let's move on her
section may or may not be essential characteristics of consciousness how about self identity ha there's a more controversial topic i think you'll find when you're talking to your friends and family about consciousness and you say it can you define it for me this is probably the most common of definition you'll get all the meat pies in the equation consciousness does need be able to self reflect self reflection self identity comes i'm not so convinced
that equals consciousness personally at the very least nine maybe even say that it's not even an essential characteristic of consciousness will get to that in a bit but you can imagine for the sake of argument that self identity self awareness may just be almost an evolutionary add on to consciousness there's a theory goes if you have a fury of self self identity you can adjust your actions in accordance with your fury of self
your theory itself may be a simulation of help others see you soon intelligent minds will learn what things to do and not to do in an environment don't touch hot stove to eat the food will take that up a notch and learn about ourselves about who we are in an environment and we can learn how to adjust properties on ourselves so as to better get on with those around us this theory
self identity says basically the self identity is nothing more than running theory of how others see you so that you can make proper adjustments to get along with people not having a solid self identity means making enemies and maybe getting killed having a strong self identity enough to improve upon that algorithm means making friends allies and having sacks of course not everybody agrees upon us some people think indeed that self identity is a core component
consciousness but one more retort to that concept is the idea that certain lower species seemed to lack self identity is unclear the dogs are word themselves for example for higher species appear to have self identity like chimps we've got certain tests for testing whether they exhibit self reflection things like they can put a dock on animals had an unhappy animal look itself near death and elephant reaches it's trunk towards the mirror versus towards its own head and then we think maybe ago
and exhibit self identity certain animals pass this test are mammals fail we'll other such tests for self identity the tests themselves or even controversial the point i'm trying to make is some animals house itself identity and some don't know say your dog doesn't would you say your dogs not conscious do you indeed personally think that your dog has nothing rattling not there simply a mechanical computing device and it's just as reactionary
biological organism that isn't experiencing the world around and i don't think you do i think if you sat with this for a while you would come to the conclusion that most if not all biological organisms are conscious enough fundamental way even whacking self identity so self identity appears to be a very controversial component contributor to the definition of consciousness but indeed in an important modify we do hope that if we can achieve our official consciousness eventually there will
how self identity so the very least self identity is a powerful module at the very least it's a powerful contributor to high steel consciousness but is unclear whether it is a necessary component of consciousness definitively by the way my own personal opinion is this if we go with the theory that self identity is what you were running simulation of self so that you can prove yourself an environment does that sound familiar to something we've mentioned to you know
episode to me that sounds like seed ai in my personal opinion the moment we build self identity into machine learning algorithms effectively and powerful a. is the moment everything begins seed ai recall is an artificial intelligence agents able to improve upon its own algorithm another word for this is actually a medal learning and is indeed a hot topic in the space a machine learning going on right now lots of companies are
exploring machine learning algorithms capability to improve on themselves okay a few other aspects of consciousness when we're talking about it's our memory and learning now these go hand in hand you can learn without memory and by the way you can have self identity without learning and memory now nobody would say that learning and memory or consciousness but these are things we may agree upon as essential to the grand picture we need these things in place to get there so we've talked about
sections self reflection of memory and learning we're not quite there yet is something missing is something something feels not quite right about we we've been talking about consciousness with these rock components that they don't really get always there is one thing we haven't mentioned one thing that in my opinion all the cognitive scientist agree upon as essential to the equation of consciousness if not the very definition of consciousness itself
it is awareness awareness or sentence subjective experience quality of these are all words for the same theme which is the lights are on something it is on up there okay you as a human when you see in you hear things you're the dog barking a nice enough why are you not just some information passing device that crunches that perception internally but that that perception is never experienced know you hate spear
inside the c. the hearing of the dog bark when in your ear and some little here's near your stimulated and some electrical firings happened within the brain within your biological neural networks neuro transmitters are transferred from attacks on to dan's right chemicals are thrown left and right electricity goes all for your brain but none of that explains the fact that you heard the dog the dog's bark was heard by you us
objective experiences you cannot explain it here is to exist in a different dimension and that is why we call of the mind or the soul the heart problem of consciousness will talk about a bit awareness it's hard to define me you know exactly what i'm talking about yea a fairly maybe we can define consciousness explicitly as awareness of the lights are on the one people say nobody can come over
the definition of consciousness so we can even talk about it well we can all agree that awareness is not very definition of consciousness itself is at least a claw or fundamental component consciousness and my friends if something could convince you that it exhibits awareness which you not agree that is conscious that is the reason we believe our dogs are conscious they up here to exhibit to you that they're aware inside the something is going on at the lights are on notice won
things suspiciously missing by him many people's definition many laypeople i dare say and that is emotion interesting to many people will say will be well i can never be conscious because it can never no longer can ever know sadness it never know pain and sorrow in los happiness joy ate all those things to some people the motion appears to be an essential characteristics of consciousness in my opinion and this is my own personal opinion i'm not going to find myself the camps
part of science they go one way or another on this topic i think a motion is not even a characteristic of consciousness i think it is an accidental drop then of evolution a reinforcement learning a mechanism things that make us happy stimulate some positive reinforcement mechanism to make us what do that again in the future and things that make us on a happy are some negative reinforcement mechanisms that make us want to avoid such actions in the future in other words ammo
fans are nothing more than a computational mechanism for seeking up or avoiding certain activities for our own survival sake i think about all the things it emotional about sex and love physical pain losing someone death these are all things that have evolutionary explanations in fact will talk about how o. possibly an artificial intelligence agents quid experience emotion on some level maybe not the way he
and experience and that possibly experience pain and pleasure no less real we than we do we'll get to that nugget so we come upon something that most cognitive scientists can agree upon and that is that the very definition or at least decor component consciousness is awareness that is subjective experience another thing that they agree upon it is this distinction between what's called the soft problem of consciousness and a heart problem of consciousness the soft problems of concha
snaps are basically things we know so it's always gonna be a moving target or running definition things that we know through the hard sciences we know for instance the consciousness comes from the brain it clearly comes from the physical brain the brain is the biological substrate of the mind is what they say that sack hatch phrase the brain is the biological substrate of the mind we know that we've known that for long time we've known it since at least the renaissance we know it's substantially more accurate
we these days than we did in the past we have things like them are rising cat scans and cat scans we can look at the brain and cigarettes specifically which regions are firing when we asked a patient a question think about your mother or plan some activity we have to wit heed the centers of the brain associated with what specific mental processes be to look under a microscope and we can see the activity of an you're on exactly why and where mental activity calm
swim know the speeches and broke is area that planning is not free from all cortex so the soft problems of consciousness or the things we know the things of science the heart problem is but why a consciousness okay if little bit difficult to explain but on the one hand we have this physical world wherein we can observe the blame for m. r. i.'s microscopes any other hand we have this other dimension the spiritual world this matheson
old world above the mind your own subjective experience it seems that your subjective experience when you're thinking we're doing math are planning something we're daydreaming especially daydreaming is a good example appears to be a different dimension of course this is what gave rise i'm sure the bully for life after death and religion that your soul lives on pasture body because they appear to exist in different planes and different dimensions on this heart problem is how
all how's that possible is a paradox this paradox is made manifest by the concept of dualism tanaka get too much in the history of the exploration of consciousness but i will mention one thing that is the concept of dualism put forth by rene descartes when a car was one of the biggest mines in the early exploration of consciousness was fascinated with the topic rene descartes is on the mind behind i think therefore i am an evil genius that gave way to the matrix okay how can we
oh we're not in the dream rather than the real world all aesthetic and see how that all plays into consciousness he was obsessed with topic the concept of dualism is this very concept that the minds and the brain it exists in different dimensions now it's a paradox how could they possibly exist in different dimensions if they exist in different mentioned is the mind is something known physical foaming metaphysical then how could possibly interact with the brain which is something physical at some
point the mine would have to become visible to enter the physical world to stimulate the brain into action but that's just paradoxical at what point cannon on physical thing become visible it just doesn't make sense descartes believe that this seat of the mind in the brain was the peel glands where he came up with this why is that a lot of structures number wayne have a mirror image on the other hemisphere so we have one thing on the left hemisphere and the mirror on the right hemisphere and you're a big structures
well that okay neal gland was this tiny little structure that didn't happen your image so it physically looked like a good candidate for laptop log and out what were you plug in the mind from the other universe but he couldn't defeat that conundrum how can the non physical become visible and so scientists just don't believe in dualism these days anymore dualism is relegated to religion nests people who believe in life after death and so if you are religious now waiting you probably are the less but many today go
we've that's the mind is physical somehow somehow the marini comes from the brain the brain and does the stuff in the mind comes out of that but how it is a mystery is a mystery wanted the last remaining mysteries of the universe we can manipulate the brain and alter the minute we know through certain patients in case studies that alterations to the brain in repeatable fashion calls alterations to the mind thirty's case studies late fee
is the jew had a railroad spike driven through his brain in cars specific alterations to his personality okay people even dualism and that that camille gland is this sort of an outlet wearable mine plugs in one how those sizzled image was specific other chunk of your brain has nothing to appeal gland substantially and terminally a factor personality which is an aspect of your minds just doesn't work that way to see brain and cases leading to mine the cases in re
pupil and unpredictable fashion we know they're too wet to lead to high but how does a mind to come out of that the reason we asked that is because we wanna know if we could perfectly replicate in robot they bring like structure which then have a mind so knowing how old the mind comes out of the brain is what's called the heart problem of consciousness and it's basically nothing more than a paradox command from some people think that we can never know how the mind comes out of the plane because
mind is definitively it's subjective it experience consciousness is awareness awareness is subjective experience i know in my mind what's going on you can observe my brain in an m. r. i. a. b. you can ever observe what's actually happening in my mind you can guess who i'm thinking about was specific actions i'm planning you can see certain centers firing but my experience is definitively subjective similarly and i can see what any of you guys are thinking nor can i even be certain that you all
are by all indications your minds must think the way my mind does as a result of my brain firing than yours does similarly i must conclude that you were similarly have consciousness that you are aware but i can prove it because your experiences are definitively subjective so something that the brain mind problem heart problem of consciousness is definitively on scientific on knoll and all we could do is get us all we can do is take it on faith that say
isn't that here are two how the consciousness house consciousness this is basically the idea of deterring test alan turing was one of four fathers of the computing revolution is a big believer of artificial intelligence he was very fascinated in space of our official consciousness and he came up with a test called the charring test and he made it sound so complex and convoluted and all it says there's a lot about farsighted out at the back is the thing that can convince you that it is
conscious then it is conscious we have no way of exploring howl in one way it's conscious so all we can do is take it on faith that if they can convince you that conscious then it's conscious that sputtering tax there's another idea which says that we may eventually be able to observe this stuff actually the connection between the brain and the mind how the mind comes from the brain if you think about it the mind is madge
it too last right now and that's why in the mind that kkk this whole episode inextricably tied with religious fox the idea that the full lives on after the body dies it's magic well we've seen this before in science we've seen things that were previously magic be calm science for example we used to think that sickness illness was evil spiritual infestation or a curse of god and we later found out it was back to
ray and viruses these were just theoretical bacteria and viruses they were observed directly under the microscope which at magic when a science and in a similar way that we recently been able to really hands-on can observe this theoretical concept of gravity we've been able to sort of interact with it and observe it physically perhaps we can calm to see the mind through science is unclear if will come to that point but it is possible i heard one sam
idea that philosophy is basically a moving a caravan of fingers a bunch of settlers moving westward constantly thinking about how the universe works thinking about all the puzzle settled there when something becomes observable experimental test of all outcomes of branch of those philosophers to settle a colony of some new colony over here a hard at science it's a cold neural physiology psychology and the like
but that philosophy keeps traveling west work ever in search of the infinity of mystery is to the edges that philosophy is ever at the front here thinking about stuff we don't know at any time we come to know what outcomes of science science is branched off of philosophy comes out both philosophy settle the colony and sits with its lat well perhaps as we've done in the past where we've created science for magic
philosophy pushing ever west words into the mysteries of consciousness will find something that will stay there will actually be able to make science of this mystery this heart problem of consciousness so those are two takes on this concept one is that gave you we haven't gotten there yet with the science and the other is maybe it is not signs of old maybe it is purely and definitively a subjective thing the heart problem of consciousness so let's talk a little
and about some ideas rattling around the space of philosophy and cognitive science an artificial intelligence about we're conscious this comes from theories within the heart problem of consciousness remember this is called a heart problem for reason we can get observe how the mind is created from the brain and so we haven't spaces lot of guesswork one of the most compelling ideas in my opinion out there is something called the merchants emergence the idea that dumb
i mean this simply emerge and property of the brain that cures to be the case we have the brain it's firing it's doing some stuff come to some numbers planning some actions the unpredictable and repeatable an observer always an outcome the mind perhaps that mind doesn't exist in a different dimension they're not two separate things as dualism holes but the minute it is the byproduct of the brain the brain is the thing is doing the thinking in the mind is simply
the experience of that in other words one f thinking is simply experienced they are the exact same thing this is compelling we know that intelligence coms in scale by human is more intelligent than a snail but snail is still intelligence we all accept that sales to humans everything in between our intelligence in their own way because they're thinking machines are computing device is navigating their way through their environments will perhaps
consciousness is simply the experience of intelligence intelligence is the thinking the brain is the device on which the thinking occurs in consciousness is the experience of that thinking is experienced in fact the case we would say that humans are more conscious than sales because intelligence in consciousness are basically the exact same thing we just think of them differently intelligence is the capacity in the action and consciousness is what comes of it now
it makes sense we said that humans are conscious we all agree with that we asked whether our pets are conscious our dogs and cats conscious well we said yes it appears to be so their lights are on at least well of cats and dogs are conscious of what our rats what about fish house nails no word on snails well okay to say that one thing is conscious and not the other where did you draw the line and why we don't seem to have from neuroscience some definitive center of the brain without which we say
you there is no consciousness and biological organism like i said this is a heart problem sorted pure arbitrary where and how you draw the line between species which are aware of what's happening in their environments experiencing a quality of subjective experience and those which don't so consciousness cons and scale is directly tied to intelligence says one school of thought out there asap field within that this emergence property theory is something called the computational theory of mine too
and it says well if consciousness is an emergent property of intelligence in humans and dogs and snails well what is it that these things have in common there are information processing systems within their brains we're computing the stuff that happens around us using our intelligence architectures if that's the case then wouldn't any information processing system any computational device experience consciousness as well off now we're getting into artificial consciousness
computational theory of mine to the c. t. m. holds that the consciousness is the emergence property of computing computing we've defined intelligence we've tied it to humans and snails and that robots we said that consciousness maybe the byproduct of intelligence in a merchant property the experience of intelligence we agreed that artificial intelligence by it's very name does indeed exhibit intelligence
and therefore after all that artificial intelligence shorty indeed thereby exhibit consciousness as emergent property of its own intelligence again everything in scale the idea is humans are extremely intelligent and extremely conscience as far as we can tell by comparison to other lesser of biological species but let me be the case as well with artificial intelligence perhaps ai is extremely intelligent in it's own week way in specific over
polls ai is better at image recognition than we are in some cases certainly ai is better at math and really hard computational stuff than humans but there are many aspects in which they ideas less intelligent than humans in that regard perhaps a giant experiences consciousness it's over coals and which it's very effective so for example perhaps experiences very highly lucid visual experience
perhaps by the c. t. m. ai when it sees a thing with its convolution own works and such experiences seeing an extremely loose incapacity better than humans but in the areas were false shorts which are many it is less conscious many humans and certainly am i present relax self identity so it doesn't appear that ai is yet in our playing field but it may according to the c. t. m. at least be all we're experiencing this
insisted are happening consciousness and scale because intelligence is in scales to consciousness is the same as intelligence just thought of different way one example of the city she discovered keeping your head it once we start moving forward these episodes is this thing called word to vet word to that is a component of language modeling in natural language processing solo stuff about language read it to it's like chaff box and ca
defying tax as being about this or that swore sentiment analysis figuring out if what someone said it was emotionally happy or sad or map reading as new language falls under the category of natural language processing and these models that we use in that space or call language models one of the pieces of all that's his cold war two vet can work to that is basically an ai is vocabulary no word to that is very fascinating
because what it does is it's the war of words in vector space so imagine three space of all these dots all over the place like stars in the galaxy and each word is represented by a dot and through a complex algorithm that wanted to if you drop us a dots or placed near other dots based on their word similarity so if you wanted to come up with a similar were like a thesaurus you wanted to build a thesaurus out of work to that call you gotta do is land
near your doctor and take any of the ones nearby and you have all the related words were to bank has some really cool properties that you can actually do word math you say queen plus keen minus manic equals and outcomes wallman is really cool stuff well if all of that computational machinery yields a highly accurate vocabulary system that can while less than a caucus on its capacity for understanding words then whose
say that that understanding isn't just a mechanical system but experienced understanding exactly you the way we think of the word understanding who's to say that ai it doesn't exhibit understanding already in things like quarterback okay computational theory of mine i'm again i'm not finding myself with any theories floating around about heart problems of consciousness i just want to sort of bringing down all label
lance to get you into sin spires need to start exploring the stuff on your own one more idea of branch of the computational theory of mine is called the integrated information theory i i ate he i liked he says that's effectively all weirdness or at least see woody awareness comes fromm accuracy of the information contained within the idea is that if you have a machine learning model computing in computing in computing initially all the
waiter random they're all over the place the model doesn't understand the picture it's like static on a tv and the more you train it was supervise learning for example a maury training training and trade it's more accurate an actor inaccurate and that awareness is basically the level of accuracy in the system the level to which the system accurately represents something the lack of which is called and for pizza that's the crux of the integrated information theory is that entropy exists in information systems
according to information theory is that the whole plan to met it's called information theory and most of information theory seeds directly into machine learning for training our models and when you were relaying information or trying to come up with a accurate model of something entropy or chaos basically is howl in accurate your system it's something has high entropy an intern in to guess whether things are capper human then it to go either way space we're flipping a coin fifty fifty had no idea the more you do
greece and turkey the more accurate your model pecans and you can go back to him in that the cat as c. n. human human cats had given him a cat perfect score good job machine running model low entropy means high accuracy and i i'd see says basically that awareness this crux of consciousness we've been talking about calms fromm reduced entropy scott an interesting idea it's kind of like if you think about dreaming for example where or what more do we mean we're here to how the ole ole ole we're in
yes in fact we use dreams to make comparisons to consciousness and on consciousness we think that we're kind of conscious in a dream yeah but not saying consciousness that we exhibit in waking life well in our dreams up here is that the information being processed is in some chaotic fashioned high entropy there's lots of theories of what's happening in dreams why we dream something that we're basically practicing a bunch of spins on things we've seen in life the base
a machine learning about this know how bout that note we're like trying up lunch of different light predictions from the day that we experience and doing great in dissent allowing ourselves to think outside of the box in order to improve the accuracy of our productions the i. t. says that the dreams high entropy low awareness look the waking life low entropy high awareness who knows a lot we don't know about dreams it could be that we have our memory white when we wake up could be that
our attention mechanism is out of whack tension is seems to be a core component of consciousness by the way some people really don't like the computational theory of mine or the emergent property stuff because they say well look here's a big difference potentially between the brain of the human brain and pewter bring a computer x. determine as thickly in hasta do with whole based on him with a keyboard or running a program where reason women registers deaths completely determine its degree where humans have free will
free will we can do what we want a very interesting point my friends free will you say you'll really like the tv series west world west world explores the level to which free will defines consciousness we are not home free with that statement free will is believed by many to be a non existence is our own brains or biological structures built on cells which
built on chemistry which are built on physics etc their own brains are completely subject to the laws of the universe as well presumably our brains act exactly in accordance with our environment proof of this is a better brace earth acted by chemicals drugs the things we eat our exercise regimen that all affects depression our own personalities are called sure where we live defines our personalities were families are the people we surround ourselves with brain
damage studies like this thing now is talking about with the news gauge getting a spike through his brain and altering his personality the alteration to his personality is well known by neural physiologist stay they know exactly why and how that aspect of his personality change based on the part of the brain that was affected if our brains are so physically and biologically and chemically determines how could you possibly say that we have free will we indeed our subjects
to the laws of the universe of the brain that is subject to laws of the universe and if we believe that the mind comes out of the brain and not the other way around aka dualism then you must believe that the mind is subject to the laws of the universe indeed and if you believe that then you do not believe that humans have free will of course we still need a word for choice and decision that prefer not cortex of our brain kendall's action planning inaction planning happen
it's so decisions are made but many think that the magic is not there the magic of free will do you think is there that makes a special and a daily el ball from artificial intelligence does not exist in the way you think that you like the topic of free will how it relates to conscious are highly recommended t. v. series wes world's lots of fun okay let's switch gears a little bit we're talking about the c. t. m. an emergency the idea that the mind is simply a necessary byproduct of the brain that any computation
all the vice any complex computational device would thereby exhibit awareness at the very least unconsciousness depending on how you define it now if you sit with this forbid you say okay humans are conscious maybe doctor klein just maybe artificial intelligence is conscious of how low do we go organs a fish is conscious recess knows conscious armies a calculator is conscious a calculator you just said any computational the vice now according to the c. t. m. yes yes the calculator
respectively experiences a flash of awareness of the thing that gets calculates don't remember everything in scale intelligence in scale we have humans are more intelligent as nails humans are more conscious than snails by this idea the c. t. m. according to the c. t. m. then will it help leo doesn't have memory doesn't have self identity it doesn't have attention it doesn't have perception at lax soul very much of what makes high scale consciousness heist
no consciousness but according to the c. t. m. the only thing that matters is awareness week on jan one plus one and we hit the goals the budget connotation happened registers or turn on off its reflect from zero one and potentially according to c. t. m. that flash of one plus one is experienced and he goes away a very interesting idea indeed not the most widely held lots of ideas one round less which years now we have the c. t. m. saying that the mind is unnecessary byproduct of annie computing device a different theory holds
that may be the summit is greater than the parts of the human brain or least biological brains the way even your wrongs overworked specifically in your old physiology and neuroscience in mammals and reptiles and fish and such maybe there's something special to that the summit where the parts perhaps the brain must be just sell in order to get consciousness and we don't know how or why but possibly you have to have new
aunts were least something very similar if this is the case then in order to achieve artificial consciousness we would need to approximate biological brains it's called biological plausibility the phrase a biological plausibility means how close does an artificial thing represent it's biological counterpart so for example a plane can be thought of as something of a functional approximation to a bird
it's wholly different hubbard has feathered flopping means in its tiny and cute and a plane is a giant metal thing was stationary wings propellers and wheels it doesn't biologically approximate the burden to achieve the flights therefore it is not very biologically plausible biological plausibility however it functionally approximates the burden in that it flies able to achieve flights now let's turn to neurons we have you
and brain and it's neurons which biologically and functionally create intelligence and consciousness we have artificial intelligence which functionally creates intelligence now the question is whether it can functionally create consciousness we explored the c. t. m. ballots explorer or biological plausibility in the artificial intelligence a popular and powerful technique employee is called people earning which we're going to be exploring in the next set
of episodes in this pod cast deep learning is based on a functional approximation of the human biological muir on what they said in the beginning of this episode the creators of the artificial miron came fromm the heart of brain science as they were trying to come up with a mathematical representation of the human you're on a mathematical approximation and they came up with the perception on the perception on became eventually the artificial neuron and the artificial neural net
work on artificial neural network we have used to great success in a wide variety of fields is given us the screen flexibility in performing mental tasks in machine learning in artificial intelligence the level of flexibility and accuracy pirate machine running algorithms have not given us what's more is that we have had to tweak the artificial neural network to create dedicated architectures for particular tasks we created
a recurrent neural network for achieving natural language processing tasks the record neural network a convolution only all my work for a vision based tasks a deep kuna work for planning and so on so we have this sort of master algorithm of the artificial mule mel work and in weeks weekend over here for language or therefore vision over here for planning etc at first that here is to take away from the magic of the neural network but if you look at the brain
and that's how the brain does it the brain has the center for speech call broke as an area of center for vision called the primary visual cortex the center for planning called the peripheral cortex and so on their structures there architectures and sometimes even their new ron's or tweaked your q. optimize performance of their particular tasks so we're starting to see you little bit of biological plausibility we saw some level biological plaza believe that
the neural network itself came out both sides of the brain the perception was a mathematical model of the biological me on and we further advance the architectures all of a neural networks can work to achieve specialized capacities which looking back seems to biologically approximate human brain as well so we seemed to be achieving decent biological plausibility so according to the fierce to say the sun is greater than the parts we may be addressing them
with what we already have indeed learning now that may not necessarily be sell the human you're on of course is biological and chemical we have neuro transmitters being transferred from acts on to again fright whereas in an artificial know my work that's not how things work additional eight it appears that neurons in human brain seemed to fire fire fire fire but continuous level of chemical interaction where's neurons an artificial know my work are one shot feed for more importantly there save
very big hedge to the biological plausibility of artificial millman works compared to biological know my works and that is that while a artificial neural network when diagrammed on that piece of paper when a symbolically represented in the mind's eye looks like a biological neuro my work that is not how it looks inside the computer the algorithms and davis structure is associated with an artificial mailman work or stored in ram
as registers over here in ones and zeros reflect and bitter change in variables are altered and things are crumble hard drive and things are taken off the hard drive and it does not look like a neural network when put on the top computer represented the mentally ill on a piece of paper or chalk board in your mind we're lookin' at artificial me all my work represented physically our computer we're working with a computer's architecture nothing at all like a neural my work so that defeats biological plaza bill
a biological plausibility is not helping us here if we believe that sounds great are the parts so there's a minus one for artificial consciousness if you believe in the importance of biological was a building over many do not believe in the importance of biological plaza believe instead they believe in functional isn't as i said a plane and functionally approximates a bird in that he achieves flight well can a artificially on a work function only approximates a biological know my work here
that gets that she's intelligence well already yes we know that it does to do so in order to achieve consciousness that's the big question the heart problem so there we house biological plausibility and function was on those two were sort of head to head competing theories within our official consciousness one of the main proponents of the necessity of biological plausibility is a man named john sir all and see my friends you'll hear his name as you
dive into the world of our official consciousness he is a hater a hater of artificial consciousness they disbelieve are for everybody that believes that we can create artificial consciousness he is there any hit on the one hand you have re kurzweil who's a true believer in on the other hand you have john stroke was a true disbelieve or his main argument is called the chinese room argument that it goes like this you have a man who speaks english who does not speak chinese and he's inside
but the room in the room has a book on a counter of instructions on how to translate surgeries symbols to other chinese symbols this chinese symbol of the place the records as summary slides a chinese symbol under the door and he walks over the door in accepting it looks at you looks around he's trying to his bearings you what the heck the money goes over the instruction book and he okay so i wrote a chance so you influx of pages of sully finds the thing in his hands and okay so this xinhua
surround room that instruction book says that this symbol translates to some other set of samples apparently some is asking a question and we can contract an answer and there's a map i'm acting in this instruction manual so he goes over the korean instruction manual member of this this in the other cards and you look someone's left hand he looks up once in his right hand is not so yes those special instructions say that you walked over the door he slips the three chords ages picked up under the door
and he answered that the chinese question correctly is the chinese room the man understands english is not understand chinese however following instructions he is able to speak chinese it up years however we know that he does not speak chinese man in the room is not speak chinese he's simply following instructions so this is sort of the argument that following instructions it does not yield understanding does not
old consciousness now back to honor argumentative chinese room is that the man that does not understand chinese true love the room what does the system of dies the man is simply a cog in that system the idea here is that a new ron does not understand language but the whole broke this area a cluster of neurons is a system guys i make goes back and forth you're starting to see that there's a huge debate in cognitive science going on between people
the left side people on the right side people to believe that we can synthesize consciousness artificially and people who believe that that is impossible to do so if talk about biological cause a billion we've talked about functional isn't how those go head to head one more point on the topic of function was a big deal functional ism is that if the thing functionally it she's it's gold then it doesn't matter what happens on the hood doesn't have to have biological plausibility if a plane flies it doesn't need feather wings if the machine is conscious
then it doesn't need biological neurons but that's sort of the question reverse how do we prove the machine is conscious we just hands so we rely on things like deterring tasks there's this whole conversation round zombies in open debate of artificial consciousness imagine we have a bunch of zombies in the world urges you know brazen there are walking with their hands out now add glance right away you think there are not conscious of casey turned como or the transfer just now
there there's there's nothing there and this is sort of be on the biologic plausibility side people saying well what about what about zombies animal functional isn't side is if the zombies appear to be exhibiting intelligent behavior imagine them not to sing brains walking round up of their actually conversing with u. etc the function was say that there is no such thing as a mindless zombie if you have something that is exhibiting intelligent behavior it necessarily is intelligent
and necessarily is conscious okay okay there's a lot stuff this is a guy just dumped on you call lot of credit thoughts and theories in the space the point is that there's a big conversation happening right now the big conversation is especially begin hot right now because look what we're doing in artificial intelligence look we're making big things are happening over in google in facebook i. b. m. and by two etc big things are happening
in those bring big questions we have the human brain we can look at the activity in the brain the electrical pulses through with the centers of the brain under an m. r. iraq pet scan our cat scan we can look at the centers are activated the purity associated with conscious box with awareness we can boil consciousness down definitively twiddle least awareness and possibly a depending on the many other aspects such as memory learning self identity attention perception it
cetera and we know that all this comes from the brain but were vexed with this conundrum with the brain is over here creating a minute to minute years to be outside of the physical dimension how can we possibly think about the minds in such a way that we can answer the question of whether artificial consciousness is possible with it is possible to synthesize consciousness through our endeavors in artificial intelligence it's an unsolved mysteries in and settled the bait car
and if scientists are debating to and fro left and right you see this cloud of fists as people are fighting effect actually there's these two cognitive scientist philosophers in particular that just he'd show there it's really fun to watch daniel janet and david chalmers david chalmers is actually we get coined the term the heart problem of consciousness and daniel than it is a very very good author about all things consciousness but that these two are just like a name call each other they fly in the heck out of the choices that big fight
going on in space the current size like this said earlier in the episode we have seats at the table of this discussion that's why this is so fond that's why i'm so interested in machine in an artificial to others that we can participate in the conversation about whether or not is possible synthesize consciousness and if we can synthesize conscious my friends we can create the soul of this is prospectively the most important thing that humanity has ever done has ever accomplished and i believe that part
shh lentil engines and junior's machine learning engineers and that sort might be the missing piece to understandings equation one of the most lasting quotes that his stock with me was between these two bears the date about artificial consciousness myers will those guys who says the site is something you can talk about codified to skinny you agree upon the definition just let him go and he said look look here's what consciousness is consciousness is a camera looking at s. c.
e. we have cones and cubes and spheres and we can dissect them we can do all the science and math to understand the scene that were observing weekend science the heck out of the room that the one thing we cannot science the heck out of his ourself why because we're a camera we're looking at the scene and we cannot look at our self consciousness is definitively subjective outside the realm of science him
possible to make objective in some guy in the debate said young masiello mirror he smiled there was a pause and everywhere you look to him a smile faded unless you have on me or my friends i think that the artificial intelligence engineers are the ones we're going to solve the mystery of consciousness we're gonna build the things that makes it possible to interest acts
we're going to build the mere we're going to make a minute one more thing this thought that this this the creation of both or fish want oceans has been mankind's purpose built into our brains from the very beginning according to the theory of missing you larry the technology is advancing at a exponential or at least plano me old case why in such a predictive graf why so predictable some believe the
technology is actually an extension of evolution that evolutionary advancements has been on this whole no no graf as well since the dawn of evolution on earth through biology has single cell organism become a mauled by cell organism weeks became real explosion and dinosaurs mammals monkeys humans and out of humans cons technology advance in advance and investment and out of technology calms the next species visible effect evolution and technology if on such ed
to her monastic unpredictable graff is necessary an unstoppable all we have stopped evolution through medical science now anybody can live there is no survival with its maybe you know evolution can't be stopped and the next logical step is necessarily synthesized life that this house to happen is a necessary extension of evolution and if you look at our technology through the ages we've had made of artificial intelligence in the columns on
jewish folklore to the fascination in the renaissance of ot hama in perpetual motion machines rene descartes and leonardo de vinci to the fiction of frankenstein and robots and of course violate to the obsession of artificial intelligence ever since the dawn of computing all the way back with alan turing an old mines since the nineteen fifties the creation ai seems to have been an obsession of humanity since the beginning it may be our destiny
and it may be inevitable were on the cost of something enormous my friends you cannot deny that what's about to happen maybe the biggest thing that has ever happened in human history consciousness okay that's the end of this episode i said people may be coming to this episode for its own right outside of the space of artificial intelligence a machine morning if you are interested in participating in the creation of consciousness in artificial intelligence is the right place to be yet no experience of our
the show and hold hands than my recommendation is to be involved in machine learning machine learning is the gateway drug so start at the beginning of this high caste series for the resources for this episode i'm going to leave you with one major resource i've recommended it to claim and time again it is a course by the great courses called the philosophy of mine brains consciousness of thinking machines it is it pays to see about everything up and talk about this episode and more there is great stuff out there by
aren't philosophers and cognitive sciences daniell dammit david chalmers et cetera but those are little bit more specialized one is star from the beginning degree courses series will guide you where to go from there some just believe you with that one resource and pro last episode but julia is going to tell you the game plan for the next few episodes whether estimates put this into two seasons stopping now i'm actually made you at least a few more episodes and they do some stuff on recurring allow works and calmly
shone on iwerks and then we'll reassess from there whether i'm going to stop and take a break but you've got at least a few more episodes out of me before my brain is run dry enough that nothing else to teach so let's make as much headway with my brain as possible and then i'll all and then we assess from their okay guys see you in the next episode


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and algae it time also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks the sum which could prove beneficial in your machine learning education jury find that atlas c develop dot com forward slash l. l. h. welcome back to machine applied today we're gonna be talking about developing within the p.
dub us environment in other words using eight of us as your local development environment now before we get started let's reflect back on acquire episode about doctor i didn't episode where i said you can a package up an environment and it's dependencies and their projects source code into a darker container and a ploy that doctor container to the cloud will we do is we're ready doctor file literally called doctor file with it
it'll be at the top of that doctor file you specify any operating system you can be using an end within the doctor file your gun as specified any number of operating system will packages you want one stall s. s. and k. gore are put up to tiananmen and you might install some pit packages you can either directly in line the doctor file suit against all tax wian c. or you can have the record as stock text file the kids copied into the doctor container in that thing gets kicked off with a pittance called the ripper
and stock tax and then we'll see in the doctor files is copy some host directory to the container directory or capitals see opie why copy space the location of the source code on your computer relative path so if you're working within your project route and at your project route there are a handful of miscellaneous files and then within the source to rectory that's actually the source code for your project lawyer python for
owls and then within the darker container it's expected to be running out of the forward slash actor director read one you'll see is caught the four slash source space for slash actor and what that will do is it will take your python files from the store structure in copy them into the act directory of the doctor container and then you'll typically have a darker composed r. y. m. l. file and that file will spin out a bunch of different doctor can
here's the wallet handle the orchestration of multiple doctor files the small scale few really wanna be deploying lots of doctor containers that scale and handled the networking above them all together you'll be using cooper nannies not darker compose doctor composes for very simplistic and barman slick on local host intruder in eddie's is for large scale deployment of doctor containers to cloud or not talking about cooper navy's your talk about that no layer episode it sort of me
mutually exclusive to the type of deadlocks that i'm talking about in this episode know last two episodes local los you have a darkened goes far to spend a good bunch of doctor containers and handles the networking of those containers to each other now has to developing on local host against you doctor containers what you'll see in those doctor compose files is volume mounts mounting it will take your local source directory and it will muir or it into the running die
her container so that copy command will actually copy of files into the doctor container one time snapshot fire and forget but the volume directive will mount your local directory into your doctor container at that destination in an hour way you can actually developed within your doctor container on local homes as your programming in tight charm on your local host your changes to your source files
give me your word to those source files location the darker container so makes editing your files on local host wall you're developing a breeze bought back hobby directive is the best thing that will actually be used when you actually build your doctor container your own doctor bills and then you put that up to the cloud that hobby directive is what takes a snapshot of your files mr structure to copy them over to that location in the dark continue
one time builds the doctor container and then pushes back to the cloud so the copy command is a one time deal the final is asian of coppinger files over into doctor container that the volume directive allows you to me are you did on a barman into doctor containers that you can and it's within your doctor or container on your local environment now this seems like alot of we're dark herself before getting into the tub us by you'll see why it is an analogy to the stalker and doctor compose staff when we're
talking about developing on a wor spin your local environment the other reason i'm dreaming of darker get in this episode is that doctors still can be used quite extensively in a job us and in this episode when i talk about using a u. s. as you all barman doctor is used very sensibly all over the cloud all over the internet all over a dub us even when you're using their manage services like a ws land aided us age maker east yes
for gave these are all things that you can use doctor within and is recommended that if you have that option did you do so for example in aid of us lambda which led to deploy its single python functions to the cloud as either rest and point or some python function that you'll be calling won office within your tax that well you can write your python function and provide along with it the requirements doc text file and a ploy that away and a function but if you how
whole lot of requirements being your requirements stock text file is preferred that instead you packages all ought as a doctor container and you deploy your lambda function as a doctor container and asked for various technical reasons one is that if you deploy u. lambda functions in the traditional way which is actually to build those pit requirements on local hostess it to them put them into the best reebok as is it filed and then
and you deploy your land of function from there which is pretty sloppy in my opinion and also there's a file size can't believe how lot of requirements jurgen exceed backpack very easily and oftentimes indeed the fall lambda environment safe for example the python three point eight landed in violet doesn't have the same sort of operating system set up that is needed for a lot of these picked installs an umpire being a huge one number doesn't work out of the box in the d. falwell
the environment sergei working edie fall and an arm is committed an act is that just doctor rice you're out then you'll use that doctor container has your lambda function because installing on pie with the doctor container was all done for you that's the magic of doctor containers the silent environments that the operating system while okay so bunch of doctor bill let's not talk about eight of us here is a pinpoint and i suffer from wow what to deploy a no fee to the cloud well i have a doctor compose all
i'll file on local post it has a post press darker container in fact the p. r. darker container a client doctor container in the server is able to communicate effectively with the database just fine because they're sharing the same network bridge it just access is the ports on local host five four three two in the choir they will access the server because again the containers the facts tape yeah i'd doctor container sets up all of the rock scene of network for quite
stan exposing the porch to local host on my behalf as of the clay hisses server the server is the database and then dates back to serve back to client fine and dandy on what the host though when it comes time to deploy this to the server that's not how they tell us operates in the u. s. is much more complex than that in orgy get my a server doctor container in the cloud i have to first push the doctor container to dc are the last
container registry on eight of us then i need to set up a u. s. four gates which is a sub service of peace yes elastic container services on eight of us for him at the c. s. is for doctor stuff is for deploy new doctor containers in any of us manage and byron i have to pull from dc are too far gate and then set up my whole for gay stuff okay so step one with the c. r. step to setting up for gate
step three is getting network requests to my forte container nakano why do that quantity of us you'll be using elastic low balance or a dolby or you'll be using the p. i. gave way and then bull slovo services need to be set up for handling network requests boxing each t. t. p. requests on down to your doctor container and those services have to be tied to a domain name that domain name
is gonna calm fromm began the u. s. roque fifty three and if you want access solved if you walk h. t. t. p. s. then you're also been a need to use the aid of us they see them service amazon certificates manager my database is gonna be deployed to party s. s. relational database service and exposing that database true mine doctor container on far gates is no small ordeal i have to set everything up with
nemec a dub us v. p. c. at the virtual private cloud i need a private sudden at the public's out neck with a year that gate way the server is to be placed within the private sat next an inward have outbound traffic it also need a gnat gave way the data bases to be within the private senate and also needs to have database submits associated with it all the parts within this stack need to have secured
groups set up so that they can communicate with each other and then at the part that is exposed to the internet user application load bouncer or to gag a way it is kennedy within the public seven that so that was a whirlwind did that confuse you good that's the point i'm trying to make is that everything you do on local host is almost entirely different wine wedge to how we're gonna be putting this into the cloud harding be putting us on a deed of us
and so what i offer to you to do instead is start developing on local hosea doctor compose file on prepared forty of us when that time coms and that time will come instead developed everything to wreck leave within it the dub us develop your entire environment from the get go in in a job us tech stack now how the heck would you do that
you have your local computer and then aliases the cloud business for hosting things in a cloud in the production or staging a fireman we mean developed in aid of us on developing on local host an ada bua says the u. n. at that doesn't make sense we'll try it doesn't make sense but there are ways to do this and that sort of be discussing here there are a number of ways to do this as on a tackle them bit by bit the first is the most obvious way to handle this and that is this
that your environment in the cloud yannick a dub us first we'll do is go go on a deal u. s. consul you printed count going to the console and the first step is to go into the dub us of the p. c. the v. p. c. or virtual private cloud is sort of the entry points to all your deal us stuff it's wedding caps awaits your services together into a single unit and allows them to communicate with each other through networking and keep some private from the internet can
safe and secure or an annual decide how you're gonna expose that the p. c. to the internet as the case may be so for example few walk the rest server or a web sockets server graf to wealth you'll set up your v. p. c. by way of securing groups and sob nests and entered a gay weighs in at aways molly sings such that one port is exposed to the internet and strung up to some servants bus service may be easy to work p. c. s were
forty eight or lambda seagoing for your v. p. c. you set your sadness and your security groups then you go on over to the i am console and you set up your users and you set your user and some permission stuff that you had on over to eight of us lambda and then from within lambda you can write your python code has worn off python functions at any time that up to the guy gave way and you expose it to
a gay way to the internet and was cool boulware that is that there's actually a coed environment on the lam that consul you can actually write your code for your lambda functions in the web browser using their python aidid or their jobs get aidid he saw this is option number one option what number one is not having the local garment whatsoever you write all your cody in the cloud in these lambda function handlers because there is an i. d. d. on lambda
the less you can get your coat your python code and deploy these land of functions test them the cetera is also an aide of us service called cloud nine is an entire aidid and server places adam on the local host or pie charm on local host not cloud nine does not wholly candle to supply charmed let's be clear cloud nine is meant for simplistic editing of coated in the cloud on eight of us but it is another wait
to handling this option one i'm discussing which is not having all the host whatsoever instead doing everything in the browser one year or the other u. s. consul so cloud nine you can get clone they get repository for get hot and you can get in your files on cloud nine and it keeps track of course of change files from get so then you can actually make those changes get commit push those changes and cloud nine is strong not to bother the of us sir
says like dc art am lambda so that it lets you run your services for testing your python functions so if you were serious about the your coat macleod you would not be using the lambda echoed editor as your bread and butter you'd be using cloud nine to endure python files and string nose up to lambda within cloud nines functionality for connecting your coach who lambda the lambda coordinator is
or for quick and it's to your land of conscience and of course perot last two episodes i discussed this sage maker studio has white python notebooks in the studio on fire and so you can and it's an orion and test and deploy aid your machine learning code in this age maker studio is using a different body didi vivendi cloud nine aidid is using high python used to burn no books so the amazon didn't lie
the jitters that is just was commonly used by data science and mushy learning engineers said they just set up an environment on stage maker said you can use the tools that are familiar with namely jupiter no books like python no books so few writing in testing and running in and deploying in your machine learning code you can do that ball in the cloud on stage maker studio tonight python old books as you doing the same for wed at four server
location code you can do that on cloud nine now that's a cool option developing everything a cloud has a handful benefits there one is that you not tied to your computer in our stock with a specific operating system supporting some functionally that you need more or less say you have a work computer at home computer anyone at the old pop back into your development environment well it's in the clouds so as can be propagated from your desktop that worked
we're just opera home 'cause just running in web browser or something i faced recently actually my computer doubted my mac guidance i had to transfer my environment over to my apc announced to cannibalize if i had set this all up in a cloud nine and sage baker studio i would have to deal with that the down side of the senate is that cloud nines and sage baker studio these are not enterprise gray yet by the g. e.'s these are not super powerful ids what times we really like our local host of armand we like
the tooling that we have on our computer by law of pie chart and i'll love gina grant degree it is for database management it said jack brains i a g e four day yet i can browse the tables and look at the table meditate itinerant sequel queries the consort and in wine center of an anti charm it is my preferred i t. e. for python development is is wonderfully powerful cloud nine sage makers doodle on a camel the pie
china comes to just law python editing capabilities so i'd still rather use these things and therefore i don't use option one but i just asked instead let's move on to option two options you is that you set everything in the cloud like i'd just mentioned you have your vp c. n. you're by am rolls nearby am user you have your party este de sewer lambda functions you have your v. c. r. doctor can to doctor repository eight yeah gay way wobble
blocked by when you want to get your coat on local hose and run some units casts against services in your tax back on a dub us you have to take one step and that is to connect to your local computer truly you ortega be west tech stacks of the pc by way of something called a client v. p. m. so you can set up something called
client vp an what that does is allows you to connect your local computer all right hon all and delegates from all truly your age of us is the p. c. and n. now when you want to connect to a database in york v. p. c. on any of us your part yes database you can do so because you're operating within the vp see you wouldn't otherwise be able to do that because your database was to be contained within a private v. p. c. does
wanna be exposed to the year that you only wonder database to be accessible to services running as servers whether as lambda functions or on far gate or peace yes or beseech you and those servers also want to be within the private side net of your v. p. c. and only accessible to the dinner next by way of your net gain weight on the public's avnet and that is hiding in to buy thirty p. id way or application labelle
sir so you don't have access to these resources the services on local host but you can get access to them if you can next to your v. p. c. by way of a client of the piano and so what would you do well you're right you're lambda functions as python functions you would attest to those functions now these functions may be making requests to a database on our t. x. or they may be grabbing secret accessed he's using
each of us secrets manager that's one way to store private dana like database user name and database password is not things you wanna stored in a can feed on jason file or as environment variable caesar's things you want to store in a secret manager which handles secret rotation encryption all these things automatically for you another thing that you don't really think up until you're starting a pusher stuff to be dubbed us sold your python function can access your party us today
it's in your secrets manager secrets and maybe ask us an sms and these other aid of us services and you can write your python function and you can run a unit tested against that python function and it will work because you're operating within the vp seed of the deplored the other us and by running a macleod and then when it comes time to actually deploy those functions then you do this set of packaging up those python functions in says it files or doctor containers and pushing them up to date of us lambda
and then finally the third option the last option is there is a service called a local stack local stack it is a open-source project it replicates the aid debbie west tech stack and it's all running on doctor containers on local host you have a doctor composed out why am i all file that spins out all whole bunch of local stack services and
the services are running on local host not on a wor s. on local host and they are fake the other u. s. services so you have a local staff s. affiliates service so you do is you don't you're doctor composed out why mel file you would enable the x. three service you'd say doctor compose out deep and local sacked will spin out by doctor container whose end points that you'll be accessing with your pry
jack to replicate everything and then to be debbie you wes is f. lee service offers same for dc s. and p. c. r. and co deploying code pipeline in r. d. s and every service and almost every service that they give us offers as actually quite an undertaking that's a really powerful service and i'm surprised that it works as well as it does that their accounting for all of these and pouring secure make
and coals to beat the job u. s. backed by way of rest calls or c. l. i. calls the aid of us see alive or bono three calls this local staff to project tries to replicate all of a dub us services and all calls they can be made it to those services and also replicate how were those services ron all on local host using dark
containers is pretty impressive and it's a daunting an overwhelming undertaking and there's gonna be som loose ends i imagine within the local stacked project i haven't used it extensively by the amount to which i have used it to his work surprisingly well sonnet keep using it for the time being but one thing to note is that it has a free tier emmett page here and a free tears all the typical sur le stuff
like lambda and past three and dynamo g. b. and the page here is all the rest of the stuff like r. d. s and v. c. yeah monroe fifty three in all these things as fifteen box 'em on sight if i recall correctly and so if you're gonna be using locals that extensively for setting up your deal us stacked bill bestselling what use their pro version and the benefit of using local sack over and actually deployed the debby west act is one cost savings
because if you're deploying and they tell us a stack and you're getting me into that stack from local host by way of a client the pc then running the server says macleod may cost you quite a lot for example party yes the database service it's kind of an expensive service where previously i was running my post crest database as a local doctor container now all i'm running it actually has a hosted a part
he asked database on a date of the west the main reason being that i want to develop against well actually be here facing way if in the real world when i'm developing against the local doctor container hosting post grass well that's really easy to work with it bypasses all the stuff that i would need to set up macleod so instead i deploy in a party yes instance to ada bua spite played the piano into that
the p. c. and i had actually have to make sure that might i am policies and my son that's in my secured eager for all set up correctly so that i can connect to the database so it's making sure i do things right the first time measure twice cut once and running that party este based is a little bit expensive is not terrible with little bit expensive but since then i ran all that on local host using local stack which is using doctor containers but the entire is set up such that it
acts the way board yes would act in the clouds are still do need to set up those v. p. c.'s inside nasa security groups nor to connect to my local party yes database then i'm saving the cost of running that database macleod on paying fifteen bucks a month for the program but that still cheaper than what i'd be paying for spinning up an art yes instance multiple times throb week in a cloud hands in or once a lot faster because it's running on local host there's low latency in making
he's now work requests to local close as opposed to the clout connecting at any dinner facing with these services will be olaf astro mobile homes as opposed to climb vp entering into a v. p. c. and if i make changes to my eight eight of us back by way of terror for for example which are to be talking about midday here then that deploying those changes lot faster on local host using local sacked venues on the job us when you act
the kickoff infrastructure changes aunt ada pos is actually lake bringing on wine hardware and moving things around from one physical location to another physical location so that your architecture changes get reflected in the cloud before it's available to you to now with the p. and back into two tester changes doing that all all go host through local stack it just brings all the darker containers down and brings them back up the way needs soon it's really fast it's really easy
to deploy your infrastructure changes using tear form on local host of local stack now i brought up before the idea of mounting your source directory into your darker can cheered as opposed to copying the source into the dark container for a build there is essentially an equivalent of that for local stack there's no way local stock for mounting you'll pull hosts source files to your lambda functions are being deployed as law
i'm the rest and points in your local stack infrastructure and that really is due to see here to treating the job us as a local development environment because otherwise without being able to mount york hold true lambda functions that are as quote unquote employed you're gonna have to basically rice and cohen and a ploy to landed in the work no okay tweaked echoed deployed for an end to the work man that can be really painful process so
another huge benefit all local step is it allows you to mount your local python functions to your quote unquote deployed lambda functions and that way you can continue to be in line it it to your land of functions in pi charm waldo vaulting running and testing your code so those years three options we have developed everything in the cloud you don't even use and i eat eat eat you use cloud nine and sage maker studio best option one option shoe
what is spin everything up in a cloud by developed on local host black why old developing on local host you're still connecting the services macleod not connecting to running a darker instances on local host know your connecting to running a day of us services or doctor containers running on a of us services and you were connecting to those by way of applied the p. n. n. in the third option is that you're running everything on local host in local sack
which replicates the p. l. u. s. text act and now you can access it all mobile homes and eventually connects to a presumably with a client bbc is well so if you wanted to set up your client the p. c. on local host to local stacked so that you cannot do that again in your cloud and barman when it comes time you wanted just do a few quick test against you or are deployed environment before picking everything off one other major benefit of developing against each of us
first is that you become acquainted with the offerings of the dub us and start to realize that they're all lot more offerings on a fabulous they can replace things that you would otherwise using your project as it packages for example and that will actually get the job done better more securely and hosted in that if you had developed everything on local postman doctor can tear you might not have known about you might have foregone and then when it comes time to deploy duke
out you wish to use that dwi service instead so for example in the sage maker episodes are talked about using data wrangler to transform your features and impute missing day yet in all these things well if i had known about the wrangler were i was writing one of my machine learning code in the past i would not have donned that costume by hand in hand as if his dear wranglers handling of it is gonna be more robot
stick simpler to set up an upscale all importantly with data coming in from some stream of the deal wait a minute be using eventually deploying no feed i need that to pipeline to scale and the waves with now is not steel ball so i will be transferring might cost i'm a feature transformation coded today that pipe one so they can scale so that there's various points in the pipeline and to tap into some for example getting the
normal and getting us at different entry points of my machine learning architecture i wish i'd known about that first 'cause i wouldn't have written it the way i did i assumed that this needed to be done in python another component being sage maker experiments their hyper planner optimization capabilities would rather had i known and heavy use experiments so they can distribute and scale running my hibor planner optimization jobs rather than though
way i handle them all on one computer at present in my jeep you based doctor container will doesn't stop the sage maker there's a whole bunch of services on eight of us they can replace components of what would be for example york python server sold another doctor container in no fee is the entire web server the whole thing is running on fast eighty i've been fasting yeah i hamilton whole lot of things for you and your third party plug ins for fasting
god for handling other things one examples the proxy never requests well the gag a way war application low bouncer old to that automatically for you you don't need to deal where all like engine attacks and how are moving day she p. requests from the internet's to your container one other thing is actually a load balancing or for example let's say you want some rest and points of your server to be throttled so that anyone user on the internet
he had hit that endpoint a million times a minute well the key i'd gave way has a built-in you can just click it check box and say throttle this route we only want one user to be a bullet hit this and point x. amount of times per second where previously i was using a fast the p. i've plenty n. for throttling specific rest routes that one is called slow eap aren't given fassett the isle of slow them down welcome a guy that an any use a gag aways built-in
handling all of fraud owing and finally another example is the storing up of the user accounts this was really important for early i'm using a fast the gagged one and that handles storing user counts to the data base their user name enhanced password an e-mail invalidating them as necessary sending the forgot passwords if necessary sending it a activation code and i have to string all add up myself i have to spa
safire those often routes as throttles be using slow eight yard and i have to send out the jade of the u. t. jocks we call it which is sort of that authentication tokens this commonly used in the web development space when you log in to use your count on the server big shot gets sent down to the client beckett stored somewhere like a beast teach people only cokie on the web browser and then you send out that jocks flu
mercer virtuosity to rest requests for that user and you also have to handle invalidating that jocks if the user changed their password you have to handle expiring that jocks every five minutes or so and then requesting your refreshed token all this stuff all that stuff around authentication took me weeks even with the plug ins and tooling available to me from the fasting p. i'd ecosystem
i'm just not comparable handling authentication on my own even wasted being managed as open-source projects i would rather there be a service dedicated to user can storage authentication and all of the security and e-mails of new account forgot password and activation and in validation appointing someone serves it does is automatically for me and blow on the hold it of us has a service called courtney yell i did not know that
no it into asserted put no water my stuff on any of us if i had known that in advance i would have developed the usury count system in the court need elk now i am migrating the user can system of fromm fast tape yeah i to cardio and so one benefit had identical being against each of us first instead of a darker based global environment is i'd probably would have discovered this service early on in the phase
and i and then i would measure twice and caught wants in fact as i'm going through my server module by module and point by ten point one finding myself doing is getting the entire server stack the entire server stack and instead moving and briefing into single a dub us lambda functions so previously i had won the old doctor container was a bit jail
in python functions all as fast a p. i rest and point's well now all with all of the cooling available to me by way of the gag a way a gag aways web sockets supports their authentication and authorization handling it by way of carbon yo and the authorization hair hitting eap egg aways jock to validate or i can just write these functions as single python functions without
any tooling you know like database connection cool management stuff like this is steady leave it to be a u. s. to handle all the men at cooling that i would otherwise be handling myself by deploying each function as single lambda rest and points you know way all of eight of us is offerings sort of replace you're not only your infrastructure sack which is obvious because that's what they give us is is clough
i'm hosting thereby infrastructure but also replacing one of your modules and plug ins and framework say you'd be using at the server are hosting lovell now setting up your environment the trish always to go on to the key of u. s. consul in the web browser and then you click around you set the vp c. n. i am user in r. d. s database some way and a function string everything together put all the hunted yeah gay way clicking around into
being in things this way is not manageable very on manageable and becomes more and more on manageable overtime and you want to be old track these change sets these changes you make to you are stacked in some way that you can replicate see in the future so for example if you were spinning not a text back in the infrastructure we call it on a dub us for setting up a database in a gag a way inland
functions and you're doing this to get away from your local doc curtain closed out why the alfaro and you previously had your doctor composer ueno filed in gates one get hot well then future users and future you will miss the tracking of infrastructure changes in gates and you would have to sort of describe two at a future use your heart set up you text back with clicks keeping track of all of that and managing your infrastructure in the web consul isn't
non starter it is not the way to handle meiji era for structure on a ws instead what we use is a concept called infrastructure as code word infrastructure in code and there are number of projects out there for this chef and ann sobel been very popular one is called care reform cheat or rpf or can care reform as to why use and tara form lets you write code
that sets up your tax that quantity of us its infrastructure as code and our way you can put your chair form files in to get into your kitzhaber posit tori and so that when you move from one computer to next computer or another user calms on board and they wanna set up the same infrastructure for testing the code and running that project they just ranchera formed in it's an inter form applied annual take those terror forms
owls and it will set up the infrastructure on a job us so what you do with terror form is that you the first step is to make sure you have your ada be asked by eight am credentials set up on local host and we use those i'm credentials to make sure it can spin up services in the cloudy has the right permissions to create services managed services on your behalf on a dub us the money raised her form files
one might be for setting up your priam rolls on other one might be for setting up your land of functions another one for your party yes database another one for your v. p. c. for securing groups your side that's your client vp and for setting up your client began and to your certificates on a c. m. for s. s. l. h. d. b. s. and your route fifty three domain amen always thinks you use some off the shelf sample terror form files you modify them to your liking
and then you run for four minutes and her form applied and it will set up your entire infrastructure or your entire text act on a daily us in the cloud in the new king king next to that text back with your client c. p. n. and then at the end of the day when you're done with you or developments when you're done coating against this stack you can run a turf war and destroy it and it will bring all of that infrastructure off line so if it weren't for terror form war
c. t. k. horror server less framework for one of these other things all be discussing a bit these infrastructure as cold projects into war for these projects i would not be doing this episode because it would not be worth managing the setting up of your infrastructure all through the web you why because if that's what you had to do anyway if you were stocked with stringing together your entire infrastructure on any of us with clicks and typing in webs
earns them in the pain point then mentioned at the beginning this episode of your local of armand being different than your cotton foreign well that would be a moot point you have to set up your cloud and farm about her weight anyway why because we have this infrastructure is code projects liked her form we can orchestrate the development of our cloud environment in code and then we can put ourselves into that environments and that way we let us say we measure twice cut
but once we set up burn barman the way it it's intended that we can develop against we do the hard work first and then we can replicate that environment to a staging environment to pay development are meant to re testing in vermin and finally to production and terror form is not the only option other players said ann sobel and pop it in chef these are all popular framework says well on their framework so little bit more popular and modern but i see in me
use today there's amazon c. t. k. that is the amazon cloud of development kicked it's essentially liked her form is a infrastructure as code framework written by any of us for a job us for deploying your services to the clout and then taking them off wife union the future now why the hell why not use c. k. and championing everything a debbie us in this in the last two episodes this
ek is what might be in the u. s. forty debbie west where the heck where i'm not be using c. b. k. and said i'm using tear form well this a couple of reasons and their love it more personal for mining project one is that at present currently see the case alluded knew or didn't care for one or the limitations i find lucy became visited does not support no on manage services like cloud needed services for example so one stinking coy
i got hung up on what i was working on a project that's so one point i got stuck on one hours working on a project at dat was that i wanted to deploy as chat walked in i did this by way of the dub us lacked speedily as lex busy chat box serbs elect's you set up a box with a handful of conversation flows and any number of responses to give in utterances and then the slots
he named entities that we're gonna be pulling out of the users' response is so few had a customer service bonnet said hi how can i help you to name the person said credit card help it would pull out record as a slot well setting all the spot this is not a service did you manage like beseech you work for gate in the traditional sense on eight of us is a cloud need of offering cloud native means these rest calls rudy's got a three calls that you make any don't have to manage the
running in hosting of u. or servers c. k. does not support currently november twenty twenty one does not support setting up alexa box that there's a bunch of stuff you need to do is set out black spots sure it's not i can manage to service by there is still stuff he needed to set up the data set up these example conversations and the types of slots we walk alone staff as a terror form supports that bessie case does not terror form supports all lots of
nurses lot more services and most of the other infrastructure is code projects out there that i've found including and most of the cloud native and server less capabilities on eight of us in azure n. g. c. p. so that's the main reason why it shows terror form over c. t. k. another reason lots of people choose terror former over c. k. is it your form is universal it works on g. c. p. and the microsoft
azure and other clouds services and other cloud of supervisors were c. k. is just for a ws the benefit of c. t. k. is it easier to use and it's amazon first so first class citizen so c. t. k. will continue to improve and rapid clip supporting more and more debut as services and tying into them very effectively maybe in a way it's such that where terror form because it covers
such a whiny the territory would give anyone a job you esther russell little bit of second class support just by the nature of being such a big project in a big undertaking c. k. would be loaded more fine tuned and dialed in first class support for the sake of us services i don't know that that really is something you worry bout i have not had any shortcomings on a hill us with terror form at present it supports everything i've
ever needed to support in a very efficient and bob fried capacity but the biggest thing i see the biggest difference i see is a tear for ms very evil blokes in this complex is a huge learning curve worse c. k. is really dial in and sleet and very efficient and stream winds it's less code c. k. is less code then terror for the terror for covers more services take your pick us up to you on aka champion one over the other bride personally
stir form and then there's this thing called the server was framework which is really popular infrastructure is codes framework but it really you mostly supports her was technologies is really dial in for dwi slam dunk and dynamo deep eap aidid wake them less so for party yes for example or g. s. is not the server less service is a managed hosted the basement clark and so the server less framework is less like
lead to be compatible with these non server lists services and more compatible with the server lust services like it at this lambda and dynamo t. b. so that's another reason i choose terror form or server list now finally we talked about local stack hosting your entire or the dub us tax act on local host care reform and c. t. k. are compatible with local stacked so you write code be a new war terror form
files that tells her former we're actually pointing to a local host knocked to the cloud you set up a whole bunch of it and points the inside your turf on file b. c. 's for us to local roads dynamo dubious local host a gag a rigorous local host and then the next time you enter form applied actually runs it against your local stock environment and it works it actually spins out all those them barman using tear for infrastructures quote the way we do
against the actual he got us cloud and then again huge benefits of terror form of infrastructure is code generally is that if you wanna make a modification to your tax back you ron terror form applied and then it applies at just that and it just apply some modification annabelle propagate modifications through your tax act as the case may be which is super powerful so for example if you make him
modification to your v. p. c. or to assad met with in the vp see your security group within some sob ned well if you were to do that in the browser you have to then go to all your services every single service that needs to be made all we're all that modification and you would have to then click into that service and then i just for that modification as needed and some things are in a slip through that
racks you're not forget about some services that need to be made aware of those modifications terror form will not if you make a modification to som deployed service in york infrastructure and terror for mizell way or the dad is a dependency and all other services in your infrastructure they will first make the modification to that service let's say v. p. c. and then it will propagate the necessary modifications through your entire infrastructure
to those other services it knows how to make only the modifications necessary without doing anything to destructive and another example of non destructive modifications a tear for ms cole act is if you write multiple lambda functions on local host and let's just talk about landed here this is really powerful bissell showcase how efficient her form as let's say you have five when the functions top five python functions on local host maybe a handful of that
our web server functions handful are worn off functions and that you want to call from within some other area of your stack let's say one function is actually a prawn job so you can connect your leno whole host to reorient us the pc using klein vp and so that you can run these python functions better eventually to be calm lambda functions you can run them on local
host maybe you're running these functions in a high test sweet the unit tests suite is to call for these various functions and then it is able to you on within your v. p. c. so you have access to the services that you need within york eight of us infrastructure and then after you run your tests against your functions you're content with these functions and i you are deployed east the lambda and tara form well how these lambda of module chunks
with your turf firm file where you point that block true that file and they will handle automatically when you enter from applying it will handle packaging them out as is a thought putting in august three if asked you deploying the lambda function and then if you have specified to find that out to the p. id way and setting up the necessary permissions so that's cool you're a bypass a whole lot of the steps that you've had to take wake packaging that is it
file perignon s. three and then deploying to lana for mass three s. three step process that you're able us get just by using tear for them the next step is let's say that you modify one of those functions only hands you leave the rest alone us a new test school we're back in business and terror formed attacks that told you that file has been changed based on zombie hashing our rhythm above like the modification date of that file one desk
this sort it all it hoist unnecessary changes to a single and a function not to the other land of functions and if necessary it may propagate some additional changes let's say two deep yeah gave way where it's tides to that specific land a function so everything i'm saying about developing against a dub us on local host as opposed to a darker got wine all file is only really feasible if you're using infrastructure is cold
liked her form is made especially feasible at least a lot quicker and save some money if you're using buhl local stack framework but i'd have antacid local sacked super for ollie so as far as the amount of coverage of capabilities of aid of us that they handled your mileage may vary but is four feet trying out first and of course we're in the machine running up white hot gas so within you were the pc connected
through the coin the piano's you are you have access to sage maker you can kick off stage maker training jobs so free this episode which are praising sage maker's studios like python no books to write your machine learning code so that you can kick off machine learning jobs within the sage maker environment well you can opt to create a sage maker studio project you can set them all up either in the web browser in aid of us is console using sage major studio
or are you can orchestrate the infrastructure set up all of sage maker through terror for months that and then it buchanan came next to your sage maker environment within your client vp and so that you can kick off training jobs and all that stuff on local hosts rather than being on stage maker studio now why would you wanna do that wall let's say you're doing the rest of your web to thalmann servers development for the way
i've described in this episode or maybe you don't wanna mix it up too much so we wanna keep our machine learning developments on the same time in byron clyde beatty and connected for the p. c. so that we keep kids off stage maker jobs but to move so they could use by charm on all white guy python notebooks personally i don't like you burned nearly as much as i like jet brains pie charms such a powerful id the so violence is continued he use my id eat for developing my machine learning code on local host but then i'm able to come
next to the stage megan firemen second still lion sage maker or training jobs and inference job zen experience of hyper prayer optimization all less stuff on local close enough so when when and of course you still get all the bells and whistles of sage bakery can still then hallway into sage major studio on the web and ruled that the future porn says that's just it out from shack in autopilot cornered spearman for new training job the model monitored clarify
all these things is still be all the output based on the features that are provided by sage maker but you can simply do your development on local host so that's how you can develop against a bill us on local most measure twice cut once start building your project on a dub us as if you're running these ads like test data bases or test servers connect to it with the clyde beatty and so they you're within the environment to run your test co
and now way when it comes time for deploying your stacked in our canadian side swiped by all that set up you'd normally have to do this on terror for infrastructures coat or siddiqi a horse arose from work because managing your infrastructure in the wet interface all the aid of us or azure orgy c. p. as in not gonna happen simply not happen for complex infrastructures and give local second try soon you might beals do all the stuff on local host now oh
like i said this is all a bit mutually exclusive to an alternative to height of infrastructure orchestration systems one of which is true in any case i'm actually in be talking to somebody adapt here soon to give me a rundown on how krueger nannies and crew buffalo especially for example for machine morning pipeline orchestration compares to cloud hosted it managed services the way i'm describing in
zepa sewed they're entirely different ways of handling infrastructure krueger nannies what true burnett he says is you give it a bunch of doctor files so all of your services for whitney has darker files okay rather than using more real wine on the manage services that are offered up on a dub us or azure orgy c. p. you're relying instead on open-source projects contained in doctor containers
so for example in a yummy us if you want a message to use ask us well the open-source equivalent west us there's something called zero m que anders rabbit and you so you can use rabbit times you doctor container and then for the server you do you see dirty job us lambda or forget the hostess or that i might use the ws lambda for running a server on any of us well you would actually hoster server maybe as a doctor eyes fasted he eyed container two
bring many states all these containers strings them altogether handle an hour came between them put them all into its own version of the p. c. and then deploys all of these services to the sea two instances if your ear be running this on eight of us or the equivalent of instances non je c. p. or microsoft azure so krueger nannies is intended for re legal relying on doctor and open source projects
ooh slurring all of your services together you're not gonna be using a dub us as your backbone really ought to be tapping into the various services are made of us you are to be hosting it on eight of us with cupboard eddie's would you see pure azure wherever you are a host it but the services themselves and the way they communicate with each other and away the scale pop and down that's all managed by trooper nannies not by each of us it's all open-source stuff that handles itself and
and you deployed the trooper nettie staff to be of us into run eddie's has what's called this master server on the control plane that does all the orchestration of your services with respect to each other and i will do us smartly and as cheap as it can so for example to have multiple services and they all expect some number of ram and c. p. u. n. three of them are running on one easy to container that actually has extra program
and c. p. you available then it will spin up but doctor container on that dc to be instance rather than spinning at a new dc to instance so anyway all essay that cover nannies is entirely different mutually exclusive fashion of deploying your architecture to cloud one which relies heavily on doctor files and open-source projects and what it does is it has this mask
sir and it keeps dibs on who's who within this back in order to decide whether we're going to steal upper scaled down certain containers with him this back as opposed to care reform or c. d. k. or service framework which is infrastructures coded is actually managing services that are offered by a dwi asked these are not necessarily open-source offerings they're not necessarily darker containers though they k.
and the terror for manages your stacked on a day of the u. s.'s cloud offerings by a dub us other mutilate squeezes the internet is actually consulate cos there's a feed the crewman eddie's feed to run your control plane in the clouds as one downside of kern eddie's went outside is that is sort of busy universal solution so you can move you work for many sack off avail us and on to g. c. p. and aid of u. s. c. c. p.
and azure they all support to burn eddie's they all have a service that supports to one eddie's this is not just as support out of the box each of us is services supports to burn eddie's is called the tape s. and by running your true bernice clustered in ek s. service that's it is iniki guess that you actually encourage that feet to run your clusters control plane and the reason i'm bringing up to one eddie's here is that it is another alternative to infrastructure esque
oh terror form it is a project for infrastructure is coat and so was cooper nannies but that they handle deploying your stacked the cloudy been totally different ways another reason i'm bringing up is because as i mentioned i'm going to be interviewing somebody from debt on how they used to burn eddie's to deploy machine learning pipeline to the cloud by way of cuba flow and so rather than using sage maker this person and that is actually using cooper nannies
to orchestrate the entirety of staff everything i discussed in the last two episodes of ingesting your day death and transforming the day impugning knowles feature store everything it's scales my cursory says dana pipelines training monitoring debugging him and deploying this person rather than using sage makers hosted offerings on any of us is using cube flows open-source containerized offerings in or
to string together the entire and and dana pipeline in a universally deployable and open-source fashion which has it's perks no doubt as so i will be reporting back here soon on how things like tube flown to burn eddie's compared to manage services like sage maker and they give us all orchestrated through terror form so hope to get you that at this
but in the near future at all see you then


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and algae it time also starting a new contacts which can use your support and it's called black mayors like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode eighteen natural language processing part one in this episode i'm going to
ark about the sub field of machine learning called natural language processing and l. p. and l. p. is a sub field of machine learning and is quite the radical i'm going to take you on an adventure away from machine learning proper us specialization we've been machine running this is going to be a three part series this first episode i'm going to be describing the general topics in an l. p. in the next episode will talk about the shallow learning in traditional approaches to anna
he and in the third episode all talk about work for me own networks and more to that that is the deep learning approach state of the art in an l. p. l. p. is a microcosm of machine learning proper it's his own world of machine learning our rhythms and l. p. uses maybe half traditional machine learning algorithms and a half of its own algorithms dedicated to its own fields semester i mean i'm going to be taking your rabbit hole excursion away from machinery
hopper by now as the time is any to start thinking about specializing in machine learning you get to a point in your machine running career or you know the fundamentals of an owl algorithms the math the general approaches but nora big anywhere new career you need to get really good at something specific in machine learning there's that image guy is the reinforcement learning guys endgame ai and self driving cars it's hind forecasting keep
all who do stock markets are weather forecasting etc and there is the language people battling with processing is everything related to language in the domain of the knin machine learning spoken word written tax machine translation if words are involved in any capacity natural language crossing is what we're dealing with micah said at some point you choose to specialize i can think of it like in our keiji we reached level ten you have to choose area b. m.
age you're warrior or robe well a level ten machine learning once you got your basics down even have to choose which specialty wanna go to an issue and so i personally actually it's hatch r. p. g. classes to these various specialties i think of the time series people who work with time series forecasting especially in the stock market is the rogues right because it deals of money i think of the reinforcement learning people who were dealing with action sequences like in the game ai insult driving cars and think of them as warriors because ago
well with my warrior defeating the dark lord analogy from a prior episode i don't really have a big class for that image people maybe honduras because they have to have good eyesight omelet support our g. but there's the image recognition camp people are doing is classification and tagging light are processing for some driving cars and damage related machine learning is especially popular today with the rise of virtual reality and augmented reality be able to handle motion detection in room skill sets as well as on 'em
it seemed imagery onto real life objects etc and then naturally with crossing i think of them is the mages wizards because they just have to cast spells with language right with incantation say cast spells the real lot of books so i think of the mages is the language people naturally which costs and incidentally it is an l. p. that i've personally chosen for my own career i do want to promote an elk he has a very viable career choice specialization in machine learning it just so happens that
every machine learning to interview i've had thus far has been an l. p. related i don't know why they're not targeting me because of an l. p. in my resume and i'm not targeting them because of an elk he'd done the job descriptions i think it is so happens that an old he is a very rich and promising ecosystem in machine learning in the job market today if you think about it makes sense is so much value they can be hat through natural language crossing think about the various
cesspool companies in machine running out there and what they're working on sunday siri and google assistant though the matalin was causing you talk to it whether speech for taxed and a response to you after performing some action advertisements are still want most profitable industries around facebook while let company is particularly good at image related tasks such as facial recognition in the image of loads stiller bread and butter is advertisements and how did it
i mean what advertisements to show your users by natural language processing on the things they write the things people write to them et cetera and of course the granddaddy company of all natural language processing ever is google everything gould as is an l. p. when you perform at a google search you cast in a query you'll find all related pages on the internet raids by relevance your query if you're query was constructed in question formats the whirlpool
you're really answer your question at a relatively new feature this card at the top of the search results will literally answer your question if you go balsam and c. n. organization or a person will show you a bunch of information about that and be on the right side bar few duel with it he it'll show you a picture of their lobo when there were founded where they're located who their founders were etc an image or your revenue made at two boys by ads advertisements relevance to query you searched on
customized to your typical query history everything is language everything is an l. p. so there's money there's potential and there's jobs so i highly recommend considering an elk he has your particular specialization within machine running okay so let's talk about an elk he has is a microcosm of machine learning is an elk he is little bubble machine learning that looks in general what machine learning where previously in the world of can
shooting we programs discreetly specific tasks one thing started yielded too complex for that we had set enabled the machine tool earn a generalized to perform particular tasks whether it's productions were making actions that's the same in the world of an elk he and all peace started with linguistics linguistics simply the study of language with grammatical structure with what parts of speech words represents how semantics or hold
the sentence structure how certain things means certain things linguistics study of language it's been around forever on natural language processing is computational linguistics the another word for natural light was crossing is computational linguistics so one point in time the field an elk he was specifically dedicated to encoding in the machine all rolls of linguistics that we have enumerated heart coded and l. p. version one was hard coding grammatical struck
your sentiments oriented words parts of speech rolls into a database or spreadsheet work sri some sort of hard code system that will enable our algorithms to parse tax as a result of that being hard coded l. p. in its infancy was not considered a branch of machine learning it was considered a branch of ai machine running as a branch of ai an adult he was a branch of ai so machine learning and an l. p. were siblings then
a machine learning started to untangle and help you with its whoops sorry to contribute significantly to the performance and flexibility of the algorithms in the space of a penalty so much so that one might be forgiven to say that an elk he mostly consists of machine learning our rhythms there are of course construct some data structures and tasks which are trying to perform with the n. l. p. which give it specificity and autonomy uniqueness of its own words
still it's own fields people and steady linguistics people could set a penalty with that anybody who's anybody in the world and l. p. today is applying mushy learning algorithms to penelope another comparison i'm going to make to the microcosm of which she learned is that like machine learning with world is now moving towards deep warning models deep learning models acting as one models rule them all they can do sort of spring cleaning on all these dedicated task machine learning models improving
complexity inaccuracy in certain circumstances and oh he is doing the same the current state of the n. l. p. is a myriad of shallow learning she learning models and cutting edge of an elk he is steep learning specifically algorithm recover in the third part of this series called rip current neural network augmented by way of something called word to that deep learning and deep learning what those models us is one more complex and flexible systems for
taxed parson but to one model they can handle so many tasks that previously required dedicated models with in the world of natural language processing soul talk about the learning in the third at some burnout way you can see penalty is all three branches machine running in it's own way autonomously has it's own agent it has gone through sort of the same growing pains of experiences the machinery proper has gone through in a way it's also a microcosm of machine
and so like i said in this episode i'm just going to talk about the general goals and tasks and parts of an l. p. i'm not going to be talking about algorithms toby for the next episode but i wanted to see two space and motivate the different types of things you can be applying an old piece you know like i said simplistic we can just think of an elk he has anything related to language taxed spoken word anything if it had seduced language it's an l. p. now what makes penalty special compared
other fields and she learning is that we are dealing with tax machine learning wants to work on math on numbers text is not numbers and so i know he has construct for converting tax the numbers were handling tax and some bag of words model et cetera that some ac unit compared to other machine and system's another thing that makes an l. p. you need is it is sequence based sentences are we're we're we're we're we're we're words can modify h. other than
grammatical structure influences the meaning of the overall sense so the model that will be using an old he or sequins based models or kind series based models if you recall for prior episode what's a good algorithm for time series the stock market's chains will get to that in the next episode with the market models now even though i'm not going to be discussing our rhythms in this episode of his name dropped out rhythms as i go through some of these parts i find that helpful kind of like reading chapter hundreds of a tax bill
knowing what you're getting yourself into so that help he i like to break it down into three three layers three tiers that this is not something that exists in and out yet never seen it presented this way this is a tyler thing at the top level we have goals like large tasks that we're trying to perform very high level very lofty big things that we're trying to do like machine translation or be answering questions the big step below that are sort of medium level two
asks so i call it tasks that are essential in order to achieve our goals things like teasing apart the sentence structure of the tax cuts call parsing syntax tree parsing or figuring out what role at each word play is in a sense that call part of speech tightening or figuring out which components of this seconds more important for particular task pulling out names of people were dates were things
it's called names and city recognition so these medium level tasks are essential to accomplish our lofty goals to have goals we have tasks underneath that and we have parts and i just consider these the sort of text pre processing the little bit spots in and to just have to do and get over with things like cocaine is asian were you have a document was the word to me at the top but it's all works to turn it into a list of words warlord k.
seen all of those words if you do go wolfert queried well you probably don't want the capitalization that the user put into the queried to matter for the capitalization as it exists in wet pages to matter though probably lower case there fiske word lower case the documents seeking to ease your matching another thing will talk about it called limits his nation and standing where you turn something what we're hunting hunted hunt her etc into the just the word hunt so that if the document has all worth it is important you but not in the morph
logical contract that is presented it's still counts so these parts this lowest level tier of an elk he is this just basically this text pre processing so i'm going to talk about these three tiers and individually before we get into that i wanna talk about a very important distinction between two words what is syntax together semantics syntax and semantics it's important distinction to have your minds matter what walk of life doesn't have to be en el pizza maybe is that
initial hope you outside of this pod cast syntax is sentence structure said was at syntax is the structure of the language as it is presented its it'd be like parts of speech so what role each word play is whether it's a noun verb adverb is cetera parts of speech syntax tree parsing so part of speech constituents so what amount freeze or more full logical presentation work so does the worse for without proper case or leonard is asian and stemming in all
things so it's basically just the structure of tax syntax semantics is the meaning of taxed the meaning of the fundamental take away after you read a sentence so semantics is important especially for our high level goals i love machine translation for example machine translation can't just work on syntax as you'll see we talk about machine translation have to be able to encode
it's being said in some fundamental way in on the computer so they can be de coded into the intent in the other language or we get to the n. l. p. deep learning episode this thing called work to back is basically building a dictionary of words as they relate to each other can text sully is really like building up the meaning of words in the dictionary not just the list of words a bag of words a cog not just the list of words with nothing attack
so then billy conn with some sort of meaning attached them in a fundamental way so syntax is grammatical structure send structure phrase structure even worse structure and semantics is niemi important fundamental to take away from all we're worth sends word documents syntax versus semantics and you'll see those serbs sort of categories of different types of tests that will be performing okay parts of the low-level stuff
like text pre processing tasks like that middle level of stuff which is basically you sing tactic parsing of words and senses and goals for the high level big things we're trying to achieve what machine translation our search engines etc so let's start with parks we're now beginning our lesson on an l. p. a. documents is just all blob of taxed whether it's a sentence or a paragraph or a chapter or a book however you want
the fina documents that a lot of tax a court this is the list of documents so when we're working with natural language processing they're very popular core cora which is the plural of corporates are very popular core pour out there what is called the pen tree bank basically anything with panda work hand in it p. d. n. a. and stands for pennsylvania is in the university of pennsylvania you pen is significant
traitor to whirl of an l. p. and they've built all lot of court for a list of documents out there for use in training our models in and out the sole one court this might be a list of news articles in the view that for detecting something about news another court this might be a list of documents which is really useful for learning how to parse sin tax structure from sentences for one reason or another maybe there are examples of very
easy part trees mixed with examples of very complex partially cetera that one in particular is the pantry day so we corpus the list of documents which may be useful for some learning process in our cannot he endeavors okay we have a court vessel was the documents we are the document any tax blobby won whether it sends a paragraph chapter books etc and in every document is composed of words we call every word of the token token i
token is low but more complex than just the word token can include smiley faces or punctuation or anything like that it's on the individual component of your document is important and can play as a contributor to parsing of tax then we will use it as a token so simplistic we can think of tokens just words more complex an addict natalie being a number of things like punctuation smiling faces another bits so corp hora house documents documents have tokens
then we will operate on these tokens in any number of ways for example in pre processing our documents for use in our machine learning models we may want to remove shock garbage we call these stop words words like called handed ah vaught these awards dinner so frequent that kind of grammatical fillers they play an important role in grammar if your doing very complex machine learning tasks we
indeed depend on grammatical structure than you do what we saw around busy doing very simple task which simply depends on the words in the dark medulla say you're doing a search query any disqualifying documents that have a high school or heart height weight as it pertains to the words in your query that you don't care about these stop words so one tree processing step might be to throw way stop words major machine learning over the morale of the faster because yet once they get the war
on another thing you can do with tokens is to reduce them not more full logically produce more full logical variation so morph biology is the structure of work whether it has the upper case in the fight or what affix says it has whether it's by angie work edu past present future tents stuff like this so the structure word is this more fallen c. n. you can reduce its morph
what he by removing some stuff let's say you one removal ion g. o. p. d. or any of those things to reduce all words that have past present future tense whenever it's just it's one day's work it's stam it's called us so this process is called stemming so what is it previously if i'm doing this google search crews and i'm looking for cutting equipment or what is hunting season or something like that will do will probably wants to reduce any variation
oh hi it's whether it's past present future tense vintages the base for hot sold chop off the ends of both the words and documents that it is green against and my query i don't think it actually does that is is an example so that's the stand this damn it is the core of the word by removing the thing that can make it very from tends to tense or whenever a very similar concepts to stems and stemming oral lemons and let his asian
oh p. m. and a llama now for the purposes of this pot cast episode you can think of them as identical stan is the same thing as alana stemming is the same thing as one of his nation their differences on standing as as far as i understand it as little bit more of willy nilly by fast action where are limits is station actually works alone with more on the semantics a word in order to find the pure root of work rather than chopping off the
and so light the way i understand it i could be wrong is that limit to station is a more pure sophisticated but computation expensive version of standing so stems and lamas then within the world of tokens we have another thing called and it's distance now we're getting a little bit into the territory of algorithms machine running out of bounds at a distance is how different two words or from each other so let's say caps and car there deferred
by one word so in a simple way you can consider them and at a distance of one so you might use at a distance for example in spell checking for spelling correction or google might use at a distance in suggesting were were you misspelled it's okay so let's just the list of these little small bits odds and ends on an l. p. we've got core pour out which is lists of documents they're sort of related to each other in some way
it's about maybe one corpus is about news another court this is a list of books better public domain like moby dick anything by shakespeare etc another corpus maybe a whole bunch of chat room or kites so pore pore over lists of documents documented this tax blobs dawkins can paint hawkins which are words and punctuation other things and you can operate on tokens in order to change their more fallen g. in whatever way see journey
aides for your machine learning tasks with islam accusation or standing removing stop words etc okay now we move on up the ladder you're at the bottom working with tax pre processing nor moving up the ladder to this middle tier i call tasks were operating on the structure on sentence structure using machine learning algorithms last we're using machine learning algorithms and these tasks primarily relates to
syntax simply working with grammatical structure of sentences like i said these tasks will feed into the ultimate goals high level goals and other animal p. so on this level we have us a category called information extraction information traction extracting information from the sentence structure one such information that we can extract is cold parts of speech he oh that's the hardest
each tagging parts of speech or the roles that individual words play with in a sentence so now owns verbs adjectives et cetera very simple i'm sure you understand this right out of the get go now part of speech to hiding is simple to understand conceptually but can get quite complex with computer has to do the task of part of speech typing and so we use machine learning algorithms such as the market models
and maximum entropy models books that sees next episode in order to automate part speech tagging pos part beach taking another type of information extraction is relationship extraction let's say we have a sentence like apple was invented by steve jobs we have a relationship there we have steve jobs inventing apple so you would have a relationship where invents is sort of like a method name in it takes in parentheses to arguments one being
steve jobs in the other being apple so relationship extraction can extract within the sentence what things relates to other things and how another very important piece of information extraction is called names and steve recognition and he are named entity recognition this is actually a very very important piece of an l. p. i used in your pretty heavily at my last job in an l. p. any art is vital for things like chaff
watts c. yuri google assistant to the center of what any organized is it looks it your sentence at it takes out salient parks things that you're interested in the end so if you ask siri had lunch with john to my calendar on may fifteenth you'll read the sentence and it'll figure out who what when where it'll pick out the important parts launch john may fifteenth case of other entities in the sentence wunsch
john and may fifteenth and then they will name those entities launch his watts okay so what name goes to launch who is john and when is may fifteenth and then any are typically also corresponds with another bit which is intent extraction sweep lot of that the intent to being a adding something to what hollander so we're talking this theory that push a button and i firmly say siri avalon
with john on my calendar for may fifteenth because the it translates speech to tax that's a whole other piece of an l. p. speech to tax id parsons the sentence that may use part of speech tagging combined with syntax tree parsing etc in order to perform a slightly higher goal of names and stuart ignition and eat our perform the task of pulling out of that sentence he says they're important in order to purr
form an action that pulls out an intense that is the action that is add something to a calendar to add to calendar is the intent for the method name open parentheses and then it passes in these arguments these names entities lunch john may fifteenth close parentheses executes the action them so named as the recognition is is a vital piece of an l. p. parsing specifically syntax streetcar
sing parsing teases out of the sentence the structure of the sentencing overall structure of sentence is very related to part of speech tagging part of speech saying is figuring out which rolled each individual workplace noun verb adjective well parsing does the same famed bow with larger chunks these are called constituents and builds into which free so high is slowly have a sentence in you
break that down maybe over on the left we have a noun phrased in lower right we have overworked phrase the boy with blond haired jumped into water on the left we have been known phrase being the boy with blond hair and on the right we have the verb phrase being jumped into the water and they can break those down on the left we have the boy with blond hair okay so boy is a noun and with blond hair is a proposition all phrase if you just keep breaking the senses do
our dna hierarchical format tonight sri structure until we eventually get to pee oh that's part of speech had it so parsing it is a hierarchical structure old grammatical approach a higher level approach and pos part of speech typing is a low-level approach work work work whereas parsing is a tree structure grammatical parsing of the sentence okay so the role of things that i consider
tasks of course these tasks are based on the parks in order to perform pos part speech tank or eighty or named any recognition or relationship extraction or parsing we first have to pre process are taxed by feeding into adult court this or document token eyes a man document steadier limits raising those tokens removing stop words etc okay now we move up the ladder to the top the high level lofty goals
an l. p. the reason for which we have the tasks blow was there with a simple example of an ultimate goal of an elk he spell checking and spelling correction a pretty solves task if you ask me any we've had spell check sense microsoft word of the nineteen nineties spell check may depend upon grammatical structure in order to determine what is billed most likely word you're dealing with but you can easily think of spell
check as simply working with at a distance that i mentioned before see aka tak will maybe you meant to say car were kept find some word which is all of minimal and it's distance then what the user intended to write an edition of clark or cat which is the most likely word the user intended given the sentence they've written thus far so spell checking is maybe it's simple goal come out
number one classification text classification we already talked about text classification with classifying e-mails as either spam or not spam busy buying eerie classification task and remember the machine learning power over them we use barely is naive days indeed naive days is a very popular out revenues in the world of an l. p. s or get to in the next episode so weevil are
talked about classification and you artie understand basically what classification is all about you haven't document and you trying to classify it is this that or the other thing related to classification or i might even grid hundred classification is sentiments an analysis determining whether what's being expressed in a document is positive or negative or maybe even more complex he might break it down into all of you rainbow the motion second the exterior
it's angry sad happy nervous scared et cetera for the most part applications of sentiment analysis no real world tend to be relegated to positive and negative emotion common use cases of sentiment analysis are determining whether a movie reviews are positive or negative see him come up with the overall sentiment about the movie made ill scrape these all from twitter are in facebook et cetera you don't have at your disposal monsieur
early star ratings well sent him in alice's skin below the more complex than you think sarcasm can exist in a sentence which could turn of the apparent sentiments of what's being said upside down or certain words which would usually be associated with positively like fantastic or excellent can be modified to flip the sentiments being expressed that movie was fantastically haw
and then we was not excellence sentiment in alice's is used for urgent harmony overall sentiment towards a product or or a company or other such things there are high frequency trading algorithms alvare which hearts the fire hose of social media in order to determine the overall sentiment towards a product in order to decide whether to invest in that product it's very interesting so sends an analysis has very
high value in business it's that sense analysis showcases very effectively the growing history of an l. p. in general the past would have said no he was primarily based on hard coated rules pulled from the history of linguistics so we might just simply look for words like excellent or bad or four more happy were wonderful like that said sarcasm or ma fireworks might mock that up till then we move on to mush
you learn move from we moved from heart coated systems to machine learning systems like naive days and then bag of word approach it's those get us closer to the gold but they still have problems we still are not over calm sarcasm or ma fires we may eighteenth hole in the system that words like not preceding the motive word we actually then be calm one word not underscore good but that still feels a little bit
i am holding feature engineering and we have to sort of knoll the feature engineering baby-sit take place in order to work with these documents soul state of the yard sentimental says his move us towards deep learning deepening uses things like work for your own networks which will read the sentence left to right and sort keep were running tally accounting for ma fire words and sarcasm and all those things while still learning
pull up for sale in words and patterns in senses senate announces very important pretty complex watson machine learning algorithms used here varying from sdn support back to machines hit markoff models naive days rucker neural networks will talk about all that later another category of classification document classification is to hiding documents that is different from classifying documents classifying documents it
giving it one class in any number of classes acute be a buyer classification and his spam detection work in the mall tino will classification the case of sentiment in ounces but hanging alternately known as topic modeling or keyword extraction is actually figuring out what keywords to apply to this document any to be any number of key words so if we're looking at some programming blog post on the internet him
i'd ask be talking about no j. s. and reacting reacts native and post grass and all these things so it's going to learn to tag this document with all of these tactics us away think above top model it is the machine learning approach to automatically to hiding blog post which you see all over the internet already have tags are manually tags well in certain instances we don't have luxurious manual typing maybe were scraping documents from some corpus and we want to bottom
i play tag them to the week present that as a library of documents that people can sift through by category the common elber the news there is awake and your chalet at our location the old d. a. okay another lofty goal of an elk he is search search engines finding relevant documents of document relevance as well as document similarity how similar are documents to each other so searches really adia
to tighten at query and find relevant documents a popular algorithm used here is called t. f. i. d. f. common frequency inverse document frequency and document similarity is all about how similar one document is to another so for example in a recent cable competition remember cable is a competition board of machine learning it asks where a team of machine learning programmers can compete with another team and maybe for cash cry
is for employment opportunity etc the recent competition posted by cora the website core of cue you or gate which is a question in sir website similar to stack overflowing the like of the competition like this one a user is asking a question that hiding in a title and it's hiding in a description with their question on the core website they want to be able to find similar documents automatically and present those journalists format on the right
sidebar so the user can see if their specific question is has already been asked and answered in the past so they're not submitting a duplicate question so as the task of documents similarity again a common algorithm here would be t. f. i. t. f. r. right let's get a little bit deeper let's talk about natural language understanding now natural language honor standing or an alley you by comparison to natural in which processing
the general field of an elk here we've been talking about last far the sub field within the machine learning of everything related to language that's an l. p. natural language of understanding is holing out of what's being said in a sense the semantics now we have symantec's holing out of them need the intent of us sentence somebody asks a question you have to have natural language understanding you know
urge to answer that question well i said the case of machine translation you have to understand the i'm betting were being coding people were all intents of the sentence in english in order to translate it to its equivalent in spanish so natural language understanding or an l. u. is all the tasks of an elk heed their require a fundamental understanding of the sentence for the word that whoa
hey so common tasks with the natural language understanding what i said question at the answering soaps yuri and ghouls assistance they answer questions if you ask a question you will answer the likeness of the four if you type in a global search query in the form of a question who will actually answer your question that obviously they extremely difficult task in an l. p. i actually don't know the algorithms at play here and try to do some brief
church before my next episode see if i can address which used state of the art in the field of question answering but it's i mean is obviously a very difficult task but requires natural language understanding another thing is textual entail meant as interesting problem is if i say one thing does it imply another thing if you read some bit about donald trump winning the presidential election and then you ask the system a question is donald trump the president issued
no the answer is yes because it can do some sort of processing about the parts of the facts that party knows in order to address the question is being asked is related to question answering have a lot to do with logic tax will entail meant and finally of course machine translation machine translation they call this a guy a complete it's an interesting phrase the i complete what it means is this task machine translation requires all the pieces
c. of ai to work in other words once you've achieved perfect machine translation one maybe could say you've achieved ai well if he asked me we've got some darn good machine translation systems out there by google study art and levy's worker neural networks so have we achieved ai well it's always a moving target hampshire somebody's out there saying no no no but traditionally they've always considered machine translation to the end ai complete problem require
ring of all the pieces of that ai to work completely at least within the space of an l. p. machine translation requires all lots of pieces all lot of components we require parsing we require pos we require a handful algorithms like a shannon's and naive days we've got to encode the meaning and intents of what was said on the left hand side so we can translate it into something on the right hand side tries
waiting list to spanish machine translation is also an excellent example of the power of deep learning because like i said in scholl learning approaches we use dogs in gobs of algorithms where indeed learning approaches you can use one algorithm like the mighty worker in neural network which gives you increase simplicity elegance and yea even accuracy next step we have a natural language generation this is that
to leave generating taxed matches parsing tax we're not in putting tax only we can also output tax golf course natural language generation machine translation would be what those examples another example of beach hat box cat box holding a conversation with you series and google assistants tablets used any number of components within an elk he like an easy or for example a very simplistic chat by my take why
you said compare it to a database of conversations and find the most probable response utterance didn't hire sentenced to a throwback actually k. you say a sentence and when the database of conversations it has its disposal defines the most probable sentence full sentence to a throwback catch you at the simple chap by a cool and complex chat baht will
actually generate a sentence were for work they want to throw you a sense in the database it will all end co what you said using natural language understanding and indeed code word by word or a probable response using proper natural language generation that chap officer very fine work with there is a a huge uptake in chap box in the world say i'm sure you've noticed companies are going to have bought wild there is
it's a push for the concept called you why free or no we you wire know you ax any number of things that the boy they're trying to say is companies are trying to build a chapel i'll answer that your interacting with the chap by either verbal a war on your keyboard and a chaplain is so good performing actions that you don't need menus and buttons and slider isn't hobbles you'll need a good you wax you'll be us at all all you need to chat
a series of push towards us direction so this is kind of his id eyes of an l. p. in today's generations chap octopi think they're confining you know i i think it up properly donna you jerk spurious is going to allow users to perform actions so much faster than fighting it's a bunch of tax on a keyboard so i don't know how this chap walk freezes and handouts whether it's gonna be a bottle or whether it can be successful will see another apple
asian of natural language generation is the image captioning now are boring between the image of people in the natural language people were combining our efforts in order to perform a combined task of image captioning and so you see this from time to time i don't remember facebook in caption images for excessive ellie purposes for blind users that weber who's doing this but this is somebody on o. capturing images for excessive billy
on the reverse ghoul is translating search crews into the damages they represent an if you actually use google photos you can search in your own surf box on earth on some phrase and it will actually bring up for you images that look like that phrase it's pretty cool so image captioning an image searching the condor reversing each other than that image captioning zen example of natural language generation we see it in the image and actually does
cried zeke image word by word automatic summer is a sham this is a very useful and powerful task also very difficult an example of automatic summers asian it in use today is summarizing legal lee's summarizing legal documents so for example how cool would be if the website with privacy policy or terms of use contract emina we reso's sings half the people don't read them because they just don't care sure what everyone is at the service
but maybe the other half wants to read these documents to the so long and it's still have the time to read every privacy policy and terms of service under the sun well at the good automatic sunrise or might be able to boil down a privacy policy into the most important it's sort of a reader's digest or an abstract of privacy policy automatic summers asian is in use today in summarizing actual legal documents for
legal purposes i believe gould one answer your question at the top of the search results uses automatic summarizes the figures out how to sunrise and answered your question without showing you too much taxed all what's natural language generation okay some other odds and ends on aka really cover the algorithms that play optical character recognition or o. c. r. is being able to convert case stands documents from a physical bookworm
physical paper into digital format golf course the primary algorithm at play of an optical character recognition system is indeed the convolution own neural network c. n. n. or consonant an algorithm we're going to discuss we talk about the image recognition that primary algorithm of image recognition is the conde nast of course that's going to be at play in converting the damage into dijon format by certain letters
maybe incorrectly translated and so an elk he will come into play in order to figure out baby in one word were given the second's thus far was the most likely letter for this mistake so both c. r. is another example of a marriage between the image people in the language people and then of course we have speech speech by whole author or ball game that speech to text text speech when you talk to siri ver
or bully it converts what you said early to tax because series reads from tax and then serious response your query with tax and your phone read that back to you with speech so converting from speech to text and back is a whole world its own or you have to analyze wave frequency in structure of audio files of all those things were aka talk about speech
this is a couple buzz words for you there segmentation is figuring out how to choke a body o'flaherty maybe segmentation in two sentences for example and another thing is called a dire realization di id or by easy t. t. i o. and dire realization and the gold there is a nation is to figure out in an audio file let's say we have a conversation between one person another person and i ate a customer service representative any customer
or even at a dialogue between multiple parties the recording of low companies need dire is asian is all about separating the audiophile into the speakers so speaker a. said the following shot speaker be said the following chalk speaker a again saying the following talks of school dire as they should so speech to the whole world its own involving audio processing which i know nothing about and so i won't be talking about
in this context okay a giant lay of the land of an l. p. we went from the bottom where we talked about little odds and ends like core porn and documents and tokens how to operate on tokens like wanted to station and stemming removing stop words finding word similarity with and it's distance we talked about syntax and semantics syntax being parsing sentence structure such as part of speech tagging relationship that track
ten and syntax sri parsing those role in that middle tier according to the high were three car parts tasks and goals semantics is all about word meaning sentence meaning document meaning etc pulling the underlying meaning of all we're whether it's contents jules similarity in the case of work to that there were enough talk about a third part of this series document similarity using t. f. i. d. f. etc so we had death
second tier of tasks primarily relating to syntax parsing that includes information extraction such as pos any your that's part of speech taking named angie recognition and relationship extraction as well as syntax tree parsing there we go toward third tier of the lofty goals of an elk heed such as spelling checking in correction document classification is well it's document sentiment analysis and hiking with top modeling
or keyword attraction classification search document relevance stocky man similarity natural language of understanding tasks that question and shrink textual entailed and machine translation natural language generation such as the image captioning chap box and automatic summers asian and o. c. r. and speech in the next episode all talk about the algorithms but if you wanna get started on that before that episode comes out
unwarranted talk about the resources now first the resources for learning natural language processing have really done my best to boil it down to booth most fundamental resources natural language processing is mine jam is my particular specialty within machine learned it's my favorite topic and so i think that these are the three best resources out there is a text book called speech and language processing by daniel drafty he is sort of that
the andrew being of an l. p. daniels raspy also co authored by james martin then there is a and help e. u. tube series by a daniel drafty again which is basically the u. tube series equivalent to that textbook what's nice about that is that the speech and language crossing textbooks and probably a thousand pages the penalty u. tube series goes pretty quickly i think it's twenty four hours
and convert its audio and that's what i suggest is you converts audio like usual were i talk about my video recommended resources what i usually do is converted to n. p. three put on my ipod andrew on running are claiming zara all host the how to convert the videos u. m. p. three things on my resources page was called the stanford and l. p. series on youtube finally there is a library how they're called an l. c.
hey natural language tool kit is far away at the most popular and it helps the library used by professionals is on it is written on python and it comes with capability for handling all of the things that we've discussed in this episode from ole ole to the high so it calms of all those tax free processing methods such as tokenism sherlock says asian stemming etc it comes with math
as for part of speech hanging that's the recognition that sarah and then a cons of algorithms as well for classification of documents a military and the like now technically it is insufficient for going the distance you can write a search engine using an l. t. k. you wouldn't write each hat box or a machine translation system using an l. t. k. instead penalty kate is a very useful tool kit the u.
tell the belt all library i like to think of it if you know java script is still low dash of natural language processing for those schools that python developers are hard pressed to not use it in their natural language greer no matter what final solution you're not planning on that typically like i said the lofty goal in the middle of deep learning ricardo networks and we're to that that's ten sir floor pie tortured him using these deep learning fling works blight you'll still find an old he came very handy for
or tax pre processing are accessing a court it's is all that popular court cora available just a method away now what's interesting about an l. t. k. the resembling ineptness freezer section is they have written a book and i'll teach hate got work for words slash block that isn't warmly about penalty k. is primarily about an elk he did penalty kate book which is for free and aids and h. t. m. o. format is cry
merrily intended to teach you can help feed in general just using an l. t. k. as a vehicle and in fact many professors intercourse is a sign the n. l. t. k. book as they're reading assignments so you can either do this speech in line with processing text book which is very gay and more theoretical or you can go the way of the n. l. t. k. book which is more hands on and practical and faster if you want to
a common and l. p. experts don't recommend a speech in line with causing an l. c. k. book but if you wanted to sort of get a quick label lands were just hit the ground running don't recommend the penalty cable i knelt c. k. that library it calm there are alternatives out there as well open an l. p. n. stanford n. l. p. is we're all different libraries that you can use for shallow learning machine learning algorithms usually in the space of machinery you're going to be grand
being in too deep learning for more powerful flexible and elegant models so you probably can it be moving on to something like ten sir flow or pie torch that's it for this episode alien land of an l. p. m. the next episode the second part of the series i'm going to talk about the show learning algorithms traditional models use in an l. p. now my friends have terrible news for you in order to continue working on this pie cass i'm going to have to take on advertisers i know of
dean of every pot house listeners life but i've reached that point in listener ship were actually have the download weren't can start reaching out to advertisers to sponsor the show and so i'm hoping that you guys can actually go to my website goes to belgacom force life pod casts forward slash machine learning where i'm going to post a user survey is required in order to sign up for sponsorship with lives in the pike pass who sings service by e. u.'s fellow that user form for me i know that
insults injury to do me a favor few good and so sorry please i'm also going to try to make my website little bit more useful were maybe all pose announcements for what i plan to pose nude episodes for example this episode was two weeks later i apologize for that i had to collect some resources on this topic so hopefully i can add some incentive for pop in motion the website okay guys thanks for listening siu next time


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. eight and i'm also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. welcome back to machine applied in this episode i need to be interviewing co workers from that matters
that expert in architecture and your watch is an expert in dialogues were developer operations these two skills combined into deploying of a full fledged products they can be used by consumers awhile back on mobile lab most of what we've talked about miss park s. series this machine learning how to develop and train your model inside the darker container or even developing in training your models on the cloud by way of
the dub us siege makers studio no books now our skill sets are on the machine running side but eventually why did that model into the hands of a customer after you have train your model whether on local host or on stage maker you'll deploy your model through sage maker today sage maker rest and points or two way model registry they can be called as safe maker batch transform jobs or sage may
kirk server was infringed jobs so now you have your deployed machine learning mall ready to be used but who's been the use it as we're all the stuff from this episode comes in now i'm noah have talked about whole lot of the tooling and concepts from this episode in the past and i promise i'm knocking attorneys bypass series into a false back slash architecture slash dow bob's pod cast and it's actually war and bring us up so that i wanted to talk to my colleagues who are experts in the fee
gold stock bomb going my way through these episodes waste year guises time and just say hey messenger was let's nail kaufman far do you do this as a machine learning engineer how do you product eyes and deploy your machine learning model as a full product in the cloud should you do that now to get you thinking along that track i want to talk about my journey with no fee no seed was inspired by the publication inaccessibility of the
transformers and opie models hardy face repository of all these models summers asian question answering i thought you know these would be really great for use in a journal apt to get people recommendations and resources and see what a couple of crack my knuckles candidate typing away by deploy these models previously as doctor containers running on a dub us that now i'm moving everything over to the dead us age maker onstage maker you can deploy models that you train or you can deploy preach
rain models as rest and points or for use in the server was in france jobs great now that i had a machine running model had figure out how to get that users of the day custard journal in the next step was to deploy a case server are using python fasted the eyes to a dub us the sea astro lasted container services and wrapper on a. b. c. 's that makes it easier to work with his call for gate forty will manage the scaling up then
down of your darker beasts servers based on loaded on the server great malin have a server but i knew database so i created a a dub us or d. s or relational database services host grass database and i had to create and i pay them a roll attach that to my forget container smith has access to my party yes database then i had to make for good accessible from some others
chirps namely the front to talk about and that so i a spa not an application low bouncer or a l b that has an ikea just the iq dresses registered as a domain name on a route fifty three in aid of us fight that bail be to my four gates service now the whole thing is accessible by i p. address finally i needed a fright and so that the users can write their journal entries in communicate with the server and servers in a kick off this
each make or model inference jobs to ephron and was written in reacts that is java script beach tim allen c. s. s. and then i dropped the bill the artifacts of that react code base in shoes and asked for reebok gets us three is the l. u. s.'s service for storing files with industry you can check a box made this bucket publicly accessible as a web site now few warrior esrey bucket to be a public website but thousand stop there you actually have to do in others
and that is to put a what's called cloud for its distribution over your bucket but far from distributions gold is to cash those files of various locations around the world so that it's more fast lead accessible by users in various locations that conference purpose but it's actually required stat if you want added domain name again through route fifty three star register domain name no c. dot ai point it's a record so that cloud frontage b. c.
far from distribution points to the ash tree bark at best reebok its files are reacts transcripts yes as a steam out there making baby i calls to my forty container through the application load bouncer the forty containers sitting on top of the c. s. which is managing the scaling up and down above that service the forty container has attached to it and i am policy begins access to the database on r. d. s and afar gave container will make calls too
the sage maker model and points in order to ron inference jobs using machine learning all and did i mention your washing all this stuff up yourself by pointing and clicking your way through the dub u. s. consul the reason for that is it's hard to track changes of your architecture overtime or snow what services you have running in which ones are affiliated with each other or less say you make a change in your keep track of that change we want to keep track of that change unless
all less stuff is stored in cold as a as a project i'm using called care reform perform it is what's called infrastructure as code it lets you manage all of the deploying all of your quads services in a cold file you can check in that file to get up and then you can use our file to track changes to your architecture or time or so that other users can repeat that architecture on their hands or so that you can make subtle changes in let's say your product dad and staging in bar
once just by way of code so i just mentioned a slew of cloud hosting services on eight of us and how you'd do that in code and it is your head spinning because mine is this so that was the impetus for this episode was that the last year's worth of bumbling my way through architecture n'dour box when my specialty is machine learning and davis and says i'm sure is the case for most of my listeners want to get some back spur
tom online and say what are the popular tools out there in which schools should be being used for all of these skills valuable to the machining engineer or expected of the machine learning engineer or in the job market should we be bothering with this stuff or is it too big a pill the swallow and we should be leading instead on a par dell vox colleagues or hiring somebody to do this for us is so that's the kicking off point for this episode i hope you enjoy one
max's show today we're into good-bye gertrude and read talk about that box or developer operations and architecture and we have on the show to co workers from doubts matt merrill of new jersey solve the imam arrow i'm a director of engineering at the u. s. icon from aback around about sludge on the development and no just the boatman tides dabbled in dialogue said something about won't we usually the guy me out to you is he's reaching across the aisle to need be ops books and things like that i bid you know
for fifteen years into a puppeteer by my baby's blood which idea i am de de la scott does lead at that i start out my career as a job on parole python poll rehmer and i was doing double us before is that from the wild scheme out because of my varied interest i would always said cockpit oscars allied unity ministry should i would always do the c. i. a. c. p. jenkins because no one in them
wanted to do it no was made to the dead servers again because no one to do it and i started to automation all-star abdication because i do wanna do by hand and i learned that while performed and sibyl look this selective in my career from job the development to the box so for our listeners a promise on our return showing to dubach slash architecture show or false that had been taking another direction about monsieur up so's
carcass age made thirty of us but this might be the last episode where talk about that lots of condom bumbling no way through dialogue steinhauer awaits to machine learning for anybody in machinery to actually belongs to deploy their models to the cloud for quiet they wanted their model online but in this episode were cut willing tile down dialogues between huge use what is it the place into the machine and ecosystem whether you'd been developing models on sage may burn your local whoa stands now reward betting on
wine so regard to the experts here sure what's going to disturb them lots mask and talk about architecture received our does that fit into the picture from she murmured in perspective so you're alive what is that lots sold a is a philosophy and uphold that the dead that bob store together to the help deploying cold and applications safer quicker into production soul it may be good to highlight zone
the problems that the boss tried is gonna solve originally and then still happens today the beds and aunts even though they both were keeping biology and they both were missing company they don't talk to each other to silo and we see that numerous conceded now when that there's an engagement so don't also weighed june have the common colds in common communication process so that way east to silas can work together well and as you know we go wanted to apologize to be part but some specific cool
within some specific processes that that makes the mob's work yeah so low on my own blisters early sino on sheena mean coworkers are calling this impasse then i'm doing is you they write a model or mobile coaster using cancer floor kara sir pike for such a large ideally often though be using a darker contain i'm so if they're on a map that all have a daughter container the ron's machinery not to be able use g. p. because of mathematicians if they're using windows they might be passing through to dub us all to
by way of darker and writing machinery models who might be using or doctor container and insurance fromm andy and containers were hiding fish containers more often the case were icy is people or writing their models on a slide to burn no books in a cloud so will create a g. c. p. were a ws account and they'll all go to the machine learning tool kit sony ws it's called sage maker they opened sage record it creeps for them a juggernaut
run off the bat open up that no but they started impact on toad the right i'm on all there are they're using care off the train and ended them when they deploy it save maker has all the stewing for deploy amalia same all but a ploy and you'll do all suffer you background the wold spin up kronos b. c. two instances yes as instances we all know exactly sometimes using darker than you're hearing from pacific are dark and inert sometimes it does all that country recision for you itself at all you'd
who's rates and python protocol debris rosebud what happens is machinery and using nope jupiter the race a python they deploy oman all the rest was like where we go from your heart we make that available to be done next to your clients in ames that's kind of where are we stopping upon cast and you began as a summer is an expert in the orchestration wham yes so that's a great dead requesting 'cause i actually experienced this first head so what
working with the team which bobo telematics c. a t. the dead which he led a team they were out playing around with siege maker did not understand boss of the dead body and what became the likud's pyramid if he were really popular to analyze speed up process data is played ada sold one day be cheated me you say date we got this disease maker in that of them knew what to put production and i did delay the bus age major i barely knew anything about
jupiter nobles and that they knew was sucked out i'll call you why for python that is going you about a soul know what the dialogues velocity is collaboration right the debts and aunts working together to shiva common going to put so it was a two way street so they caught me about jupiter no books siege major things a bad egg on my end you know it took a look in their bed the implementation and i saw all what they said had to be modified before we could deploy to production with it
i'm right this cop it is so complex that were not expecting wish he would dance to understand it is so just give some examples i had to be could be desk because it had you should be done in there was a hard core the database use the passwords on there so we had to be seized in court i am execution rolls to us in short annual get access to that the database seekers measure to retrieve the sea shooting password and make backups in case things crash
so there's a bad things that you have to do it for good production and that's why you would work with your de los ti eap to do that we're not expecting the dentist to think about all the stuff that has to go before court production actually know the macey ta da imam you contrast secrets manager ashley what is architecture imagine trying an honest was architecture how sad differ from dialogues imagine islamic crossover so mad you'll sperling architecture juror appeals paroling them ups you always are
as rolls some are different shepherd is one be any other hand yet but it doesn't interest aggression of actually never heard of belov some architecture computer dwight abbott was star was what i think architecture it's not step back by saying wait you know beers's perception of the guns off for engineering not an architect is gonna wake up some kind of amending the stone age you click god like figure who sits on an ivory tower and sends down yuck spur our team needs to build an sob and then there certainly are those the market tax but i think about spry
with the worst i'd move toward the court the kicker possibly house icy architecture arrows assistance it in howl different systems and tractor to each other they and how okay and you made a healthy whole system that is as simple as you possibly to make it maybe each individual peace is complicated within itself by how are you going to make all those different things in your business talk to each other whether it be the customer relations
management system feeding d. deck into your machine learning model or whatever it might be how are those things going to play nicely together how're you going to make shore about your teams of people with their rates skill sets to support those things thinking back far ahead thinking about how all these applications are going to get deployed and making for that you do how to ago bob see more crowded during team or whatever you wanna call it becomes
or that type of stuff i was with the given us across coming technical person who news at the service both different teams who could help connor road map these things i really like the discussion you into juror or how they did your one mentioned about what we can expect wish you learning a junior to know all the stops became rightly it's it's unfair it's it's on talkback it's that spot on the need at least one person to stop or people to do so do things the latest into the role of architecture in sky
of the person you could take a step back or people back and take a step back and think about hope all all this one's together before it goes out to you susan i would say that most small companies you windstar with an architect but would you start getting bigger that's when you need to start to cuba having somebody in their role in an ideal you have somebody grow into that role organically who how's although the context in history and things like that always commonly comes from a full stacked knows how to surprise today big stars the guest on how
get 'em all okay then excesses so am i right to satan architects blueprints whole thing and added them ops makes it happen that we snuck while the nets for my confusion was a slip out of that those two rolls of somewhere in the way that the applauds would string the services together actually in implementation still a big just depends on the architect and their expertise soul i'd go with new respected architects from numerous to three clients and companies some architects are there
recyclable right abroad diagram is able to we have a dictator the senate for miss mead fine lines up high we needed clustered we need to have noticed wold is security here here's a diagram the box you fake iraq the committee should get to work on their luck architects specially built for promoted from the trenches okay well i want all of this and this is how i wanted rape by white guilt for gate and i want couldn't eddie's you or whatever cool to the light he is the recipe
you just follow when so just as an architect and the skill sets and the personalities but i like to think that the law or effect of architects are somebody knows when to do read tableau springs upper right time do you not you'd you don't always want the blueprint because you might have an effective team i do because as much you know part of science and also just human interaction as typical work sell it inclusion in is that's hey xiang damn box as a role as a
she learning in junior is to be looked old school if you know one man band a project as machine mining engineer maybe for a client would say it's a one machine an engineer who just when it a client who may or may not have other people working on the project taking on a bunch of del box technical know how would be a lot to ask because in a way it's a it's a role in it's own rights it's not just the suite of skills it's a doll
rubles for any technical person to have on or about stow your watch that that being said to me speak to serve all levels complexity wolf's been doing dialogues on wine may be a machine an engineer wouldn't want to take on all this burn themselves right so it specially security is the one that's curious rate especially with mushy wording you're probably gonna be touching since the day diana was insisted says sedate out so in this state is exposed whoa
the bad times for you that country are quite a bit coppinger working for the back wait you probably want to hire a professional to take a look at your implementation and in oak your dad's the specializing that to make sure that the application that you wrote your laptop can be deep lloyd quickly safely repeatable each huge production insecurities potable supporting we did twenty eight clients where it is not just the application to the cbc chirp this battle in the car
infrastructure to write this didn't work you see plenty a kind word to them work is called a dyke he wide open to the world easily hackl notice these things called as the blockage we use the steuer data that's all been sold securities the one thing where it pays double potential come in and take a look and tied down cool jury when i'm curious we do is but i would say that everything you said is spot on what that is not so good at what every bias on them now and you're to try to die didn't you
not hardly but i think that there are certain things that animal engineered can do in certain skills as a big instructor bro shop on bellow position but m. l. project well to be taken over by adam upstate and i think we're going in and talk about some of those that some curious big bad rings true if you as well since i nodded about oxford discounted it's on my periphery soul usually what i've seen is a powerful is pretty easy to take a break
oh where there's an aisle or dennis is all take a look railroad this turf war to spin up the guy d. c. today to be seeping it is fair for me to look at school and i'm glad we did it issued its learned that baby had the better for her to marge bolsa know put on the right settings inside that soul i think it's great that they need the learns doctor carrick for an answer bowl where it is but it still requires a collaboration today get professional big organizations did it
the club based on the trigger point in him back to old you're into church and about lots to m. l. n. junior shouldn't think that they're just imitate isn't taken over the fence to the lobsters and be downloaded on the balance for soon as the magically hoof make this appeared production there's been a beach in twos and conversations going both ways been ordered him a backpack in collaboration is beat you to double ops working together to to shiva common goal that said that stuff is something that's going to highlight colburn over this conversation
architecture cliburn with the box in the dow box and doesn't it would work together to a new engine ridge utility hands in move the land which the bill you have a single into chartreuse carl you know what the two largest dive into the tooling and i have a juror wants talking for now it's your hand he's got really good looks to be the journey through the history aunts and one spires the creation in use of the school's soldier rock doug's going to join so we'll get into the landscape
bobby's them up stools but it is really important to remember we just talk about like this is a full-time job right we're in a ghost rue all these different types of tools so that you how to live away and you can hear these ability out as the configuration management tool donna we're not suggesting you go into deep dive in any of these this is so you can talk to your dubach steeple communicate with them better or just key role to be a little bit more intelligent and osgood
questions when these topics come on so listen to it sure what's that expert in you know wannabes areas in he's gonna greed greed greed or reviews in and out of lead most to details follow your head into sherman reliable concepts and you're gonna be a really really gucci all right so were the goggle back loaded to the old days ago days where the bad old and it's cried the problem that the mob's is dryness all four have stalled for a double
hard so i don't days there are people would go way back to the u. why does look pretty good to be in crete servers and create a porch would apartments or do whatever they need to do it would do it manually that is colors are on the u. s. consul website or he'd been given before a dub us forget the 'em where you have had the honour command lines rape even with fizzle servers people would go with log on to disturb there and this had been commenced many only so assault was a good dark c.
hundreds of helping to be done balkans are configured ending with daddy g. dear laughter that office last raid and all that no documentation no idea what that person did it's hard to do a managerial potential business that way no way related issue on house that was a mass just a god what's going on and the serb or this trash will look for that but the and then you know what the classic settle we see all the time is okay one dead or one ad man goes into one severed does it what
wait a minute and it goes into another summer doesn't know the way you know this was the right message that deferred to break into star burst into boxes sold the manual towards a call box you to operate soul would go bob's it was take some of the velocities of deb rape you have cold that you're right you have cold that you can read you into an old to milly checked into local repository get selway is a record of the changes is a record of what
didn't and he in and all the people can run it sold it seems like a pretty basic concepts now but when it was first introduced again she injured and knows a lot of resistance to write almost wiped out all your try to take over my job so what infrastructure is cold those is the schools like confirmation terror for holding the bag well instead of you going into debt eagerly was cause we're going to be cut to be to create a server to create siege maker
you can write cold to do what they could be could be in the den would you would check the dead and is yet to make changes then you could make that change in cold review with two teams in all we're gonna change instance to you know m. l. m. five large took them five x. large check again when it and you'd see the history of what happened to have a code file and spit deploys all the aid of the u. s. infrastructure for it without which is your clutches run some screw
it's an old spin out all your nice to your data bases are so with this infectious code does yes and all the game changer was what i mentioned before we get through the differed off to the digs differently now that he had called you can we run this cold in another environment rape by discussing a terrible would say you ran this coat and death you tested out there was some blows with the infrastructure happens and then you know after what you got okayed this works she could take one to one that cold passed it
able to wait or crawl on an exact same thing will happen to weigh in chronically short would be tested down it seems to where we testing to us in prague and in that old days else i'd guarantee that all right the mayor daley to the steps yes to run to deploy a manually the guarantee you know stepan debutante boarded the d. of the wahlberg recently by a deploy service are using tear for us or p. diddy span the working of the the scene at
the b. b. c. block coat was was a new twenty five one to go do it created a bunch of stuff i didn't know i need to create so round table in a game way in or d. c. else always sings i would've known to have donned myself an urge to expose the serbs to yaron he ended up us you see like a picture sure truckers cope with their same the faults also helps with its simplicity of that point architecture oh definitely that's another because it from death
they came into bob's you what did larry the reuse ability all he had this model of all the complex it behind us off ox lapd's pass to prepare a bulls and what do the right thing already mentions constellation turf wanted to meet how to resolve the particular took a confirmation is a baby was specific and had to perform is more generic you could do more to cop out a terror from it is that the most popular in the structures cool cool is the one that if you would think
urge all of us would you should learn gloomy comes in seoul to care for him is very structured in the way was it means this canal was my way or the highway plume he takes that this concept and adds we'll call it good so you wanna go for you in this state men in the more generic you can the so it's a great off he could be more dynamic but at least once they get a cold with gloomy because if you're a cold big get that flexibility it's good i would
that we recommend care reform as it's the most popular and it's double structured it's easy to teach it's easy to not messed up too much also go on over on wine supported the number two has a very very by brick community when i found an early to do it but on of support examples an independently n. before even talk to these guys attendance in conclusion richer for sport or else we'll have to cork
does have a one point four billion dollar i appeal so it's a real company it's not some open-source being the nose and maintain so would you do need perpetual support you can be cut out to have to court or that the court to say okay get this turf war can you help us and skip one cool theater florida as you or mention is that small black clouds were so if if you don't like give us a jews were reporting on this episode you can use turf a g. c. p. and azure and others but in many of us world
there is confirmation and out c. k. if we're solely of us heavy why are we saying turf for over to begin work on rage is so c. t. k. within the debauchery he is a baby intense debate on how do you prefer sure is called the state had one can lead to lay on the table and they were i believe that if the soldiers coolidge be structured you should do a lot old we're dynamic stuff to but doing that polly do sunday
we're on the flight control words care reform and the confirmation is in ways just passed neil bush can configuration and i'll figure out how do it with cold lonely it would seem the tape is taking some of the complaints that structures it will all i wanted to discuss eating or i wouldn't say no if deb you this is prod do that and i wanted more like how i'd call bill bowls c. b. k. of clooney sport type are on supports could scrub acting job was in there too
okay this is a procedural purse declares come right exactly showed have a better form is an aged immobile issue tendency to john skirts are out of jeff's exactly but it's very great analogy performs as this is what must be darned see cases do this we're ready okay so that's good for structures code mr fung on it and gun and friends i've heard a lot or cancel where is that fall into place yes so what the bad to be seen things about the dubach schools is
this big yellow the interest or should call the bank and configuration management in the two different sped casualties and the coolest that as it is for sure caldwell and it wasn't as configuration management well is different cole's different companies soul let me describe what configuration is because as a rural road example and we see this pattern lot you would use care reform to create an easy to server in a tub us and widespread in there
it's easy to server is in the cloud what about you got to put your special this is sauce insured right you got to configure ill you saw security patches you got to add unit users you got to ashley installing your business at what do those pass to reform is not very good at and it so does indicate the cool club and stubble which does that worry that began to configuration you can say okay install this patch no young's
bob had forty three at the next ad uses these ran as wise and then when installed on as each so that what has what does and that's what configuration management that is what you have the generic server the configuration management cool installs your business logic to it to the way it actually work now i will do that with darker right soul a lot of these configured ration mesh the clothes are getting deprecating 'cause they were beside affordable day
east where you would actually have either fiscal server or the brcko serb ready would have a unix server that set up like sleet and you had to go with it and configured soul containers or another game changer in the box where is the old joke will were so my laptop read the doses were so my laptop and offices will assist deployed the laptop when the greetings by containers is that all your laptop you can do it they're beating on your lap kok ready to go ronnie
you did serb are you could test and deployed and the development on that doctor container what you're satisfied that this works on my laptop human combined into a dark image would you just a cold everything and give it to be good about seeing is a ploy it were so my laptop attested take this entire thing and go the other dudes but it is that they start up almost its biggest almost instant its we were as these other methods can take a long time to start up the big deal
the jury is would be doing down is would you do in prague you would run it easy to serb are all your laptop ready to act this coolly to come in make it but doctor dictatorship would go with it was a major gauge featured when ago was first introduced okay so we're out of infrastructures code that is writing a profile although create a whole stack on a dub york's whole stack on she's seeking to pat asked you would walk into the sea two instances that you may have deployed with
configuration mission tools to install the patches on the soccer and you mentioned in seoul what were the other ones were there's a base chef puppet salt bozo that the four main ones and they all have to which he called mind share that big difference between an supple intrepid cub it is that as was the pusher are you run a command for mr burden you push out the configuration with chet to put baby have a peach and on the server in a poor sick
the grecian i'd prefer to push model does to show my bias because i let that control but they are privileged to the same thing that and now all these configuration is rituals are slowly being phased out in favor instead of barker digits you can deploy garden shears crew tariff on to wiki c. s rate for dates and then there's no alternative for daddy is so we have doctor containers now so hard to us about b. c. 's for aid
when eddie's when you use morning news the other one is to run eddie's promised to do everything turf on etc yep so you made a very good point raping so they can support shots are being superseded by a doctor file so that catches you would do it and sibyl and chef in installing an absolute you and ask one just that would just you know darker file and getting allied arkansas again rejected in it's a real father you could pass around seoul cooper daddy's so what you have a duty dirt on your lap top when you get it to
the bosses say the point is that where the real spine b. gans and we can provide a link to decide if miss a cartoon or a diagram were hot be deployed a continued to eight of us this literally fifteen ways to do it would squeeze july confusion and even abbado botsford professionals there's a glut confusion about how about the best deploy eight a darker container and edwin hester old crucial favorites and that we recommend peace yes last the compute of tobacco
i'm busted him to measure rich battle we recommend the professional environment because out of all the options is the easiest it has to that support and cried support but that's it you have a good job osteen if you're just that the one man show inside track runner that's good that simple there is a lasting peace dog that's nice and simple it depends on what you trying kind do and in the big granddaddy of them all as could be dead east and that has a lot of hype ron
the thing that made the onus for us what could be daddy's is it's a berry complex ecosystem to deploy container and people have careers just dedicated to baseball without any other skill sets and they made great money 'cause it's a very complex to get raped and what could be dead east does is it tasted container and that's all the production that used to it right clustering you can employ able to pull containers of local copy
the container away didn't manage go read it there's a pirate users you have two containers inhibit bauza users have three containers for computer specialist security recovered nice to have it does say to talk about the containers or two other extra services and that's just scratch mr fiske what could reduce provides for your own styles the impression it's become keeps with the debate was it is all need higher ups for for orchestration serbs it though all the maid
clouds have doubled implementation of cupboard eddies in aid of the west is card dk yes but at last the company's service and t. c. p. could raise was developed by global selling go cloud that the agent cooper is engine the shore has their own implementation scooby babies and the good thing about these it's called managed services were you could get a cupboard is all your old from scratch right and this is the message blog caught could reduce the hard way the old baby step by step and oddities you would go to buy
and that would either be west of the copper wires that is okay would lead to good eddie's no riders perform that despite dole's read accompanies coaster for you that borsch with all the dog was services does not so much competition is just another way of deploying containers into the crowd and is coming to the most complex way most complex why would we want to see ever since the most complex isolde especially the production loaded you have a really
all of intense production require misled yeah blood of teams in these teams you wanna my course services so you would say you have eighteen containers eighteen different images yet to maintain graced me for that window on at the scaling up and down the world is great for that and to be on a special wanna say parana break so that people will type but the new shiny red was a new toy like to play with his record that the up to the driver in milan architect here at that we sit
write to keep it simple the simpler the better the simpler days is easier to me team you'd manage easier to teach other people so good with babies is opposite of simple big than it should be paid right so accommodating to mere ten commando pretty high salary solo wide as i've seen this so many kinds well i have an opportunity to use cruddy so work i'm a do it because i wanted skill set the other man a bit like just full disclosure i mean
no major gordon is expert on any stretch when i figure this german architectural perspectives i do see go bout stafford i think no one were expressing here is actually one of the hot tape humid about unity but how would you be a certain portion of people were like well we mean not to run eddie's we mean star these yes that i'd get out because we're the advantages of musical burnaby says even though those people to command a premium that skill set is quite available it's also open-source so young couldn't be a more files that you specify for
the money's there been source don't work largely no jury what would you would grow to be honest large we work between different snot manage service providers for good earnings would choose those are nice but it's too but i conclude your re with voyager one as soon as i wish you a good team but ready to do that and ready to deal with the complexities of managing cooper cooper nannies services much better starts about an article about what will occur surgery was granted east and by previous job coogan is plus or read
air raid us so it was the big optical learned and i made soul many mistakes production level was takes inside no permanent volumes security networking it's such a complex caught they did that it's not just maybe will you take all day why did this decor real it works cool you have to deploy the production which we'll users rollover you learn soon enough that is to cold drastically wrong and it's a dildo here doing it hearted debug a
and i'll actually yes annuity it's a mess and fall short of it things go wrong commodities hears all would literally clinic and then just run to the dogwood going on to you said to reduce is is overly complex and now i realize you not just saying that companies to use today's over complex process saying the complexity of the myron you might return orchestrates course on to the complexity of crude many seen use so if you're a start up a sailor and employees he probably
by using only a new as services if your project is wildly complex than your own ones that cool to task which is in the public burdens and it definitely if we both agree is a true that is a is warm decision and make sure that the car to people who've been doubled to one car right to our people or passion about grenade isa production to a cop to dinner too and get their due put on this but it's a huge investment hard to high
eric expensive to hire those almost like a lamborghini right well bloody piece but it's hard to drive you to crash came today that logic works like if you look hot adaptable ever dvd you can use the rearview mirror gosh they had told leonard borden says the book behind it will oblige jamie's but they sure are you know nobody's in the big another thing about grin and i would like to remember that cupboard eddie's was created to help grow and creepy
it's been for structure of the closet google great weight it's inherently warts very complicated like it's gonna be really good but it could be really complicated it's meant for the gloomy scale problems maybe you'd if you're successful up oh my god by hat's off to you you'd be benson complexity and he's a new disarm now but i don't know i don't know find necessarily agree with the stark symbol cured allotted break your wallet to keep using that container so in the star
and with p. c. s and he says is wanted for you pretty far to match point did you ever get to the situation we're while we have cool skill low one hat's off to the youth youth as success you could deathly likely to go buddies are right what is the c. s. c. ok i'm gonna actually honor read this flow chart this maybe we're in audio format and a drop of course all of the images that juror was talking run the show nods for greg what about was weary this platoon see how does this in which a dub
let's come to your shirt show use where you wanna speedier on premise then you to burnett is yes the chaos know the c. s. and an open shift which i've never heard of rosa mina says would you wanna speech here in the cloud both geared to run build jobs could build them that's jobs that she wanted abs scurrilous lincoln still real bogus lambda no find it so brutal slaying know who'll man just ready
the b. to us the other us managed that if he landscape but such an awning and manage it may be doing a lot if you know the myself yes the p. c. to anna says that if she was sort of bolt comes here on promises were in causes green grass i would election thought worked hard wilderness elite curb the mix of currency these yes we have a lot of container services they can use tear we have each a s. b. c. 's way and afar date you start out for
unlike sailed the seas to go dylan bats cyril i'd say the blessing of greek kovach soul even among people who were doing this day in day out to do when we predict what containers it's a complex issue on itself that's why if you'd just doing the shooting learning a new because drug that and not worry about this complexity whether it's a great way right you just take a reflected here it be that the west as the bidding for you
cynical about secreted network on yet uses provide the dinner at corner is a very good being with her son rob reiner goddamn it was just to do just about your ago just before the pandemic at the core may be right after the pandemic so again that's all the data but the box okay what the new bike music don't need not be relevant and it's not all of it today a break at wonders of daring it's their way this point to deploy containers and they put on their knowledge of what would right and wrong with other
one expert who kept runner men during atrocities are blasted the stock is the hollywood bowl this way stupid way considered it does a lot for you to so would sell the war would shore bowed war battle tested in court as a couple more clicks rape but in the game gretzky mcbain still barry symbol it that could be cut to the guy she drew up would you would use his old lady what it'll take on earth we want jobs will know you what was she wearing or you want to do they know where that
so so what did crashed near the beach is to go on say okay got my stuff so coming into this discussion i thought it's really boils down to do it he guessed remedies or p. c. s then we also know women find it so that the whole other good cop together says the courts are rulers et un would put that you can see in the park has been an air quotes your list there's no sir bristow be on the scenes but they sleep you know how to manage the servers with land of the disabled
his leg hold any good either be cold that you will or get that could be talking to peter pan lendl say okay i'm way just sure purge you for actual usage but give us by people hits your land of that is the trigger couples fight users so it's about the book cos seabees but when death is a berry complex kopit and landed in service is a complex politics so i would recommend that for a beginner more for carsey being son than anything
most the elena's stress that even the bomb people would be doing this for long con i've seen so would that include patients all of them does seem so many betty boop imitations of the sea yes considine fuentes of cupboard eddie's great wizard well well that's a thought rape it so complex n. n. n. deposit back on me i'm sure if you look at label bid to shirk remedies for major goal absurd and that is in the beatings we're doing there one take a crack three of them on
you must know the one way i've thought about to run eddie's versus host sister says measures isn't a while it would have us leisure giusti i've seen them as almost two different approaches at war south korea is very gentle approach almost awaits her forties general cross flour in interceptors cope with oscar nannies as a way to go doctor files in a matter where you're running a need to be on premise to begin your company's deducted center you're on the same man
we're just created us an answer or on premise and this can be using open surf tooling instead of the clown providers service for that equivalent so if you're used to read asia we using a post grass darker country are as opposed to if you use a job us to be using r. d. x. there hosted did a disservice so with kerman eddie's you have a mirror your fingertips all of the world open-source tooling you know how this thing
works you're not walked him in cloud lambs using a stutter form on the be u. s. you're using their services are d. s is their implementation of relational data bases it's not open-source you don't know how their postscript stuff happens you to burn any signal have its own secrets manager ws has it's own secrets manager there are pros and cons in case of a u. s. you by using their services you get all the patches that updates everything's black box so it's more
manage and secured by using trinity's you have more visibility more control everything's open-source any dinner plate anywhere is our right so far so the cut johnston very good at selling points of cupboard is rape it's supposed to be generic so that the system and claudette to cope take a duty to yell if the people cannot says if early to control could see to deal but with thy command and the animal file you could deploy your ad with their kids
hold on a dub you episode piece that you know she seek your rouge or the sestak promise him in reality what gets you is that teacher new model right to deploy could be daddy's in aid of us the production level environment you would have to tie into that i am also a dub us lhasa you'll still be using these services even with currents the target corporate macleod right you just can't help it would expect that is the jury model
you know it's pervasive among all the services and again to lead singer to communities clustered at your back and he did you know you'd cut it deployed on to that cheesy peak when it usage and i almost guarantee you in a series called bad you have to access resources you're gonna have to get back to tomorrow and even if you don't write so let's use a principal railroad out it's going to write beginning of the west wing or pottery right good
spree file or as he bought it if he did what is on you know t. c. p. you counted on to deprive it to the g. c. get good wind of the astrid market sold yes committed these promises made the generic in common but in practice in god and then i can at least to this other hot cop they caught take off the box or to cloud rape you hear this lot were all our application or our products you can deploy on a debbie
less or t. c. p. henri shore it's easy it's not easy to do that the sales pitch what it does is easy to say something that's the idea of deployment readies cluster on different karpov ayers cold how about her form with the way it or shirts differ club providers break so they could be good but cheer for miss that is to see configuration language rate so did a good thing is this gil said that you learn to cold
therefore my maid of the west is transferable to t. c. p. one a policewoman show is a platoon career god i enter compare cloud on its they'll be willing to buy it compares okay he did is needed the west is we do duty c. p. great equivalent services so turf war is not trying to accomplish universality and she stratacom push your slow excessive going right to sponsor sits right and it's a common why would try so you know how did you tear for many debbie well
but say it could example all here to care for the grand espy bucket in a tub us neither did create the equivalent in p. c. p. each say okay why would she c. p. here's the equivalent to audit storage since doctor from cold just to dc keefe labor moray is so speaking of mostly cloudy tell us about azure cctv yep if you're number one eight of us not adverse event of the companies are gonna use that if you know put under
for jobs right you call that it be that u. s. issue or is the second most popular one in the business world because my curse off the somewhat upbeat eight of us it was beach kugel so if a ladle dusty street to negotiate well you almost a decade of your body short for free radio disc can it do that for just didn't want us to account for the other one is set a good joke is it by chris offutt order company donated us do it do donated us you'd be off the shore their target for should go
they're stolen i have some friends who are about accord and butter sauce and that there would be project g. a. c. p. it's great for the latter local t. c. could probably has the best you why both but to read the bad thing but she's seeking go with general is a political products and inactivity pepper to compatibility in she seek the database us look been over backwards to to accommodate being spewed don we'll head fifty years ago right this week
you have the c. s. classic you still have you know the way you can predict history but it's eighteen years ago or whenever it the best we came up because to do today for the most part google hates backwards compatibility that's why it's it's the least favorite of mine that three babies to the scene and earnest in services go beyond that risk category entirely defunct yeah there there's actually a great list all the g. c. p. thing cinema define the winked and posted but this agreed listen meal
hate is that the scuba breaks great to be the city's public to chill for you why perspective it's it's accidents but the best you why i'd use the aid of us is very cocky you why why is that the real conversation for our listeners as you'd be around them she manager says offered by the two providers and what i think we should talk recovered eddie's machine on unser says i personally usage maker on a dub us but the part about moss few episodes but just brush up it's a suite of machine and tools mom when she won
to aunt ada be ross it offers stringing them all including her role as asian distributed to training in monitoring the model training process when you're don deploying into it and couric than point to the arrest and warn one and you call as a bad batch and boring so if you just made you won off influences on ward rounds of data so they do not charge further juanita west and cord or recent weeks or west model calls in just
fully amazing and then all wunsch cogema motoring schools on top of your mom also is new again it comes in from new users does that new data distribution over lying drift away from the type of do you trained on animal you know you if so which is really powerful and is there applies not only does all the stuff for you and spine completely unfamiliar with g. c. p. or azure is machinery offerings in that domain our youth
familiar with them and i will know that that thing that's where the clever to come to that played in aunt ada be west to be describe it is new to me i see the intended this you know too old to two points of this contest is one you can't know everything's raise him such a complex deal and especially you know the trial where both the loss in which she learning that the big apple do bite already yelled wanna pick one or the other the other thing is collaboration where
oh well look cease bigger can do all these things and you know if you go to de los ti me you explain that rate is caught in a beam youth or nine like when a person of the muscle into party collaboration of all hey you saw this debate in the city's major tunney shorty you would get him down and then the dullest he didn't take a look at this and say okay will we be to secure it is you know we need to let do this with getting married saw patch is inside that she and her doctor file and saw this he didn't catch what would
the collaboration and learning the spirit for both sides but yeah from the ceiling this prospective i'm very ignorant of all the duma to get better but listen to your potash they had to apartheid stand and we're in the body go through any my label asked to obsess on stage maker of the prior that is assume she ninety six is not our cloud anything until the very end their why do women shouldn't when eddie's on a dusty drugs all talk about that competitive open source machine
the pipeline cooling because it does mention sage maker has a full sweep of pipeline tulane but it's all hosea carbide of us for you couldn't eddie's hasn't song's called tube for a candidate to orchestrate a pipeline close machinery trolling for you that includes the deed engineering the date injection engineering transformation and then various forks more like analytics system but the soap if you enjoy say drapery wanted to open source for ron anywhere near my
look into companies cube flow there's a concert sutherland apache airflow and some others although one thing they do with image should get up comes up a lot is cos and that's you know if you're a pig which you a production bethel you're cia o. or edgar see deal would you see out of those who say hey what's this expense but it's really do your its we deployed to your purse look how and i've got hit with this all law that bolted to the status quo i deploy into my purse look out the dawns
what the dared to deserve to get to tear down that scene you know you get a bill for ill t. box hundred bucks and i can be a surprise news of the one day guide thusly recommend is a jerk on a play with this all your purse look out there to be caught her by there has a bill monitor kisses all you've stood would put a box that you know you know you spend more than eighty bucks calls it a credit card rates would e-mail uniform you to that fled before you do anything turned out on arose
no you may get a surprise had been aren't so we talked a lot about what schooling thus far though look a lot of the history of god's hands we alluded to this than ups might not wanna be a wall did you take on as a machine engineer that would be to be a little to swallow stuck your guns machine and space or in the sage maker cooling thrown for a sofa my listeners machine running engineers with say
they've created the b. s. account it's been a bus age nigger projects in open sage makers studio to burn all booked the sub typing away treating aids care on small they were on the big they really learn say snickered not symbol spending maybe they'd even to korea signature model now as for rest and we use him all the way now that machine running in sheer maybe by a one-man band but if so what that's the small arms while it may be
what a spate what a fraud and you are air react friends or ministry butterworth far from distribution in a row fifty three new rocks and that friends in half to communicate with some back and was sale at ford apes python facts c. p. i's are then kicks off the machine learning ahmad all inference and courts so everything i just said is i would consider horny doable law
i buy my listeners maybe they would work assuring that entire staff there just mentioned using care reform so where does that word is that wall starts effectively why would be a good sweet of nene to schools in aid of us presumably using term for that my listeners would get started with actually deploy a product and it where is that wall begin where now you actually wanna really consider having and damn
and professional helping out with the rest of this process you had a great crushed and so i don't wonder dissuade anyone from what a turf war or doctor can score could read eddie's i just want to highlighted some beast was we have the steeper learning curve than others that and ashley that's how i learned to ride so i started three as at job with us and i thought oh well that's a blitzkrieg cold would play with that truffle was cool loaded with that and
a deadly it became my career so we'll i wanted to sweep it was from learning it that for me to be marked he should point is security powell valuable is to deny or halsey churches were data is your dear god hath be due to the wild is it okay bill was just no public didn't no big deal or is it no says if the idea medical data consumer dana identifiable didn't it occur answer is if my this
it got me into that kind to me yeah my company then that's when you wanna hire professional to take a look and make sure that the proper security precautions are met yet i'll just i don't talk about like you know if you're in a player around with the development instance with some think they know that's creepy should do that but if you're gonna release it to some friends are friendly people you should do that which are not trying to scare you but the lichter would say you have anything identifiable there yeah
truly want somebody dies on uppermost curie perspective those alot of very easy mistakes that you could make a new c. news and the news word for corporations couple weeks you'll wanna be one of them you know people that are really really good at this stuff things look through the cracks minutes to spur needed for the best people how about percussion said was look at this period things the other thing but just to consider is like if you're just using barry see two instances you wanna make sure those things are locked down and people could just go in use them
so good-bye curb the line item it's kind of the little bit paranoid but but the real sick if you really dirty user your open up forty you shouldn't people can hijack appreciated run up your below the intensely sweden watch out that the stuff to go to war exception should scare you just things that you were at all on the step up schools with the infrastructure is cold and configuration management technique in a trip to la repository it's about more berkowitz all the people working with other teams
you don't want personal shop show then you're weber is a job you know i did documented it's limited so experiment cried learned you know why we go what to say you know you can't do this right you teddy bears does you were the complexity of fraud but let me you may learned and end up wildfire on what this war the java devoted so did come in and she's my career been recruited journey are right so so secure in the city mark year
relate to pull the complex b. c.'s you're in over your head and you'll know we see it restores tinkering in or your projects distortion on doing security and he i was that was asked and for her so personally i'd general elena don't fire she would furnish the young voters of the good guys you don't security in a professional did these guys man manager watson the air ducts and see if you're just tinkering around this one has a funny billion machinery magowan sage maker be deployed
so as ear rest and point man can you guide us through an architecture to talk us through it and what services then argue shaming engineer or would want to use on a nebulous to expose this model to you know so years that sage baker i mean if you asked me to come up with the cub you're not in camp was a blood sunburn and save three years that excited on the street from dubai cloud front porch you or are concerned you mentioned media to bite on service
without beauty daugherty jeter running on the c. s. hook him up to her wish molded baser resonant presume you need to have some kind of states were there you always use pose grassroot oppose press and then it her former credible curiously sniffed your what would she do you know about that those are all good recommendations my advice is to sleep if it's not teacher says the data experiment that's how we all learn rape trial i'll see what happens then
when the good things about eight of the west is that there's a lot of documentation succumbs to much but the point cried out and see but the goodbyes maggie the senate is desolate spot on and what i'm ordering you to add that we totally forgot is that you should really put a automated build a duplicate what what's too so use exhaust like code by blind or your circle c either something like that choose your flavored that is something that will pay off in spades it's now hundred percent necessary boy you'll fly
and that it can hardly what about the decode friends first control push an automatic we ought to hear structure that for them are called an episode is c. i'd c. d. yeah and as he did use in addition continues when the ski run the process for according to commit u. n. c. would generally they mean is this really a measure of where we're so good but in general lee he will ever watch war periodic we build orchard you continually trigger from your source for palm jury will assemble your code
run tests run security since whenever you need gone to assemble your code package up into what's usually generically referred to as an artifact which is package of files basically that's the you know usually be continuous integration peace banana continues deployment peace is taking a package and pushing it out to your number sure turkey and turning it on and releasing it to users so it's really important to do that to make truth about processes repeatable
then you're not it's not prone to human error and also just say gee time i've seen a lot of projects in the past that that skimp on that step in the beginning then you just end up paying for it she go along so it's usually come away by number one piece of advice is don't skimp on you see as c. b. you would you start out that it is a bigger investment is not something you wanna wear along with everything else but it is something is she sick about deploying a piece of co now to the wild if you run into him upon to mrs gonna be trouble
you'll think of specially something that you want one boston the end it's called the sub index for myself as perspective that you wanted you to write this is how ashley i'd learnt automation use a job a code they worst locally now i want to deploy to to death so that you did you way folks another dozen could look at it in all the good man really wants a great i do namely twice a day after ten times i'd love cannot be the by the order so what it is on a busily
he does you know ticketed and then a machine behind scenes does it for me and i did you go get some coffee awesome space guys are right well shit io contrast another of depths pockets network we can show notes likes to do picks up the end of their episodes or my wake that's an effect now wasn't it because it's ability to be anything anything that you are currently into the new t. v. lately doesn't have to be technology really delighted the papers
when technology root out so i go first to do our part my big use of the show succession are not hbo max moore hbo i am blade to begin on this and biblical your life to be but that show is the impasse that journey strong medina claes campbell roy on a show the use of music so now i will pass she knew tyler are right my will be apropos this episode actually create her form skirt that spins up
clogging streaming service the iwai cloud giving them by my good held for slash what's your alyeska mary for slash yeah right hasn't found hastening will care for script or use gulag quest to become the west the roadblock was somehow this whole thing works and we do is use them out the windows nineteen server on a dub us that has a pen and video graphics car drivers and stalled ready for your gaining
you install virtual just hop on the server and you can next to that virtual desktop instance using our killer's crests choose virtual desktop back and now you can do triple a. b. are gaining without a pc so you can play half italics as guards wrath wanna go although really impressive we are titles that can only be played the game in peace economic cliques are beginning to see i'm keeping my eye on it and video with veered g.
force now sir is it looks like they've got their eye on cloud yarder korea cloud export they have a server and climb back up the direction in work so deny on them but you can actually star game without a p. c. in without an n. b. summer services all t r y maybe you first get your hands dirty with terror form right you're right on my to be a little boring but this is a lake which would go and ashes would care for it is behind the scenes reined in and
and it's very see like language the sole i'd mention in the blood gas beg you to the job until upper than i did as a terror from it as a ball and looks pretty cold and ice today by cleared your courts that after doing double for while asleep i got two subtle granting cook the grill programming so alluring goal which is a new code red line which is my corrupting in the chi chi's indeed the boss is a lot of the above cool suspicious patch cord it should go so glad to hack into
your scorn of the view that one language so that there's something very free about actually creating a deity of executed a not needing anything else terrain isn't doctor or dark and those go based on the sole doctor it's all about the composes interesting to start a couple of years the rain python and then i'd be converted with the latest version of darker compose is called a solid lead believe you're correct it said it was from a python ago and yet in the cool thing is the
now look that the reason doctor compose use do not work very well with doctor itself is because was written two dozen lemmings can show gliders as a bad now it can have a symbol of where we're friends espy one against damn wore shoes are restore kind of be wanting when we're x. jobs for a new day of stuff python nor the server some go as my courage that they're rough one of others in the old turning those me say what is what is said by violent ouster to spy on joe's been plagued by the laundry bag and human
those that you'll never current not to disturb but when you reflect feathers says plants are at the soul but it's funny because that i had this excess income like my son is in college and he's about to graduate so he's going into job market rate and he has a computer science degree and they caught him see plus plus as the main language rate is under traditional computer science education adorable secrecy blessed blessed your job market slow limited soul i actually could get python
as a python you could use it from but the real developer language could also they do what it could kill bob's you can know python is a very good system language to the white i recommend a pecan become soloist teaching him you know talking about learning right as it will actually if you wanna job that is to deter for many of us i could teach you a mouse that you can pass a control global interview and you know whole flooded and fake it to you make it by the o. okay but it was lorna stuff big cloud grew
you know we're so they have i lessons on flower so azure juicy dna of us their lessons or coolly formal end dialed in because they're intended for you to pass specific certification to pass this floors to know nobody writes about elena learning attract were there or there like these really dial professional view horses assault was linked to their learning tramp flow charge one of
which takes you down the path of designs which you marked specifically by way of the be honest on the sill unsavory person but hold the whole half is like really professionally don knapp and it's how i boarded speeds entirely however to ws the second o'clock group is my son is going to a similar path all of learning initially i was twenty jim but god didn't turn out that the greatest so that he would do you do you would to officially give us the deals or riley who are
in eventually the cancun route is to what he likes the best you know but that's how i need to do with crashes


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt heat it time also starting a new contacts which could use your support and it's called last year's like tax and teaches conductivity focused tips and tricks the sum which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. welcome back to machine applied in this episode on tap into turk from debts agency about
q. flow k. u. b. s. alpo believe is an extension on coburn eddie's for machine learning a pipeline orchestration in last few episodes we talked about sage maker for compulsion is same thing and then in the damn ops episode we talked about just general dev obst pointed out your web stock to the cloud and if you recall there are solutions for hosting web stacks macleod like to be of us the c. s. and then there's the open sore
to quiver went to that's called cooper nannies and am u. s. service called ek astro lasted to earn any service all was used to raise on any of us even either you ccs or he chaos the cspi be using it in a u. s. is a managed service for hosting a darker containers and sustained easier to use and he k. s. allies occurred many sir is isn't like you to kosher to run eddie's cluster on eight of us the benefit for many years is that is open source and cross platform you can hold
your committees questrom debut as g. c. p. board azure setting up in those various providers and be a bit different so as not just a turnkey one size fits all but as can be more cross cloud provider compatible then if you had set up your kloster on the c. s. so any of us p. c. s their manage container hosting solution is easier but there's bender walked in and then a ek asked their managed to cooper nannies solution is harder it's more calm
lex for certain as you recall good discussion with juror was but it is open sores and it is general case cross cloud compatible and you can use on local poster on crime and so the equivalent in the machine learning space is amazon sage maker for their hosted machine where in offerings verses cuba flow an extension on top of cupboard eddie's for the open-source cross cloud compatible version of the same and before we get into this interview i just wanna talk about
these things up or these high point orchestration colds because sage maker is not just about hosting machine and model as the tip of the iceberg it's a very useful solution the war ended your machine on a model macleod is it easier to use sage makers model and points or batch inference jobs than it is to try to spin up your own machine learning model who sued solution even on a piece yes or the sea to or anything like that so
even just a ploy your machine learning model by a sage maker is gonna be a simpler solution then roll your own but the main dicey vickers not just close your model is all the other stuff that comes as part of a package what we call pipeline orchestration so we're training your model you have your data comes from some more like a of u. s. s. three you were there relational data base your sword yes or or dynamo de bier something k. and then that it's piped into a feature transformations that and then that despite
to training stabbed the training phase of this pipeline that training phase might be splitting out your data were spilling out your compute instances into multiple instances of it the training can happen in parallel and then we'd join the progress down stream as the next step in the pipeline do some checks some bias checks some drifts detection and an employer models of the cloud and an additional is part of these pipeline orchestration schools there is a cunt she knew
listen monitoring aspect where if something goes wrong in a good model pipeline that might trigger a week training job by u. reusing the entire pipeline from the beginning or let's say that new data comes in over time the users of interacted with your website for awhile let's say you have a matchmaking service for music or videos and a thumbs up thumbs on certain things and so you want to retrain not because anything went wrong with because your little bit out out of date so these mushy
morning pipeline orchestration tools like sage maker or cube flow are the angle of creating a production ready deployed machine learning and stay and solution not just the machinery model macleod but the entire process or goes eventually into the trained and deployed in from small macleod is so all those steps of the pipeline are important in so everything we've talked about was sage maker save maker has these schools for stringing together steps of the spy
one is pipeline orchestration one will be ingesting the day that and will be to engineering so feature engineering an imputation all at another will be training including parallel is a sham and so on and so let's say shoemaker which is a dub us is hosted offering it's easy to use but you're locked into each of us so today we're talking about cuba flow which is that an extension on top of coburn eddie's it's the open-source version of sage baker
effectively it is universal sukhumi hosted on all the different cloud providers and again for the discussion the juror was the downside is gonna be more complex to manage so that's an introduction to the topic unless i've read them are i'd welcome back to show everybody today we have heard from dad proper enter coverage is she so yes thank you tyler hide my name is the gown and a one part of the dutch dennis feinstein the debts i held my clients of billings
or solutions to discover valuable insights under dana and an end to salsa critical this is shown to us both ways she learning as well as more like low-level day analysis and i were gulag also on the machine running engineering ford and it's also are gonna talk about today necessity in the past few episodes we've been talking a lot of doubt lots with the with the goal both of which you mean camelot the machine model into the clown named in his journey of ammo on
so it relies is not just a matter of getting your your model in the cloud as some rest and point which sage maker can accomplish very easily but what time's up pipe when you want to build out so you by your training phase of old periodically in just the day that i may be short that out to multiple nodes and that's what's the date up crushes it is certain way brings it back together and says you know she money mom and so there's all these different headlining tools on familiar with sage make represented on the show alas
ups and we talked a little bit bowed down box i'll look about cooper nannies in doubt lots and soup today your specialty reined in and all odds is she and flow that yet but so we read them and a complete touches upon the cuban eddie's as well as it's basically they obey orders traitor for pipelines on kennedy's soap operas popeye born son general under haven't choose the topic smile was yours but don't know that dawns officially down the motivate
and though the entire process and eventually will get into what were the options out there and white cube slow the caucus about pipelines get the food so the first thing and it's it's good to know is often when you have like peas these examples these coy examples of mercy learning problems or any day the problem it's very much focus on on the solution itself and training the model to predict something and then use that are put to optimize this is perot says he's sore or decision-making however what we see are
clients mainly is the belief lot of our clients of different this is powerful which we develop different morals would you get the point where you have so many models in place then it's knowing how well i'm on was still performing n. n. retraining the model is gonna think a lot of time and you basically ken's just spent time on that retraining and checking if someone was still proper be working if they're all son you use cases that that that juror working on for a client so
and a pointer kind of force to to move away from stand alone wattles and popcorn machine and pipeline train because that really felt shoot to first of all keep track of the performers of the model that helped you to automatically a retrain models legislature life much more leisure as it's taken a lot of time wayne and keeping the quality of the models of it almost sounds like one continues integration can she is the poignancy icy need is for the software world and i knew she
on a model when she learned pipelines is for the machine running deployment world is so as having good analogy the supper for the most nothing that i think that if he if you look at what is science field and in general it's much more moving towards a continuous integration and a continuous deployment of models of course by the big companies like golan face when they already do this for a long time it's also widely known as these free morton these platforms that basically
allowing u. as it is scientists are machine an engineer to do that as well and they have invested already a lot of money in those solutions but i think like more like the smaller companies they especially in the beginning it was more focused around killing a model ordinance all the numbers the solution and sometimes it can elect the continuous development in and deployment an integration off like the machine and solutions gonna think a lot of companies now start to realize that meeting and pipelines origin
as important as spilling a model because more without a without a proper popeye by the hundreds is eventually not gonna perform as well as when it was just the fellow because david james overtime especially amount for example with covert so one of these models that predict customer behavior or did take the simmered behavior into account to predict something else so special is colored and returning cord is more important than ever big
as customer behavior really changed during the periods when he can imagine when you use a model that was trained on variable four day period then describe the reform as well as there was a during michael the dorothy code because behavior totally changed so it keeps up so these pipelines keep up with the times of the change in didn't do you find yourself i know that these three works and what were you call is from irksome as a right to call them pipeline framework swore i think one of the proper term would
the pipeline orchestrate ers you find yourself using these exclusively or do you still do your mom all the velma on local hosts justin talk about spencer flow wherever they can in a band so especially with clients that reward we've already longer we now move to words developing from my pipe one perspective and of course the development of the motto in the testing of the manon first version of that evaluation of the model that can still happen outside of the pipeline
what everything around it and the structure is basically said out to be financially be used for pipeline kind of architecture and then we also have these kinds where we'd sure started peeling solutions and they're sometimes we see that there's first the needs to be as c. how old machine learning our designs the solutions can help improve their business so you could say they're less mature and and we often see the pipelines are not really good way to go yet big
three first need to really does go for what they use thousand machine learning can bring for his client but for long term relationships and cleanser really merger know we really move towards developing for my part my perspective rather than a stand alone mortal like that it's not measure twice cut once the new town are unique in the minds and popcorn developing course the pipeline once and for production or three steps ahead again sorry for all the diversion you are
spurred she sits in this area so even on the discussion when you drive and take us on a journey talk to us about que flown in the various other options in alaska yeah and so the reason i made it to confer with that because that's one of the main orchestrated or one of the pipeline focused project that the pleaded then chose to go with and to maybe start off with like would really cute flow is already mentioned it before your phone is basically a solution that folks is to
poise the machine or empire buying on a cuban any specter and and that helps to make the pipe livery stable when it's one of course dickie benefits of cuban eddy says capability that you have and also the same time que flow into graceful lot of these different kind of framework sways in so it allows you to reduce sensor flow or psychic learning or pie torch and all of those modules or packages are you like to call on you can for
the easily incorporate those in q flow salute and que flow itself doesn't necessarily have to be in the pipeline consoles offer to possibility to basically submitted job on any cuban eddie's kloster so just like a training job ward eight data processing job but it also the same ben's unless you're using de que pro pipelines architecture to builds of four on pipelines and that really use to flow as a whole lot of flexibility in them
size also it being open source so a lot of people the fellow cotton and corn tribute to the framework itself and in which you'd see because it's open source for want of these challenges that we use the company do you with us will honor companies do with those low scores and because of that the open-source nature of q. flow you see that the fellow who goes really fast and every day there's like something you released wage might also saw a form of your problems which otherwise she would've had to spend quite soon
i want to slow it yourself in the last episode with juror watts workers comparing cube flow with in a u. s. is you see a service last containers worse and so we have a big cloud native offerings microsoft azure to plow platform hearings as on when serves as an episode we recommended it and us if nothing else but popularity segment which is like you or find a job on when most like the buyer john invaded us marketplace because of it's not a popular and say shoemaker lamb
and the way they pipelines is what is this a lot of overlap with q. flows off and attorneys are so sage maker would probably be equal of cute flow in cloud native on a u. s. and p. c. s is equal and of cougar nannies on aided us on a bed could hear perpetrator platform but talk was source we don't have is going to haneda stuff works by by that standard you would see it coburn eddies and cube flowed be preferable why market open-source a feud
and in the same thing in first place on arab us and juror wants the man was good for nannies is quite complex award told to swallow from about but the thing compared to use yes they are and i wonder if that similar to your own experience and if so how does cube flow comparative nana non je to cloud noon machine and offerings as so maybe did the first good thing to know is even though cured flow in summer
it's really built on cuban mideast it's not just limited so wiping goo cloud because the full sullivan of the history of carl pupil region and scum from google and how date internally deployed she learned by applying for all their services and they built like his entire tens if opec it around a difficulty effects which we will touch upon layer is long but q. flow is not just limited to cuban eddies and also in the same thank you for pixel lot of this complex deployment froze
is that you have with cuban eddie's takes it away and doesn't for you so especially as the machine learning engineer or dana scientists you might not be very knowledgeable about curing eddie's or no audience an ounce of how to deploy something cubans would cure for the facilities that for you and help you with that and the same time que flow can be deployed on coming most big lead from flight data we were as airflow is your cool clouds and in
cool clergy even as the option to deploy don't cuban eddie switch it originated from or more like a ai a pledge form would they now call ago for dick cia platform so i think whereas when you focus for example on more like a bill us or just an azure you're very limited to that platform we've que perot you can reach across just want platform in hefty possibility an opportunity to know just the two won by firm but also the employee in honour platforms
if you need it of course is surprises can relive the different but a lot of the complex of the cars with it is handled by careful and i think that makes secured arguments for using cure for together with the open-source nature that has the added another great erred in with it being cross was formed to paddle is low blows the bellman with sage maker one thing i'd pop against in developing with the clout in mind the benefit is that again not measure up what
once you're in the mindset working where the p. c.'s and scary groups and i am policy use before you can even more on your moral pronounce i'm good luck with the body in local de beaumont the key to be a v. p. c. narrow your spin us up on your local p. c. your behalf to create an action host macleod or declined the piano say look max the bbc ourselves so the complexity that you're on my disgust of cooper no use in use case in general in
that's one thing brawl were not accounting for many times the complexity of developing towards the sea yes when you're gone and fireman's local house since scenes like this one is pros and cons on both fronts for what smooth talk about the debate of complexity and and you guys use crew were nannies for the general about stuff for web web hosting swirl by thing did kind of depends reply and i'm not really aware of that poured design warwick for the day the science thinks i'm not worthy of the ia weapon
not at the filament team i know would dare orson appointments that have been on and cuban eddie's what for them that i'm really not sure how will that the fog shorty and fomenting said that tackle the court your preferred choirs g. c. p. is that correct i wouldn't say preferred it's more like a lot of work lions work with that i'd faint in europe in general for long time ago was together with a scheerer has been one of the lead to bigger blow up of forms a ws is kind of
taking over i think at the moment but i think it's also because we're in timor cutting business or a lot of hard finds that the solutions to weep built for them or more cutting related and a lot of those lines were grief google products like away at school analytics of those in a cool based products and it's also the reason why they often go for approval cloud because it's logical decision to make if bowater fewer our systems are also by goebel develop ago that makes sense to have azure we do it
dealer fresher but we do see is under constant use azure or often less more cutting focused issue might say it's more like operational base of operations financially focus solutions that we can build for a yes so for example we have a kind of did indeed the real estate developments business and rebuild them warwick operational solution for them and they're also an issue that adds a lot sooner the puzzle mine and people who are trying to use
or macleod miss the witch winch but the guy was aided us just because it's most popular so it's easy answer but i never have any real technical argument for any of the specific cloud providers right yeah i think if you're in a marketing this isn't your real using our big herbal products it's one of the most logical steps to make from any connectivity standpoint of course she can get to the dado from cool analytic for google and slowed to any of the outer platforms but some of the integration sorry no
it's more straightforward with cool club than ways for example analytic school one of the nixon data we were as an art as yours foreign live on a suitor microsoft up the schumann mud and potentially c. n. n. g. finance as an example or operations people and finances so used to excel that's my piss off products seems like one easy step towards that platform if you're coming from an environment in which microsoft products would have made sense on the desktop daredevil am also for example are
a lot of the company's center for jumping using microscope dynamics are those kind of packages are those kind of self worth their walls would make sense to stay with the weicker soft related close i know for some companies that aren't allowed to use go go and solo mainly a big thing again nothing switzerland lot of the company's there do you wanna stay away from google sewed very wrong like your phone calls them or still on prayer or also measure and it also
the reason is g. c. p. have its own version of sage made birdsong non que flow based m. l. pops off when possibly including pipeline orchestration yes so this is a thinker couple of months now with gold clown shoes to have the wood because like a diet that for bids were all your de ayer related stuff live in so your jupiter notebooks your jaw blogs your model and coins all beckham the stuff and
dave recently changed that to something they call for dick sailing and with that day did some more possibility whereas a guy but for before yes what the pipeline option for the pipeline option was mainly consisting of acute flow kind of pipeline was there is some cuban any specific would you had to do is spin of the cuban is closer the board coupon bad and then you crude bill during wattles live for example with your facts and then when you
would go to the pipeline section in iowa but for i would exceed open up a need to you why you team to face a foot of kubo now with for dick cia they have full it a bit of a different approach is still can run que flow lad what you can also orchestrate your pipelines but still from within your phone when all the cure would know the stove is handled four years we don't have to spin obligated to reduce costs are aboard those famous thanks so it makes the little bit more piece you're done deal
when is more managed yeah it are we bringing are the migrating their regional offerings to this noon vertex the isolation is re branding risen and all toward a solution of thing that sullivan above so some of the stuff for my eye but for right now they're read grinning at the end and including would only within the new verdict cia section nine hundred of her dixieland brands but for example the option for pipelines is still be traceable in both they and but for as well
all as the four decks account for it and also interfaces are different if you go to da hybrid form by planning to face you kids basically to cuba and her friends however when you go to be the interface of pipelines inverted cia you're really getting dedicated cool clouds pipeline interface attribute of fellow by google and not necessarily by que flow to potentially vertex ai is this age maker cleveland's one juicy even managed machine in oxford yeah raises all by google for ghoul
our visitor have first-class orange juice you keep tube flow is the most by goes wrong yet is the fellow by the denser floating okay or not mrs an intense afflicting him but it did spilled around tens of love and to seville pipelines to facilitate those fanciful apartments okay dolores tucker full pipelines what is he a fax to sweat and gadsden slower senate ethics emanated thus far senate basically started to win in google and dead wrongs or courage
only a bond on on cue from us think hugo's still the only option to which you can deploy this flux and pipelines and it was a kind of away from google to basically make pipelines for all their big services so all the models that are behind their gould e-mail or gold calendar or google maps like all these big services in which they have water for seeing moaning models running they all needed to pay a continuous that
one of course this is a future visits and also it needed to be scary aboard because you can imagine those services argues body by millions of people incentives for how would they developed world as an internal service the bills what is now known as que flow and tens of low itself for a long time blisters model bill into law a deepening models fishing and almost problem listed models but with the extended baggage it reaches for didn't just a more boarded the city
except entire port before billing and training amano so like cleaning up your deductions forming your day and checking for anomalies creating a schemer from new delhi and buddies and reading your dinner from for example bakery or katy the club are getting cool go on and also the port after de machine on moral so the devaluation of the modern checking if the retrained for show him on oldies is performing as well as the previous version curtis
we're jumping can imagine and if you dude continuous training for a model using those pipelines that you cannot end up in a situation where you return your model but for most of it is not necessarily better than the previous version so also that is something enough to take into account and finally of course pushing your model to you you're an end point to a location inga will cause storage for creating a claim on all of it all those steps they basically bills more jails
in the tens of flow package along that to facilitate of steps and all the steps together create a wood equality effects pipeline which has been deployed for youtube flo why is that the song like a lot of overlap between t. f. accent you full court the differences between the two so basically opens the flow expanded is just the logic poured so how or you the building up your model how are you generating your skin mahal your transforming your model holly you be fill you in your model
and for dead they all made ladies components which they call it components serious like the trainer components you have a refill you waited components and a sadistic generally component all these components an open forum like the pipeline and they ordered lloyd on cue burnett is for example group could cure for so basically the back end of the pipeline askew flow and that executes the tens flowed senate code i see is so very easy
q. flow especially on she's seeking you're also gonna be using to support stand there like to have a few are working with a sense of lamotte over definitely butcher not limited to you good as well use high court you could use psychic loan or are in any of the other major machine known in frame or some rather but originally it is to filibuster rounds tend to float feathered because it was like an internal servers and google is area mao
of tons of flow extended you can use if you're developing a model and by torch to select a lot of nice utilities in there yeah so so then you get into the compatibility issues between high towards than ten slow like theoretically you definitely use those blood the way hard forges beloved and intensify was build up the road leader is saying but technically there are sometimes quite different i'm not sure future dried blood to try using both applied for
jan tons of cocaine you're saying she running goat is gonna be a lot of debugging and in the law the cases you're gonna end up using a new one on the other young actually and that high caste come a point project is this juror was in any guide you have to use bulldoze framework sensor cases justice on certain packages are making use of unit is by using doleful then the same i actually kicked off that term jobs other running on two separate python numbers based managers need to get out in a part
schiphol battle with each other yet they're fully n. n. also just like the types of power would be outputs in imports are are are the findings in both beckett is a very similar bird is sometimes when goes little details the rooster compatibility i know there are like arson cases where people do manage to come blinded to button so painstakingly presence of blood or rat we have you wanna get your machine amal host you have azure juicy diego us an awful lot to others
those men treat each of those three all while you to do yourself using open-source cooling though would the u. for nannies and cube flow and they also allow you to do it through their offerings which makes it a little bit easier when you're locked into their offering so those are called managed solutions dwi says is sage record she c. p.'s is vertex arid lands it sounds like a whole bunch of other
losers offerings is well we've got airflow an owl flow and well where else are you familiar with and to those that you can speak to how they compared to reach out so what i'm familiar with this pretty limited to airflow bands to float and uncool clouds of pipelines i know that the wound also measure park west not like practically a book or theoretically what i like about the
cube flow sometimes like one of these discussions with colleagues bothered us and they work for example in an azure pipelines barefoot lot of these small issues with indication issues of compatibility issues information being shared between different components that's all handled by a the combination of t. effects and and and que flow so dad's early why a diaper for q. flow but it's also i might be quite bias because i use at a lot of things afloat and what i do and
solutions i built and i just find t. combination between tia flex and cube flowed to be very cute new beginning it's quite a painful process to plug very steep learning curve and you can get very frustrated and during the prices of trying to understand how are they were together an especially in the beginning because i remember when i started working with your most recent version rose zero point twenty five and renounce version one point four and just a time frame of six months'
the frame words have improved so much that a lot of those folks little dishes of all been taken away and you just see that there's a lot of dedicated time and efforts bent on improving the services them for what i've seen for example as yours that sometimes it can take quite a while both for some three basic issues or refer which you would expect that that they would be so much quicker done what is actually happened and reporting like air flow from what i've read than what i've seen so far away
is what is airflow record is so airflow is is basically the new order straighter dead that would fit one backs and then i have to wait for twenty official tournament is directed at the cyclical glance yeah what is so so in the pipeline emptied out each step countenance huge other stop yeah that is to develop and would the thing with airflow is is that it's not resource heavy buying wherein you deploy something on cuba eddie's deed for
assistant our inner resources that you can use impurities or by big fight could scare you boliden can take quite some heavy loads however a good thing is with their flowers that you basically for all the computational heavy stuff you have to submit for example a job through a habit for so when you wanna train your model on cube fell on cuban eddie's you can choose to submit a job to wear platform which you can as well just traded on the back end of cuba which is given and as you can just training all
acumen and so we carefully dawn of the option because it's basically just an orchestra nerd and executes the different steps in your pipeline but when you knew something that's more resource has a new always have to have something like andy i'm good for the spin of the machine that contented to have you were close to deep competitions in the model turning unknown suspect information by ceo can see better stand so tube float is bad headlining it and model order
trish an income when eddie's with machine in mind yeah in any higher floors of the general purpose jostled stamper i yeah and it just so happens again what machine in about one sherbet you just to be using some machine tool kit or claar offering what juicy piece that platform that you will never never be able unless you have a very light monastery the little thing that but when we're talking much heavy models machine learning the burning model for example that the key
and in millions of rows of data that you always have to have something else and just airflow because otherwise senator conrad to use air flow does it have a place if you're using grazing cute closer your reason you never use air flow for eating so what we do adept is we do a lot of data engineering tasks on their foot cell trade harangued the export of data from one location to another or the creation of tables all that stuff that happens within and
burn farming but still nice to be scheduled we need to know and mainly through airflow in for dead and works for you will want to swab for machine morning okay so if a call by moret in this you need a job us lands above managed step pipeline orchestrations is a thing called a conscience the husband step ingenue my hundred of slam those two men have a series of just roff auction that feed into each other and in a specific war or non branch out and come back together in a smooth general
are the sounds like maybe there flows for that purpose yeah i think you would you could system and then how an elf lord you have any new areas issue with that i have no i've heard about and low flow but i've never really worked with with their services to get it sees as anything in releasing some middle c. forgive somebody here in the future to discuss started to see emma flow coming up but often that that i have not really seen anyone within our timidly said uses a mile from okay but
it's none of megan rocket that not that i've seen yeah so the main contenders that i have seen really in my world is u. n. meet when did you have flown into his safe return as funny thing actually i'm gonna work besides maker for riches feet which is profiled in u. s. run and a delivery in oakdale all your love so that was as lose hanjin so there comes from babs the agency is the master or organization i actually working for can
recall rocky which he says suitors recently acquired his law allows smaller companies within the agency and so it's nice that rockets mean skill set as a dub us in clockers man that's mean skill set is a azure a dingy city for the record at the burger grads get together we basically convert the biggest acero full of blood firms and as you have anything else to say about to reduce fuel flow to answer for extended or any of the other offerings
that i think in general we talk about the benefits are false tupelo compared to what your options are rather the most important thing is not really what you're gonna use eventually his discovery your options and and see what is out there in and what fudge on purpose because in the end all these solutions of options they all serve for purposes battered another i can imagine that the few lord completely set up in the middle you as they would really make sense to go with something of
and so trigger even dole queue flow livers the street cool solutions and of course you can still the point you're going in and the witness of course the thing the main lesson is to just move to words a more like pipeline or just a thinking machine on a par binds rather than just then alone models and then which confirms searched the best that screwing up to your situation personnel file was found on your phone anti effects to work free world together and is has served
so for most of them the news of our plans are nazareth situations in which to flow is also not gonna commit free yeah i like the idea of starting to think towards orchestration instead of just legs and savages local close marlboro man in ensuring if you're in a trainer amal by a quickly just starting on one of these platforms your training job we'll be able to shorted out to multiple nodes unit and potentially macleod ideally in clough
so they are confined to your own local was cheap use limitations or if you don't have a jeep your no law go devolve on max and that you can do machine learned very well on max is that's you would say she's eighty just kicked off the cue flow by byron sage baker by buying and it will assure your day out if you have a huge days second world trade on multiple nodes manrique combine the results downstream our history our training does a lot faster doesn't have to be an expensive because the
news on eight of us you can use it old spot instances other super g. in our little less reliable was super super cheap and just he has it's own guevara was orozco machine charging like a different the fertile machines of the orders of the spinal denied a and s. b. spices are actually is this like access can keep capacity from song would lead one organization reserve some out of the new can reduce our happened i'm not using it at that time you can still when baghdad and you're paying substance
what's the record you by your machine might get taken offline if it were is issue comes back you basically house sitting for them i'm not sure if it's cool caucus and one good 'cause i haven't seen you before printable instances and it's the cruel blow equivalent for them but you can see disabling ghanaian deeper sense of what's all on my friends her doom shimon inhibit amount all say you'd you use eight of us should often training job on a jeep you based instance the use of spot instance
you say ninety percent this beat for an inspection and so that is since costs one dollar per hour to ron with a g. for two x. large cos like one or two one or forty around one nine eight percent say is it's young fourteen cents per us huge difference by the downside is picking come off line actually somewhere where you been on it's you say it i'm willing to pay two dollars is the price hikes and everybody else wants that house sitting job
i'm going to like payable over the ridge open arms but you'll find yourself in a regime we rarely and so the spies are radical useful invaluable and not as unstable as they would see that once you when you see suffer like production rickety wooden uses for long running instances was phenomenal for training jobs so you would recommend them also to use of already employed in and models that are running for by the desired unimagined and so how we ended the caterpillar stock is that everything is
the pendant on one another so the d. output of a model is gonna be taken up by a solar job to basically getting our goods to the location for one for one of them can be used but with the spot instances has called how would that affect for example when your worker is taken away because it's utilized by the regional organization is your job than just waiting until it's finished for the cigar for miller how was that i'm gonna play out we definitely would music in hope host
training this is only for the development venice or the training like was in your mom all drifts needed some learned that it needs a retraining job the new in response is it depends on the framework being able to pick up or less on focuses on essence goes down big idea buddy epson juicy kids call printable instances if that goes down it depends on cancer clubbing it'll take snapshots and then picked up from where the last snapshot left off in wooded then have to notify him
self to try again going unused on instance pick up or left off the force saying that the job is don to kick off the next occupied one that bunk and this time the idea that you could do this and settle because the bellman of your model training art you can get this off to cloud using sponsor says from cost savings and i'm steep yourself with an orchestra she heard on a thousand is a very good solution to the conflict training local your paid full price for
and for machine so again what would what iowa's what to do is to strain on a small subset of the adjusted the book and then when everything is working just submit the entire tiring day is said to the model was in the pipeline and so far mr morton pretty good but i guess this would also be quite a good solution to come down costs to surf on what's your abdomen and are you the sea legs and go so when i'm at home i use our windows
the seed ai values linux than men and warrick would also be peace you could back mac had known the next i've tried it like a bad university was but i know my focus is war two were slim on a building and theoretical designs about life on all the hassle that goes into setting up for the next in the proper way they didn't really enjoyed them much a devil am acre estate haven't sea otters john remember growing up bad in the early days of limits and the war between all distribution flavors neverland
jonathan boston is a bunch you on the seaside had on what the managed limits find anna was gents you on the other side and thus him and it turns my wife with the bodyguard gents into this there are makers opponent equally to funnel yes i know i'm under the old days engineer was like a teammate jesper it's you pretend that he wouldn't they would use linux at home i use legs quite often and all three a usenet windows undermine its windows for gaining two clinics for all the bomb at north increasing
and try to do on my own vomit in the cloud of first i would like to ask if you could take us through a project beginning to end journey the beginning being i know i wanted to older i'll as second class are there any pending i want up whoa someone how would we decide which car buyer use were for shoe that was first coolant at what is the training process look like within the tools that you would end up selecting what is the pipeline wide
like when women have we got in on one hundred and four website so from my gay uproar together client or more like a general personal project was say our listeners they went quiet klein says look i know i need a bra commander sister maureen image classified or even as an alternative the climate may also calm with their own specific and barman neither microsoft shot or are there are using who will further how it's so in various forks in the roads during this cross
us about why our machine an engineer might choose one solution over another and then as you go through those steps huge slip by as we're braun won i'm choosing kids love them incense never sure yes so i i think in and the first abandoned for leaving davison for where the first up would really be to go back to d. d. drawing table and an have alluded to data to really see if what they want super one day you request is also what they need when we often see is that
klein's come we're paying direct question and already some bizarre one id what they want however one week is what we always do first resort to it the perot says it's called david discovery so we see what systems are employees how's ortiz says things connected what they had to we have what do we see from the date and so like a small version of an eighty eight can we even supported that solution or questions that they have with the current david innocent lives so that law
really be step would even closed up zero go back to the drawing say we'll see if the question of declines happened to the client has this really would they also need because for good or very very nice example is due to stick to lug recommend assistance when we entered kind of came to us with the question my gal we wanna have recommended system on the web site when we started looking into their data and an investigating customer behavior and an end to roll the system's already had in place we actually discovered dead than you do better
heard function so did in the demurred leper commander on the website they needed the way for for people who visited a web site to find products for easily because what we already could see from wonder first discovery phase is that he was very ford for people to search for products because often approach one's really sure what that when the search for so that really would be steps syrup to scholastic surgery implement some assorted the client went with a tool which i've forgotten the name
for think it's something called like a seeker but i'm not sure is so yeah best of cell where maria where roads but okay let's say decline goes with the question and then that isabella question and it's a valid problem that we cancel all fun and awaited they see it after we have donned a bid to discovery we see that the riots is some sort of place or some system still need to be implemented for everything too were blokes where was first luper the bay is they're already a platform that they are working on
a lot of the closet come to us or are they your clients that already have a cup of four major a deli where school cloud or or measure what in case they don't have such a thing we look at what's their services award today or do you resent which club but for my needs to be integrated with those other service most days when so in case of likewise and can increase if it's there are companies that use a lot of cool products then theological recommendation would be
stuart using cool cloud one and the others are other side is a howard hughes and a lot of my curse off early reports as your would be a recommendation to that would then be stabbed one setting up their direct foreigner violence and then step two would be for today the engineers to make sure dead all the data is in place and everything is so valuable but also does the pangs of course all news guys because there's a limit of a day in separation approach because in some of the k.
is more sore doing this for c. ok decline wants to see if the output of the eastern n. n. no sir tomorrow is even gonna help them improving your decision making and is some cases they really want to have feared continuous employment n. n. use of our bid from wanted so in the first is to say okay we first on to develop an amman old fella dating now put in an emerging that to the carpet for unemployment on their board bit more happens that the clients like okay i want this
princely to live with him a couple of four and then we start developing this together with the dayton jurors who first major and all of the discipline is that all the data is fleeing the distance while some most important things in dislike entire process yes they have creme de otherwise not really gonna get anywhere with the article from will only cleaning today yet in your journey you or are we just looking at the data deceived its quinn is so in this case cleaning of the dermal full steps in cleaning the air
he's already like a deep freeze selection so if there's any false the degas did we already know all waiting to greet him or getting all the data together making sure that david is excluded from the data so does finally gonna be used by the day the scientist to build a solution is in my opinion by the right thing to do because when you're out already noted this faulty data than there is not really any reason to intrude on blessedly group very specific used is of course so there's already like a pre select
and the premises but then once all the data is in place and this is also for slated for diff process that goes back and forth in between good and the third step which is today those scientists explore new day and doing some beauty aids is he ok what do i know over d. c. as in statistics and fish realizations from what i've done it seemed to and then he goes back to date engineer because maybe for some of the day that a lot of fellows are missing or though make sense or not
i'm not going to ride forward to like stepped up to three top or camilla can picture differences so when the two three are finished that's when we start developing the model so transforming year to intruder for dried include what we also like to do is we take multiple wobble purchase so we have like a baseline model so let's say certification problem retained collector brennan for st ago gerber and fortification model which is very easy to emblem
and minority can give you the first results confirm that we see okay where is it performing well where is about performing well maybe we need his chains and stuff new data so we go back to step three into reagan but then also known from their restored to maybe do felt any more advanced model if for example the base one was no warning one half or so then will we get more to disparage of developing that the defense mortal the wheels of getting to the stage off making sure did you more can be
employed and if we wanna build a pipeline around and make sure that all the components that are gonna be employees around the more blood cells are being built up an indirect well with the porridge on today the engineering side so this kind of like a pig pelham parallel prose as we both the vote goes the more toward the advance wobble attended the same time rounded we'd build the pipe and then when we have evaluated the custom ana winds performing well an end
how could make sense we made sure did the entire pipeline is the point so that bundy did engineer kang also used the outputs or toward a model train by the pipeline to predict on new dado which there can be used for any activation purposes that were determined didn't like the discovery for this so this kind of like the due process that we walk through which all water replies but definitely not all the glands in tiny decided
that had been face between lasater on g. c. p. vertex ai versus q forum yes so currently the depend so that a lot on offering more deter using those currently you when you use consumer obstinate even like tens of floyd self is saying that it's for now better to still use didi cube flow like the cuban eddie's kloster implementation zodiac affirmed the implementation it shows the verdict day of the medicine is quite new skill and my mobius
evil is dna i've been formed into attention so for now we're whenever we deploy such bye-bye is still an epoch for bugs essentially it will the bans on and if there are any find specific requirements that might not be met by eatery i'd love for verdict cia or ai bedford park was totally disappear and that merge with inferred excessive force in all man so many solutions i guess you know in in wendell melman
this annoying one framework says well there's captain expressed fast if i fasted yang and all those things that are for the coarse sand as a mix of selecting a tool cents to become an expert in the moods and with a puzzled a difficult this sounds incredibly powerful winds popular tu que flow that islamic beverly keeps coming up is almost repeated boils down to the so i'd ulysses mc or oftentimes i reconsidered charges sir learned dudes
on the other girl lot of issues listings in yeah i think it's also kind of the nature of depth we work of lot of different clients so we don't really have the luxury of color that that we can stick with one platform you have come on our own yeah and no one of the known fastest ways to connect with other platforms with limited knowledge or not knowing all the dedicate the pipeline orchestration for reads the firm platforms is use
if you follow because they can connect to all those different conference and of course in some cases it might be better to use a dedicated solution of but for nothing especially when your in the business of that's where you have a lot of different lines lot of different but for some work with finding a solution dead expands across those platforms and his knowledge dependent on just one platform that really helps us to deliver fell you to all our clients and it makes us all so much for flexible
the dissolution so we can build center clients for which we can solve the problems that are really powerful deciding corridor where agency we have lots acquired securing the same boat it probably pays ago kerwin eddie's tube full because the general isolation and i really think so therefore should also trigger rudolph having different people with different expertise is of course but from what i've seen a lot of foreign if there is just a couple of people that do one thing you'll never get his ski
who does one everyone were shown by the same thing in and can basically improve themselves but also the team you made progress much faster than wonder choose one person focusing on one solution another person focusing on another solution then the knowledge sharing is also we learn some of the with your flow someone or so for climbed as a billy where's the news can flow want another colleague who were to the clown the goo clerical she's cute when men still in some way they can share their knowledge with one another way
she is gonna recorder between someone who's dedicated to say trigger and some were dedicated to ensure bye-bye infertile bathroom in the supervalu ohlmeyer really pre shape as their diaz you wanna say about your floated fog spend it or anything else no i think whoa we just what's above like sums it up pretty nicely cube flow is the inflexible solution of that serves most of the bradford's and not just the board freeze her blood froze but also to
she won from works but also in the end it comes down to woodward's as freer but i think for us is an agency the biggest point is that that we have a lot of different plans to flow is really something that i think in the agency that the board truth she learning engineering an engine problems should consider using i'd like to end these interviews episodes heard the way ship it handles there's wednesday that i caught isabel you senator religious indicted book and
getting audio format and souvenir ruined fond relax character so on that scenario little bit funny some new your interests there things you like to do off o'clock old that's i developed quite an lot of new hobbies during her colored unlock the the suffers a law on the big keyboard nerd alter the bill keywords b. i. bill thousand keywords so i got into that holly huxley during cohen and it's like a rabbit hole once you get in you don't get on something disqualifying most recent private hobbes was you could say
i'm very interested in playing table tennis in which also recently pick up again as the bug is sports so i guess it's basically a private interests and i need recently is finished oh my master degree is sending data science but always have been working on the side as it is scientists and marketing companies before depth of the motor company and stuff like two years will add that so yeah besides it just being my word it's also my private interests and drew my time off from work or else it just we'd like to read
research papers seawater mute of feldman's was allowed to enter watered latest trends alone machine learning and it's a good patients so yeah i think that both sums up my pride in the morning or interest on the topic of the master's degree this is something the law my listeners are rare in sudan and says is so fresh gee maybe you might have snared sector and they wanna know what amount of added value his master's bad over bachelors to finding a job and innocence as sex a good question receive is often popping up on right
this will lot of people are asking this question and i think it really depends on on your situation so thick for example someone who comes from computer science or from a sadistic background or from any mathematical background if if if you wanna get into d. the papers signed sealed and have a bachelor in one hour still owes three so absurdly computer science mathematics innocent to six personally i don't things and master degree is in cages
i don't specifically is then gonna serve much going for you because in gay assigns the most important cordis understanding that the mathematics behind it and the technical part i think is easier in some cases is easier to learn that on the mathematical in statistical court so i would say a few come from the ground where you have had all of us manage statistics nothing but computer science at the same thing you basically have a combination between mathematics and
and technical subjects of thinking and those cases the decreed in your master green designs is that i really had much kill you for at least i can add more philly done when you stuart exploring the beside herself and what is also wanna see iranian lot of people in the day the sarsfield are not necessarily people who get the message agree but they have some technical or mythical mathematical background and they were interested in this signs and they just read papers and and and and did course
i'm calling themselves and i think at that point with such a bare ground a massive agrees are gonna add more affiliate themselves and however a needle in my bachelor's was more like a business and economics focus of course irony had it interesting data and the nitride to always combine what i've learned in my bachelor's whitman today the field blood could convert the film for durden like economical mathematics and statistics and business
mathematics as the six and for me it really felt through basically frank up my technical knowledge as well as mine are more specific they assigns mathematical related dogs so i think if you come for background it's like this is really did i think it really helps to have a mass of the green bay the size of that sway wanna go to and then ultimately you end up with his prefer combination between understanding the business side and the day aside one of those three you said toussaint
as the sake of pure science statistics or around and then animus in the third option is is machine learning work were due to science as an actual master's degree yeah which of those three would you date if your intent is this signs urging the sounds like a honestly c. is the right answer but it doesn't seem that that's necessarily so you have a bigger masters for good sized shaman would you go to pure science math spats bird is cents an hour personally i've thing
i would choose for stats steady for mathematics and the sock and as a weapon with the conclusion is coming to you know it's almost like machinery instance band is sciences you'll know what my calm next as the glue that field the future yeah but there's old years old faithful all the time just that's that's true and i think in the end flaking you understand the solutions the new bills of the best and how would boards if you notice that some of them out
behind it and what i feel with these sometimes fees from what i've seen of forces depends for university group but what i've seen from ladies' day the signs of bachelors that might sound like they're very related to the subject but sometimes i think they stayed too broad so they don't reproach upon like a deeper and deeper mathematics and statistics behind it and of course in the end it also comes down what subjects you choose like the selective study after enduring your bachelors in your master
which can also really change with what kind of molecule finished as agrees but i think the statistics would definitely be the best backbone to have one going into wanted to send troops i agree agree end i've seen water grin on africa's long line and end up in universities one thing i recommended a long time ago and i would stand by and we're just because i haven't kept up with that slid open ended and complex witness ron was all when nancy asked george were
to tack on line master's degree was really in expenses with five thousand rape us marshal but for masters we sell ripper an slick want two years masters on line and so is a really attractive option option as opposed to the boeing says bedecked universe ye guests code now servers on one knee when you find that there's i mean those new gasoline and agrees those are really going to march in the way of credibility and the job market n. n. is like traditional university for rationing is seen
to be something um no brownies there's with degrees offer online maybe by a well-known universities but see the new me on an author like an accelerated program more yes and i've seen him coined by like advertising for for those kind of degrees lot especially on on platforms when read it and friends as far as i know like yet to be very careful with with addict reclaim the dismal lot of those online agrees they seem very attractive because of their prices
however would you really have to look out for is that it's not gonna be a show what i've read from it is that a lot of fun people took those courses they had barely any interaction with professors marauder students and personally i think the size the courses itself when i found brief l. u. bowl or during my message reason go fifty percent of my message agree with your immortal soul so they can go to university physically and it's also where realizes that even more is the interaction but
green you animal nourished eugenics is freak is is the smallest as important as the courses itself because everyone there has a different background and you can barely color from one another i've had a lot of his crew project in an needing full daughter she runs during which i'll learn so wunsch new stuff that i didn't even get to intercourse is itself says because they were for exempt from a computer science background or from a mathematical background i think and that's a huge component that you missed
when you take these online degrees on the other hand i have seen some repute wants that offer course is strong and free credible professors i know for example that this eventually freed but i think most of what the patriot mit is publishing also lot of these people running lectures and if i compared those two deep lectures that i got regarding the burning in my university is that the current them there is pretty much the same
do anything you're missing out on us like e. king friction with baby daughter she was under professor in the practical exercises a big and so what i would advise is to really dig deep into what the curriculum is to offer an accord of people behind it and also just check sources like read it war annie forest because there are a lot of people that she heard their opinion about he's gonna fall one agrees but often is really a hit or miss lewis on the river completing on sons and inoperable really learning in it
in which you would never when you just watch figures on youtube as sometimes there are those that are really good and i think that the beatles up until the organization was hunted i'm glad i asked you those questions come up with some insight on on the top and yet because this is huge discussion that's going on on rented for example as well what a lot of people as for the way they need the master degree in and i found this online version or does online degree isn't really necessary or valuable up for me to read for
no my career but under the tree lot of discussion going on about these laws tried to what kind of keep up with that those thing 'cause i find it quite interesting to see where those developments are going to this especially in the field of data sounds in education is changing very fast and curriculums also for white universities themselves or changing every two years now a new one another vote for nafta that area learned has flown by torture deepening you know you're sick who knows what comes next
yeah i desire member line enduring minute messer jury like the focus of a lot alike on machine on and you can but it's now or were decommissioned thing to more like reinforced and learning is as that of the next step i'm not mrs of the next step up its level of wood more advanced than your d. c. the nurses shift going on to push for all suffered those kind of subjects this was a wonderful wonderful area and so are we had a really enjoyed it so i think it


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. eight and i'm also starting a new contacts which can use your support and it's called black mayors like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twenty two d. n. o. p. part one last four finally here deep natural wine way
processing it's taken us so long i apologize for that we're gonna discuss work for a neural networks or a man's word to becky in word and outings in various architecture details like the l. s t m long and short term memory angie or you did work for unit cells after preparing for this episode i realize indeed is the video mall fight for soul to see how far we get in this first episode deep natural language processing i mentioned it for deep learning has revolutionized an
nope he too incredible degree work for neural networks have common with sword and porch pillaging the entirety of an l. p. leaving no stone on towns are n. n.'s are incredibly powerful and versatile in an elk he why is steep learning so plentiful the penalty there's a couple reasons the first thing the complexity and new wants of language remembered from the neural network episode that neural networks and people earning shine
particularly in circumstances are extremely complex by comparison to circumstances of her very simple predicting that cost of a house in a housing market is very likely be simple linear model linear regression fits just fine but predicting whether there is i kept our word tree in a picture is too complex for linear model to handle it so image processing is really entirely the domain of people earning and convolution all neural networks specifically with which will be two later what is the basis of
complexity in mushy learning malls it really boils down to the fact that certain features in your day that combining your model in some way it is nonlinear so very simple example t. is to try to protect the salary somebody you might use their degree their field of study where they live race gender always things will feel the steady and degree are two very important features more talking about somebody sorry but they're not additives their multiplication they combine together to
create a new feature which is field and agree with underscores the multiplication if you have a bachelor's in computer science that combo will make you more money than if you have a bachelor's of english as so we don't consider them separately it's not bachelors plus the english versus bachelors plus computer science now if you knew this ahead of time as we do with field of study in degree you could manually combine those features together and still use your linear model like linear regression this is called feature engineering men
lee combining your features and in putting them into your model for very simple situations like this yes that works fine both very complex situations you need the machine tool learn how features combine enough for a new rome at work guys that explicitly when neural network is best at doing is combining features together so in this particular case you could just around the gate into neural network and it would have learned in it's thin layers to combine field of study and agree together multiplication what is called feature learning n.
there are multiple layers of feature combinations that when you actually have to go with deep learning in the case of image processing there are indeed multiple layers of feature combinations the first layer of feature combinations is combining pixels together to form lions so began the layers lawyer pixels those are all your features the first in layers job is to combine black pixels together or dark pixels together to create lines are borders are edges and in this set
in hidden layer combines those lines together to form a objects like i. a.'s years mouth and nose and in the final layer the output layer of this neural network the small fry where percent chon is going to be a buyer classification as to whether this is the face or not you need that second thin layer because you need to combine things heretic lee is not sufficient to combine just the pixels into lines in order to make a production you have to make one more combination adding new layer
the language is just this way language is so new wants them complex first off things do indeed combined they have to become mine would you say this movie was good pursue this movie was not good not been good combined multiplication lead such that it is now the different feature then not and good consider separately says sequences of words should be combined in such a such away on top of that you might have a second skin layer which could combine constructs of words
as they relate to each other you might even think of this as syntax tree parsing now making a recurring neural network with two hidden layers in the architecture that doesn't give you syntax tree parsing her say we don't know what it gives you neural networks are black boxes what's going on inside of us in layers is impossible to interpret all weekend into it is generally the things are being combined in one layer and then those combinations are being combined
another layer and however many layers deep you wanna go the depth of your hierarchy is sort of the depth of the complexity of the situation and hands and that seems intuitively to be like syntax tree parsing if we're talking about language but it may not necessarily be that way under the hood in the black box of the neural network but that is just to say that language is complex language is new wants in words come by mrs way that would be impossible feature engineer see you want to feature lauren m. by way of deep lar
in an high arctic lee combine it soon tactically so that debt indeed learning is significant and helpful that's language complexity the other big benefit of deep learning in the space of an elk he is by way of something called n.'s tech and models that we saw this small last episode on machine translation in shallow traditional machine translation we would have multiple models all stacked together all feeding into each other who'd have syntax tree parsing we have to
coding indeed going by way of bees in statistics we had alignment models and we have language models nice all displayed in together they were written separate we maintain my different maybe teams are individuals on a project for a company an important leave shrinking one of these models does not feed into the training of other models you have to train your models independently and danced and model like rucker neural network that bar n. n. is all you need him
sheen translation that lorena and learns everything you need and therefore you only need to train your wanna organic and so open and stick and model is a model that is written maintained in trains all in one mile so deep learning and neural networks bring whole lot to bear in natural light which processing now the first at understanding r. n. n.'s is making this distinction between a sequence models like to mark off models from prior episodes
and on sequence model sings worry to snap your fingers and outcomes in output by the support vector machine or logistic regression class a fire you input something and out comes your classification or your aggression to the sequence models anderson on sequence models and we had that in our prior shallow n. o. p. episodes we had a hit mark off models mark got chains hands joints probability language models being sequence models and we had it
maximum entropy support back to machines and logistic regression being non sequence models were gonna carry this information over into world of deep learning and religious have two models a regular neural network just a neural my work and work for a neural network so horny use a regular neural network for everything that is not sequin space you snap your fingers the neural network happens and out comes out of our youth worker neural network for everything that is sequined space stay
step step step step we're we're we're we're we're in a sentence or even send send send some sense in a paragraph if our task is the sequence of times that's for you sir replied your network and by the way sequence models as we've been discussing another word for this is called heinz series forecasting to find series models because they're too kind steps were talking about language we're we're we're we're doesn't seem like times that's inside words that but they really are too high in steps and sequence mon
also for trying series modeling time series forecasting is useful natural image processing it's useful in whether production models and it's useful and stock market forecasting anything that has a sequence of times that's so return your own met works for c. quince paste and opie tasks and regular neural networks which we're also going to call the neural networks or artificial neural networks or mall tyler perception on its case you didn't see these words all the time referring to a regular vanilla bean
rome at work that's deep neural network for d. n. an artificial neural network work day and an mo filer curse at tron arcana l. p. and specifically by comparison to recurrence neural now works are going to be calling them in this episode feed forwarded neural networks feet forward meaning you input your input it feeds forward for you than you on through the thin layers and out through the out band's blown way we understand that i compare
son to new recruits neural networks is help or n. n.'s work so this is how on and work in our hands we take in our input a word we're gonna be taking in one input step at a time we're we're we're we're so we're on were won we take inward won as arkin put it enters the hidden layer something happens in that black box in that neural network in layer and out comes an outlet kill us they were translating from english to spanish so i say hello howry
doing it wants to translate to glaucoma stocks so step one is entering a hello and out calms olaf now step to we moved to the second english word how hello how how goes into the neural network it goes into this saying that neural network okay and there are not multiple neural networks for every time step is only one neural network that will be reduced for every scene
time step this time through the neural network it takes in his input is second were how hands the output of the prior hats whole lot sell it takes into the inputs second word and deed output of the prior pass the first output in other words to him later loops back on itself there is a circle arrow coming out of the thin layer and back into the
they had layer and the second outwit the second time step is cuomo very interesting so we have one neural network a regular old neural network it seems like but with one tiny little twist and that is that an additional can put that comes into the thin layer in addition to the regular input is the output of the prior pass it in your own i'll work with the circle luke luke being bitten we're back onto itself and it's
and spin your aunts or functions they can be signal way functions or can it each functions were rectified linear units etc these are actually computer functions either mathematical function to get executed on the computer says it's calling itself will recall that in computer science as for courage and it's function calling itself and itself in itself can itself until we get to the very end of the sentence and then we backtrack her way up the stack anger results as recur janelle swelling color work
currents neural network and that is by contrast to eight feet forward neural network fee for being the opposite glove with current now we get short third step hello how are you weren't hello how can our i. r. a. are ye we feed all are into the thin layer arkin put layer is our it is fed into our hidden layer can spin comes from the last step veit how we get out
put thoughts far okay so what said last time there were taking the output of the prior step in it as an input is a bit of an over simplification what we're really doing actually it is all language model task for member language models do this kind of probability thing sequence from left to right you like multiplying george probabilities or running through markets chain or something like that we're kind of doing it is to colleen our way through the output sequins and that sobbing
and it came to this pass as an input and what they can and do it's going to look at all are gay and if we were not using the prior inputs feeding them back into the thin layer they were just output sewn in seoul and the direct translation of all are from english to spanish the decimal we want we want the language to be complexes smart so be it carries with it sort of this meaning vector that it has been building up as it when along through the center
it's an it's sees all lock como and it knows that combined with are what we have thus far this to how leave after the we've built top of our translated seconds thus far combined with all our in english should not to give us a stone what we should maybe do probably stickley has learned in the thin layer of this neural my work is wait wait because what comes next is what's important we're going to see
skip this word to move on to the next word you we pass said in as inputs to begin where remember this is all the same neural network we used over and over at each step in calmness this vector just how weak it has been being passed we curse of lead into this thin layer incomes that tally vector combine it with you and how calm space stocks because the words we've translated thus far plus the crime we're we're looking at will give us that word and so we learn that
like cities and nuances of things like mata fire words and sarcasm and all these things were words combined in a special way that brings new meaning to the output information is carried through the rucker neural network through each time step by wind out putting words into looping that back into the thin layer for the next time stand the way this for colonel no work works makes it all language
model language model from a prior episode so an r. n. n. is the language model shall we haven't been put sequence of the words he eats these words of one word of the time and spits out some stuff one word of time including if necessary skipping the word because that's going to combine with the next word and some special way as we saw with or you becomes s. stocks know was the sound like this sounds a lot like a hidden markoff model from a prior episode indeed and such
ollie anything they're very similar i think of this formulation of record neural network as like the deep learning equivalent of a hen markoff model now of course the record neural network because it is a neural network with a thin layer of new lawns or in lady or is it will be substantially more complex and new wants and powerful than a hidden markoff model you'll learn that mort intricacies benefit marked off model could learn
out here is what's really cool about our current neural network this architecture that i just described to you can replace every single tax cut we described in the prior episodes we didn't put the sequence of work work work work work work and for every word weekend i'll put a part of speech tag bird noun pronoun adverb right part speech tagging that was one task where we could use to mark off models or we could use maximum entropy models were
report back to machines well we can use the ricker neural network for won the r. n. n. will learn attributes of the word that make it some part of speech intricately in the depths of its black box within it's thin layers but who he will also be considering sort of it's location semantic lead in the sentence that thus far by way of that luke structure it sees where it's sadness sentence as has been how lead from
left to right bringing us to this point and it considers that hasn't been put that is a feature a factor hoping determine what part of speech this word is so the more complex new weinstein shirk its version of parts speech tagging one model the rucker neural network to be used instead of any number of models we might have using prior episodes such as the head market model maximum entropy support actor machine etc are speech taking names and city where kidney
should we input or sequence of words and al calm steady number of names and cities okay so many outputs will be skipped and certain outputs will calm directly with its name to entity we say steve jobs invented apple it's acting little bit like a kid markoff model we're going steve jobs for collecting those two together how pudding as the second outwit her son steve jobs k. we go to the third input invented and a determines that that word by way of the war itself
in combination with where we are in a sense thus far by way of that loopy structure is not a name to entity and so we skip that the fourth input apple combined with a hidden states outputs organisation apple so the news of recur neural network to spread out named entities as well as parts of speech in this way we're using an r. n. n. like a good market model were working the words and putting out little things that we walk along the way being the inputs one word time and coping outlets any number of words
time we can also use or n. n.'s for sentiment analysis classification machine translation and every other tasks that we've artie talked about now in order to use our n. n.'s for those more complex tasks we're gonna need to re imagine r. n. n. a little bit different way and i'm going to present a version of record yeoman work hold the sequence to sequence model or alternatively and and coder de klerk model and again
yes take alone or in an instead of each word and putting out an outlet for every time step what we're going to do is scan put all of the input first from left to right gather art how we we curse of leak through the thin layer and then we will stop we've read our entire seconds from left to right hello how are you period that we stop when output anything yet we're building sort of a vector to how we being me
in full vector representation open to this point it's like r. r. n. n. is listening to us speak you say hello bob bob bob bob would succumb in his nodding it's had on okay hon ago on our hands at the very end when you stop talking it sinks a little bit and a kind of re for military thing you said in its head it builds up this meaning representation of everything you said and now they can respond to that first step was cold and coding
you and coated the sentence and now the orion and we'll see cody what it seems to be a proper response to listens to you talk to aha okay okay when you stop is like you know okay whoa was or what we were saying that there was so did depots response based on the encoding it has from what you said that the two steps are in an ankle or de klerk been another word for this is sequence to sequence you gave it a sequence and it
i'll put the sequence sega sequence and thirty color and we we use this secrecy sequence model for more complex r. n. n. n. l. p. tasks so the case of sentiment analysis or classification what the r. n. n. will do is read all the word love to write first ah okay okay okay it and what's your die and it's been like you sounded mad right that sentimental assist it will give you a predicted the sentiments once it has heard the entire sentence or a predicted class of fitch fitch
a class we're talking about sports or technology or news stuff like that classification so you can code your sequence and then you are or n. n. can outwit another sequence now you see in this case that sequence is one word it's all one item sequence we can imagine like in programming you can have a single element ray break open bracket and one item an enclosed bracket that's basically what we're doing here is creating an air raid the sequence it just so happens to be a waugh
the items sequence and then finally in the case of machine translation i posed to the problem as being a vanilla or n. n. translating the sentence word for word as we go along that's not how we do machine translation neural machine translation using r. n. n. architectures are actually sequence to sequence models and that makes more sense anyway it's a little bit more difficult to try to translate something as you're going along from left to right in
real-time then it is to listen to what the person said first think about it and and translated to spanish so the more powerful mural machine translation models use this sequence to sequence and coder de klerk architecture now so let me give you an analogy i use for understanding sequences sequins r. n. n.'s in the case of single item sequence to sequence r. n. n.'s like we saw with classification and sentiment analysis you'll need an analogy were
i'm betting the sentence as we go from left to right and the result is a single factor which is basically your class is not much to that but if we are in coding our source sentence in english okay and we want to translate it to spanish an indian code process we created a vector which is sort of this running tally of the sentence is sort of meaning vector summing up the entire sentence how could we possibly go from that
to be reconstructed sentence in spanish elected think of it like this if you've ever seen the movie boondocks saints this is big fight sequence deceased to irish guys in their american friends they're in their apartment our member exactly what happens some big flood comes out then he changed one of the brothers to a toilet in the bathroom and he brings the other brother down to the alley with a gun against his head and the first brother on the second floor of the new climate change the toilet breaks the toilet out of the grounds and throws
out the window open it lands on the floods head and the first brother jumps out the window and lands on some other five there's also to shooting in things in knocking over and people get shot is blood being splatter on walls memory runs away so there we had the sequence of actions think of that like our words word we're we're we're we're hello how are you this is our sequence of actions that comes in chains brother wanted toilet take brother to downstairs coins donegan said brother one throws toilet out of window and softens step step step steps
and by the end of this whole scene everybody runs away flees the scene there is sort of an aftermath left behind there's toilet shards and bullet holes in walls blood snatchers on the floor maybe i'm gonna left behind in a shoe over here so there's a whole crime scene left behind that is what we have been coded from our sentence the sequence of steps has occurred in this sort of crime scene has been
being built up it's like the shadow of what actually happens and by the time this whole row down is don a crime scene is left behind that is our encoded sentence now in the movie this climate investigator gumshoe guy shows up at the scene and you know he observes the whole scene all what's he's looking at floor he sees a shoe when i'm gonna some bloods matters he looks over to the right he sees all holes no wall can heal
so that the apartment and see the shattered window and so he's able to reconstruct the meaning of the sentence based on wall was left behind me and colby so right now he is the coding the and coding and in the movie i don't remember exactly how this on fold but you can imagine he walks over to some broken pair of handcuffs on the ground and he kneel down and picks up and turns it over in his hands and based on his understanding of the whole crime scene
i'm buying with this step one he has made it his first word in the end coating process will lock step one wise thought chained brother wanted toilet he looked to the whole crime scene see some toilet shards except the handcuffs and constructs step wind cbc he doesn't have access to the actual events that unfolded he only has access to the crime scene saffron colored de klerk are then does it builds up a crime scene messy
coating process and then from there the key code or picks up and says i got it from here on the translate gets higher in the sense hello how are you into an entire spanish sends all local what's that's excellent so now you have a basic understanding of organic entity or loopy neural networks now how do we get these words into our organ and remember that machine learning doesn't work with tax it works with numbers machine learning is math it is lynn
around for brought statistics and calculus so we need to turn our words into numbers or vectors how do we do this in the prior episodes one representing documents in the database we represented those documents as a bag of words so each document is a role and it has a hundred and seventy thousand columns as the number of english words in the entire english dictionary and there is a one in the location for the word if that word is present in this day
hakim and so eight hakim it is just the vector mostly zeroes and won yes that word is present we call this the sparse vector because there is a spore city of winds and amy jordi of zeroes now the equivalent representation for words not documents but words individual words themselves is that the word would be a vector any would have a wine in that column on that word itself okay so what would be all zeroes
a hundred and sixty nine thousand nine hundred nine nine zeros all heroes except for one which is in the location of that words call on is a sports vector as well because it is mostly zeroes and very few wines but specifically only wanda wanda and so we call this a one hot vector so sparse vectors mostly zeros someone's a one hot vector is a sports factor that is only one that won the one hot
actor so if we had this sentence hello how are you emma's pretend that those are the only words in our entire dictionary there would be for columns hello how are you and if we're looking at the word hello than the first call 'em would be won and the next three columns of the zero in our word hello that's how we would represent our word as a one hot vector now this representation of word does not carry it any significance any meaning any semantic importance is almost
just an arbitrary representation of all work but you might as moses have a u. u. whitey some serial number for every word the way that we represented as one hot vector gives it no significant that's by contrast two bags of words which have been them a little bit of significance when we represented document as a. t. f. i. d. f. bag of words we can perform at the search query by finding the documents which have the smallest
co signed similarity to or query okay so there's obviously some sort of semantic meaning in the document and our search query if we can connect them by way of a co signed similarity but there's nothing like that but we could do with the way we're thinking about words here they're just random so this actually will not hopeless in our pour in and an analogy here is in our crime scene investigation if we represented events as one hot
there's then the gumshoe shows up at the scene of the crime and on the floor there's no wind and yield down to accept the number one he looks over to the right and on the dumpsters at twenty four calf and he goes over and he picks up and turned it over his hand and he looks up at the window and hovering in the broken window is a sixty nine c. n. he's looking at ali's objects like what the hell they all need anything to him he needs objects which curie meany he needed beyond that of bloody shoe and broken the law
as he needs objects in the crime scene that means something semantic lee if he has any hope of reconstructing the crime scene in his mind so we need to do is represent words in the dictionary it's such that they carry semantic meaning it within that their representation we call this word and betting its we and better word if we can put it in vector space in a significant way so we're not talk about the word it to vet model
which is a neural network for creating word and eddings is the main mall that we use for doing such a task in an l. p. let's build us a little bit first all were in betting is the war are represented in fact your space so imagine all your words as stars in a galaxy now with our prior representation of words as one hot vectors those are just be random replaced but if the words were imbedded in order that they carried semantic meaning they
stuck on their location in space than what you would have is that all nearby words to one word would basically be synonym it's so if you found the good quote unquote docked in vector states the word good then all of the very close by dots you clean the intestines remember you clean distances physical distance all of the ubiquity in close dots to the word good would be things like excellent and best and
perfect and wonderful and then you might imagine words that are canton ams such as bad and horrible and worse and terrible would be very far away from this cloud of dots so they would you clean far that's what we're in betting is placing the word in vector space they carry significance semantic significance know what we're going to arrive that by using the word to back model to achieve this goal is something very stash
will not only will you find that synonym sparked physically close to each other you clean clothes but you'll find that certain projections kiri significance so the arrow the vector pouring from good too bad in vector space would be the same kind of arrow warning from baths two worst and wonderful too horrible and so on and classic example they use for the system is that if you been quite keen plus queen
minus man it will give you woman you will have performs vector math using when you're algebra you'll get back and arrow pointing to a dots and that guy isn't i'm betting representing the word woman seek actually do word matthews inward and addicts and so remember now you clean similarity is similarity of docks to each other physically so cinemas will have high ubiquity and similarity and co sign similarity is angle
similarity so the difference between good and bad you use that same sort of co sign metric on the other words in order to perform word math this whole concept is really cool philip is online you'll actually see the two d. or three d. representation of words always docks around each other under pseudonyms of each other and you can do projections to get the capitals of various states or countries by way of analogy using co sign that chicks from the capitals of the other states and country is in all this
but now an embedded word has left a five hundred and twelve dimensions number of callens so visualize a knees words in space is impossible because that's five twelve the so we can project that down to two deep or three d. by way of something called feed sneed see a hyphen s. and a. b. which stands for it he distributed stochastic neighbor and betting that is essentially the same thing
the principal component analysis it's boiling are large dimensional vectors down two small dimensional vectors so basically think of t. speed and p. c. a. has essentially the same thing they usually you're gonna be using t. snead for visualization purposes so these word of the actors have some number of columns that's the end bedding a dimension some number of callens usually let's say five hundred and twelve basically the number of dimensions is indeed the
mount all moved generalization you wanna boil this down to write smaller dimensional word vectors smaller dimensional i'm betting its means more general bowl less accurate word representations and higher dimension all word i'm betting is means less general but more accurate that is over faded were in that it's okay so our goal is to put words as docks in space with significance we call this an bedding
the word any type of machine morning model we will use to perform that task is called the vector space model v. s. am a model would put something in vector space and the way we're going to do this with words is by learning their contacts that's how urban will learn the semantic and bedding of work is it's context so certain words show watkins certain contexts
over and over and over so when i say this is the soda drink or the busy proctor ink or i like to drink soda and i like to drink hop coca cola is a good soda coca cola is a good pop you have learned by contacts by the context of the second step that word is presents that those two words are very similar to each other they may be complete synonym score they may just be very similar words to each other so that's how we war
earn took place words invective spaces by learning which words share the same context and where will get these contexts is from whenever pore pore always have available relief just download all of wikipedia and just go through every word word by word by word by word by word look it's contacts with the next words contact with the next words context and will learn what context every word it's easy ian so we're learning word i'm betting is based on their hind
tax there's two different ways we can do this one approaches called predictive methods and affording use with war two vet and will order to do there is we're going to try to predict the context given the word or predicts the word given the context and another one is called count based methods and these don't do any sort of machine learning these urges mathilde predict anything when you do is you lay all words out and columns in all wars al in rows c. have the whole english dictionary a hundred and seventy thousand by
a hundred and seventy thousand gay and you count all the words corcoran says with each other and put that in those two words crossed cell and then you do some way your elder brother paul while some coal current stuff and then you have your embedded context matrix we're not in use that approach i will talk little bit about that but first let's talk about the predictive methods mainly in our case new role probably stick language models like i said the goal is to predicts the context of the word
or to predicts the word given the context so we're trying to learn more in bedding space on their contacts and we're gonna use a model cold war two vet word to back it is a neural network which will earn his skin bedding matrix based on word contexts so was going to do is make a context prediction using a neural network feed forward pass and learn from its mistakes using something called the noise contrast it this
a nation lofts function remember every machine learning moll has lost function and then use a great set by way of that propagation back through the network to fix it there are so we're to that is a neural network is learning that the word and bedding this for every word in english dictionary by predicting it's context and optimize thing it's parameters if it made an error as two types of context production methods you can either take the word you're looking at
in a jungle words the cassell matt you can take the word you're looking at and try to predict the surrounding words so we're looking at cats in the sand since the castleman matt in this case what's called this get grandma call we're trying to protect the surrounding words blank blank blank blank that blank which i predict all those words the opposite of that is trying to predict the word given the context so if we park at the word cat in our window we will look
at the blanket sat on the mat and try to predict that that word is kennedy cat this is called the continuous bag of words approach for c. bal you can use either of these approaches when you're trying to protect your contacts year skip grandma foresee bow again that is context from word versus word from contacts you don't have to get hung up in the details is pros and cons versus dataset size most people you skip graham cell
as far as you're concerned skip gramm is the only thing that you care about so our word to that model is a feed forward neural network is trying to predicts context words sir rounding the current we're we're looking at that is the state gramm approach we're going to go through wikipedia one word it's time we look at one word which record it was our round it we obviously made some error and so we use this noise contrasted estimation watts function
in order to back to propagate or gear or through the neural network fixer parameters move on were to that neural network to try to predict this stick grandma context around the new word we're looking at we make some error that propagate allies through our neural network adjust our parameters next word we do this over and over and over and over and over and so we have built up the table the word and deadbeats what we have in the end is called an m. bedding and matrix
every word is a role every call on it is somewhat nebulous and getting it to mention it doesn't really matter what matters is in the end of all of your words are stars in a galaxy whose position in space holds significance okay so that's the word toback model now like i said that approach is called a predictive that vector space model meaning we're trying to predict the context situation that by contrast to hate
and based vector space model or distribution old semantics model and the way this works is we lay out all the words in english dictionary rose by columns and we count in every cell the number of times they need to words roll by call on coal occur with each other in a context window then we have a matrix of coal occurrence keiko occurrence matrix and we will use a bon jovi linear algebra on that matrix
things like principal component analysis or latent semantic analysis or singular value decomposition you'll see these used in machine in la p. c. a l. s. a. s in the deep in order to haul out the i'm betting still rack to leave from the math from that co occurrence matrix and we call this small the globe model g l o v with a capital v. the global factor
or representational words and this has various pros and cons versus war two vet usually pros and cons with all these situations boiled down to amount of training didn't you have come out of lamb speed and other things you may see glove used were discussed in the wild but as far as you're concerned reconstitute a state of them bedding words in vector space the only thing you care about his word to vet ma
wall which is in your own iwerks using this get gramm approach will that was a lot of information just you and that our words that was eighty feet forward neural network on those snap your fingers now come the thing we can use if the forward network for other work related tasks like a part of speech tagging and names and city recognition but you'll find a think that record neural networks are preferred for those tasks because whenever we're looking at
they were were trying to determine apart speech remained entity and we have the prior context of the sentence thus far which actually helps determine the part speech tag or names and sexy and soul really the main place you see the standard feet forward neural network in an l. p. is the inward to back isn't coming out with bark and bedding matrix of words everything else tends to be organic and the mighty work
i'm yeoman work itself let's step all way back now we have gone through all of wikipedia and placed all forwards in vector space these are now imbedded words rather than one hot words so now they carry semantic meany now we can use those to take into or or n. n. n. coder so that our de klerk has something no work with so this scene of the crime were the gum shoes standing by giant bright flash of light happens he
resides in staggers backwards when the white wears off he looks and sure enough there's a gun on the floor and a bloody shoe and some bullet holes no wall as some broke in the toilet shards on the grounds the inputs carried meaning into the encode or or unmanned and so what is handed off to the d. coder step also carries meaning or n. n.'s anne words to that now that's the end of this episode but that's not the end of or n. n.'s we're gonna talk about
traditional neurons and they're used the n. r. n. n.'s namely rectified linear units we lose as well as what's now state of the art cells not neons in an oar in and cold l. s. t. m.'s cells were long and short term memory cells by contrast something called g. r. e. u.'s it's gave recurring units and a problem that these things so all which is the vanishing or exploding greedy and pro
alarm when you back propagate your training error in recurring mural networks soul talk in the next episode about little bit more of the nitty gritty architecture technicalities of or n. n.'s as used in an l. p. this episode gave you a very basic label lands and the fact that orangutans basically replaced everything in an l. p. to bring state the arts deep learning to the space now for learning
people earning an l. p. first i'm going to recommend a handful of articles for you to read a very popular one which is called unreasonable effectiveness of organic and still very easy read very visual is more conceptual all on this episode and then another couple particles of her little bit more technical from there your next step is just to learn people earning deep and help he and oar in and start a section of love deep warning
rock or so whether it's deep learning book got bored or facts not ai the traditional deep learning resources that i've been recommending r. n. n.'s will be inside of those main resources as one or two or three chapters so just continue along you are basic deep learning learning curriculum in order to learn the details for deep and l. p. and finally there's a great stanford itunes you course c. s to twenty four and we
his deep and l. p. by christopher manning chris manning it if you'll recall is the co author of the main and shallow learning and l. p.'s book that i've recommended are episodes as well as the u. tube series on shallow learning him and dan sharansky but then you to play list together this is basically part two of them you to play west this is the deep learning equivalent of that u. tube series and is a brand-new it's i think it's two thousand seven
team and it's not as two thousand and sixteen it actually replaces a prior stanford indeed an l. p. course called c. s to twenty four b. which is taught by richard soldier so if you've seen that one floating around c. s t twenty forty stanford deep an old he needed curious skip that wand and do this one instead because it is a emerged and updated version of that class with chris manning's material c. s to twenty four and
and in the past i have often recommended converting video series top audio and listening to them while you exercise this series of is highly highly visual i tried doing it audio pan ayatollahs the dust so i highly recommend you watch this one put on your bike padden prop up on the treadmill wall you ryan and watch the video is very good course highly recommended and as for my last episode this contest series will be kept alive by tony
shuns soviet any amount you can donate weathers one dollar or five dollars go to the website go see develop a compost left contests for slash machine learning in click on that atrium lake if you can't donate the show do me a huge favor and rate despite cast on itunes which will help the spot have succeeded that's a wrap for this episode see you next time indeed an l. p. part two


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash and l. g. did climb also starting a new contacts which can use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twenty three deep dopey part two in this episode i will finally wrap up on end
opie is the second episode of deeper than opie or n. n.'s marina cover a few fine points about are immense will talk about bi directional or a man's vanishing an exploding gradient problem of back propagation and it's solution through l. s. t. m.'s orgy your use let's start with the review of organic hands because i think that last episode with a little bit fast and so on make sure that you understand are enhanced completely before we move on so
and people earning we use neural networks and there are various flavors of neural networks for different tasks the vanilla neural network also call 'em all tyler perception on or a fee for network or usually just the neural network is used for sort of general tasks general classification or regression problems so if you wanted to use the neural network for predicting the cost of the house and some market you'd probably use a vanilla fee for neural now work or classify
it's something is this saturday other thing you'd use the neural network then there's the convolution all neural network t. c. n. n. or consonants which are traditionally used for the image related tasks categorizing images for example as cat dog virtually and then we have sequins base tasks kind steps steps of the overtime mit things like whether production trying to figure what the weather's maybe tomorrow based on what the weather has banned the last three hundred sixty five
days or stock market production or as we've been talking about natural language processing or sends this is sequence of words through time the premier time series deep learning our rhythm is the replied neural network and so we use our n. n.'s for deep and l. p. now real quick beg you wanna make a mention of reinforcement learning which is also something you might use force sequence based machine learning problems we have
or smut learning recall is the third category of machine learning models we have supervise learning which is the majority of things we've talked about throughout the series you train them all to do something or recognize something train the model to pick up some pattern you get a trading day which comes with labels in that war is the patterns to generate labels on its own unsupervised learning is learning at patterns in the dado with our label so for example clustering data points in
space as simply things are near each other based on their features now be unsupervised learning and finally we have reinforcement learning which is sort of the entry point into a night proper reinforcement learning is the task of learning what actions to take over time in order to maximize something some reward function we call it so as you can see reinforcement learning is also applied to time series how
asks every car neural networks for deep learning oriented times series machine learning we also potentially have reinforcement learning and you can use deep learning in reinforcement learning a thing called the cue networks so before i go on or discuss the differences between these two camps or n. n.'s or g. q. networks supervise learning or reinforcement learning which do you take when the thing that you would use reinforcement learn
for is when you need tool error in what actions to take over time in order to maximize our goal so if we think about natural language processing there doesn't seem to be some sort of analogy for that in this case looking at sequences of words how might you try to let the algorithm train itself may be what words to generate next or what actions to take that generates awarded nor to maximize some weak value based gold it doesn't seem to be anything it
makes sense in this case whereas with supervise learning your training the r. n. and explicitly on sequences of words that if you try it on hello how are blank it knows to generate you as a max were in sequence it knows that because you fed it gobs of texts degas from various court for like wikipedia or on my conversations and it has seen the word you calm after the sequence hello how are all over and over and over and over it
labels data therefore supervised and if it's very well in an l. p. reinforcement learning would be a good fit for video game again i'd botsford sample they're trying to kill the good guy that's their goal and have some sort of measure of how well they're doing which is gonna be maybe your hit points bar and have some actions that they could take it any point in the game thus far not the sequence of steps sequence of states namely up until this point all budget stuff has happened steps that
step step step the good guys down various things in various other characters in the game have been various things so is the next best action to take given your current circumstances is the actions themselves that the reinforcement learning how grim news to learn how to perform the maxim is asian of killing the good guy so those are two extremes and l. p. n. b. okay mckay on it that make orangutans verses de que networks for time series tap
it's kind of an obvious what metal glass harp about something that i think is maybe a little bit less obvious stock market productions so this i think is a great case where that line is a little bit fuzzy between whether you use supervise learning in or an aunt or reinforcement learning in a deep que network if we were using aren and what we would do is sort of try to learn to shape of a stock graf unless it were trying to build a trading bloc we want to learn
whether to buy or sell stocks in order to maximize our profits over time they have a graphic goes up and down up and down the whittle up man at the peak of the hill before you start going back down you'll want to sell stocks generally because they're going to lose valley it's your cash out your maximizing we have right now and don't lose the value at the bottom of the next frocks as the graft begins to creep upward you want to buy stocks now there
she and our production is gonna be that if the goal of them out so we're going to gain in the value of our holdings through return on investment so we might you was in or an end to build a model of the stock graf specifically what would be doing is we're aggression predicting the next steps value on a graph in numerical value on a graph spelling bee regression based our end and that we would train at one years and years of historical
stocking and and having played with the stuff myself he can actually get some pretty decent representations some pretty accurate models but then from there except you to decide what to do your behalf to build into your code a trading strategy you're gonna have to cody in the system that's what it looks like we're going downwards based on a major down take in the next regression in france and yarn and then you might sell your stocks
it looks like the next predicted value is very high and we are to furlough than you might buy stocks so you're building in the action you not believe into the machine nine model you're writing isn't python code calling some eighty odd for example the model simply spinning out a value the regression another thing you could do with an orion and it is classification you can actually tag dots on the graph points where you would classify this point has cell
live or holds so you'd have to have some previous know how about the trading and stock graphs and made east dillon says the more you can click on a graph and mark eight points as a point where you would sell buyer hold those in the u. or three classes and now the machine learning our room the recurring neural network will learn to classify points on a graph has one of those three classifications buy sell or hold so the prior case of regression based ariana and our law
asked me you're on the output layered would be a linear units like linear regression the case of classification our last neuron would be a soft max units which is a malt like class logistic regression unit so you'd use logistic regression for by mayor classification and soft max is malt psych class would just occurred russian so that's how we might use an orion and to work with stock market time series day that in order to protect whether we should bio
whether we should sell or just the numerical next step and an arc hold behind the scenes would actually act on that information inward to make the decision to buy or sell in reinforcement learning on the other hand the model actually learn on its own whether to buy or sell you give it the goal of maximizing profits so all you do see give a goal you say make a lot money and don't lose a lot money so the end goal is a high dollar value you give us a
actions it can take those are by solemn hold and then uses let loose what the algorithm worms specifically is what actions to take when you teach at the trading strategy that you buy when you're alone you sell one your high or anything along those lines using supervise learning methods know you let loose with the gold and you'll learn sinus cold weather to buy sell or hold given where is on a graph so that's the difference between
supervised time series stuff and reinforcement learning one learns numbers are classes and the other lawrence actions and indeed for an elk he'd supervise time series models is the ticket not reinforced when at least not i know of and specifically r. n. n.'s are the deep learning model for time series so neural networks for general staff c. n. n.'s for teenage stuff or n. n.'s for c.
it's stuff and let's go over what our amanda's row quick we have two types of our hands we have the vanilla r. n. n. which takes in an input and spits out an outlet for every time step mount the trick of this what differentiates it from a regular neural network five neural networks left to right is that the thin layer loops back on itself is worker says it sold the neural network so let's say we have fi
i've time stamps time step one we see it and put an end up with an outlet time step to we see it in input and it takes in the additionally the output of last time step out put him out what time step three we see in and put it takes in the running tally all the past to find steps thus far and outputs in output so we go left to right conceptually the best i actually would happen to actually
happens is one you're all male work the loops back on itself the thin layer luke's it's out the back into it's been put at the regular old or an end things you'd use this for for example are part of speech tagging and mean density recognition part speech teddy you didn't put word in outcomes i tag labored adverb now an adjective etc input word outcomes attack input word outcomes attack now accordingly you could use a raid
your neural network for this incomes awarded out comes a tag but we do want to cure east room of the system the context of the sentence thus far we wanna sort of build on meaning of the sentence we've been reading from left to right and didn't put that in with the word that we're looking at right now because the contacts will the fact that tag were appointed for so that's the magic of an arn man who takes in the prior steps to aid in the classification above the car
it's that al puts a part of speech tag so that is a one to one mapping every word gets a car speech had input output input output you may have a date many too few mapping like named entity recognition the sentence steve jobs invented apple would be named density person named eighty percent nothing named as the organization so how do we account for that nothing that blank space in a traditional r. n. n. we're just now
put some point symbol some like the letter poll is common or zero or an all or something like that so you didn't put steven out cons mayne n. c person be a good jobs and it would combine that with the last outlet and output name that's the person you didn't quit invented in you you combine that with the fact that there were two person that name and these are this annual output blank these are sort of simplistic or any hands i call them the no law or an aunt
it's one twist to this architecture they didn't discuss in the last episode is that we have been reading that movie sentences from laughs for rights to inform the decision at any time step that's well and good but sometimes stuff that comes after is also informative we can accommodate for that's by way of the structure called a bi directional or an ann what it will do is it will read the input sequence from
left to right and that will come in as an input being in that the decision being made at that step along with the actual input asked that additional eight eight thirty and what will be coming in from the right so the sense will also be read write still left so the sentence is being read left right and right to left like arrows coming from the left and right in a meeting in the middle at the current clients that an arrow coming from the ball
and that's our actual input word we're looking at the current time step those three inputs all combined together to come up with an outlet so an example with this might be useful with sand saying the phrase george's my friends or so white fox will of thing that came after friends the sequence of words that came after friend completely negated everything i'd said thus far so reading from left to right is well and good up into the pointed friends were building sort of omni
main factor by it turns out what comes after completely reverses the meaning of the sentence thus far and i would've known that unless i'd seen the park after warren matt so as the bi directional r. n. n. does is it considers what came before what comes after so regular are then goes left rights in time and common use cases for that would be part of speech hanging in the intensity recognition a bi directional r. n.
and takes input also from the right going left so whenever times that the rats will meet you in the mail from the right and from the left in order to help with your output and you might also use a bi directional r. n. n. for parts speech tagging or name density recognition it may increase the sophistication of the model you would try of the mill are then and then you also tried a bi directional are nan and you look at your evaluation metrics and see which one performs better and if the bike rack
lauren and performs lot better with not a whole lot of after computer crime then you'd use that instead you can't always use a bi directional r. n. n. it's not objectively a better solution for example sometimes you don't have the future date like if you're trying to protect stock market values you exist in the present you have all the day of the past and you have nothing of the future so you can't use a bi directional r. n. n. similarly with weather pattern productions that all
the weather for prior days but you don't have the future to you don't use a bi directional aren and so there were two flavors of our n. n.'s right there when is the no-one and sort of maps inputs that outputs directly and another is bi directional as same thing at matt simplistic outputs directly however it also considers future times that information these two by comparison to the third type of our n. n. which is calling sequence to sequence model organ and coder de klerk
model and what this does is you we'd get your sequence of steps you do all of your processing with your aren and we'll help put anything york output anything for each time step you just listen says listen and listen and listen and listen and when you're down listening when the sequence of steps is beyond we've heard the entire sense honor and then you stop anything and then you respond to you have to hear everything first sobel listening phase is cold and coating
your encoding your sentence and the outputs at each step are completely ignored them they're useless as as outputs in and of themselves but remember n. r. n. n. also feeds the output at every time stepped into the next step is an input so has is running tally it's building from left to right this is your contacts of actor or your meaning vector this is your encoding and that's what's important surf runway ali out with every time stepped in the end coating process
until the very end and you were handed a pen and coating package all old vector all that is headed to put your hands can you look at a tremor or your hands and then from this and coating you will recur schrock to a deep coating you'll come up with a response if you're chap by so you'll answer the question or you'll response to what the person said if you're chap what is your translation system then you'll reconstruct h. frenzel
mission of the sentence that was honored in the language that you translating to say you heard the sense in english and coated now listening coding in vector space that said doc somewhere in space a star in the galaxy and you can take that and it has meaning to it it has actual semantic meaning package within it you can reconstruct the meaning in spanish word for word that's what's called a sequence to sequence model u. n. code your sequence
and then you d. code that into a new sequins and and coder de klerk the sequence to sequence so this is for any task were you need to hear everything for steve you can't just translate as you go along the way like to think this is what a standard arana and you're writing and can see you can't go back in bed it's prior output stiff new information gave new meaning if something kind of messed up along the way you philip wanna go back and makes
and changes we can do that with a no on and so in banco de color is useful for situations we you need to hear everything first before you want to start writing because you're riding with pat machine translation is a good example because you're so much it's packed into a meaning back during cured and coating that if you were translating word for word is the one along you might lose some subtleties in my analogy that i use for this is let's say you're a crime scene investigator and gumshoe anna
sequence of steps happened in the unfolding of a crime that was the end coating process and i got shot and i lost issue and summing up question we ran away secrets of steps unfolded in a crime scene investigator arrives at the scene of the crimes which is york and coating the scene of the crimes are encoding you'll have access to the sequence of steps you don't actually have access to the time based crime you only have access to the aftermath what's left behind at sea and coating that the gumshoe
smart enough to know how to weaken scraps the crime from scratch cool looking just at the crime scene a little crime scene is scratches his chin and walks over and picks up a guy and so he has reconstructed step one of the deed coding sequence which is a man had gunshot and he walk so we're picks up a shoe and reconstruct step two of the deed coding sequence and also some real lost issue and so on now the sequence it yew jen ray
in the de kooning process doesn't have to be multiple i am sick appeal one step sequence so we might use this for sentiment and l. s s or classification for example you could use and then clergy color or n. n. model for either of those classification four cents med analysis where are you would want to hear everything that said first before jumping to conclusions and once you've heard everything once uganda full encoding now you can de code what is
arson is mad sad happy nervous scared etc that sends an analysis but soleil one i am sequence so imagine an earl ray brackets with one eye and it and that identity is sad its classification example again this is a perfect use case for those freed scenarios that laid out machine translation sentimental says and classification this would be a forfeit for something like stock market production or whether production your way
here everything first you wanna sort of be collecting data and making real-time estimates so there you have three types of orangutans regular old r. n. n. a bi directional r. n. n. and n. n. coder de klerk slash secrets to sequence model and remember the last piece of the prior episode dealt with her in words and some numbers because we're taxed and any machine learning model requires no
mercer vectors of numbers to work with to do with math isn't she in earnings math is just when you're outer boroughs statistics in calculus can work with texts to that see you use a model called words to mac which is gonna or regular old neural network which will convert your words into vectors by relocating words in vector space close to their counterparts context jewelry so words that show up commonly in the same types of context story
located physically knew each other and so let's all of the war to that model does it transforms your words into numbers so we can take them into the r. n. n. okay so here we are that's where we left off that i apologize that that was all just a bunch review by you kind of want to solidify it because i would really fast in the last episode and as some important steps now read talk about the problem of an oar in and it turns out i've been fooling you look into thinking that are an answer you
so wildly as is there not i think our n. n.'s are very rarely you used in their current form instead of an r. n. n. in its current form was commonly used is what's called an l. s. t. m. or g. r. u. style r. n. n. in order to build that out let's talk about the problem with an r. n. n. the prom is in screening model remember from prior episodes every machine learning model has an error function differently or five
and for different models in fact a model might have a different era function given the task so on are a man may have a different era function given the different application you're blind to whether it's named any recognition or machine translation etc so we're not begin air functions but we're function does is tell you how bad your model was doing and then you'll use training to fix that there are one separate time train train train train in slowly over time that there are found
shen starts to reduce in reducing reduced your optimize in your ear function optimize in your error reducing your error and improve your models accuracy so some error function measures how well you're doing and then you use gradient the senate's gradient descents wished how's your model which direction to move it gets parameters to every mile has incited that these parameters usually they're called fader parameters sometimes are called weights were doubly you their numbers they're cold
fish until multiplied by the input row the state foreigners are moving in space or so left or down not show through greed the sandwich just calculus says calculus tell your model which direction how far to move its fate of parameters in order to increase its accuracy namely to reduce its error by way of their function greatest sent out many models use grades and linear regression logistic regression support back to ms
jeans and yes neural now works when neural network uses great descent it's called something different called back propagation that propagation is applying breed descends to all the new alliance and in passing that mirror signal down through the ranks from right to left from that output which got some error signals using their functions as by some way off from ever remember analogy for neural network being at that company or chart starts with its employees as basic
the deed in calais air moves on to supervisors that in layer then move on from there to the boss the head honcho the company in that sea out where so we took in some input got passed through the ranks the feet forward part of the neural network lens and from the boss and the boss man looks at the results as a piece of paper his left hand he's comparing it to a piece of paper his right hand he's shifting his cigar from last train is now looking at what was predicted in his left hand with the actual value is is right here
then he shouts back through the ranks cries when i stopped by ten mil supervisors get to scrambling fixing various values in their books nerd crossing of some numbers in doing some math and the same time they turn around you shall back to their employees guys boss man says we're off by ten and a key correcting some numbers no books an employee star correcting some members in their books says gradient descent past hair piggly back through the ranks of the neural network that that propagation now a worker in neural net
work remember is sequin space the inlay or loops back on itself for every time step in our sequins whether it's a sentence of words four times that's in the stock market we're making a prediction and then we're feeding the prior production back into the next time step where we take in an input and the prior outlet for cardio my work is one neural network by looks back on itself how would that propagation worked in this case what we would do is for
did sent all the neurons send that your signal back through the neural networks just regular old neural network remember so you work the same but that is really only in the last step of the rucker neural network sequence that would be that propagating only the last step so what we do we back propagate again we just feed the air signal on through again so the forward pass of rucker neural network doing its production and in the back
forecast of rucker know my work is back propagating that they are multiple times once for every time step it's called back the propagation through time to go forward to make a production and you back propagate your errors through the neural network one storeroom time for every time step now why would that look like he would happen or charred boss and his supervisors an errand boy he's the boss would look at what's on paper and look what's on file and yelled back at the supervisors error
errors he's written a paper and a supermodel hear that and so they all start changing some numbers no books no real back to the employees error error error and now it's been next to pass back of the r. n. n. the boss gets the second to last production he made compares it to what's on file error error that supervisors all here then there'd still kind of writing in their books may look at him naked guy grimace nurse furnace weapon is for changes in a reasonable troy fascinating yell back to the employees error in the employees would pull never on mr redding
boss gets the third to last production he made new worker know my work a deal that error error in the supervisors are just pulling their hair out that practically in tears they're trying to modify numbers in the books but things are coming so fast that they just can't keep up with the biggest scribbling winds everywhere in the lead back to the employees are our in the employees just quit they say we're i'm john you can back propagate or error too many times too far back because remember when i said this is all one company one
organisation one you're all male work it just so happens to look back on itself over and over and over you can back propagate your hair too many times what happens is either called the exploding gradient where the error signal becomes tool allred basically an your fate tremors get moved too far towards infinity everything is becomes infinity as kind of like how i'd describe it here or are the opposite happened and the error signal because it was too low to begin with it's quieter and
einar inquired so to the boss man's like air air and supervisors would look what i think you said eric dyson so we so they start to announce numbers no books may whisper back to the employs her an implicit why the cup and their hands is why i can hear you that vanishing gradient problem the grain is solo that we gets back propagated over and over and over goes towards you were or becomes zero know what's the prom with this why don't we have this with a regular neural network or convolution on on a worker something like that but promised
we're back propagating are terror over and over and over and over no this isn't usually a problem for very short sequences boys are sequences become longer and longer say forty steps or fifty steps nobles be very long sentences but in the case of whether production of stock market analysis those of the very tiny sequences indeed vanishing in exploding gradient to be a very very big problem for stock market whether analysis but it turns out also actually that can help he indeed suffers for
the vanishing an exploding greedy as well so you joie play it safe and solve this problem with the solution to this problem the solution is something called an elk s. t. m. or long short term memory cell or alternatively there's a competitor called a g. or use cell or dvd we curried unit date for the most part serve the same purpose in very similar way is l. s. t. m.'s are much more common in yiddish riyadh
fines from what i understand and l. s t a m soule is sort of a general case of the g. r. u. sell so the little bit more all encompassing can handle more situations don't quote me on that but i'm alice young like mrs hughes much more than eighty are you in the industry so for now let's just focus on l. s. t. m. and you can think about you are you in the future l. s. t. m. or long short term memory sell what it dies the is the replaces the new ron
now in a week for neural networks in later soap in a neural network in your on it is just some function some mathematical function of statistical function like i said not passe said it deep learning is stacked shallow learning an example i used is a most eyewear perception of the no-one yeoman work may have as it's in your ons logistic regression basically every neuron is just live just egress and that any el pueblo y.
asked me on it in your neural now work if you're trying to do regression in it's going to be a linear regression new on just linear regression and if it's classification it's binary classification as can be logistic regression and if as some old high class classification is going to be soft max soft max this mold psych class logistic regression now he gets a little bit more complex than that lot of times these new ron's aren't necessarily logistic regression war
or what's called a signal we unit the other word for logistic regression as eighteen you're on is a signal when you in it because the signori function is sort of the crux of logistic regression cynically sig more units but they're not often signal you know sometimes and probably more often than not a neural networks we use a different function one is called a tanning each function answer similar to signal a function on to slow the different and others call it a rectified all
in your unit or re lulu which is quite different from us ignored function but these are three kamen neurons a u. might see in the wild signalling unit scanning each unit and re louis units and they have various pros and cons under different circumstances that make the math worked out given the situation you need to know that stuff right now begins at the craps out will work and i'd do in this particular situation is we're going to take out those ten each unit says
maybe in an r. n. and red take out those neurons what i'm out of the neural now work and we're going to hop in this l. s. t. m.'s sell dona alice t. m. soule is not a simple near on as such is not a function is not some mathematical equation is actually like a machine it's a little complex near on that has within it multiple neurons so you pop in the circle into your car and and you replace you work in the way your knee
ron's with this alice t. m. circle in your graft and it has on it the label l. s t m long short term memory resume again into that circle and inside of it is can have multiple little neurons one might be a matrix multiplication unit another might be a major expedition units one might be a ten h. function another mightiest ignored function etc soda machine that saddam many neural network and it has a very specific purpose the purposes in the name
long short term memory so before i talk about these units let's talk conceptually about what it's going to do each lsd am in your skin layer or however many thin layers you have is going to latch onto a specific sob sequence in sequence is going to sort of honing in on some concept within a sentence and focus on that so let's say the sense is after
we're gonna go get my aide license of the d. m. v. then i go grab some drinks with friends and i have to work the rest of the night was cut three separate things happening in that sentence it's sort of a medium length sentence so it actually indeed could suffer from the vanishing gradient problem each alice t. and sell in our bar n. n. may sort of lawrence whole latch on to sob sequences like after work i'm going to the d. m. v. at that my license to sort of like latch on to that
sean annual learn to ignore the rest then the second ellis t. m. ike latch on to the middle part bellinger grab drinks with friends so it learns to ignore everything up until this point and everything after that point to the zealous himself learn to slice and dice and says soros into sobs sequence is there more manageable which also makes for representing sentences hierarchical e. even more effective as well you might soros
in code sob chunks of a sentence and then combine those into an overall encoding rather than in coating the whole sentence as it is so that's pretty cool it can seem to learn to latch on to some sequences and it doesn't have to be as far as i understand it doesn't have to be contiguous sequences it may or may not be old to take chunks from various other parts of the sentence serve whacked on the concepts more inside the sentence he never know
really what exactly happening under the hood of the neural network because they're black boxes but this is how we might conceptually think of what analysis him does is sort of slicing a sequence it's manageable chunks edible chunks for the thief forward pass in coincidence but also importantly manageable for the back propagation pass that propagation to time because what's going to do now is when you train your r. n. n. on its error backdrop through time each spell s. t. m. will
so to listen for when it's it's own turn to train as to what sort of handling its own mini chunk of the sentence is not that's not too long so there'll at suffer from exploding are vanishing radiance it sewn up highlands of a chunk of a sense become charade on a small sequence without causing issues in the back propagation third time park so i'm alice tim souls the vanishing in exploding graham problem by sub sequencing your sequence
the more manageable chunks now how does in a bar in and work with a look like inside of that machine inside of that cell we call the cell because it's not quite in your honor and you're on it implies that a small function of small unit just as a mathematical function so we call it the cell is of the feed your honor really fat neuron have a bunch of stuff inside and what those things inside of it is they forget unit were sometimes more complex lead forget lay your home your honor dedicated
to know when when to stop listening and everything that comes after its it forgets or everything that came before where we're supposed to start listening he forgets so it's still getting the inquest every step that knows it learns went to forget what it's hearing because it doesn't matter it's not it's own dedicated chunk of that sequence in tandem with the input gate layer which decides which values it's going to up to
it's the inside of the l. s. d. m. cell and then it can at each layer does the actual copying and in the alcove where does the outlets others machinery inside as alice t. m. sell that learns to know when to start listening to get sequence going to stop listening and within that that sub sequence what values to update and how to update them and of course sending out the output cool so no sta m. makes all
murphy forward pass of our bargain and more sophisticated more complex more powerful but importantly it also prevents issues in the training phase in the back propagation through trying phase by limiting what and l. s t m cares about we've been a sentence so that it's sequins isn't too long that will cause a vanishing are exploding gradient problem like as a competitor sells to the palace t. m.
cells coolidge he or you sell the big recurring unit we'll need to worry about that for now alice t. m. is so much more common in stock in weather in language and everything that you're going to see in the near term and l. s t m is so popular in fact that oftentimes when you're working within our coal or or open source library were a model they wanted recon or n. n. or they will call ricardo my work those call it and l. s t a model or sometimes
the bi directional r. n. n. you deceived by alice t. m. b. i l. s. t. m. it's like the l. s. t. m. is almost the most important piece of the equation as though they just use that to describe the whole architecture are really an alice tim all it does is it replaces the new ron it inside a glove the thin layer of an r. n. n. with a live more complex machinery then was there are to begin with sell an l. p. time series
r. n. n.'s alice t. m.'s you're now an expert you can go forth and make an elk he models there's only one more very popular at neural network architecture to discuss and that is the convolution all neural network them points talk about next episode the resources for this episode the same as the last episode so all just drop 'em in the show notes your fine i'm opposed to val dot com for slashed by casts four slash machine learning
if you can spare any change to keep this pod past alive go to that same website click on the petri on link if you can't do that do me a huge solid and give my pie castor review on itunes that'll help keep this pod cast going base for listening see you next time


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt he did the i'm also starting a new contacts which could use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twenty four tax that hello again it's been a long time my friends i
all of choice for the delay unfortunately i'm not finding ways to find this contest to make my primary endeavor the patriarch us and going super hot so i'm gonna have to make this pod cast a as i'd find time thing i will be offended if anybody pulls out of the patriarch but i can no longer promised a two week scheduled release of just have to make these high caste episodes as i find time that being the case i am looking for work so
anybody is looking for a machine learning engineer or no somebody who's looking for one please contact me by contact information is on the pie cast website bossi develop dot com i am looking for remote machine learning work or portland oregon unfortunately i'm grounded here in portland for obesity you're half so i can relocate so if anybody has machine learning work please contact me i'd be happy to help out on your back in this episode we're gonna talk tax that
in episode ten we talked about languages inferring works we talk about python our job stalin and we landed on python as the best programming language to use as a machine learning engineer and we talked about all the different machine learning framework specifically people earning frame works compared to each other and landed on cancer flow as the best deep learning framework to use so in this episode we're going to assume that comparison saul that
we're just going to assume we're working with python and ten sir flow as our bread and butter if you didn't listen to an episode in you wanna know why we came to that conclusion go back to episode ten buffer here python is the winner of the languages war and ten sir flows the winner of the deep learning friend works now real quick there has been some changes in the deep learning suffering work space since i last spoke with you namely fiato is dead fiato is officially dead did actually announced
on the mailing list or former something that they're no longer going to be contributing to the project i imagine it's kind of a day if you can beat them join them were the competition's too stiff against cancer flow in world where open-source considers are paid for their work not further open-source contributions so make sense kind of did they get pushed out of the space it's sad that affairs but hey it makes the decision of the learning from works easier for us cafe your calls another combat
gergen the deepening for him works it seems to be a bit dead in the water as well i haven't really heard a whole lot about cafe around hacker news or discussions online these days however to hoard shoot a competitor to ten sir flow has actually been making major splashes recently it's coming up hot on the heels of ten sir flow as a second place contender were a lot people or swearing by torture over ten surf look specifically recall torch which was
previously written all ally has been imported to python and so now is called high torch from what i hear some educators and practitioners think it's a little bit easier to work with intense or flow for the same net gain on jeep you performance all last stuff making him that way and i actually haven't played around with pie torch myself yeah so they'll have to be something you dig into yourself i will still recommend cancer flow or torch merely because to answer flow
substantially more popular than torch which means that there's a higher availability of resources in books learning material jobs employees sporkin's tutorials all those things sometimes it's important to sort of just take the most popular when it comes the framework some libraries in this case to answer flow is still definitely much more popular than pie torch up like torches common strong says just something to keep an eye on that brings us the beginning of this tax that conversation rib
assuming python intense or flow rules again be talking about things like hannah is numb high sai kit learn care ross tanzer force as well as a practical approach to implementing machine learning in architecture where you have made yet how wet that framework communicating with your machinery framer by way of a message cumin sorts before we get into all that let's remember what answer flow does for you you can write machine learning however you want it's just now
medical formulaic which can be expressed in code in python a lot of times when you take a course like the corsair of course for example will actually have you writing these machine knowing equations by hand from scratch in the language that they're teaching an s. fine well but there's two advantages to using a framework one is that these framework to sort of a library of common machine learning functions like convolution all layers of convolution ono met work
rectified linear units and other activation function somalis things but the other more important advantage of using a deep warning for work like to answer flow is that your code get executed on the g. p. you the graphics processing unit not the c. p. u. n. executing your coat on a jeep you can gain you ought to about a thousand acts performance thousand acts as a substantial game informs usually it's not that high usually some more and you
or maybe twenty acts or a hundred acts of something like that and it depends on the task at hand and the amount of marshaling the goes back and forth between your program and the jeep you etc but the amount of performance game that can be hat but executing your machine learning math on a jeep you oversee pew is so substantial that it really gives warrants to using one of the suffering works that does that task for you like to answer flow for those who were for miller
you're with crypto currency mining like mining bit coin or fear him is very similar in that space haven't really powerful jeep you makes using sep you not even worth it often times if you're mining using something like nice hash which allows you to use both your sheep you and your jeep you the difference between the two so substantial that what people just turn their seek you off for mining because it's not even worth the electricity cost that's the k.
you're in deep learned so that's the major benefit that these deep learning framework to bring to bear is that they do all the math on the jeep un specifically the thing that makes how the most is something called auto differentiation it's the execution of both the populists that the back propagation step remember that's the goal earning staff of the machine learning process is called gradient descent gradient descent you're taking a gradient of your
error which is a calculus equations taking ingredient is differentiating grade descendants applies to support that the machine tool logistic regression linear regression when you apply it to new role now works it's called back propagation it's applying grade descends to all of the neurons that propagation is the same as great as sent for neural networks as dawson complications deep down in there where implementing back properties
yourself would not be a very fine task well these fine works automatically take care all of the differentiation phase in other words the back propagation phase by to write evening automatic we'd be calculus equations they have to perform in and performing them which saves a whole lot of effort and a significant amount of time in performance for one reason or other insist on to differentiation specifically that the suffering works tends to tell the most that they dwell on that is like
one of their big benefits i don't know why specifically on a differentiation the great sent part is so significant by comparison to all the other math that these framer to perform on the jeep you is why all the other statistics and money around to perform a wimp but that's the victim in court on notice so pike torch and cancer float execute bodily death for you an automotive happens on the cheap you and not just any g. q. it turns out that and the biaggi p. used specifically are best suited
and to the task and video that land of cheap use the other main branch being amd solicited to common competing graphics cards in the space for gaining people go back and forth on which one's better indeed tends to be more bang for bought the maybe has some compatibility issues and also burns hot and videos older more like an iphone you cannot pay more for the name brands but also has a lot more compatibility with various software the conversation is a law
different in the machine learning space and it is in the gaming space in the machine running space you really do yourself a favor by going and lydia it really is a matter of compatibility and in fact at present tense for flow only supports and idiot so if you could be using the most popular framework out there you're only choices and the the reason for this isn't the improvises sort of proprietary driver set for doing this kind of math on the cheap you that makes writing to this kind of math as a framework to
alper whole lot easier than doing it from scratch and this driver says called kahuna see you da cunha it stands for computer unified that device of architecture and is basically just a parallel computing platform driver set provided by in vienna for framework architects like the developers of cancer flow a comparable alternative for amd would be open c. l. witches and open-source kind of equivalent stick
who wouldn't but which tends to get a lot more second class support by framework architects where two idiots first-class support and on top of today you can download from an via the zipped up folder of deal els called food tiananmen which is a little library of neural network whole purpose to interface with the jeep you and you have to download all screwed up into d. n. n. for your jeep you in order to install ten sir flow with cheap use it
or so i just said something significant said it cancer flow only supports and via jeep use because of this to nothing what that means is unless you have an old man but perot which has none video card a mac computer is not an do for you in machine running world unfortunately my friends i know mac the pros are very popular development machines especially amongst the weather and mobile at developers you can still use to answer floats with
c. p. you support on a map or a matchbook pro by ten sir flow does not support max with jeep you support going forward and i think the main reason for that is that the modern matthews amd so i have switched personally from a matchbook pro to apc i'm a big mac night but i have bit the bullet with machine learning the velma not switch to a pc with a envy at ten a. t. t. i'm graphics car
that's it hobbling graphics card you can get on the market these days for consumer purposes bang for but why is an it's a very very very fast so i built a slight build a custom pc within ten eight yeah graphics card and running at the boon to lenox not windows and not any other flavor of lennox baboon to lenox gets the best first-class support for tense afloat and other machine learning framework schoolwork windows and other flavor
so when experts i recommend a custom built he seized at how or better than the laptop just due to the ventilation issues with its hannay the cia and video graphics card running to boon to lenox that is my recommended hardware stack delegate said you can keep your matchbook growing use to answer flow in c. p. you know if you want to feast on wanted dabble with machine learning algorithms models but if you really want the real deal you know one right on some heavy duty hardware namely of
eric powerful jeep you you could get away with not having that your workstation be the primary heart where'd you run your machine learning models on you could develop in on your math book pro or your windows laptop and you could deploy your machine learning models to the cloud like amazon web services eight of us were google cloud platform g. c. p. or microsoft azure and run your models on their jeep you instances of a very common what is called
p. two topics large instance spike a doubt us any gives you a jeep you you can run your machine running models on running your models macleod it can be quite expensive it's about ninety cents per hour presently to run a p. two x. large instance i get us if you're going to write your models in the cloud i personally would recommend google cloud platform they have faster jeep you instances for less money so faster
for cheaper and they unveiled last year that they're going to be creating server instances which use not a jeep you the something call atp you beat cancer processing unit which is eight should design for running machine learning models specifically graphics processing unit is very good for running machine running models incidentally because it's very good at math but was a bill for that specific purpose t. p. user built specifically by google specifically for running machine learning ma
rolls and specifically for cancer flow and they think they uranium major major performance boost by using cheap use they're not out yet their server kansas is currently don't support tiki use but they do have g. p. use which are more powerful than what you'll find on a dub us or azure are at present being october twenty seventeen and for cheaper than those slower alternatives to boot so i'd recommend g. c. p. over the competition however
i actually personally used a doby us rather than g. c. p. there's two reasons for this one is actually got free credits for dwi swan running my instances for free which is better than she an amazon is famous for doling out these eight of us credit seeking get them from various things in the start up marketplace whether you join an incubator or accelerator program etc see might be able to land some freedom us credits additionally eight of us off
for something called spot instances spot instances what they do is they're basically like an auction up on a price per hour or forty seats you been stands so where are the a. p. two x. large instance the g. p. you sir for instance on easy to costs normally ninety cents per hour a spot instance says how much are you willing to pay per hour okay i'm willing to pay up to ninety cents for example
it may end up costing ten cents an hour or twenty cents an hour it's sort of this price fluctuation and happens to route the day that amazon and if it ever goes all were the max come out i'm willing to pay him then it will just terminate my instance just kill it is needed in the black so this sort of us gareth air is a fear that you may just have your instance hold out from under you as long as you said you were spot instance price high enough then that may never happen and you could end
saving substantial amounts of money i think i average end up paying about ten cents to twenty cents per hour using a spot instance p. two two x. large rather than the standard ninety cents per hour or so i just sat my spot instance max price to ninety cents an hour which is this enterprise never actually gets that high and i never lose my servers and anyway this would be a problem for developing in machine learning models running them in the cloud because usually you'll be
using something like ten am mind that has to answer flow support built into it so spitting up another one of these instances will be no problem on like all web server maybe it needs to have very high availability and reliability so the spot instances line really well to pass machine learning engineers wanting to ryan machine learning models on that sheet for multiple days on a very high and graphics car seat could just use your windows laptop where mac but perot develop him
all locally deployed to the p. two packs large spot instance on eight of us and ron it's fair and then not have to actually switch over to a custom built pc with its heyday eats yeah graphics car running a boon to you could do that but i would recommend still do in the pc route because in the hands unless seagate free eight of us were g. c. p. credits you'll still come out saving money with a custom home bill pc over running amok
loud after maybe six months to a year it's still cheaper to go with a workstation home then with the cloud eventually run in the cloud to deploy your models yak shalala production system and in play though we're just talking about our work stations here celeste the hardware tax that either a custom built pc with unenviable ethics car or if you prefer you can run your models macleod malice talk about the soft for tax act actually we're using sensor flow or to be using him
all the other libraries as well hannah is not high a psychic learned maybe something called salary for a message queuing baby carrots or cancer force will handle is one by one so first off mushy learning works on t. f. you have to have begun to crunch the numbers to build a machine when model to come up with an estimate of something maybe you're coming up with an estimate of housing costs that the linear regression model or of classification that's a logistic regression model
works on numbers in those numbers come from somewhere someday us at your days second either be in a spreadsheet c. yes the work itself file it can be a folder of images if you doing damage classification working come from a database like host grass or my ass too well or maybe and a key i'd like quan dole queue u a n t l a popular financial data set a key i've if the da is given to you you really have control over the situation it's you
sully coming due by way of a spreadsheet c. s. b. file but if you have control over the situation i'll recommend using a full fledged or d. p. m. s. aid to pay slut post grass is that close grass specifically is my favorite or g. b. m. estimate it is very popular monks developers to spend the main data bases in last five jobs i've worked for city get to choose how you're handling you did a story in post grasp if you don't get to choose made it's coming from a p. i'm going directly into your machine learning model was coming from us
right she then you start with what you got whatever the case maybe you're going to be consuming your data from your dataset you can be poignant your program and your python program and the way you would probably do that is by way of candace pandas is a library that's kind of like a spreadsheet in python it's a very interesting little tool first it provides a step for pulling your data from the dataset that dataset may calm from us spreadsheet so
can this has a reed c. is the function or may come from a sequel database like postcards it has a weed seek full function which interfaces with sequel alchemy a popular python or ham or is it coming through in a p. r. a. using requests or what have you you would then just manually right code that types of independence once your d. n. a. is out of your days and independence pandas allows you to what the coal mines de mon jour
clean your date at seoul's for example your dataset may have a lot of knowles in the various cells in various rosen clones arabia bunch of noles and anti value not a zero in all adding nall can substantially scrub your machine learning model before you start doing she learning you might think i'm sure my machinery mole lawrence who ignore the noles right and in a machine running model special is something so complex is a deep bureau mel work learns what height of david this horrible lasso
to murder come to its conclusions no no not so machine learning models and indeed the neural networks are very sensitive tunnels those can really does make smoke and fire come out of the hood and kill the engine say happy do something with those knowles you either sat down to zero but sometimes that doesn't help you very much for example in the case of stock market to death or financial did you how you have prices on a daily basis if for whatever reason you're missing a price
on some day setting the price zero will make it look like the stock market dropped on that date which can totally messed up your machine learning model it's assessing its zero you prefer to forward fill it with bid price the death from yesterday this is called for filling in tennis has a function called a feel for forceful wilson of function called the filter back phil so pena's allows you to fill your knowles sane way
however the case may be freer circumstances the allows you to turn numeric data into categorical day that vial one hot including or vice versa categorical didn't maybe your columns represented the strings turn those into numbers so pandas is all about manipulating your date as to clean it up so that it's ready for your machine learning model hand as is about the amman j. cleaning up your day that they may be thinking what if i'm using a sequel to
it is like pose grass quin nie do that for filling or are coalescing knowles into zero values or are transforming string values into new numerical value solace of can i do sir the data munching as the day it comes in from the database directly so it's all part of the sequel query the answer is yes of course by not everyone's dataset is a sequel database some people stopped this comment from spreadsheets sometimes from deep yeah
is hands as has been the case on recent project of mine i've been bouncing around between different data sets trying to find the perfect date is set to work with while building my model so i'd rather they dated cleaning step exists in that can the slayer and because my dataset mcginnis source may change on a week to week basis so you had your dataset whether it's a spreadsheet or sequel comes into your python code by way of
and those which then cleans up your dailies pianos to clean your data and al your day is clean and ready for your machine learning model what you do is you take it into something called numb hi annie you and p. why number high non pie is all white bird for working on vectors or me to seize the portents ers of any dimension linear algebra basically not high is linear algebra so you would use non pie for trance posing a matron
for converting the matrix were doing and got caught between the tutsis et cetera the recall that's statistics in calculus and when you're out your brothers to the three branches of mathematics use the machine learning and specifically sort of that coat local most of what you're doing is linear algebra and that sort of what nom high provides so you're dedicated firma dataset they got cleaned up by canas and i you convert your date aflame in pay
those two a matrix a matrix of numbers and you're gonna do machine learning math on it so numb pie does that kind of math you could write any machine learning our revenue needs you in them pie the danger of being corsair of course was hot in that lab known pie is very similar to matt labs the language not high sort of gives you the things that python lacks compared to mal lab c. can slice nicer major season to all that stuff so
injure being credible threat of weeks hot course entirely in number high on python rather than that lab but he chose to do on our lab now you may be thinking wait a minute i thought that we want to do all of this math and do when you're out of account the statistics but we wanted events cancer flow on a jeep you yes indeed yes indeed we wanted you dead intend to flow there is a heavy overlap of numb piet with cancer flow so everything i said
about using umpires slice nice you were major season two linear algebra mala staff you don't wanna do an umpire you what you intend to flow because you want to execute on the jeep you alf course sensor flow is a new or framework so the staff didn't exist ten years ago where number idea it can serve was basically bringing them high to the jeep you in addition to all the other deep learning framework utilities like like activation functions and convolution all layers of stuff that cancer flow provides
that is a huge amount of overlap between empire and cancer flow imagine them as of then diagram with aids very large sort of overlapping space a lot of the function calls all of non high with the same and let the message of the same name take same amount of parameters is cetera et intense are full sensor flow try to make the transition from nom high for people who were used to that too tense or float this seamless as possible and in fact it
you're using to answer flow without g. p. you support say you're just doing development on your map up throwing you only have c. p. you support them to answer flow will actually use numb pipe under the hood to execute its math black even if you're using ten sir flow for everything on the jeep you you still have to have known pop a you still have to have our round known high a tends to sort of be the common language spoken by your python program
anticancer flow now umpires sort of marshall's day that between your python program and cancer flow back and forth back and forth with does it wraps your data flames they came out of pandas as a major sees the cold and d. or raise an umpire raise endear raise is basically a matrix is either a bit of actor or a matrix worth three tents or wherever have you some empire wraps these as testers and enhance those oft
spencer flow an intense are full receives a number high matrix and it brings it down to see executed stefan the jeep you and it comes up for the response down here in c. comes back up to python and puts its response into a gnome pie ten sir again and give that back to you so sore this common language that your python program and cancer flow speak so with all that said you can write as much of your machine learning
program in them high instead of cancer flow is you want and you can write as much of it in its hands are full o' instead of an umpire as you want at some points at some point you have to have a little bit sensor flow and at some other point that they have little gnome high so magill like this sliding scale bid to win one in ten we you can slide this scale from the left bingham high into the right being ten souffle could have as much of your machine learning logic in
cancer flow versus non piet as you want ideally you want to write as much as possible of the linear algebra stuffed into cancer flow because that's a big net gain you get from executing your math on a jeep you will find sometimes in some get harbor posit tories some boilerplate code they came from an on line tutorial they're not very rigorous about getting as much as possible into the cancer flow code seal see a whole bunch of sort of custom now meet jai lai
jake that could have actually been handled by tens or flow instead in which results in slorc code execution performance but they don't care in there just put gather a quick tutorial but in the end for your production code you want to use as much ten surf lowest possible so you did it comes out of the dataset wetherbee oppose crusted days for spreadsheet the pipe it into the hand as you used hand those two minds and clean your degas filling knowles with c. rosa war
for filling them from prior entries turning categorical stuff into numbers turning strings and some numbers all accepted clean up your data with pandas you marshal pandas into nom high non pie is the way you represent your data sets or your data flames from pandas as major she's worked for answers you do as a minimal amount of necessary steps on your number higher raise as possible before piping into
to answer flow cancer flourish seizure an umpire ray goes down to see down the stairs you're in you're in the python room you handed to answer flaw package and numb height and di re that's flown up not to said they fully and he turns around with this package you walk salvador goes goes down the stairs to the basement where that see people are c. c. plus plus in your twiddling your phone service anxiously you wanna go down there things are loaded too fast down there people running around like their heads are cut off your budget
he's in bones and steam and then the door closes from the basement fanciful walks up the stairs opens your door hands you a number height india repackage noises and walks away you the python program on rapid you were numb pie or ray and there you have your answer your prediction for a mushy lying productions that that may be a category apply to an image etc can those number high a tense or flow now to answer flow is an odd duck coming from
a traditional programming background if you made yerxa curious with weber mobile active elements or any other sort of python development or anything this away you write code in a procedural manner put away your right hand sir flow code is very strange indeed and the reason is that cancer flow chloe you're right he is an abstraction it's not the real code it's executed because the right hand sir flow is some sort of simplification ads
fraction layer that the cancer flow people provided to you in your python worlds and what happens in the end you write hundred lines of code on the hundredth wine of cohesiveness seal the deal the criminal waiting perhaps awaits all the cool you britain's thus far and sorely prescient jerky when you executed tester full code it sends down to see that hunter flow framework it actually gets ready in a different language and what's get executed it is entirely difference
that of instructions so this is similar to the few familiar from the database background check with object relational map or skoal our campus common one in python being sequel alchemy what you write this python code using this abstraction layer is the key i but we finally executed the code would you run a boar and got execute or have you would actually get executed a sequel eskew elko naturally and
or am rats sable coat it's a very simple thing i mean what happens is your python code is translated to sequel code in python it's so simple in fact the lot people issue the use of or n.'s what people don't like using more and they say the heights you from sequel in sequel isn't easy language anyway you might as well gain the flexibility in power of knowing and writing sequel directly rather than hiding away from yourself with in or around
that's fine well with your rounds is not fine and well with computation graf frame works like cancer flow and high torch because they perform that guano differentiation and other sorts of math on the jeep you say you have to write yourself see you wanna use these things is similar concept you write your coat and python we're doing is you're constructing what's called a computation graphic photograph of no it's so every line of cancer flow code is like you're us
i mean a graph know the circle some sort of operations call one o'clock opie to a variable and then a subsequent wives of code kollek these variables and combine them in some sort of way whether they're doing matrix multiplication or piping them through or rectified linear unit etc combining combining to mining until the very last line of code sort of is the last combination it's the thing that has connects the whole graf together and you execute
it passes that often tense or flow framework which sort of rewrites the whole thing and see it wholly different sequence of operations in executes the thing on the jeep you in order for us all for work the way the cool cool looks when you write it into cancer flow is really odd it's really awkward dealing with things called fetches and feed it gets in variables and place holders and initially it seems very awkward n.
on intuitive but actually it's a it's a hump yet to get over there is a definite learning curve to writing cancer flow code that the peace that you'll actually get over that learning curve at the pre fast i think it took me about a month of writing to answer flow code before i got over that haunt the first month was like what the heck am i looking at and then month to month three right okay things is smooth sailing you really understand how worst is similar to our china one functional program and it's it's horribly difference
i know of code but once you get used to it and inmates lot of sense in a case of functional coding delia skeletal never go back now that's ten sir flow brought spencer flow yo con awkward for right because it has a sort of encapsulate should paradigm will wear what you're actually reading is a computation grafted then gets passed on down through the framework there is a framework howled fair that eases the burden of this process that makes writing cancer plough code all i easier lot less awkward that feel
a lot more like writing traditional procedural python is called care ross katie or gate s. is becoming a lot more popular these days care ross is or wrapper on top of cancer flow so you just answer float under the hood but it basically reduces the whole point of cancer flow boilerplate into substantially fewer lines of code in care of us so something that would take you fifty lines of cancer flow coach right a lot of pain
fusion you would write in care austen maybe five to ten lines of code instead and which would look a lot more elegant care ross used to be wrapper on top of fianna in all that did the writers of kara sir very happy that they decided sue fully support answer flow with the recent news of fianna as death because now they're still writing detail coats of a popular machine learned framework and care austin's the very popular with books
spans tutorial code basically where an author is trying to convey how you might go about writing some machine learning code conceptually under less interested in the coat or tax specifics because it care alzheimer's u. from all those oddly details when i see common in practice these people we use care ross has that sort of widow who strapped school or actually there is a c. s. s. framework in wet development called whoosh
rap and i think is a very good analogy to see us as framer called bush trap that allows you to make the website is automatically designed so like every age you know element you put on your website automatically has this theme to its that's very pretty this very basic and very common you'll spot up bootstrap website mile way black at least it's not sort of black on white sort of the melody came out you give us the matter that box at c. s. s. theme c.
you can sort of proof of concept you were wet that you can try your web out out see if it sticks see he'll get any customers and if you do if you're wet that proves itself out you can then take away the boot strap fima and now you can start custom designing your own c. s. s. to give your website a custom designed in a custom look and feel i think care ross is very similar to this way you can do with kara s. is you can write your machine
earning program again carrots first because it'll save you a lot of headache balls with confusion and scheerer lines of code and once you've proved out your model and you've decided that this is a useful route to go you actually want to build this project then you may pull out kind of chunks of care os where you preferred have more flexibility by diving down to the rocks hands are full of the game that terrace gives you any
the city youths you lose inflexibility it's a very natural tradeoff so we can do is start with care awesome sort of start hauling cure us out and going with rocks cancer flow as you need to sort of customize your model with higher flexibility this is a common use case i see so the net when i highly recommend investigating care ross so there you have to answer flow and additionally and optional rapper cold care arts to ease the burden in other calm
and library all see use is called sai kit learn psych yet learned is a library of shallow learning algorithms cancer float is a framework of deep warning algorithms cancer float actually sort has as high or low level as you want to go in that sort of people earning stack it has built into it neural networks out of the box and work for neural networks just out of a box you can kind of like ten long
in the work for neural network or you can build and spencer flow from scratch using linear algebra and statistics formulaic you can handcrafted and neural network c. can go as low or high in abstraction in ten sir flow as you want but it doesn't have functions for shallow learning so has functions for deep learning for constructing deep neural networks whether their work for neural networks are convolution ono met works or mole tyler
perception ons or ongoing colors etc it has all the deep mourning tools built and but it doesn't have shallow learning tools now all linear and logistic regression specifically you can hand code intense sure flow pretty easily there's plenty of other scholl learning our burdens that we've covered that you couldn't easily do intend to flow things like support back to machines naive baze decision trees ran of
arrests etc is so pure gonna be using shao learning for your task that you probably know what used psychic war sai kit learned is that spencer flow of shallow learning center flow for people earning secular and for shallow and the two libraries have very very little overlap so you'll be using one or the other depending on the task at hand now if you are using to answer flow and steep learning you still may benefit from psych hitler and having it
one hand it has a few miscellaneous utilities actually come in handy no matter what you're doing median machine learning for example one thing it has is the library of dataset speaking just hold a data finnair so for example common dataset is the amnesty hand written number is the image dataset and asked them and i asked he actually don't call me on that but i believe it's been a secular combine know for a fact that at guy you are
sunflower dataset which is a very common example is in sight get learned any basically just like imports iris from sites it learn dot data sets back and it's that easy to just whole idea set out of thin air six are developing a shimmering moll without hath no worry about that step another thing that psychic learned provides that is useful no matter what you're doing is date as standardization and normalization sold in machine learning models the learning
stat of greatness and we're back propagation learning step works all i better way and the features or normalized or standardized is a cell difference between normalization and standardization which i will going to hear the edges if you're trying to figure out a cost of a house based on its number of bedrooms it's number of bath terms of square footage is doesn't sound town all these things all the number of bathrooms ninety two
the defense to downtown might be like five thousand whenever a speeches i don't know yet ended just for the difference between those two numbers is either on wholly different scale search wholly different types of numbers and that that's gay old difference can thwart gradient descent and so feature standardization is the process of bringing those suits numbers into the same ballpark and all end up being some number that you don't actually rapid
the number of bathrooms mighty point five and a distance about town might be point seven the point is that it brings them all to the same playing field and there is a function psyche alarm for stealing and normalizing data sets which is something you'll use and people earning as well seeking keep psychic learn round just four of the day dead standardization step that you'll then use you are standardized a debt to feed into tents or flow now that sounds like
actually the job for him that's right hand is all about the amman jean de at cleaned the of preparation well actually i don't know of pandas has a function for standardization are more mobilization and expand as is meant to be more general purpose not you specifically in machine learning onwards is standardization way that tends to be a little bit machine learning center so that might be why it exists in secular or lease is more commonly used for on site of learning
in their repositories that i've seen so that's your tax act you have to dataset be it host grass or spreadsheet or what have you coming in to hannah is being minute marshaled through an umpire it into tends to float tester float execute your coat on the jeep you brings it back up puts in cinema plank is it that you in your python program you make optional we use sai kit learned to standardize your day at warp hole data sets out of thin
eric etc now let's talk about something that is not machine learning center but i imagine will be useful for a lot of the listeners here and that is the general kind of architecture idea for urged a company how you might fit the machine learning server or pinch your system architecture the most companies sort of bread and butter is aware that acts are further their website were their mobile act you're facing with their customers buy me
the website ramallah and so are their main server is their web server their aft sir were you call it an act serb hands technically this is going to be the very front thing for your machine learning server which usually called your job sir are so you're packed server is taking requests from web clients people browsing the web site on chrome or are there by phone or using the mobile app on android these are all your clients
and all sending requests to your webster or your acts are a new web server might be written in no j. s or go something that has very strong current currency support for handling multiple requests for example i actually personally disliked python as a wet act server language even flask in janko the traditional for an acute and python i think things like no jessen go have stronger can currency support for handling multiple quiet
so i personally like no j. s. for writing my web servers and you run this may be on a dub us either bees stock were easy to something with an auto scalar set up again this could be a totally different servers in your machine earnings for so now over here you have your machine running for it's called your job sir are and this is no rhonda on a different type of server one that has a jeep you were multiple jeep use eighty two x. large instance if you're using a debbie west for jay
ample one reason you wanna be's to be two separate servers is that eighty two packs large instance has very few see pews very limited ram been a very powerful jeep u. n. is very expensive okay it and that one's well the machine learning where is a whack at server wants more see views once more ram and doesn't care at all about jeep use and because it doesn't care about cheap use you can cobble law
it costs see you want them to actually be on physically different architectures for want one to save costs and too because the architectures make different sense in different situations to lose you want them to scale differently you know why you're wet that's sure to steal up and down pretty fast and pre sizable chunks where's your machine learning sir or may not scale ever or maybe wanted to new instances here and there on a scale back down to one okay so if these are
to separates servers how they communicate very common way to communicate between your way at that server are accurate in your job sir or is by something called message queuing software very popular message queuing southward called rabbit and q. and another one this paupers called zero and hugo c. n. queue for message que in the names of these software's now you may be thinking well could i just send like a simple west request from my ass
sure to my job sir to do some machine running task yes you could but there are a lot of benefits for using a world last pieces software like the message queuing service if your job server is temporarily offline then it wouldn't have received a request sends by your tap server and vice versa and as a result that message gets lost in tune no man's land gets lost forever say for example you're let's pretend
we're building pandora the user or songs ops songs or thumbs down nussbaum from an app oro website that their clients that requested cents to the acts or the weather for the web server may put that action in the database just keep them on hands a very good is in response down a korean saying i have received your request here's your new song but such them to that new sonnets are playing it worries that accurate as stream the music to a client all let stuff in the meantime
i'm the act server sort of busy dealing with this customer talking to the customer with this communication turns around sort of like a jobs or with videos hand was mouth payout this person didn't like the song can you think that it's your machine learning thingy the job sir reflect got such cordial way on the g. p. u. z. intense are full of afterward as knowing about that okay good medicine scientist over there he goes all the crazy stuff if they catch server could turn round and tried as a job serb and jobs
or were sleeping aka offline job sir would never have gotten that request so switching to a message queuing system white rabbit and you will happen is the acts or will take a request one-on-one bloke says this user didn't like the song go to a mailbox and put it in the mailbox and close the mailbox and come back and communicate with client whatever envied jobs server is ready to do machine learning stuff
wherever it's done with some tax cuts currently on or it comes back on line from being offline maybe a crash for something or just wakes up in a yawn isn't it looks a good time makes its coffee and then it goes out the door and goes to the mailbox and picks up the requests close the mailbox and goes back and it does it's machine learning stuff with their requests an additional benefit to this is if you have multiple jobs jurors multiple machine learning servers running cancer flow
they're jeep use one of them will go and pick up a message on the mailbox and handle it and that message no longer exists in the mailbox so by the time the second job server comes around it will either get an anti jobs you or the next job in line so you robust these socks were like brad and que makes for more false tolerance reliable sensible handling of message passing between an act and a job server a common
piece of software for a message queuing up in python is called salary salary abstracts various message queuing software like zero m que rabbit and hue and more than our way you can just use salary and not care about what message queuing software your system architects have decided upon under the hood or switch to a different message queuing service as the case may be salary lastly and i've got a little
bonus software package to talk about a framework called to conserve force tense air force is like to hear ross for reinforcement wanting so we haven't talked about reinforcement warning much yet but reinforce the learning it is awesome it's a burgeoning space in machinery for all of the best and coolest research is happening right now reinforcement learning it is where are the machine learning be calms artificial intelligence in fact reinforcement learning
equals artificial intelligence so for those of you who've come to the space of machine learning it because of an interest in a high well what you should be to harding is or will reinforce them i guess we should have your goals set on a job eventually in our l. will talk about reinforcement learning in subsequent episodes reinforcement learning is a very different beast then supervise learning an unsupervised learning for one that deep reinforcement warning it is a much
new we're states and if it's much less explored the reinforcement learning is the stuff you see coming out from the mines and open ai playing video games playing go driving cars all these things and recalled reinforcement learning is action based machine i think so supervise learning it is coming up with a production based on past day yes so predicting the category of an image are predicting the sentiment of the sentence etc reinforcement lorne
his action basis deciding what action to take on earth situation that's why when as well to self driving quarter deciding whether to turn left at the time right et cetera and it's very research center crying now not very developer friendly on like supervise learning which is very developer friendly by way of cancer flow and care rocks supervise learning it is developer friendly because supervise learning has a lot of common that developer apple case
and skin industry in standard of professional settings where is reinforcement learning does not quite how that yet so this is really more lines were better to research than it does to industry deep reinforcement learning and as a result trying to implement your own deep reinforcement learning algorithms means you have to like get to read these papers you have to relieve can craft bees hyper parameters to it she is not all our resources out there yeah
for practical implications of deep reinforcement learning our rooms except for ten sir forced tens air force is a framework which makes deep reinforcement warning accessible to working along with him recently and combine that can recommend an cancer for subject that are right big label lands we talked about your workstation ideally being a custom built tower pc with a high-end ten abt i've been geographic scarred alternately you can duval
your models on a computer you want and run them in a cloud eight of us should g. c. p. i recommend g. c. p. we talked about auto differing works high torched cancer flow fiato in cafe keep an eye on high torch used to answer flow pandas numbed high and psychic learn our free libraries if you're going to use in addition to cancer flow in your machine learning tool belt tennis is for retrieving and cleaning dana no
hi is sort of the common language of nature's season cancers spoken by your python program and you're tense your flow code and sai kit learned is bowl thirty library of utilities that you may find useful no matter what you do and the framework of shallow learning algorithms if your task is a shallow learning task as far as system architecture goes you'll likely have an answer or the w. wet stuff and the job server that does all the erma she learning
stuff you why your job sir of course to have a high-end g. p. you you'll communicate between the two by way of the message kulik rabbit and fuel and there is a rapper library in python called salary which makes working with message to southward easier speaking of rapper libraries making things easier care ross sits on top of ten sir flow to make ten sir flow code more palatable it boils cancer flow down into fewer lines of code
makes it look more like a traditional procedural programming at the cost of less flexibility to answer force is a rapper library on top of cancer flow for deep reinforcement learning specifically because reinforcement learning sort of a different piece then unsupervised and supervise machine learning resources for this episode or are the usual keep learning resources which all postma shone out and a book that i have recommended the past is called hey
hands-on machine learning with psychic learn intense are full of actually just finish this book and this is my favorite machine winning book i've ever read the fact my favorite machine lying resource i've ever consumed except for that andrew being corsair course that's a solid gold and pedestal that you can't touch and shooting for sarah's course always comes first but this book is phenomenal it's actually very applicable to this episode specifically it talks about secular an intensive what talk about hannah is unknown pie
a and dataset slight databases and spreadsheets everything that i've covered in this episode is covered in this book and the author does an excellent job of explaining things i've found that there is a dearth of well explain to machine learning concepts in the industry which is why i've made this pod casson first place i think the sings don't have to be so complicated they can be boiled down really well and this author action is a very good job of boiling the scenes that he makes mush
she learning called runs very understandable he talks about shall learning he talks about id gone so close that in the show notes usual place boasted of eldar comp or were slashed broadcasts forward slash machine learning by the way if you're interested in purchasing a workstation pc to run your machine learning models on i would recommend going with a custom built tower pc building yourself don't go laptop unless you really need that mobility because neil lot more performance with
desktop p. c. and don't you were those prefab desktop p. c. is like cyber powerpc for example there are great company and have great computers or a constant sheen learning you really wanna milk every last time you will drop out of your purchase never the same price you can get on the order of maybe fifty percent back sure a performance for running him she learned models by custom building which can mean the difference between training for three days versus today's so on my website i'm going to post a perch list
of the bill that i used for my computer and i'm going to try to keep this part was up to date with state of the art components as new components release so that no matter when you listen to the spot cast you can go that parts listen bill your own p. c. for example this is october twenty seventeen the techniques yeah graphics card and i seven series intel seek user topple line but very soon or kennedy getting into release of intel c. pews and volta graphics cards from ten video which should be a lot
faster all optical it's when he's released all also posed a link to a video series tutorial on how to build your own p. c. also speaking of performance and this is our random aside when you start really get to the metal with your machine learning development the old cancer flow from source you can milk all lot more performance out of tens full bill from source then through the pit installation it's kind of a pain in the buy so start with that when you first get set up and don't worry bout
ling from source until you're really cooking with fire with your machine learning models what you really start getting deeper new development you gil lot more performance by building tanzer flow from source doing it that way also allows you to use the latest code out and to d. n. n. releases with a stable releases of cancer flow tend to be on very cold versions of good and true d. n. n. n. newer versions of today to milk substantial performance improvements off see you in the next episode i don't know when that will be again i'm not on a schedule
any more the next episode will either be on a convolution own neural networks for image recognition or neural network parks like bavaria's activation functions batch normalization various optimize years like adam and stuff like that so i'm not sure which of those episodes and i do next that aussie when i see you also a new job


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash m. l. g. eight and i'm also starting a new contacts which could use your support and it's called last year's like taxes and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twenty five convolution o'neill now works today we're already talking about consonants convolution all
neural networks for c. n. n.'s before we do that little bit of that man i've been told by a handful listeners that they wanted donates the show except that patron charges monthly and they only want to donate wants to have created one time hey pal joey been as well as posting might get coin wallet address for you crypto junkies if anybody is willing to donate to the show if you not willing to donate please do we will review on itunes that brings more listeners to the show which helps keep
so why then well soaked convolution all neural networks for one reason or another consonants tend to be the most popular topics discussed by new aspiring machine learning engineers i don't know why specifically kong snaps are so popular manner stand that vision is essential to key components to robots in ai in all our stuff but no less so with the natural in which pricing by waiver colonel met works in the like but anyway connoisseur super popular in the deep learning space calm
gnats or this thing of vision in machine learning in the same way that rucker neural networks are the things of an elk he'd matalin was causing as well as any sort of time series problems of the stock markets and whether production compliments or four images image classification image recognition computer vision and cons next to me were real clear case of the machine learning hostile takeover of artificial intelligence asset this entire episode i think that
crux of ai is an al that m. l. is fast simpson ming ai a significant way so much so that the terms you're almost becoming synonymous that's definitely the case with an elk he machine running came in maine ahead began to wither colonel my works on all the various aspects of an l. p. s not to say that an l. p. was entirely conquered by machine learning that the machine learning has contributed very heavily suspects in the case of computer vision i think we see that even more so
oh consonants really truly dominate the space of computer vision and so already talk about that today with respect to this classification image recognition alike now for those of us good memory and you recall from a prior episode now stock about facial recognition know is using a malt highly your perception on right of the nail in your own mel work in an l. p. as an example of an algorithm for image recognition i said that first thin layer might be for detecting things like lions n. a.
just a second hand where for shapes and objects and the third inlay or four things like eyes ears mouth and nose and in the final we're being the signori function if you just concerned with protecting whether or not is facing a picture or a soft max if we're trying to classify history dog catcher human so is using in an l. p. for example image classification i lied to you my dear listeners nobody uses m. l. p.'s for him is classification the use condom
it's been an elk he sort of one's well to a pedagogical mental picture of the situation and we encounter m. l. p.'s earlier on our machine learning learning so i thought it made sense to give you a picture though you don't use them out he's for images use continents and yours why and an help ye for image recognition is like using up bag of words for spam detection that he may be thinking hey i thought you said bag of words algorithms like naive days
work well for stan classification does take all the words in an e-mail and you just come on just for all my bag you shake up the bag and still out on the table and case of span attention in nashua line which processing maybe you're looking for the word by agra cage cup pushing all words around all there is by agra bam this is spam easy pc yes better worth works fantastically for natural language processing in certain problems but using a bag of words kind of idea at the end in its classification
doesn't make sense what you would be doing is cutting the picture out into all of its pixels katie have a five by five picture you have twenty five pixels and then you throw all those pixels into a bag you shake it up any doubt the pixels on the table and now what howling at the supposed to detect whether or not there something that you're looking for in that picture is just up bag of pixels that's when an elk he would be giving you an m. l. p. you remember the other work for a regular new roman were
and dna anna's another word for it the bureau my word for nikki and an artificial know my organs can be calling in and out he's now on an m. l. p. consists of dense layers dense layers meaning that's all over the neons from the prior where are connected to the next layer so all of the pixels of the input or connected to the first thin layer all the pixels or connected to every neural first in layer so everything is combined with everything else and in all the
as neurons are connected to all of the neurons of the next ten where everything is combined with everything else it really is like a bag of orgies growing all the pixels and combining and every which possible way but that's not how pictures work when you're looking for something specific in a picture you generally looking for that type of object we julie located in one little window one square with say that we're looking for waldo already using where's waldo is the example of this episode we're looking for waldo
a picture that is not an ideal little piece of waldo in the left and a tiny piece of wall on the bottom right and maybe he's happened center the picture and his foot over here on the top right as not now works is already clumps together in one object and that object can be anywhere in the picture so that's why i am l. p.'s to work for him is classification instead we wanna neural network that works with hatchets windows of pixels all it wants lilt shrunk stay in the picture
can't even within one window in a picture window that may be up walks around waldo in the picture you within that window we still don't wanna just combined every pixel now window every which way with each other that still will be very helpful for detecting whether waldo is in this window instead we really wanna look for specific shape or specific sort color pattern in this window and so what we're going to design is something called off filter
all filter is the crux of continents it's the core components water filter is is an object detector imagine that you have a five by five piece of paper and you take scissors and you caught out the shape of waldo in that piece of paper so that there's a hole in the center of the peace paper this is shape of walled up at any take that these paper can you put it on top of your picture you're fifty by fifty image and i you take that piece of paper your filter
an easier thing or two slighted to write the slide it all were the picture you slide it from the top left all which the rights and you bring it down one roll star back on the left arm with a typewriter rate hike will win the right in good answer the peacekeeper goes up to start the next line at the left many start sliding your filter to the right again in the moment they years actually all waldo in the picture will be very apparent you because still sort of fill in that hideout in the center of your piece of paper
up until that point until you are piece of paper was over a wall though nothing sort of obviously fields that hole in the peace paper that was cut out with shape waller nothing very apparently filled it with all this the bonds of sort of pixel j. bridge until you got overall waldo beneath it just so perfectly right into that cut out and it made him papa made him really stand out says not really an object it's hector there is no activation function or output of this new
on that gives you a yes or no necessarily instead of the thing that sort of makes the object hot makes it stand out in the location where he it's so that's what a filter is it's almost like a separate damage the smaller image may be a five by five filter that you're going to be using to search for an object in a fifty by fifty image and the filter is designed in a way that makes what it is you're looking for hot makes it pop out of the picture now having your filth
we're sort of be the shape of waldo is a bit of an over simplification of filter usually doesn't work that way the filter is usually ill little bit more simplistic them that in case of walder detection for example one actual filter we might use is going to the horizontal stripes because wallace shirt has already horizontal stripes on it so simple filter that would make him hop out of the image is a filter that has these stripes on it or is on all eight though i mean by
that is it's a five by five phil certified by five sort picture square of pixels where every even role is filled with winds and every claude row is filled with zeroes basket of like the cutouts but that does is when it is applied it to a patch in the picture all of the even rows of that patch or disabled because they're multiplied by zero all the pixels multiplied by zero and all of the audit rose above that patch
or enabled because there are multiplied by one and so when we hover over all waldo will see a bunch of red stripes pop out on us were hovering over anything else in the picture is for looks like straight nonsense soul filter can learn something quite so complex as both come out of a human shape but it can learn something simple enough they could still give us a good inside us while we're looking at and then we would combine multiple filters together to really
increase our confidence so waldo has glasses he has ever be yes red striped shirt annie something about being call is very skinny guy kind of a occupies berkeley a very small section in the center of the window so you might imagine designing four or five different filters each of which is looking for different patterns in the patch of pixels and all of them combined sort of making something pop out a picture that will give us confidence as to whether or not whirl
looking at an object we're looking for so let's put these altogether in we'll call abortion o'neill now worked for us why say up to pixels but this is called is a window or window is a square ciampi in the picture and as filter is a filter again we have a filter or any number of filters that we're going to be starting in the top left of our picture and sliding to the right and any time is sort of hovers over something that we're looking for that thing that pops out through the filters
and makes it stand out in the image why think about filters ally to imagine them is kind of like an old school n.'s sort of a cylinder a big hole in between your fingers and you know you can be put on the picture the top left you look through it with your pie closure other i knew looking through the lens with your eyes and you're sliding into the right and most of the time or you see is sort of borer go whenever you are all over the thing you're looking for in the picture then it becomes very clear of the waldo
comes very clear where is the other windows of sort of blurry now inside of that cylinder insider that lands there are multiple filters the maggi go to an eye doctor he put someone's isn't from in your eyes you're looking at some letters and he says better or worse he puts in a dish in all arab lands isn't for your eyes he's a better and he keeps that bear and he puts an additional layer of lenses in front of your eyes a third lands and from the other two aces better or worse in you say works so he's trying to fly
ainsworth this right sequence of layers of lenses that really makes the latter day on the board hop out you be very crystal clear enough so you're doing in designing these filters to couple lenses cylinder object that has multiple layers of filters inside of it and this is what the machine learning models going to learn it's going to learn the design of each of these filters each filter ole are in your sort of cylindrical lands now we have the filth
or that that is not actually daily or we're talking about deep learning here neural networks we can design ole are hidden layer in our neural network is that we have a little too cool enough tool belt this lanza soldier in order to come shocked the first hidden layer of our convolution o'neill now work with cold the convolution ole or what we're going to do is like essay we start the top left with this filter and we apply it to the picture all the way to the right we slide
all with the right and then like typewriter church in we started the next role on the west and slide it all went right again chechens are the natural flood all the way all right again until we've covered the entire picture that flag this filter throughout the picture and while we have now is a new picture in entirely new image where are all full wall those in the damage ha all wall those are now crystal clear and everything in between is borer
the orchard were she picks fully this is called a feature a map of feature matt filter is that cool we use for making an object hop in a picture in a feature mac is that picture schranz formed with the filter in every window every one square although picture is transformed with filter in now we have a new image is called feature map now like i said this land is that we're using to slide over the picture has more
couple filters inside of it multiple layers of filters each soldier trying to detect the different type of pattern straits classes shapes some the whole thing in the center of the filter etc multiple layers of filters and so what is output in a bar called lucia layer that next layer is actually multiple feature max one feature map with each filter apply to the picture so while we have in our first hidden layer of our neuro now
work is a fair read the box of pixels and free the box of pixels with by height ok this same with an heights as our ritual picture except that instead of being picture it's our filter all flights to the picture for every window for every patch of pixels in the image with by heights and then depth death is the number of filters so each convolution ole or he is
they with by height feature map feature matt being applying one filter to your entire image and death being the number of filters you have okay kind of confusing so let's start from the top we have a picture that comes in is your input into d. with by height we don't fly may know what our neural network is going to learn is filters filters are these masks that make certain patterns in a pixel patch be we
no hop hop out of that window this is what the convolution o'neill networks can all learn the soldiers nor have multiple of them we have one filter for stripes one filter for glasses shapes one filter for skinny object in the center of window arrest after these on top of each other were to take that stack of filters were we to apply it from left to right top to bottom in our picture and that's going to i'll put a new picture with by height pixels but it
death is the amount of filters so what we have a box now that's our first inlay or or convolution all layer away or of feature maps if we want additional him layers of our neural now work we would do this again we'll learn a new filters you'll play those new filters to that first hidden layer because at first in my ears kind of a picture of its own it may not be a picture that makes lot of sense to humans bill makes sense to the machine learning algorithm we will learn these new
altars and rule plied them window by window by window to that first thin layer and what will get out of it isn't new picture a new convolution all air which is with by hike pixels and feature maps depth the third dimension being feature maps the future map is when you apply your filter to every window of the picture and that will be your second in later years second convolution all layer and then finally to sort of cap off your convolution all neural network what
we usually do is then hide the result of all that through dense layers you've made certain patterns new picture pop and stand out in now you can sort of latch on to those with your dance layers to determine whether something is in your image or not by piping after a soft master logistic function of the white very good that is a convolution o'neill now work for a talk about the additional details like shrine in padding window sizes and max pointed it out is why
you know that's the essence of convolution ono met works each layer is called accomplish all air will lay your ideas is a stack of feature maps in the feature maps calm fromm applying to filter across your picture as c. soldiers the wheeler filters that the continent learns in the back propagation stand now oftentimes indeed learning part of the process is sort of boy ewing things down step by step as we go through the neural net
we're spread this looks like a final every layer of neurons gets smaller and smaller and smaller until our final output is either one you're on the case of logistic regression or one of multiple neurons but i mean let's say ten or twenty in a case of soft max regression if we would have liked all layer of five hundred and twelve neurons and the next layers five twelve in the next layers five twelve sort actually boiling things down we're just kind of mix and match in an the last layers that once a maury
function right well first off that final error would have way too much work to do we would be depending on it's too much to sort of boiled down to see you reverse of combined features into one point we would be all for working this new onto would be better afflicted boylan down bit by bit by bit until finally it was last neurons tang he only has maybe twenty eight employees after reports him but in addition to that part of the magic of neural networks is that a great things down
hierarchical it so that they get smaller us maurice ago long soak in a picture for example if you start with a fifty by fifty picture about would be twenty five hundred pixels twenty five hundred units in your input layer and ideally you would boil mac down into let's say ten or twenty different types of wines and objects and then you would boil that down into eyes ears mouth and nose for objects at any boy let down to one says cut away the deep learning general
work's not always but generally we like to go from very very big to very small gradually hierarchical a novel way i've been describing convolution all ayers is that each feature map is the same size as the picture of their plight to we take our filter we move it window by window over the picture what comes out as a feature the exact same size if we have multiple convolution all layers like this that it doesn't feel like we're sort of boiling our picture down to its essence
over time the way we do this way we boil images down into their essence step by step is by a combination of window size stride and having a keg window strident and had a no win no we already talked about window is the size of a patch of pixels that you're looking at anyone given time in your picture so a window of five by five means you're looking at twenty five pixels awards stride is how much you move
that window polls are at a time if we had a strike of one we would move that window over one pixel at a time meaning that will filter matt sat to the future matt can accomplish all air girl bill lot of overlap between each window if we had a strike of five that would mean the window would skip completely to the next package sir filter look of five by five window and then it would slide over five pixels all the way past the last
axles seen them in the first observation so the filter is now looking at a new patch with no overlap with the prior patch of how do we reason about this stride and window size combined your speak about them in combination try to think of them is some sort of ratio like to over five five being the window size into being stride size or or something like this window in stride always be considered together in the previous example where the pictured it smacks directly to a feature man
and at the same dimensions that case tried over one if we were to use a shrine of five why that would do is take your windows your five by five windows and boil them down into one pixel each so you take a five by five window and now we turn into one pixel in the downstream feature map if ye strike of one huge slide right one and that would turn into one pixel as well in the downstream feature matt essentially we look to
to pixels in our original image and has become too pixels in our new image in our feature map so that didn't actually do any sort of compression it just did transformation if we wanted to compress the image into a smaller feature map that bigger shrine of five when you do is you take a window of five by five that would become one pixel in the feature match any move over five whole pixels and that new five by five window would be calm and new pixel
feature map and everything in between would be left out so all the pixels will have been considered because we can study pixels but bill has been boiled down substantially twenty five pixels will be calm want that's how you do short of compression in this process you have a higher stride and higher window sides belt that's not always beneficial let's say for example that waldo sort of straddled in between those two windows we have we're
no fly by five and then we strive five so that we know now moves to an entirely new set of pixels all right in the middle there half his body is on the right side of the first window and the other half his body's own westside second window neither filters would pick up waldo in the windows okay we've got the strike detector filter maybe that would ding ding ding bull what about that sort of skinny object in the center of the window filter that filters knock him
eight anything pop in the window so even though a higher stride will give us good sort of compression work boiling down of our windows it may result in for detection of objects so going middle ground is generally preferred may be ascribed to rest right of free so there's always a decent amount of overlap to see waldo because at some point you will be in the center of it a window hands because the strike is greater than wine these windows of five by five
we'll still be boiled down into smaller patches in the downstream feature met so some combination of window size and stride is how you achieve boiling things down into smaller layers elect s. and windows fred bayliss go hand in hand it's a real wild understand continents because they're so many tom for talk about filters and feature maps call lucia layers windows dried padding in max polling these are all terms were not talk about the sabbath
and so many terms it helps when you realize that many of these terms or combined with each other they're different pieces of the same thing so window in stride bayliss go hand in hand feature map and filter are basically the same thing filter is a small section you little paper cut out of five by five sizable window when you apply it to a whole image you give feature sophie sure matt is up quite a filter to feature map of filter they go hand in hand all yours
teacher mats stacked is accomplish all we're so all those three things going and feature mathilde for calm was aware of it at all here we have window in stride dosing sky go hand in hand for the image compression then the other thing that goes along with him is compression some may call patting the padding is very simple to understand padding as we have our five by five window and sliding into the right okay let's say architecture is not fifty by fifty but fifty two by fifty to some number this not divisible by five well
wendell slight although it's the right until it gets those sort of last two pixels know we can decide one of two things to do we can either stop there and moved to the next row or we can move up our window five pixels so right anyway there's only two pixels left in the picture soul will do is will create three fake pixels they basically zeroes so that the remaining two pixels or considered the sort of know what part of our window and end
access is deceased big pixels and presumably because now wall learned that this access on the right side of the picture can be ignored we call patting it this same honey coles same when we include the fake pixels and we call a valid when we exclude the access pixels i'll have a great way for remembering same versus ballot iowa's have to look about personally so they just two separate ways of handling that access pixels they might think say
amen to that is stink always including the access pixels seems like this more weight always goal should we always include every pixel well not necessarily a lot of pictures serve the borders of the damage are kind of croft me we do cropping as a pre processing step anyway many times so in many cases that schooling the small amount of borer pixels is not a big loss in other cases you do want include ever
a single pixel especially in cases where it's not actually the image recognition we're working with i will talk and the subsequent episode about how you can use convolution all neural networks for stock markets stock market prices are not looking at an image whatsoever to wholly different space and computer vision continents and work for neural networks you can use these sayings sorrows in very surprising domain c. patch think outside the box but in those cases where you're working with features that
really pixels one include all those features not processed and so having equals same is the right way to go to just depends on her situation okay so we talked about window in stride and to some extent having those freed being used as sort of an image compression technique always of boiling down your picture at one layer into a smaller convolution all layer and in doing same thing second lucia where to next call bushel layer and so i
is smaller and smaller and smaller than you paid your debts layers you're working with a small matter features which are about that as one way of doing image compression and that's compression in the machine learning cents it's compressing features into a smaller feature servile represents all the other features is hierarchical a boiling information down into smaller and smaller bits there's another method of image compression in convolution romeo my work called max pooling and this is sort of a trigger
chill sense of image compression which is simply making an image small or not actually doing a sort of machine learning to stealing it down glossy compression in the truest sense max pooling or there are other types of pulling layers we call him pulling where's you can use max pooling or you can use mean pooling celeste talk about max pulling because that's the most common when max pulling does it takes her picture and just makes it smaller that is it just compresses it down now it's different then
using a complex called blue schull error of filters and strident patting him blah blah all it does is thus arab oil in two by two window into one pixel of tissue dividing about for you making every patch of four pixels become one pixel to just down stealing it substantially all you're doing is you're taking that patch of four pixels that window of four pixels in your taking the masks pixel that hit the maximum pixel by that i mean it in a grace k.
oh the image every pixel is represented by a number between zero and one where one is black and zeroes white so we take the maximum of those pixels and we just use that in throwaway the other pixels this is just true compression this is compressing images like if you're trying to off load a photo to facebook and said your pictures to bagel you tried oppler picture that's ten twenty four by ten twenty four and pretended facebook says we only except images one twenty eight by one twenty eight k they don't do that kind
russian on their side they expect you to have smaller images to upload to their website well what you might do is open up reviewer for shoppers something and just click and a picture dimensions and just make it smaller that's all it's going on with max poll is she's making a picture smaller is doing so in a very destructive way you know from experience we make pictures smaller or bigger it's law see if you make it smaller it's glossy compression let's cut tax laid in silence of the old slow but off about the peace corps your eyes you can to
all that there was some damage down the process we have to swing your eyes nasty idea here with max polling is you can apply glossy compression tier pictures to make them smaller without doing too much damage in the process now why would you wanna do this we had the option of using a big stride and big window in accomplish all layer for boiling a picture down for boiling the essence down rush throwing stuff away were boiling it down to its essence why would we wanted
as max polling we use mac schooling for eight wholly different reason that reason is this save compute time it turns out that convolution all neural networks are the most expensive neural networks in all lands more than work hard neural networks more than an l. p.'s more than anything why will we take an image that has with by heights and accountable supplying those two stars number of features is concerns and your pie
being that into a convolution all layer that has with by height as well as death is sometimes very very deep that maybe sixty four feature maps or ninety six feature maps and you might have ten twenty hidden convolution all layers when you start looking at the image net competition a kong met architectures these things are massive this is really where you see your jeep you shine if you're working with an m. l.
he wore our colonel mal work you know we'll probably see it five to ten x. performance gain by using your jeep you instead of your seat you when you're using consonants you'll see it your jeep utilization spike up to ninety nine percent and you will be screening fast running your calls next on your jeep you by comparison to seek you combine our taxes is really where your jeep you performance shines and not just the computational speed
either the amount of memory it's used by your architecture or ten eight cia for example has about levin gigs of ram separate from your systems ram well when you're too old legends crosses inure to be consuming all lot of that ram so continental heavy very heavy beasts and easiest ways to a slim them down to make them less heavy is just image compression just make your image is smaller and that's what max polling is for using a combination of straw
i ate it and windows size to boil york images down it is something of an machine learning technique that's boiling down the essence contained in the pixels of your image of mac schooling is just for making things smaller so that no run faster and you can apply max pulling up to your image directly right after the first where you can also quiet after every convolution all air because each convolution ole or will they may be coming smaller in width and height their problem
we becoming deeper in debt of feature maps to using max polling will reduce the dimension ali of your process making your continent run faster by the way there something i forgot to mention earlier in this episode we think of the convolution all areas with by height by death k. with an eye pixels and that's being feature maps well liane put layer image also has depth is orgy be red green blue valley we call us channels so
we your input image is with by height pixels and channels deep everyone pixel will have three channels being or g. b. values to your input picture is also a box and every subsequent convolution layers of box so really every layer is kind of a picture in its own rights okay and that's it that's convolution romeo math works they're not easy but they're not complex i'd say architecturally he does have to reach
actor on them and you know maybe ought to read it twice to come to grips with what all the moving parts are here but unlike something like natural language processing where maybe understanding our work her neural network works is fine and dandy but i understand label land of an l. p. there's been a whole lot of problems you have to solve with continents the one prominent salim year is the image recognition or object detection image says not a whole why you have to know this isn't a three part series like with an l. p. that just
make sure you understand all the parts i'm sorry i'm so redundant rest are for the top and work our way here you start with the damage is a whipped by hate pixels image incidentally is also three channels deep this orgy devalued see that box battered image you heights that into your compliments your first hidden layer is called a convolution all air and away the slayer functions gets these convolution all layer has with by height pixels as well
and feature maps depth any number of feature maps deep these feature maps are derived by applying the filter one filter her feature map you look up like this filter to the image but talk left corner of the image you slide it to the right and it generates sort of a new image where are the objects that that filter is designed to make pop well it's a new image were poll
all those objects in the image pop soak in a waldo detecting filter your feature map is going to be on new version of your original picture where everything is blurry except all wall those who are very clear sir you're filter is some small window of pixels that has something sort cut out of it and you apply to your whole image when you get out is a feature matt you do that with every filter in your calm lucia where you
a new work feature maps of the convolution ole are each call in show where is with by height pixels and feature maps deep is the filters that your neural network is trying to learn you specify out fried actually the amount of filters you're going to be using with him out of feature maps in your convolution always be specify the width and height of your convolution aware as well as the amount of feature matt's being used to the death is the job the neural network to learn the designer
a filter is not the amount of them so that's the essence of it the details are the filter has a window size of the size of the filter is called blue window with and heightening beautify by five that is so small square the stride determines how much that window moves at any given time if you're using a strike of one that window moves over one pixel of the time and the resulting future matt is the same size as your rage all the image
if you were striking is five then your window moves over an entire window at a time so that there's no pixel overlap and each window of five pixels becomes one pixel in the feature map in other words your image gets compressed to a smaller image generally a good strategy is to use something in between maybe a window five by five minutes' drive to so the salam out of overlap
to prove the likelihood of detecting objects and yet some amount of skipping which results in the damage compression additionally a technical detail is what you should do when you slid your window all way to the right and there's access pixels do you include them or do you spit them if he skipped them we call is valid padding and if you include them we call this same padding and away we make that work is by adding an extra down the pixels c.
roll pixels so that our window of five will fit all over five pixels some of which are gonna be donny pixels combination of both window size padding and stride will result in image compression in each convolution ole or until your final air which is generally a good strategy but another way to achieve image compression is called max pooling max polling it is la si simple image compression used to cry mare
early to save system resources if you're images are too big or your convolution all ears are too big and it's just hurting your grandma or your jeep you performance you we use max polling okay so that's the general architecture book on wish multnomah at work that's the general architecture now you probably won't have a great deal of success trying to freelance your way through designing a continents understanding the gym
earl principles how to design accomplish all arabs and then building an image detector with that they'll probably give you kneel the decent image detector maybe ten percent error rate wharf twenty percent error rate it turns out that's the amount of accomplish all layers and where you put the max polling layers senate and the window size and stride size in all those things these are the hyper parameters right selecting these things are choosing your hyper parameters calm that
hyper parameter selection is very sensitive is forced error rate is concerned he was very very well tuned compliments then you're gonna spend an hello lot of time to your hyper parameters a soul one thing you can do instead an especially if the image detector buildings is a classic type of image detector you're actually trying to detect people and dogs and in common objects in common photos is use one of these off the shelf called
at architectures so there's this competition call by el paso v. or see image next challenge and it's a challenge for people to be aloof set specific objects in a bad day basis photos and a whole this every year every year people come to this competition and they peaked last year's continental architecture with the new architecture and new combination of max polling and stride and window and number of
your maps and although states and so one of the classic what is called laying next five is in yon looked whom that elegy for liqun and a subsequent year's winner was call alex next so that would have beaten lane that would decrease the error rate and then a subsequent one is google maps and then the next one is in section and then next one is raz nets and so on and so on you may have heard of these different net architectures i heard these things flown around so long
time resin an alex net and realize that they're all com that's been none of them are or immense none of them are penalty is so we hear something that's you're probably dealing with a continent in these kong nancy's architectures are enormous there are some big big contracts very complex and very sensitively tuned hyper parameters so if he just won the game is detector for some project to join the robot with vision you can use wannabes off the shelf networks
i could roll thumb is just use the winner from the most recent year used point seventeens winner for example it will have defeated all the prior architectures but if what you're building is in the domain of computer vision but is maybe a little bit less common than common object detection in common pictures than what you can do is sort of steady the architectures and see what makes good hyper planners and good layer style
and then use that to drive design your own confidante so i described you in this episode is the core components of designing a con man architecture the very likely if you actually plan on using continents in wild specially for image recognition you'll want to look at one of these prefab architectures they came out of the image that challenge and probably use the most recent winner cool cool that's it for this episode in the resources that
and i'm going to post linked to the u. to record a series of c. s. to thirty one and of course by stanford specifically on continents and of course the standard deep learning resources i've always been recommending all pulses no-show notes in the hands on the machine learning with secular intense are full of books that i've been recommending has a very good chapter on combat says well boss elan accept so


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash famine belt he had time also starting a new contacts which can use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twenty six projects bit coin traitor it's episodes in the little different were injured
loosen up project that we use a community can develop together sluggish sharper cheetham she learning engineering the project is going to be a bit coin trading box does a few reasons for the selection of this project what is the crypto currency is a super hot topic right now another is that a u. unreliability rich for this project i'm not getting your country harnessing the last six months nights and weekends and cross my fingers it'll work out someday will build a crack go to the main reason for using bit coin training for our project is that it's going to lead
really well to the coming episodes the next episode after this one's gonna be on paper parameter selection and one after that we're finally give us our own reinforcement learning deep reinforce her wedding and the grain trading handles very special we in hyper parameter selection this next coming episode in theory intuitively highlights lot of the decisions that we can make about choosing network architecture which types of layers to use whether they're ellis t. m. or convolution all layers activation function
as with all the layers and things like this it more intuitively highlights these choices than many other scenarios in machine running do it the coin trading is very special and that makes these types of decisions of high printers very intuitive will see that next episode slow hard to understand right now another thing that makes bit coin trading special as a machine morning project is it's flexibility training can be successfully donna she morning either by unsupervised learning shoe
why is learning and reinforcement learning all three branches of machine learning you can apply to trading and you'll see in this episode that trading is a very simple concept sister surprising enough flexibility we have in the types of machinery paradigms we apply to the scenario see start to see this quite special another bit as you can use convolution o'neal lal works or elle est yeah i'm rick karr neural met works in your network architectures freer trading model which is very unusual pickling training is will see
the sexes a time series scenario the intuitive networked architectures and l. s t m work for no man or when you're not restricted seizing that in fact many people had more success spinning the situation as accomplish on your network scenario such training is a very unique scenario that one's welsh will lot of machine learning concept exploration and which will dovetail us eventually into deep reinforcement learning so to the project in it's own right i think it's something know what the devil get
gavin hey maybe we can make money off of but importantly for this contest is going to fit very nicely into the progression of episodes from here on out of this episode is going to describe trading day trading swing trading at is going to describe crypto currency and bit coin in all that stuff if you know about crypto currency if you yourself for a day trader skip this episode there's no machine learning in this episode we're not quite as sets of contents were machine when framework in the next episode
so few no crypto any no trading status episode but if you don't feel like you know both or either of those will not listen to this episode because a lot of the details are in a comment on explains in future episodes so this'll give us the ground work okay before we talk about treating we're gonna talk about crypto currency smudged two aspects to our project crypto currency and crypto trading now crypto currency and not so sure i need to really introduce i imagine most of my listeners
carved well acquainted with it's a super hot topic right now everyone's talking about bit coin our parents are asking us how to get involved in bit coin all that stuff but i didn't choose between nonetheless for diligence and they will talk about trading bit coin is it's height of crypto currency or just quit dealt some people call it a crypt joe currency is a currency just like the u. s. dollar is a currency the european euro is a currency the japanese yen except it's a universal currency right
and being paid to any specific country that the crypto currency is it said digital currency that's completely managed and generated the giuliani and crypto currency is not managed by any one person organization or group is managed distributed to all my years in the network in may for this thing bit coin mining or basically people have their computer riggs senate usually up the fiji pew senate very similar to a machine morning pc actually whether doing is they're right
a bunch of equations calculations coal mining it's basically handling the infrastructure of the currencies existence processing transactions with the currency making sure new bit coins are released into the world in a proper time we weigh in the same way that the federal reserve candles distribution of the u. s. dollar so's to strip and with a big network of people ends all transactions purchased with crypto currencies our report on today's shredded ledger
home run record celestial of information you've never heard of kurt occurred to them either the whirlwind the crux of it is the crypto currency is a currency just like the u. s. dollar except that it's management is super secure super permanent and completely democratize completely managed and owned by people it's a very special thing indeed it's been a lot of publicity these days for very good reason it offers a lot of value into the financial market place in office lot of competition
his value over banks and over governmental overseeing of country currencies say to you may have heard this thing people say old crypto currency is a big ball because there's nothing backing it on like gold which has a physical object backing it was not true with backing crypto currency is the value that that's high security in on temporal and permanent histories transactions is called ledger and democratic management all things the real currencies laughed so the something's up stay
so backing crypto currencies that coin is one of many crypto precedes the coin would be like the u. s. dollar for example there are many other critter currency is what's called a fury among others call like coin and others called rebel they all have symbols bit coin is b. t. c. just like the u. s. dollars u. s. d. the syrians the t. h. all the coins the most popular because it was the first of the crypto currencies it was the flagship of the concept of crypto currency built on this
underlying technology called block chain all those benefits i'd mentioned previously about distributed permit ledger security blah blah blah that's all built into the technology called block chain and on top of watching we have crippled currencies and one of those could occur in seizes bit coin and dick when was the first and therefore is the most popular is not necessarily the best differ crypto currencies have different sort of steel billy profiles and additional features so for example if you're in has this thing call
the smart contracts which actually like you sign a contract digitally with somebody and then based on some of them to execute that contract with your contractor working for a company you might use the fury him to sign and executed contract and i are locked into a contract if you decide to break that contract whether you leave the company earlier do something like that that penalty can be enforced program at levi that contract is so the combination of smart contracts that should permit ledger and always
things baby does lot opportunity uses technology to reduce legal costs an investigation mitigation the government uses to automate taxes at the end of the year and all that stuff crippled currencies were really cool stuff all our relish for potential of future with this technology now we're interested for our purposes as a trading vehicle day trading swing trading discuss that he might've heard before we could trade stocks and bonds and for encouraged
he's in any of these other things that people have traditionally been trading on wall street the last many many decades we could do that that there little bit complicated get into getting into trading stocks can be quite a technical her old getting started you also have to sort of keep your ion which stocks you want to trade sell stocks and bonds us out that manual too complicated for project for us one thing we could trade is foreign exchange for ex f. bowl
or he asks for its james is very similar to crypto currency country currencies crypto currencies are very similar you can trade country currencies on a four exchange in the same way you can trade cooked up currencies on an exchange like g. dax which will get to it that foreign currencies go up and go down the ad and flow just like stocks to slip ons certain countries economies might be booming like china for example fast case maybe you should buy some chinese
red sea in exchange for your u. s. dollar if the chinese economy is burgeoning more than the u. s. economy and presumably you'd be making money on that purchased the value of the chinese currency the u. just purchased increases and at some point you can decide to sell that to your u s dollars back in exchange for the currency you own annual have made a profit because you sold after the value had increased so we could trade in foreign currency board and doing crypto currency is digital curry
see because of the twirling that's available the exchanges out there like g. jackson cracking have fantastic keep yeah it's very nature crypto currency being this new generation digital phenomenon means it has all the great digital tooling around it that makes working with it as an engineer very easy so the main reason we're in the trading in crypto currency compared to traditional instruments is just convenience by the way that we're instrument
is sherman refers to anything that you can trade so okay stock is an instrument a specific stocks so the apple stock is an instrument facebook stock is an instrument of foreign currency is an instrument the specific bond is an instrument equation instrument okay so we described crypto currency bit coin is the main crypto currency out there just by popularity were therefore going to be using a coin instead of your am rebel likely center just because it is more historical day
now available for bit coin all exchanges support a quaint etc bella stockholder more about treating this two words to be aware of investing and trading when you invest you buy an instrument or some amount of some instruments white stock in apple for example because you will leave in that insurance you believe that the value of apple is going to increase in the future investing it is putting money where your belief it's an any just leave
it does leave your money there the stock market goes up and it comes down and it goes up and calms down is like this ought to third graf can you ignore the graph you don't care about prices in the short term because all you care about is in the long term generally speaking if your theory is correct your investments value will increase trading is different traders believe that's the activity on that craft does indeed matter the idea goes like this in you may have heard of this phrase
buy low sell high let's say you buy ten dollars of stock in and then over some of time it doubles in value now your stock annapolis worth twenty dollars you bought ten it became twenty now let's say that apples value is starting to creep down wolf you jump out just in time you sell your stocks the u. own x. twenty dollars and less a apple stock crashes back down to ten dollars all what does happen you don't hold your vow
you put in ten came out with twenty you pablo use old high so the idea of trading is to look for these patterns in the graft and in what's called price actions of an instrument you look for these patterns can you buy any soule based on the patterns so that you can come away with more money than you put in the catch phrases by low and sell high but it's usually not that simple and the prom with that phrase really is how do you know what slowing was high how do you
no when you're at ole ole or if it's gonna keep going lower and how do you know when you're at a high or if it's gimmicky going higher so investors and traders are are fundamentally different people investors believe in the value of something they put their money in that stock or in that bit coin and they just leave it they close their eyes big deal would be stressed out by that wild graf they know that in ten years' time the walk away with more money than they put in traders on the other hand buy and sell based on patterns
the grass on a daily basis on a weekly basis on a monthly basis however much amount of treating you want to do if you do all of your training in the course of the day if you're just glued to the screen and you're buying selling and buying selling the colonnade day trader an important bit about a day trader is at the end of the day trader closes his or her positions in other words he takes all money out and put in his pocket as what his value to fluctuate overnight when he can
you're watching it so he manages all manually all the course of one day goes to sleep wakes up and starts over again generally a day trader at such a full time job there's lots of day traitors out there that that's all they do many of them a whole lot of money a day trader is probably what you have in mind when you think about stock markets about trading on wall street woeful walsh reconnaissance these guys in business suits glued to a screen black screen with green and read it fry
this chart so in numbers and his two grams on their monitor any other hold the the tickets in the air and they've got the visor something like that's the old style stock exchange trading now are things digitized course gasoline trading is anything outside of the tree gave you don't close your positions at the end of the day in and go to sleep with peace of mind if you continue to trade of the course of days or weeks or so minutes call swing trading they're very similar it's just bad day trading towns
the more back full-time person will work to build together six needed project is a swing schrader effective way because it can do trading overnight if you're so inclined but anyway it's gonna be a machine learning automated schrager it's gonna buy and sell based on patterns are recognizes in these graphs the chairs work on patterns in the graphs is the pattern is there a machine learning to take it up there for we should have machine learning be are automated day trader for ourselves
the weekend go to our full time job make extra money over there now can you buy a bit coin first place we go to these exchanges so called exchanges if you were to buy stocks you go to a stock exchange fury buy bonds you go to a bond exchange against him by foreign currency you go to a foreign exchange for x. and if you wanna buy bit coin you go to a crypt deluxe change to very popular exchange is what is called g. dax g. d. day x. subsidiary of aqaba
he called coin base and others called crack in there is no company in charge of the coin or any other crypto currency for that matter exchanges are basically just like bank accounts wells fargo and bank of america don't own the u. s. dollar other places where you can go in store your u. s. dollar crypto exchanges are places where you can go exchange or u. s. dollars for some amount bit coin now you hold some bit coin on an exchange and you can trade back
forth between your dollar and your bit coin t. x. is probably the most popular out there so that's what i recommend you using if you're in the u. s. crack in is a little less popular but it's also available to european citizens and others know what you're doing is still going she'd x. dot com daniel sign up any open the budget security information and you put in your bank wiring information than any transfer money from your bank into g. x. and now you'll have a certain amount of money in u. s. dollars in g. that
so now you can trade those u. s. dollars for bit coin what you'll see in front of you the black screen and some charts and some history grams is numbers a whole bunch little stuff is in these charts that you'll usually spot the patterns that as a day trader you act on the bill look at the screen is your terminal and based on activity in these charts you're going to decide whether to buy a coin or to sell bit coin or to hold still do anything by cell
all syria looking for patterns in the charts were these patterns come from become from what's called price actions they can imagine that the value that coin goes up and down up and down and age ross's long-range heart well was say we want to make the machine learning model around that would you do well you have to crime stats every times that you have a price the price of the queen at that time the price of the coin changes depending on how many people are buying or selling bit coin the value of the coin is and how many people
of purchased a coin more people by bit coin more valuable what is sick imagine all white shark and every time step is a price is so if you wanted to build a machine learning model here when you might do is use in ballast yam whitford neural network and are an end to remember is the neural network best design for tying series d. s and equate price histories indeed time series day at with one inputs that every time step being the pipes
let's say it's ten thousand dollars at times that one and it is ten thousand and two dollars at times that too in ten thousand three dollars at times that three and so on you my bullet alice t. m. r. n. n. where you hide in the history of some tiny slice let's say a hundred and fifty steps in the label the thing you're trying to protect us supervised learning scenario is the next price that you label so you can put all the time steps one to one fifty a new
training your model to be able to protect the very next time step times the hundred fifty one in this case better supervise learning situation bing bang boom we have a day trader easy easy that would be with the most basic way that you could envision this project and alice tim are then we feed in we few notes five steps one the input court finds that that's the price in your training it to predict the last times that betraying a tube
it's one time stepped into the future so if your training it to be a day trader you would predict one seconds into the future in our example where every time set is the second but then you as a developer would have to heart coded the rules on what to do next well if it predicts that the price is gonna go pop in the next train step wall baby we should make it by if it predicts that the price is going to go down again
next time step we should make it sell by low sell high right hands if the next price production is the same as the current price they should hold it should do nothing very good we have a supervise learning ellis t. m. o. r. n. n. model that will take him all the prices as strange stepson output the next time step production and then we have some custom code to decide what to do next physical problems here the first one is
it's not that simple with buy low sell high it may seem so at first but it really isn't this a lot of clever strategy is for working the system like you're playing chess the trading isn't that simple job in fact many people steady day after day after day training in sandbox environments and lose a lot of money in their early years is a traitor until we're really start to get the system right so it's really not so simple to buy low sell
either starched shirts and strategies is so that we just one of the reasons why we're discussing bit coin training as a pedagogical machine learning sneer you won first place which is that the problem can be posed either as a unsupervised learning situation i actually don't know how you do that but i've heard them before they supervise learning situation which i just described in which case you would have to program by hands the next step actions or a
reinforcement learning situation reinforcement learning takes to supervise learning to the next that reinforce hillary has incited it's package like it's stomach imagine this robot with an alice t. m. r. n. n. in it's stomach it has inside it's packaged supervise learning incentive package that has the capacity to estimate the next time step based on prior historical data and then in the outer shell
of reinforcement wearing the model learns to act he learns to take the correct action they can learn very sophisticated action sequences of actions and strategies and so forth so this is the situation in which we do you supervise learning for treating bought or to use reinforce the learning for treating bought in a car project the actual code is when to use the reinforcement working by way of a framework called cancer force i think ray
for swearing is a much better fit for automated trading unsupervised learning bubbles can be it's sour the first bomb was how we envision modeling our situation was that it may be old predict the next time step to some degree of accuracy using else tia marta and by that doesn't tell you what to do next and it's not as symbols by low so high after all so switching to a reinforcement learning model could mitigate that issue the second problem is that a single price
at a time step is usually not enough information to really build a pattern matching model is very unlikely to build a palace to him or a man that he really accurately predict next step prices just based on price alone one input is really insufficient for any machine running model so how can we get more inputs out of our situation more inputs for every time step was yours how at any given time any
number of trades could be occurring on an exchange any number of other people could be buying and selling bit coin to in other words high aim is infinitely divisible as far as treating is concerned the sounds confusing one china say is you're gonna have to decide how to partition you're trying steps or maybe maybe once every second once every two seconds once every minute every hour every day well if you decide
on the part fishing your time steps into one second bob gets well with him that one second slice of time and infinite amount of trades may have occurred now a high frequency trading algorithm they're called h. f. c high frequency trade these things will split time again to milliseconds nanoseconds in that case these algorithms could capture every single individual chew
rate they call you supporters the capture every single order on the order book as a high frequency training our room and there's no chance in hell our project is going to be old to catch every nanosecond slice of time so we're gonna take it out scale war is say maybe at twenty second buckets of time every twenty seconds renege go out to g. dax is a pianist and asking for informations on the trades that occurred during that time now what happens with air
retreat every time somebody buys the coin the valley goes out every time somebody cells bit coin value goes down in other words during the twenty second slice it time to the value of the coin has gone up and down up and down without us and many saatchi's graf within our little slice of time so what can we do maybe we can average all the prices but coin has been attacked during that chunk of time we could take the meaning of prices have something we can do it's job
really not recommended one downside to that is that outlier prices came very strongly affect the means and this may occur in a situation call pump and dump somebody on the exchange who may be has a billion dollars to their name for example they can put 'em million dollars into the coin they can buy a home million dollars worth of a coin on g. dax now the price has skyrocketed the value of that coin is now very high will buy
just traders may see this spike in price and they may think that we're entering a bottle and all very hurriedly say bye-bye bye ronnie aquanaut is so everybody will put all their money into the coin as well in our prices even more op and that initial person who put the million dollars in will now hold the million dollars out because the value has increased and he's selling in high it's a very deep move indeed but it is very common and very strongly effect
is that price averaging because the juices and out why are they can really mess up your clobber them to mess up humans to to avoid taking them enough prices in a train slice instead what we're gonna do is we're in a collectible inch information about that time slides were asked yet when i entered this time slice we enter this twenty second interval what was the price then that was the old tin price open price and what i accepted this time slice whoa
the price that was called a close open and close the all of this twenty second time slice was the highest price pick one ever reached that's called a high for example or million dollars scenario million dollars of the high level with global was the lowest bit coin sonnets times twice that's the law or logan high low close o. k. h. l. c. this method is sickening the prices was invented by the japanese they call the candlestick park
calling him but it worked fine slice they call it a candlestick the really going kill sake is at the convention it's like a box plot from statistics we have a vertical are the body and it has whiskers on top and amado the top whiskers behind the bottom whiskers below the conaway out wires in a box plot well the japanese invasion those as we explore channel and the body of the candlestick topple either be the open and close and
bottom will be the opposite depending on whether our candlestick is going up or going down by comparison to the prior candlestick if our current candlestick is going up if the price has increased in this plane sliced by comparison to the prior time slice them the bottom is the open and it's hot is the close of the body of the candlestick and the color of the candlestick is green you know 'cause it's good collar beams were going up
and if we're going down then that hop is the open and the bottom is the clothes and the color is red 'cause we're going down so that's a candlestick okay chelsea open high low close there's one more important number in that trunk time it's called volume how much was treated during this time so for example the fact that that jerk spent a million dollars pumping the price but only one per
and did that means that there will be low volume when the price is high the volume is low that's a good indicator that we got company temper but if the price is high and the volume is high that means many people are buying into the coin during this time slice during this candlestick and that's a good indicator that it actually is going up so open high low clothes and volume always shall see the those of a five no
i'm results seem very very commonly if you look on an exchange terminal the price actions the actual lying raffle price you realize is not actually allying graf it's a budget candlesticks those cables it'll be colored red and green depending on if we're going out we're going down and ohio whiskers on top and bottom and those are your open high low close numbers then below that the price graft you'll have a fist a gram of bars by correspond
it's every time step and those are your volume numbers so we start off with a price grappling graf him we had a problem that and that only a game of swine input for retired slice than we realize way out way out there's no such thing as a tougher in slices infinitely divisible and so we had actually decide how much of a graph we wanna split that so we can say split everyone second or every two seconds for every three seconds
the high frequency training auburn soulful it every nanosecond or sell or not that computational intensive sorus whatever one or two seconds well in so doing we just introduced to ourselves five new numbers that we use as inputs instead of the one that allowed us to get morgan puts out of that price graf bowl h. l. c. b. or candlesticks son are trading model start a little bit more sophisticated week we can envision it doesn't tell us
am i or n. n. that takes in five inputs every time step and either outputs the next price production or we can use reinforcement learning in that hideaway from us and it will just decide whether to buy sell or hold there's a few more input sweetie juice out of those candlesticks by the way we call these indicators in this constant cons of indicators out there very common was called the simple moving average and the idea is that looks back in time acts
some amount of time steps whenever you want say two hundred times steps or fifty times that's you specify the window and it comes up with a moving average hence the name which is the number indicating it generally it's sort of the direction of the graph of the price graf if it's positive there were growing up if it's native that we're going down it is very positive then we're going very far outfits very negative there were going very far down you get the idea is simple moving average or ass and a another one is called
exponential moving average another one is called up or pacifier or relish straight index another one is very bibles called the whacked volume weighted average price and there's a whole one to mothers buzzer probably the most popular specially for starters but i just listened but there's many many indicators and basically you could try to ease into your model as additional inputs if you want now because these indicators are some rising price history
scotch case could be made that says we're using in l. s. t. m. worker neural network which already sort of aggregates history over time they may be unnecessary is something that you'll wanna experiment on your own wife are right we have price actions that candlestick daily chelsea feel that i will close volume as fight features right then and i did every time step we gotcha we have in eighty years which aggregates time stamps prices they come up with additional members
maybe valuable for your model no one final input we could juice out of our price grass is this is called risk of arbitrage is very valuable risk arbitrage the r. p. i n. t. e. r. a g. e. it's very strange word arbitrage canadian goes like this she jack's dot com is probably the most popular crypto exchange in america i'd say spanning the english speaking countries and general
will focus on g. x. and crack and crack in the little less popular than she acted spans more than just the united states the european and u. k. countries can use that exchange but is less popular west popular than she backs of yours why that's important is a phenomenon and finance called the efficient market hypothesis in the idea those that the price of any instrument stock or bonds is exactly what should be the price of a measurement is what should be it's fair
are they caught in other words the price of the coin that like eleven thousand dollars what is today is exactly what should be should be eleven thousand dollars now the idea of what it should be quote unquote is that for the large numbers of people who are involved on the exchange on gee that's all large amount of people are involved in doing what's called fundamental analysis the researching the value of the coin in watching the news keeping i am only other car
seizes cetera fundamental analysis is c x yet chewing your research on a company on the stock on a crypt of currency any instruments doing your research called fundamental analysis and deciding to buy and sell based on that kind of the investor friend she well what's so many people performing fundamental analysis and so many high frequency training algorithms performing technical analysis of what we've been spreading in this episode stadium following charts following numbers and patterns between those
to the fundamental analysts and tactical analysts the price of a coin is going to sort of start to waver teens the middle of what it should be the actual value of the coin the actual value of a bit coin to humanity you know eleven thousand dollars a day according to the efficient market hypothesis is the real value the bit coin holds to humankind's the salon beef with the efficient market hypothesis day traders obviously contend with a
in this whole thing where if that should get made sure it sufficient after the analysts have done their job but the analysts can still be a faraway doing their job that and making some money a process blah blah blah but the point is in order for the efficient market hypothesis for work out is you have to how long it would set play had to have a lot of people leveling out the price was more people on g. x. and less people on crack it now let's say that she acts is price for it
coin is eleven thousand dollars in crack ins is ten thousand dollars different prices and it's actually occurs a lot more people and she acts less people want crack in the fish market hypothesis is driven by numbers that means g. x. is more correct then crack it's that means by in crack in because it's going to correct itself the gun of trying to get the price up to the real price spiegel
one thousand dollars that's an oversimplification there's no conscious effort in this process on the crack in exchange with people trading over there is sort of like this just natural equilibrium that occurs on financial changes so that's another input we can hide in that any kind step is all of the holy chelsea b. plus indicators coming from g. x. pipe that end but also the same day that firm
crack in tight bowl fix changes price actions and indicators in at the same time and it could help him all decide whether to buy on crack in 'cause it might catch up to g. x. sprees like half call so that's it for trading in crypto currency now i've already started the code for this project is six months old line of it's actually pretty close to working it's using the brave for smuggling by way of tension force with actually convoys
no no corks not else to him or an ensign will discuss the reasoning for that next episode at all postal linked to this open-source project on my website boasted of the law calm for slashed by cast for slash she learning so next time will discuss paper parameter selection for models in them will finally get to be reinforced for learning the cool stuff artificial intelligence r. n. c. in exam


welcome back to machine learning that i'm your host tyler not penalty teaches the fundamentals of the machine running an artificial intelligence the covers of intuition models math languages for him works and what were your father machine learning the resources to provide the trees by provide the forest that visual is the best primary learning modality the audio isn't ringside wanted during exercise commute in shorts and consider him out here syllabus with highly curator resources for you
episodes details and eaux c develop dot com forward slash n. algae it time also starting a new contacts which could use your support and it's called last year's like tax and teaches conductivity focused tips and tricks some which could prove beneficial in your machine learning education jury finds that atlas c develop dot com forward slash l. l. h. this is episode twenty seven hyper parameters par won today already talked about hyper parameters
this could be a two part episode got a little bit longer than i thought i would bless every n. or hyper parameters was talked about hyper planners before by comparison to parameters to review the human decides on planners are the numbers that machine learning model learns is learning process so in linear logistic regression your fate of parameters are these waits in front of coefficients to these numbers that model learns parameters are the bit that the machine learning all words
hyper parameters are any sort of knobs and dials the u. as the human are in control of so there's some obvious cases of hyper parameter selection for example with regular station that will talk about the next episode selection of dole won l. two and dropouts both the numerical values that you can assigned to this realization terms or even than your use of those regulars asian parameters that cyber selection is something the u. is a human shoes in neural network are
it's capture the number of neons in any layer and a number of players over hyper parameters for something a huge shoes hyper primers are they really mean anything human chooses so let's get a little bit less intuitive the selection of what type of model to use in a machine learning scenario three guineas linear regression logistic regression the guineas naive days or neural my work that's hyper parameter it doesn't seem like a hyper brammer first glance but really any sort of do
seizure in the u. is the human can make that affects the machine learning model that the hyper parameter is really only a useful characteristic when you can compare the selection of hyper parameters so like i said you could choose linear regression model or you can shoes and neural network that selection raiders hyper parameter is a useful hyper perimeter because you can compare the performance using cross elevation of linear regression to the neural network they're not apples and oranges
the result was a numerical score representing the realm of performance of one versus the other now really talking about a lot of theories hyper parameters nice two episodes things like neural network architecture but activation functions nonlinear he's regular is station parameters stick have steak grievous and optimize years like that a grad and adam things like feature stealing in bash normalization stuff like this this episode is gonna be some of the more high level parts things like neural network architecture
seizure of what types of ways to use like alice timber says cnn layers and unacceptable be smaller bits like regular is a chanel one al toon drop out but don't let that big versus small part fool you every single hyper parameter can be vital to the success of all miles training as an anecdote in the bit coin trading bought project that we're working on together i have personally found that the combination of no-one else you and drop
look here regulars asian currency you can use one or two or three of them and whichever one's you used to have some numerical value i found those rigors asian terms to be more vital to the success of the deep reinforcement learning agents they and the selection of the agent hype that will talk about reinforce learning episode things like proximate cause the optimization versus de que network elbows or huge that the difference between a pee pee owner deeds un as a huge difference but found
the combination reposition terms how strong are the facts on the outlet to every hyper premier counts in what a constant shooting hyper parameters every punishing learning moll has a handful had rumors that goes along with it so neural network for example you can choose it's wet it's depth the types of layers whether they be ellis deal where's convoy years or dance lawyers tell what else you in dropout regulars asian a handful of other hyper parameters but most
ever parameters have the same d. fall that you can start with so for example no-one else you they tend to be in the point zero zero one weighing somewhere around there tends to be a sweet spot for many researchers just getting started so you do is you start with the same d. faults let's say apel won all to set point zero zero one met death at one or two met with two let's say a neuron something like this and then from there after you've selected you're saying d.
faults of hyper parameters then you search for better hyper premier combinations to use something called grid search or random search which all described the next episode or more learning oriented approach colby xena optimization which uses the scene statistics to actually hone in on better and better hyper premier combinations overtime tesla high level stuff ever parameters are knobs to use the human turn don't
shirk the turning of the snobs because it is vital to the success of your model screening process in the way you go about this is you use all the fault out a box and from there you use grid search random searcher be seen optimization to get better and better and better accommodations over time now when i first found out about hyper parameters by comparison to parameters that looks kind of ugly i was kind of surprised about the gold machine morning was to learn everything i thought a machine going models the postal
our nuts and bolts especially was some talk about am i thought that ai supposed to be super self sufficient what we use humans have to turn these dials non citizens seem very magical seen the buzz kill to me in fact it seems like there's more hyper parameters then there are parameters can anyone machine learning motto is a human does more than a mile creek on magical to me was the deal here well you and i are not the only people thinking this removing hyper parameters from the equator
it has been a long standing goal of the machine learning community right now we sort of how linux boxes we have to compile everything from scratching you have to add your own packages with certain flags space center seek you architecture and all the stuff very complicated and very hands-on the goal we wanna get to eventually some ask you just for a man comes a box you open up and it's good to go but it's just not that easy you see this is the goal of researchers is to sit sit
i'm the hyper parameters into the machine learning models that'd be calm parameters of them she learning model can learn everything for master bolts this is difficult and it will take time but it's something that we have been accomplishing with every machine running breakthrough so for example the sooner the neural network the neural network introduced to very powerful breakthroughs to the machine running community one was the ability to represent any complex situation the caller
the universal function approximate are theoretically the neural network if done properly can basically do anything with any uses theoretically you could use in your own outward for everything forget the support vector machine forget logistic regression forget the selection of any model under the sun and just using neural network know like that said before selecting a model is itself a hyper parameter and therefore the creation of
neural network t. away with a very major hyper parameter selecting a model now of course as we've seen entire episode is not so simple what times your circumstances call for a shallow model whether it be for cocktail shellfish and sea or you don't have enough data to learn from your special case may call for shall learning model but increasingly we're getting to this point horses like throwing your old mauer get the situation it's his idea of sure you can use of this
or rifle why not just use a bazooka we've got the jeep use we've got two o'clock platform just use in your lab work neural networks also eliminated the very taxing hyper parameter called feature selection or feature engineering in the shallow learning days you need to be very selective about what features are going to input into your model in certain circumstances feature is better very highly correlated can mess up certain models
performance is so you want to hand removed any features that are highly correlated additionally you may need to steal features down and in case you have too many features you can do one of two things you can either hand removed specific features that you know or bad important or you can hide them for room a dimension ellie reduction malek principal component analysis p. c. a bid for typing it into your lyric ration mahler
decision tree so we have to hyper parameters there one is the feature engineering slash feature selection bit in the other is the decision to use p. c. a in this part of the equation the very time intensive process is not just some dial you turn like to tell one regulars asian parameter know besides the mitigation days to work with all neural networks few radically handle featuring cheering for you in the early layers of the neural network to mention ellie reduction
automatically occurs as long as you're layer width is less than the number of inputs hands in your arms in your own work learn to latch onto important bits of information and disregard less important bits of information so neural networks effectively eliminated feature engineering and model selection now obviously that some major over simplification and ruffling the feathers here i'm sure we'll get the point i'm getting at is over time these advance
and spin machine learning technology they do exactly that the remover hyper parameters and simpson them into the machine learning model as parameters so in other words over time ideally we will have to be dealing with so many hyper parameters and in fact google is so hot on this topic of automatic hyper brammer selection the credit project called on oh can i tell the concept of learning about the deal hyper parameters for your machine morning miles
cold metal learning right because you're learning how better to learn you learning the parameters that help you learn your parameters medal learning and gould has a project called on l. m. l. that they're working on very hard to solve this team points but in the meantime u. n. need we lowly developers are just enough to suffer with haber parameter selection and to me all right towards a top down approach restore from the very top of model selection
then once we get to neural networks restored to design layer architecture and then with in the layers were going to start to work on things like activation functions and regulars asian terms and stuff like this top-down all-star with mile selection you recall from the shallow learning episodes in the resources section i had a decision sri diagram that help you pick your machine learning model based on your situation is a classification or is it regression is a supervisor is
as supervise i will leave it to you to go back to that diagram that's kind of a very fine tune the model selection process on that paint with some very broad strokes here sort of that hot dogs in the shallow learning machine running models sold for soft are we dealing with unsupervised learning or supervise learning if you don't and surprise to good a totally different path of the less common sort of machine running scenario is honest and say hey k. means clustering was caught that for now moving on assuming you're using supervise learning it
assuming that you actually have the that labels to train on now or nascar solve some questions is the situation linear best first important question is your circumstance lean years the fingers heights learned linear equation honey no fits linear you can sort of think about it so for example selecting hyper parameters that isn't nonlinear situation and here is why many parameters play with each other in a specific way so for example warning rates a
and the pox or optimization steps the number of times you train on the specific batch learning rate and the pox us to play together specifically generally you want higher learning rate was lower the pox and vice versa okay so they play together important leaders for the connaught is a fresh hold it which lower learning rate or higher learning rate isn't going to help you no matter what the peacocks feathers kind of a cliff or maybe it's not
cliff may be of some something of a rabble occur anyway the point being this is not all in your situation when a situations are cases in which you can plot your data on line you know all of your data points problem on your graf an alternate scattered generally around the line or a hyperplasia dealing in more than two dimensions if you situation is linear these linear regression were logistic regression linear regression for regression to help her number like picasso house logistic regression for
suffocation deciding if it's a cat doctor tree if you don't know if situation is linear the general fahmy is given a shock trial in your religious aggression gunshot c. out as if it is nonlinear but you don't have a lot the lockup dana okay because the avalon of data binge probably just move on to the neural will work in but generally speaking lots of diego to deepen if you don't have a lot of data that there were no work with naive days decision trees and all the decision tree off
it's without rain forests gradient boosting extreme green boosting x g g that soup pot louise days which when you use try 'em all as you'll see with every however parameter the name again this trial mall any compare other relative performance of one malts the next by way of something called cross validation which will get to in the bit about grid search and ran and search so try we're aggression try logistic regression naive days decision trees ran a fourth
gradient boosting a tree maria boosted by the way ran a forests is like decision trees plus plus the better decision tree ingredient was saying is like decision tree plus plus plus it's like an even better decision tree and extreme gray and blues singer x g is even more better than all at so actually hear him decide to go the decision tree route you do yourself a favor by also trying ran for three and losing an extreme very interesting 'cause in theory all those are just barrel
mm decision trees anyway my advice if you're not be used neural network use gradient boosting that's my same d. fall by you that box i don't know what to do i generally start with gradient boosting which again is a spin off of decision tree but with a lot of optimization c. r. right now let's assume you have lots of date and you can enter the deep learning territory now regard to the blood or in this episode on network architecture and design network architecture
network designed to the first very high level decision you can make in your neural network architecture is what types of waiters are you going to use in other words which had a neural network business in the first place you really have the main three decisions to choose from l. s. d. m. r. n. n.'s consonants and mall file your perception on this is plenty of other networked architectures out there to set up web page called the neural networks do when you can look at images
descriptions of different types of neural networks like belief nets an ongoing quarter's those two for example are actually very popular author neural network architectures the size of freedom i just listed and then there's a whole bunch more of those are more advanced topics indeed learning to the very common architectures our c. n. n. r. n. n. and m. l. p. and they have very obvious use cases redoing vision soft calm lucille new network c. n. n. beijing trying stuff l. s. d. m. r. and review
in other penalty so let's think of a few examples for these situations you generally use accomplish only only work if you doing visions office you're looking at pictures you're playing a video game if you're building itself driving car division is of all you use a cnn if time is involved you doing stock trading if you're doing whether production or natural language processing you'll use in l. s. t. amar n. n. a record neural network anyway you'll have to necessarily zealous t. and t. can use of vanilla recurring yeoman work week
news of g. r. u. recurred yeoman work but the most popular that same d. fault is alice tim cells in our n. n. for time series date and for everything else he'll use 'em all tellier perception on the space billing throwing all your inputs in a blender doesn't have time it doesn't have space then it doesn't have anything see frodo blender and they'll be okay then how about or pickling training baht will die just said stocks is a time series phenomenon so well
t. m. are immense actually we're using a convolution all neural network in a market how project not an l. s. t. m. or n. n. what's nice is c. n. n. supervision and you said specifically that stock is high series entire series is else am i did indeed but this is one of those cases we start with the d. fault and i did indeed start with alice t. m.'s and then you experiment with alternatives using grid search ran and serge be easier not to ms
shit and i found through experimentation that cnn swerved to better hands after reading a handful of other papers out there on archive dot org and takeda collies were doing our rhythmic treating you would hear that many people out there greed or have had the same experience that confidence i'll perform else t. m.'s for al rhythmic trading why in mind that the i actually don't know the reason that surprises you or i. files tim's would shine here i think
the reason is something i've read as well is vanishing in exploding radiance remember from the n. l. p. episodes i said that the impetus for the invention of the l. s t m cell was that work for neural networks had this thing called vanishing in exploding gradient problem da da those that you've taken frames steps in its kind series dataset in order to predict something maybe the next time sat or some value will have too many times that you basically all
overwhelm demure on spin your neural network you sort of piling on information after information after information anything causes exploding gradient they call it were you saturate your neural it's it's kind of like yelling too loudly in these neurons exploding there's blood everywhere it's a horrible problem or the opposite to the case we don't have enough signal going back through time steps in the training process because the signal gets dampened indebted over time that's called a vanishing breed in problem in the way we made
it is was replaced the way you lou activation function or colonel minorities pluck that were you activation function out in your own and replaced it with outsell the l. s t m sell long short term memory cell which has a whole bunch of complex architecture within it's not just some activation functions like we plucked out this little marble when you're on the power of the network through way that we popped in this they would pop in this complex watch looking konzi
and yours block and we sat in that place at the palace to nothing has the capacity to learn to forget time sequences overtime in latch on the specific time sequences overtime which allows it to mitigate the banishing exploding gradient problem blacks blacks you only really see that showcased in medial ain't crying sequences like a sentence mean what's the maximum number of words and senseless a fifty a hundred words that's okay that's fine k.
no no problem but can a candle in the infinite sequence of steps will be quite price history is an infinite sequence of stat semi we go away back to two thousand nine in second intervals always home now and in millions and millions of steps at a huge going so goes off into the future to infinity so can and l. s t m are then really handle that many steps i don't know actually personally but i think that's the running theory as to why alice tim's to work on so
prices is that maybe they actually can't handle the fin soup kitchens were very very very large sequences there's always the dinner round that i've heard about stopping training for time step to a maximum window link in the past but did they get spree harry so what about cars natalie make that work for stock prices were but we're back when prices while the human goes to a exchanged terminal on the computer like g. x. dot com they got black screen and they have a price graf
it goes up and down up and it's made out of candlesticks that are colored red and green and under the price graff is if mr graham of all you will last visual you can imagine actually taking a screen shot of g. dax wants every second at saving it's your computer and running that for your comment to train on now that's an intuitive approach but clear perot just actually use the same day you're using for for alex to am namely camels the daily chelsea the air will do is
bill construct the time window with it x. axis is your time just like you would be in your exchange terminal the x. axis this crime stats ends this the axis actually the channels or death of your picture it's basically like that or g. b. channels what would be the orgy be channels of a normal picture you actually use your features they are so the depth of your picture is your candlestick and then the height of the picture is is nothing that says
this is one so it's like to be the shimmering model is visually looking at a time window of christ the in order to determine whether buy sell hold okay so that's deciding on what type of layers to use mauer it talked about the shape of your neural network later with an number of layers and where you might play c.'s alice yeah more conflicts by the way most yoga works we'll have some amount of dense layers even if they are incontinent
most other layers bark han's players may be in the very end you'll still have one or two dance players and if you're nellis c. n. new may still have moved some amount of densely years before they'll ask him where is an award after them and then slayer remember is just a vanilla neural network layer is called dance because all neuroscience from the prior layer connects to all neurons of the current later everything finex every which way so as gets okay
neural network shape there is a saying the faults that that's recommended you to find out there on stack overflow i've seen this quote it comes from a textbook and ono which textbook but see no word over ennis says the same d. fall for network shape goes like this have you can put where he's not really aware but the color layers the dispute is happier pinpoints the heavier out we're in that's usually does anyone you're on the classification or regression situation in the same old psych class classic
a sin situation bill last layer is a soft max new on soft max which will talk about in the activation functions bit see every pussy outlets know what goes in between word in layers that's the thing that you really care about was saying the fall of number of layers is one one hand where they need to be theo know anything else about your situation you just have the gas shot in the dark one or two layers star with one hand give to a shot in your hyper perimeter search how bout with whelan
we're in your own spin your one layer the same diesel would be the meanest love the number of inputs and the number of outputs soviet backed three inputs and wanted out what you one hand layer should have to your odds demeanor of your input center outlets are just get smaller basically performing to michelle de reduction on your inputs that the same d. fall some networks call for much larger structure i mean five twelve new ron's four layers deep
it's only depends on the situation but you know anything at all go with the water two layers and be with inner layers being the mien of your input in outlets by all la times you can actually think about a week into it the shape of your neural network so remember it our episode i talked about using your own network to detect off base at ours actually describing a mall tyler perceptual to to detective face what you usually uses of consonants lesko of the mold eyewear
perceptual example this because it it allows us to think a little clearer about this yeah a picture of a face by tickled by five pixels case of twenty five pixels total it's an image on your hard drive that's your input where is your pixels now you might reconstruct the face by combining black dots in your pixels into line is lines and curves and edges and angles how many of these types of things could you imagine i'll essay last say six six types of kerr
i'm sun angles lines so your first layer in the neural network will be six new rots next will combine those lines in edges and stuffed into shapes eyes ears mouth and nose for ice years now those are for neurons is going to detect for types of objects in the next layer and finally the last layer is one neuron to combine all the i's years mouth and nose into face that one me
on being faced detector so twenty five pixels boil down into six angles and edges and curves in lines those boil down into four objects spies years mouth and nose and those boil down into wanna object the face so you can actually think your way through the design of the neural network architecture it's not so old black box as people make it out to be necessarily sometimes it is sometimes you have no clue how things work come by
aim higher to play like that and so you go with the same d. fall in use hyper perimeter search to try to find better and better networked architectures from there in this case with face detector you would indeed was still use hyper perimeter search to find a better architecture over time 'cause you're almost always in the wrong the first time this is something reasonable to start with our bit coin trading by well we have only chelsea open high low close volume feels that that's our features for ty
is that now like i said i'm actually using a consonant in the code but i think it'll be easier in clearer to explain it as an l. s t m arnett song and do it that way each times that takes in a candlestick five features those your input in the outlet word essay is the decision to buy or sell the only exists in reinforcement loring moll unsupervised learning malo would basically be predicting the next price action and then from there you decide whether buyer cell
so let's assume we're using the reinforcement learning scenario we have inside are reinforcing learning asian else to model where the inputs are five features a kill sick in the top what is one you're on the decision to buy or sell which incidentally is gonna be at hand at each activation function of talk about that next up so what was in the middle we definitely want and l. s t m layer right because it's kind series we want a layer of it's building up information over time about how lucy
graff is acting in order that can predict the price at the next time step so theoretically we just need one else him where and according to the idea of the d. full width is the mean of the inputs in the output do with the bureau austen layers three three osceola cells is single layer bing bang boom malice play with the slow but we have five features coming from one exchange which we wanted to do that arbitrage thing that risk arbitrage thing i mentioned last episode that you have to
it features a camel sick from g. x. and a candlestick for crack and i have ten features coming in let's nice usually more data is better more features a better option to a certain point it tends to be sort of this ceiling we start to reach was called the curse of dimension out yet too much information now i don't think tennis too much information the last pretended is what could you do leave he added dense layer above your palace kiam way or as the first layer at a new layer
between the inputs n. your palace tim we're incidents where was it do it boils down the inputs into their essence well we make the width of this den slayer for we boil down ten inputs to four and then we send those off the l. s. d. m. weirdest are doing the history crunching they do we don't wanna overwhelm at one ascended the assets will let the den slayer boil the essence very nice very nice to have a dance where the top performing demand shelly reduction on the
it puts in passing it off to the l. s. d. m. where which is performing historical analysis on that time steps now remember entire episode i mention these things called indicators we have these numbers they can summarize some amount of time steps in the past was a two hundred times sets in the past till now you might want to take the simple moving average s. and may be exponential looming average dna the relative strength index far as i estimate for example wrappers
ends the moving average did the direction as a number from two hundred times steps in the past so managed to basically think of it like that like became call from then till now it's positive if we're going up it's negative going down now we can use a third party library called technical analysis library to khalid the t. a hyphen lip and we are in our project actually we hear is that projects to generate these numbers given the time window and pipe those in his input to talk
that is a problem that the problem is we'll have to decide which technical indicators use which indicators we were used as president thompson cons of them hands for each of those indicators what is the time horizon two hundred stepson hundred steps a thousand steps so those are just add additional hyper parameters wants your plates no will excite her parameters now normally more day as better in fact we are experimenting with khalid in
the project the present pulling it here is because we have and l. s t m layer and palestinian we're already learns aggregated information about your historical data it's already doing this ruling process is building up its own theory about what's going on through the time stamps in other words it's kind of building its own technical indicators so why don't we just let l. s t m automatically learn technical indicators rather than piping them in arts
also they can learn whatever sequence of time steps it likes what did the two hundred or four hundred and eight learned which specific in eight years in it's own little intervals are valuable for its projections in other words it's going to automatically were an estimate or the dna depending on what proves useful to the l. s. t. m. so what we do here well we here at a. neither knew ron in the lowest him we're for every technical indicators we want
the layer to learn so let's just grab bag ten indicators coming next a decent number of indicators that you might use in a typical i'll go trading bought ten indicators let's have our calloused him where learned ten indicators all wanted some solace and tenure on studio steel we're very close about allows us to reduce the amount of hyper parameters we had to deal with dry we have a decent model here for trading now are trained not if we spend it as a reinforce that learning our room which is in code is going to give a
the decision is going to give us what's called a signal the amount it wants us to buy or the amount it wants us to sell or zero for hold there's something missing from this equation though it's telling us to buy and sell based on price history but doesn't know how much money we have to or name that buy and sell where it doesn't know how much u. s. dollars we have been g. x. that we can buy with it doesn't know how much bit coin we haven't she acts to selwyn
now we could just in our code saying by only what we can afford based on our box suggestion but ideally the baht will suggest a price to buy and sell based on what you have that way if you don't have as much you'll say will you can still buy a little now a benefit so ideally your balances would also be an input now you don't wanna at your balance is as an input at the top why because in your
alan says get mixed in with the history it's like you from ru we're balances your bit coin balance and your dollars balance it's hot in this in this palace tim blender and now you have your doses splattered all over the graph all over this price grafted it's built up for us all for technical indicators of staff he had your balance is all over the history should be on affected by your balance the balance only pertains to now the present moment so what can we do we can actually pipetter balances
as the inputs to the final muir on the final later in the neural network you can't hide in the net at any point in the network doesn't actually have to happen at the very beginning to happen at any later he became pipe in additional features in you'll see this lot without image captioning where one for the network is handling image recognition another part of the network is handling natural image processing and then they connect with each other down stream so narco what we do here
this week that the present day that this stationery hearing now dana downstream in the network you know later later pass the time series all where layers being alice t. m. sir complainers focus the point there was you can imagine building the networks shape in selecting the types of layers all by yourself an intuitive fashion even though neural network is a theoretical black box you can shape even malden with your hands
the someone you know about the situation and then use hyper perimeter search search for a more optimal number of layers and width of layers billet suck out all of the more time actually sana jump into activation functions nonlinear eddie's besides spectator gets you in the next episode activation functions and bring your honor in the thin layers of the neural network have applied to them and activation function activation function or no
on linear eat these are things like sycapip install --upgrade pip setuptools
more ada chan h. raya lu ann like now and activation function is the function that you apply after the weeded som every neural inside of it is basically all linear regression units is just awaited some of the prior i'm your odds so learns these waves where is the fate of parameters just like a linear regression to multiply coefficients by for every muir on a prior where does it
so when you're aggression unit and then you'll rap that linear regression unit in activation function in a non linear function something like us a moya were ten a jury will know why do we do that why we have to wrap it in something when we just have a bunch of linear regression units all connected to each other and call the neural network of the reason is that linear functions when combines they make another linear function in other words if we did that if everything you're on with lyric rushing you
at our result would be a linear machine learning model so to only learn linear situations like the price of house it could learn something so complex is a face detector why is that this is strange if he had a bunch of lines me from all the piece of paper you don't have one big law aimed yeah something comes or looking whittling soros slice every which direction on a tapestry a new deal look how about the d. n. the parts of course learning part of the machine learning process will learn how to slice
are to partition things into their own colleagues rights is no way you'd get by combining multiple we're functions know you would get a big lie you would actually get another line that sort of the average of all these little lines it's very interesting is no you'd expect so you have to transform these neurons and into nonlinear versions of themselves so that you don't just end up with a big line so how do we do that we applied on
nonlinear function to it we played it through something that adds weevils any wiggle a wee little all that matters is that we we hold your breath because when you shake up one to wilson from onto a piece of paper you do have wheels that slice of paper like zorro zorro with curves curvy zorro you don't get one giant wiggle your budget criss crossing we goals subway we make our neural network nonlinear which is super super born that's all pointed neural network is it's you
versatile nonlinear function proxmire is that we just had wheels we just take the lawyer is there pal put that every neuron and we bend them into it for us now the classic curve that we've seen before it is the single way function that s. curve is impasse between zero and wants us to see that's autograph between zero one an alternative version of that is it's gannett h. t. a n. e. h. th function which is in pasture
so it is signori function just like that logistic aggression unit black goes between native won and won so that to haul ass to reapply either of these goodies a signal in activation function or giza th activation function with what should we use well we should actually used him h. actually don't know why a signal when it isn't used very commonly annual awards and must be something with the math but c. n. h. is much preferred as activation function in your own melor a kind of makes sense for training
i because every near on an their work is taking in with its own two cents as to whether to buy or sell right to an aged goes from native one to one in other words by some amount by some multiplication up between zero and positive one or sell some amount sell some multiplication between zero and made of one so each new ironic is sending their own buy and sell signals they all have their own lil
opinion and in the final muir on sort collects all the votes toys all boats and says you know what i think the majority is raising their hand that we should buy satanic sometimes makes sense i don't get exactly why you can transform a signal a into the equivalent of o. myself action but for every see you still see lots ignored used within your own math works why you see a lot more common than signal is or ten age is something called ray lou rectified linear unit are you
no you are really so we're one we lose it in only neary he's not lying in that wasn't born two boys almost ally it's a lie and that's why he full zero four x. equals zero to native affinities and then this angle going to the top flight so it's kind of like a hockey stick were of the babysitter take over god doesn't seem very useful into would've we come in i can't think of how you'd sort of hold myself sick
was that lawyer come up with any other sort of been into it application of this activation sunshine are really understand why it works but for whatever reason it's computation lee more efficient on you can cure it's faster to run something about populace works more efficient on this function and importantly the negates the vanishing in exploring gradient problem it turns out the vanishing in exploding great calm is not only with r. n. n.'s is with a
a deep networks and cons next specifically tend to be very very very deep bee stings like resonating google met stuff they're very deep many convolution layers and so we use ray wu activation functions in those layers to mitigate the vanishing exploding graham papa i don't understand why i don't know why it may be sat problem but it's it's something about the math something about calculus and therefore or you'll see aurelio activation
function used much more commonly than anything else is the same d. faults that comes out of the box with a neural network one of those things where id fault your dance layers in your kong's layers you saw ray will bite the fall and then you'll use hibor surged to try to an age and see if it performs better than raven there's a handful of other types of ray what is already live family is something called bullied you re loop that you will acts
and then shoulder your unit the l. u. days see lucas the alley you there's c. or yellow you korea will always different types of rae lewis they they kind of baby look slightly different they kind of had that hockey stick shaped some emperors the edge off of the hockey stick elbow and some of them don't stay at white people zero towards native if anybody actually curve down hold it to the bottom left port
they didn't say they'll have little different twists they're kind of more modern versions of re woo the researchers are trying a whole name on that the perfect way wu is another thing where you might wanna try different versions of re woo with hyper perimeter search tried you in siu in korea and leaky ray what all those things there's also a thing put out by google in their grand quest to eliminate hyper parameters called the swish estelle you i ask each so
shh activation function is actually a you'll are noble activation functional learns what's the best activation function for your neural network during the training process which shall be so sweet to be really nice not to have to choose and activation functioning a hyper parameters cell signal a soft max ray wu and the relive family and generally ray lose used by the fall trial mall using hyper search if find what works best for your neural net
or for cars in my experience in our bit coin schrader gets ten h. and those of the activation functions for the thin layers though the activation functions for the near the internal new islands communicating with each other till last miron rubble last layer of your neural network is the output layers that output function is the thing that can give you the human something of value so if it's a classification scenario we will use the same way function actually is
this time we can use the same weight is because it gives a classification see you the human that when that signal would function is valuable you could use it to an age where using it to an asian our situation is a good buy or sell signal you could use nothing is one case were you can not have been activation function what would that be as regression without an activation functional wrapping up your waited som when you get is the way that song that's the l. put that a number
now might be the cost of a house for example so no activation function for aggression scenarios and then it for malt psych class classification dark hack tree you use a soft masks soft max that same old psych class version of a signal which function and don't always just as the output layer of your neural network you would have a soft max inside of your neural networks in layers awesome guy so on episode i apologize for calling
all the resources i've listen up until now was wearing all this information from seoul is nothing new to put the resources section and next time will talk about ray there is a shin optimize years future scaling naturalization is finally paper search of how to do that with creatures such document
 Welcome to the first episode of Machine Learning Guide or MLG, which is a podcast structured as an audio course whose intent is to teach you the high level principles of machine learning and artificial intelligence. In this podcast, I will provide you a bird's eye overview of the fundamental concepts in machine learning. This includes things like models and algorithms, both shallow learning models and deep learning models, shallow learning machine learning models.
 include things like linear and logistic regression, naive bays, and decision trees, which as a machine-learning newbie you may not have heard of, but I will also cover deep learning models, which I'm sure you have heard of, things like neural networks, convolutional neural networks, and recurrent neural networks. I'll discuss the languages and frameworks that you want to use in machine learning. We'll talk about Python, TensorFlow, Scikit-learn, PyTorch, these types of topics. I'll discuss at a
 level the math you need to know to succeed in machine learning. This includes calculus, statistics, and linear algebra, and I will go into all these topics in as much depth as audio allows. And then I will provide you with the resources needed to deep dive any of these topics offline to master the details that require a visual element, whether it be textbooks or videos. This podcast is of course intended for anybody interested in machine.
 learning. But there tends to be two common subscribers to the podcast. The first is managers and executives. They're interested in knowing just enough machine learning to be dangerous, whether it's to assess what technologies are available to use in their projects or at their company, or maybe they want to intelligently converse with their machine learning and data science employees. To the second are people who want to learn machine learning. Maybe they're considering pivoting from a different.
 field into the machine learning field, machine learning, artificial intelligence and data science. I myself come from a web and mobile app development background and decided that I wanted to become a machine learning engineer and self-taught, myself machine learning, and ended up getting work in the field. I only have a bachelor's in computer science, so that shows you that it is doable to self-teach machine learning effectively enough to land work in the field.
 industry. But in order to do that, you have to be very rigorous and diligent in your self-teaching journey. And you're going to want a very structured resource guide. And that is what I'm going to provide to you separate from this podcast. It's as important a component of your listening experience to machine learning guide that you visit the resources section of my website on my website, ocdevelop.com, OCD, e
 www.el.com forward slash MLG. There is a resources tab. Click that tab and you will get a hierarchically structured list of resources with filters. So you can filter by format, whether it be audio book, textbook, video course, etc. Price filters, quality filters and so on. And this tree structured resource guide will provide you the step by step in sequential order. What resource
 Welcome to the first episode of Machine Learning Guide or MLG, which is a podcast structured as an audio course whose intent is to teach you the high level principles of machine learning and artificial intelligence. In this podcast, I will provide you a bird's eye overview of the fundamental concepts in machine learning. This includes things like models and algorithms, both shallow learning models and deep learning models, shallow learning machine learning models.
 include things like linear and logistic regression, naive bays, and decision trees, which as a machine-learning newbie you may not have heard of, but I will also cover deep learning models, which I'm sure you have heard of, things like neural networks, convolutional neural networks, and recurrent neural networks. I'll discuss the languages and frameworks that you want to use in machine learning. We'll talk about Python, TensorFlow, Scikit-learn, PyTorch, these types of topics. I'll discuss at a
 level the math you need to know to succeed in machine learning. This includes calculus, statistics, and linear algebra, and I will go into all these topics in as much depth as audio allows. And then I will provide you with the resources needed to deep dive any of these topics offline to master the details that require a visual element, whether it be textbooks or videos. This podcast is of course intended for anybody interested in machine.
 learning. But there tends to be two common subscribers to the podcast. The first is managers and executives. They're interested in knowing just enough machine learning to be dangerous, whether it's to assess what technologies are available to use in their projects or at their company, or maybe they want to intelligently converse with their machine learning and data science employees. To the second are people who want to learn machine learning. Maybe they're considering pivoting from a different.
 field into the machine learning field, machine learning, artificial intelligence and data science. I myself come from a web and mobile app development background and decided that I wanted to become a machine learning engineer and self-taught, myself machine learning, and ended up getting work in the field. I only have a bachelor's in computer science, so that shows you that it is doable to self-teach machine learning effectively enough to land work in the field.
 industry. But in order to do that, you have to be very rigorous and diligent in your self-teaching journey. And you're going to want a very structured resource guide. And that is what I'm going to provide to you separate from this podcast. It's as important a component of your listening experience to machine learning guide that you visit the resources section of my website on my website, ocdevelop.com, OCD, e
 www.el.com forward slash MLG. There is a resources tab. Click that tab and you will get a hierarchically structured list of resources with filters. So you can filter by format, whether it be audio book, textbook, video course, etc. Price filters, quality filters and so on. And this tree structured resource guide will provide you the step by step in sequential order. What resource
 resources you're going to want to consume separate from this podcast to most efficiently manage your learning path through your journey of learning machine learning. I have spent a lot of time and effort on curating this list of resources. I'm subscribed to many machine learning and data science subreddits and RSS feeds. I frequently peruse the syllabus of various courses and the textbooks that they assign to their students.
 There are of course just very well known machine learning resources out there like the Andrew Eank Coursera course in fast.ai. So this podcast will provide you the overview of the models and the concepts. But if you're serious about learning machine learning, you're going to want to also spend time on the resources list where I have crafted for you a step by step guide to effectively learn machine learning. Now this resources list is not only for the
 deep divers. There are also plenty of great resources listed on that page, including other podcasts similar to machine learning guide, some of my favorite podcasts that I listened to out there. Various podcasts are news and interview based podcasts where they interview experts in the field and those experts discuss latest technologies and inventions and white papers. Some other podcasts and audio learning resources that are similar to MLG whose goal
 is to teach you machine learning and other fields tangential to machine learning such as math, operations, research, and some more fun topics, more inspirational concepts and machine learning things like the singularity and consciousness can robots be conscious and what is consciousness. So after you listen to this episode, I recommend visiting ocdevelop.com forward slash MLG. There you will find the resources list, which includes other audio supplementary resources that you may want to enjoy.
 alongside this podcast, as well as the deep dive materials that you'll need during your learning journey. Now, as of October 20, 2021, Debt Agency, DEPT, has acquired the Machine Learning Guide podcast and they're paying me to work on it, which is fantastic. So I'm going to be breathing new life into the old podcast. I'm going to be dusting off old episodes, fixing up some erata subjects which may be considered missing in the traditional machine learning educational path, things like decision.
 and naive bays. And they're bringing you machine learning applied for free. Previously, machine learning applied was a separate paid podcast I had which discussed the applied things in machine learning, things like technology stacks, programming languages, job hunting, machine learning operations, or ML ops and all these things. So I'm going to be merging in these episodes into the main feed, thanks to depth's generosity. The value of depth to you as a listener is that I'm going to be using their projects as talking points.
 For the various subjects that I'm going to be discussing, so for example, if I'm discussing computer vision or natural language processing, I might pull in a depth project in those subjects. Further, since this company does data science and machine learning, if you're listening to this podcast because you want to tap into the industry, we'll apply to them, apply to depth by way of the links on OCDevil.com or if you're a manager listening to this podcast trying to decide, what is feasible by way of machine learning and what types of technologies are used in the space looking for talent?
 Well, hire us. Depth is always looking for new clients, and since I'm on the team, hey, you might have me as your engineer. If I don't have a depth case study project handy as a talking point for that episode, I'm going to be using my own personal project, Nothe. GNOTHI. NotheSayotan is ancient Greek for NoThyself. Nothe is an online journal, a personal journal that uses AI to provide resources and insights, like book recommendations, therapist suggestions.
 It generates themes of your journal entries, common recurring patterns. It allows you to ask questions of your journals. It allows you to generate summaries of your journal entries. So for example, give me one year's worth of journal entries in one paragraph. It allows you to share your journal entries with therapists so that they can use these tools as well. And then the project being open source, nothe is open source. You can go see the code, you can see how some computer vision out.
 algorithm is programmed in Python using TensorFlow in the wild. You can see how you would program a summarization model in the deep natural language processing episodes in the future. Note, as I'm redoing the podcast, you may find some gaps in your pod catcher. It looks like maybe an episode is missing. That's normal. It's because I've taken out older material that it's no longer relevant. And it's important to bear in mind that I discuss resources in my old episodes at the end of the video.
 the episode. I discuss them in the podcast episode. When you get to that point in the episodes, just skip ahead because this resources page on osudevelop.com is where I dump the resources now. This allows me to keep things fresh and updated as resources change, as newer, better resources become available for some specific topic. Then I don't have to go back and edit an episode. So going forward as I revise these older episodes, I will no longer be including discussion.
 of the resources themselves. Instead, you need to just go to the website to get the resources. Now with all that meta out of the way, let's dive into the next episode where I define machine learning and artificial intelligence, how they differ from each other, compared to data science, and so on. I'll see you in the next episode.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. Machine Learning Guide Episode 2 this is what is artificial intelligence, machine learning and data science.
 This is a complete redo of the original episode. So if you've already listened to episode two, I recommend listening to this anyway, because there's going to be a lot of new information, especially regarding data science, which was sorely lacking in the original recording. Let's start with a bird's eye view. Data science is the all-encompassing umbrella term inside of which is artificial intelligence, inside of which is machine learning. So data science contains AI and AI.
 AI contains ML. But before we go into data science, let's actually start with artificial intelligence. From Oxford Dictionary, AI means the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision making, and translation between languages. So basically, AI is automated intelligence, automation of things that...
 normally require human intelligence. That's the objective definition of AI. In the same way that the industrial revolution was automating the body through robotics and machinery, the AI revolution is automating the brain by way of intellect. So the objective definition is pretty clear, but in a sense it may not be that helpful. And so people have difficulty defining AI because they haven't mind more of a subjective take on it. And that subject...
 of Artificial Intelligence can be difficult because defining intelligence is difficult. So we have the objective definition of AI. We have the subjective definition of AI. And since neither of those takes are super helpful in this context, let's instead just break AI down into its subfields so that you have a better understanding of AI as a discipline. AI is broken down into multiple sub-disciplines. Straight from Wikipedia, these are reasoning and problem-solving, knowledge representation.
 planning, learning, natural language processing, perception, motion and manipulation, social intelligence, and general intelligence. Let's start with natural language processing or NLP. And the reason I'm gonna start there is I'm gonna kind of paint a storyline of the disciplines of AI. And this is a good starting point. NLP is the sub-discipline of AI dedicated to language. Anything that is simulated language is not
 natural language processing. This can include machine translation, for example, Google translate from English to Spanish. It can include chatbots, things like Siri, for example, or let's say that you have a customer service chatbot on a website and you're chatting with that bot. You can either be speaking, your utterances out loud and it will perform speech to text. That's within the domain of NLP. And then once it has that text, it's going to perform something called named entity recognition. So for chatbots.
 says, how can I help you today? And you say, I need help with my credit card. It'll pull out credit card as a named entity. That's named entity recognition or NER. We'll talk about these kinds of topics in NLP in a future episode. And it can do question answering, summarization, and any other task associated with language. Next, let's talk about the subdiscipline knowledge representation. A long time ago, IBM created a jeopardy playing bot called DeepBlue. What would eventually?
 become IBM Watson. And Deep Blue had inside of it the capacity to play jeopardy not just by way of natural language processing. You need NLP so that when the question is asked, it can take that question and transform it into something that can be used internally to answer the question. But also that there has to be some internal knowledge representation either trained on the web or on Wikipedia or on books. Some way of taking this knowledge about the world.
 out there and transforming it into an internal representation. Usually in this subdiscipline we deal with things called ontologies or knowledge graphs so that after you've gotten the question and transformed it into the type of representation you can traverse the knowledge graph in order to find the answer, come back with the answer and then represent that answer using NLP back to the human. So these two fields are somewhat related, especially these days, in NLP we have a specific task called
 question answering where you can hand the agent a corpus of documents, for example, a book, and you can ask it a question and it will answer the question either extractively by pulling the most probabilistic answer out of that corpus or abstractively, such that it will actually parse your question, figure out what you implied by it, go through all the documents in the corpus in order to paraphrase an answer to the best of its quote-unquote understanding.
 and then relay that answer back to you. That's called abstractive question answering, which to me is a little bit more magical than extractive question answering. These sound a lot like knowledge representation, but they're slightly different domains. Again, knowledge representation isn't explicitly dealing in language per se, although it will certainly be using language as part of its tool chain. Instead, it's the sub discipline of representing knowledge in the AI agent such that that knowledge can be traversed.
 for the correct answer rather than just by pre-trained text extraction language models. Next up we have reasoning and problem solving. This is related to the prior sub-discipline of knowledge representation. Reasoning and problem solving though are more typically dealing with probabilistic models about the world in order to solve problems. And then we have planning. Planning is actually the sub-discipline of taking actions in an environment by an artificial agent.
 This is what you may have heard of with chess and go playing bots. So a long time ago, AI researchers and developers created a bot that could play chess and played chess against the current reigning champion at the time Gary Kasperoff and beat him. And so that created a lot of publicity around AI. Recently, Google's Deep Mind Alpha Go learned to play the game of Go and was pitted against the reigning champion Lisa Dull and one.
 Again, bringing a lot of publicity to AI. By way of much more magical tooling that I'll talk about in a bit than was previously used in the case of chess, there's a great movie about DeepMine versus Lisa Dull, which I'll link in the show notes. I highly recommend watching it. So planning is more about taking actions in an environment, whether that be playing video games or driving a self-driving car, where the car needs to be able to take turn-by-turn actions on the roads. Speaking of self-driving cars taking-
 actions on the roads. There's two other pieces that need to be present for a car. The first is perception. So an AI needs to be able to perceive the world around it, eyes, ears, mouth, and nose and touch. So a self-driving car needs to be able to see the road by way of cameras or light R. It may also need to be able to hear the road, hear horns blaring or cars screeching. And so that would be using microphones. So this is perception and it also needs to be able to perform.
 form actions inside of its environment. This is the sub-discipline called motion and manipulation, which is basically just robotics. So a car has seen the road through a camera. That pixel data gets transformed by AI into some internal representation. It performs some planning steps in order to determine whether it should break, accelerate, turn, left or turn right. And then it has to be able to transform those action steps into physical reality by way of robotics,
 which is the sub-discipline motion and manipulation. And in particular, we call this hardware, the point at which the AI performs physical actions in an environment actuators. So an actuator in a self-driving car might be the brakes, the steering wheel, the wheels, whatever. Now you'll notice I'm talking about robotics here, and a large part of robotics is simply hardware. That's cameras, microphones, and the machinery used in robotics. "...bad weed".
 do include robotics as a subdiscipline of AI to the extent that interfacing with the hardware by way of intelligence, whether that be planning in the case of taking actions or representing what it perceives in a way that can be internally utilized by the AI agent in the case of perception. Now you've probably seen in the news all the cool stuff by boss and dynamics, those guys are killing it in the robotics field. We have robots that are jumping on boxes and doing back flips.
 There's one thing they can't do well, defend themselves against hockey sticks. I always joke that if there ever was an evil killer robot takeover, well we know their weakness because Boston Dynamics has shown us that's hockey sticks. So you might want to stock up on hockey sticks to defend yourself against the AI revolution. Also listed on Wikipedia is social intelligence and general intelligence. Social intelligence is the concept of AI being tuned into human emotions. So we may be trying to find...
 follow a human's emotional flow through time. In the case of Robo Therapists, I myself am actually building a journal app that uses AI to give you resources and recommendations through your journaling journey. You'll hear about it in future episodes. It's called No-Thee. And there are apps which are Robo Therapists that you can converse with. And these will attempt to pick up on your emotional state based on an LP, tasks such as sentiment analysis, whether the things are tight-
 typing or positive or negative or neutral. And there are also computer vision technologies, which will try to pick up on the emotions on a human's face. And then finally, general intelligence where artificial general intelligence, AGI. AGI is the pot of gold at the end of the rainbow. It's a pipe dream, it's a pie in the sky. It's the goal of creating an artificial intelligence that is not only good at its particular task.
 But it's also generalizable to all other domains so that it is as effective at those as a human if not better. Okay, so we say weak AI if the AI is dedicated to a specific domain. For example, Siri is really relegated to natural language processing. A Roomba, it's mostly robotics, but it's also incorporating action and planning. So that's a two for right there.
 Driving cars, we're getting a little bit closer to the whole Shabang, the Grand Door Prize of combining all of these AI sub disciplines into one. We have perception, namely computer vision and hearing, to some extent. On the one hand, it may be listening to sounds on the road. It can also listen to voice commands inside the car. Of course, action and planning that's turned by turn navigation, acceleration, braking, and then motion and manipulation, all through robotics.
 a Tesla self-driving car is getting us closer to the end goal of artificial general intelligence. But as you can tell, it is not very general. The word general in this context means that the robot or the AI agent should be able to generally apply itself to any context in which a human can find themselves and perform or act as intelligently as a human, if not better. So let's say we take a Boston Dynamics robot, which is good at jumping on boxes and doing.
 back flips and hauling luggage like a pack meal, and then put it in front of a court so that it's handling legal cases. It should be able to pick up the skills needed to handle legal cases as well or better than a human who is plopped down into the court, and that's the generality of artificial general intelligence. Weak means it's dedicated to a specific subdomain, not necessarily that it's weak at its task, but just that it can only handle specific domain. And general means it can handle all domains and general
 across contexts universally like a human could. Now when I say pie in the sky, pipe dream, I don't mean that this isn't gonna happen. I actually do believe we will achieve AGI, and I believe that it will happen in my lifetime. Between Google Mind, Open AI, and all the other fang companies go and hog wild with AI these days, it looks like we're on a pretty focused path with major improvements week by week. What I mean by that is that AGI is more of a concept in the future.
 goal rather than the day-to-day dealings of somebody who works in artificial intelligence and machine learning. So basically, the objective definition of AI is automated intelligence, or automating intellectual tasks which typically require human intelligence to perform. That's the objective definition, but it's not very helpful because, for example, I could write a Python script that does some number crunching on a spreadsheet. That is automation of an intellectual task.
 that usually requires human intelligence. And so that segues us into the subjective definition of artificial intelligence, which is really hard to pin down. You'll find that when you ask somebody, what is artificial intelligence? Whether you can read the definition from the dictionary, but strangely, it can be easy to get into something of a heated debate about what the actual definition of AI is. You'll find that a lot of people take the definition of AI and make it a little personal and make it a little personal.
 it subjective. And the reason for this I think is because what they're really driving after rather than the definition of AI is the definition of eye, the definition of intelligence in general. And indeed defining intelligence can be quite a subjective and dicey matter, which segues into the conversation of what is consciousness. For example, I discuss intelligence and consciousness in a future episode, but let's do a little bit of hand waving here so that we can become a little bit...
 more settled with the subjective take on artificial intelligence. The way I see it, we have manual hard-coded scripts on the one hand, on the far left, and then we have true intelligence all the way to the right, far on the horizon, past what the eye can see. And the reason I say it this way is because I think that intelligence is an analog concept, in the same way that some humans can be more intelligent than others, and humans are more intelligent than others.
 dogs and dogs and cats and cats, then rats and so on. There's sort of an analog scale to intelligence. And so too will be the case with artificial intelligence. And what is the upper limit we don't know? And therefore, there is no sort of end to the right side of the scale. It's analog. So on the left of the scale, we have handwritten scripts that can be considered AI by the object of definition. And on the far right, we have the pie in the sky concept of age.
 which I'll discuss in a bit. And anywhere in the middle is sort of what we're dealing with and trying to define AI subjectively. The layers by which the software becomes removed from the programmer adds elements of magic to the concept of AI. So a handwritten script is not very magical. A machine learning model, which we'll discuss later, that learns, let's say, the rules of the game of chess or go or how to drive a...
 self-driving car down the road. Well, that's some number of layers removed from the original programmer, and that removal adds an element of magic. That magic is the sense by which the concept of AI can be defined subjectively. So where do we draw the line by which we call something magically AI in the sense that we're all looking for? Well, Alan Turing had a really good take here. The Turing Test or the imitation.
 game. Alan Turing is essentially the creator conceptually of the computer by way of the universal Turing machine, and he was fascinated by the theoretical concept of artificial intelligence. Of course, technically achieving AI was well outside of the technological capabilities of his time, but he was really interested in the concept. And he came up with the Turing test, which basically says, if you're chatting with an artificial agent, an AI,
 Basically, he envisioned that you're on a terminal with a chatbot, and the other end of the chatbot responding to your utterances you don't know is either a human or an artificially intelligent agent. If you're chatting with this agent and you can't tell if it's a human or an AI, then it is intelligent, whether it's the human or the AI. If an AI can sufficiently convince you that it is intelligent through a chatbot where you're poking and prodding and trying to find the holes and you can't find...
 them, then it is intelligent in the magical sense. So he says, if an AI can convince you that it's not not an AI, then it is. Then it is intelligent. So in a way, if we say that there's a sliding scale from hard scripting on the left and artificial general intelligence way into the horizon to infinity, the point at which we achieve intelligence in the magical sense that we're all trying to subjectively pin down.
 is the point at which you can't deny it. And it's as simple as that. If it walks like a duck and it talks like a duck, which is behaviorism, if you experience an AI agent and you are flabbergasted and can't deny that it is intelligence, then it is. And anything up into that point is not and anything beyond that point is the future. So that's sort of the marker delineating this analog infinity scale as the...
 point at which we subjectively define intelligence. That is, you'll know it when you see it. So that finally takes us to learning, machine learning, the last sub-discipline listed for artificial intelligence, and the most interesting and powerful sub-discipline of artificial intelligence, as you'll see here. The reason I saved it for last is not only that, it's the primary subject of this podcast series, but also because it is machine learning with the most interesting importance of artificial intelligence. So all results were gucken, and we already know. Very strong, zasp. Are there any values attached to these? we have total eleven evidence for
 sub-discipline of AI that makes disambiguating the terms AI and machine learning difficult. And that is because in recent times, machine learning has been subsuming all of the other sub-disciplines of artificial intelligence. Machine learning, what was previously relegated to its own siloed sub-discipline, is now breaking out of its cage and hostile taking over all the other sub-disciplines of artificial intelligence.
 sub disciplines, grabbing a chokehold on those sub disciplines, and some of these sub fields have been completely turned blue. There's nothing left but machine learning. So let's define machine learning first and then let's revisit the sub fields. Machine learning is learning on data to make predictions. It's pattern recognition over data so that future predictions can be made. It's learning on data. That's what machine learning is. It's really simple. It's really easy to understand. The idea is you feed it a spreadsheet.
 Let's say that spreadsheet is stock prices for a particular stock over time. And what we're trying to do is predict tomorrow's price for that stock. Okay, it's a typical use case of machine learning, algorithmic trading bots. You take this spreadsheet, this data, and the machine learning algorithm will look at all the data in the spreadsheet in order to try to find patterns in the data, is trying to learn the patterns of the data. What is it that makes the price...
 go up or go down, given other variables in that spreadsheet, such as the time of day, day of week, sentiment on Twitter, whatever the case. Learn those patterns. And once we have those learned patterns saved, we combine the learned patterns into the algorithm into what's called a model, a machine learning model. So a model is the combination of the algorithm, what the computer programmer wrote, and the patterns learned what the machine learning model itself picked up.
 Now we have a model and that model can make predictions for the future. So the model can now predict based on today's context or circumstances what's the price going to be for this stock tomorrow. And then it may either just predict the price where it may actually take action on that prediction such as to buy or sell on that stock. So that's machine learning. It is learning on data to make predictions. It's as simple as that. Now previously, like I said, this was a dedicated sub-discipline of AI
 was focused primarily maybe on tabular data, learning on spreadsheets or databases, the patterns in that tabular data, by way of simple algorithms, simple models, things like linear and logistic regression, which we'll discuss later, and these simple models made machine learning maybe less interesting to the other practitioners and the other sub disciplines of AI. But after the deep learning revolution of about 2015, machine learning started to become
 generally applicable and started dominating the other sub-disciplines of artificial intelligence. So natural language processing. Previously, NLP was language theory. Basically, you go to university, you study language theory, you learn all about syntax trees and the ways in which certain words combine with other words in grammatical constructs so that you could traverse these trees in order to pull out named entities or answers to...
 These were hard-coded scripts by domain experts in NLP. While along comes machine learning with its recurrent neural networks and transformers models and clobbers these original expert crafted scripts, where each expert was a sub-expert within NLP, maybe some expert and their script were really good at question answering and another for named entity recognition and another
 for machine translation. Well, these transformers models are generally applicable, generally applicable to all of the tasks within NLP. Transformers can handle machine translation, question answering, summarization, and more, all with a universal learning model, and that this model can be trained on the language tasks generally, rather than handcrafted on a specific language task. For example,
 the difference between legal documents versus news articles. So machine learning takes us some steps away from a handcrafted script, which adds some magic to our subdiscipline of AI, and some generality, a step towards artificial general intelligence. Some generality in so far as a transformer's model can be generally applied to various tasks and domains within NLP. Computer vision. Previously computer vision was.
 handcrafted scripts, algorithms that had hand designed, pattern matching patches, these sliding windows that would slide over an image from top to bottom left to right, where the patterns that it's looking for in order to detect an object or to classify an image were domain expert handcrafted pattern matching windows. So an expert would design a series of manual convolutions, which we're looking for.
 lines and edges. We're looking for certain patterns for dog breeds, for example. Some such algorithms were called hog, h-o-g, and har, h-a-a-r, which I'll discuss in the computer vision episode, while along comes machine learning with its convolutional neural networks. These CNNs learn the convolutions, these sliding windows, these pattern identifying patches, and blows the old guard of handcrafted solutions to smithereens. Again.
 These models are not only layers removed from handcrafted solutions in so far as they can learn the pixel pattern matching process, but also are generalizable across different domains. So the hog and hog people might have had to handcraft different convolutions for different domains, whether we're dealing with animal matches or clothing matches or outdoors objects matches. Any one convoluted-
 Neal Network can handle all these contexts generally. And then finally, reasoning problem solving, planning, motion and manipulation and perception. These were all previously dedicated sub disciplines in AI, and they still are to some extent. In particular, the reasoning sub discipline dealt a lot in probabilistic models. And the planning sub discipline dealt a lot in trees. Search trees like the a...
 star search tree. And these types of trees would have been used, for example, in the chest playing bot against Gary Kasparov. Well, along comes deep reinforcement learning models with their deep Q networks and their proximate policy optimization models. And I don't know which specific reinforcement models are used in different contexts. For example, that reinforcement model, what reinforcement learning models used in AlphaGo 0, but it has become.
 very obvious that there is a massive amount of generalizability for use of deep reinforcement learning models in any situation that requires taking steps in an environment, planning actions and taking those actions. We previously had search trees and mark-off models and all these things which would have been custom tailored to specific environments. For example, pre-programmed with the rules of chess along with
 with the how-tos of playing chess. Well, deep reinforcement learning models not only learn how to play a game or drive on a road by taking actions in an environment and observing the point value consequences downstream at the end of the game in order to modify its behavior, but it also learns how the game is played in general. What game is even being played? So many of these reinforcement learning models...
 All you have to do is show it a screen through time. So it takes space in pixel values and time, the progression of the video game as it takes specific actions. And it will learn the rules of the game, the context and environment of the game, and how to play it. So now at this point, we're another layer removed from a handcrafted script, even more magical than the models used in NLP and computer vision. So machine learning is
 not only more effective at tackling a lot of these sub disciplines in AI than the old guard of previously handcrafted scripts, but is also more generalizable and frankly more magical. And speaking of generalizability, strides are being made on many fronts now to unify these machine learning models to tackle multiple subdomains within AI at once. For example, recently in the news, Google announced some...
 something called Pathways, which as I understand it is a machine learning model that can be applied to computer vision, natural language processing, and reinforcement learning. Also, as you'll learn in the NLP episodes, an NLP model concept called Transformers is now also being used for vision and other tasks. So steps are being taken towards a grand unification machine learning model, and those steps also take us towards dumb.
 artificial intelligence at large by way of machine learning. So that's what's special about machine learning is that it seems to be the master algorithm of artificial intelligence. And that's also why disambiguating those two terms has been relatively difficult. It's because previously ML was simply a subdiscipline of AI. But in modern times, it's the dominating driving force of AI in all of its other subdisciplines.
 Now let's talk about data science. Data science, like I said, is the umbrella term that encompasses artificial intelligence and machine learning. More than that, data science is the umbrella term that encompasses anything related to data. If you are a professional and your daily job is dealing with data, then you are a data scientist. This can include things like database administration, data analytics, business intelligence, machine learning.
 artificial intelligence, even something as simple as maybe a tax person. If you're dealing with spreadsheets day in day out, to some extent, you're a data scientist. So roles that primarily deal in data are roles within data science. But there are roles more understood to be data science proper. And that's what we'll discuss here by way of a data pipeline. Because the typical roles dealing in data science fall somewhere in this.
 pipeline. A pipeline from left to right beginning to end. The beginning of the data pipeline gets the data from somewhere and pipes it into the machine. And the output of the pipeline is either data analytics, business intelligence or machine learning. And then there's a whole bunch of machinery in the middle. So let's go down this pipeline journey. The first step of the pipeline is data ingestion ingestion, which is to get your data from somewhere. This data can
 can be in a spreadsheet like Microsoft Excel, Excel, SX or CSV files. It can be in a database like Postgres or MySQL. Those are relational database management systems. There are also non-relational database management systems or NoSQL, NoSQL databases like MongoDB and DynamoDB. There can be from a stream of data on the internet. A stream means that...
 But the data is coming at you so much and so fast that you don't want to store it anywhere. You just want to process it real time and route. You want to receive your data, do stuff to it, and then probably throw it away. So an example of a stream of data would be Twitter. Twitter is what's called a stream or a fire hose of data. It's coming at you so fast because there's so many tweets in the world that you're probably not going to be storing this into a SQL database or no SQL database and certainly not a spreadsheet. A data set typically means...
 file formatted data like a spreadsheet. A data store typically means database formatted data like on Postgres or MySQL and then we have a stream or firehose. This comes into your pipeline and now you want to put all that data somewhere. You want to aggregate all of the data come into some use case all together in what's called a data lake. A data lake is the aggregation of various data sets, data stores into
 one repository, one umbrella. And that data is gonna be dirty, it's gonna not be cleaned up yet. It's coming straight from the ingestion phase. And data lake technologies allow you to apply certain unifying, aggregating features on top of your data. One of these might be permissions. So for example, if you wanna collect spreadsheets and databases and tweets all into one unified repository called a data lake, then you can use data.
 data lake technology to apply permissioning on that data lake such that various downstream data users or consumers can access this data lake all by way of a unified permissioning system. Another feature you might want to apply to your data lake is a unifying schema. If much of your data has a lot of structure in common, even though they're coming from disparate data sources, you can create a data schema.
 that you generally apply to the data sources in this data lake, so that downstream consumers of the data can expect a certain formatted structure to the data. The next phase of the pipeline might be something called a feature store. Now, this can sometimes be wrapped up in the data lake phase, or they can be separate components of the pipeline. The feature store is the phase at which you take the data from the data lake, and you...
 You cleaned up and you store it and it's cleaned up state. So you're going to have data coming from Twitter, coming from databases, and it's going to be dirty. There's going to be missing entries, missing columns for certain rows. Some fields are in text format or date format. And as you'll find later, data analytics and machine learning don't like dates or strings. They want numbers, machine learning and data analytics always wants numbers. And so this feature engine.
 phasing phase they call it feature engineering is where we will perform some of these steps. One step might be to fill in missing values. We call this imputing. You'll learn about imputation strategies later. Another feature engineering step might be taking your date column and turning it into numbers. So for example, we can take a date column and turn that date string into time of day, day of week, week of year. And so now we transformed the date feature into three
 features we call this feature transformation, as well as simply deciding which features to keep in which ones aren't worth keeping. For example, in the case of Twitter, user name might not be that valuable. We really are concerned with the date, the tweets text, and maybe the engagement, like the number of replies or number of retweets. So we throw away the username. This is called feature selection. And this whole process is called feature engineering. And then we
 store all these engineered features as something of a checkpoint in our pipeline called a feature store. And feature stores also allow you to version your transformed features such that if a machine learning engineer has been working with data in such in such a way based on the way that the data engineer had previously been feature engineering. And the data engineer makes a modification to that phase in the pipeline. While the machine learning engineer won't be side swiped because they will...
 be using a specific version of the feature store. Okay, so data ingestion takes our data from data sets and data stores and streams and firehoses. These are all called data sources. Stores them all into a data lake, which is dirty, unclean, large data or big data. You've probably heard that term big data. Anytime we're working with data that can't fit on a single server, it's big data. We may apply unified tooling on top of that lake.
 things like permissioning or a common data schema. And then we pull that data out of the lake, we clean up the data. A step called feature engineering. Those steps may be feature transformations that is altering one feature into a different feature or set of features. Imputation that is filling in missing values. Feature selection that is deciding which features are important and which aren't. And then now we're done with that third phase and we're ready.
 to move on in the pipeline. At this phase in the pipeline, we're gonna veer left and right. There's a fork in the road, just like there is with pipes. And so to the left, we have data analytics and business intelligence, and to the right, we have machine learning. These are two typically separate roles. And we'll discuss how these are considered separate roles in a bit. Let's go left down the pipe towards data analytics. The first step down the pipe towards data analytics is to store.
 what we've already cleaned up in the data in something called a data warehouse, a data warehouse. Data warehouse is separate from a data lake. It's a little different. Typically what a data warehouse will do is take a slice of historical data from the recent past, transform that data to be represented in column format, rather than row format, and then maybe apply some additional transformations on the data. So list.
 the step-by-step. The way you would think about a data warehouse is that it is something like a proxy that sits between the data lake and the data analyst. A proxy almost like a cache layer between the data source and the analyst. In the same way that if you've ever worked with web development, there's a thing called MemcacheD that you can stand up in front of your MySQL server and have it proxy requests which are frequently accessed.
 it caches SQL queries. So if there's a lot of really common SQL queries that get called frequently and would put a lot of strain on the underlying database, MemcacheD can sit between the database and the server and cache those common queries so as to reduce strain on the database and speed up SQL queries. Well, a data warehouse is similar to that. We take a slice of data over, let's say the last one year or two years, we transform that data to be represented in column
 from structured format rather than row format. Row format is what you're used to when looking at data, like a spreadsheet or a database. Now imagine taking the rows and columns and just flipping them around. That's column structured data. That structure of data is more desirable for analysts than for other roles in the data science pipeline like machine learning engineers. Machine learning models do like row structured data, the type we're familiar with. But data analysts prefer.
 or column structured data. And that's because running aggregation queries specifically over columns is faster in that way, like count and sum and average. Running those queries is faster on column structured data. And then we might cache specific queries which are frequently ran by analysts. And all these things combined turn a data warehouse into a powerhouse proxy between the data lake so that very fast, cached, real time queries can be...
 run against our data by the analysts. So that's our data warehouse, and then finally, our data pipes out of the data warehouse to the data analysts and the business intelligence professionals or business analysts. BI, BI for business intelligence. These are two very similar roles, very related roles, but there is a nuanced difference, and these roles are more likely to be differentiated from each other.
 at larger companies where we have lots of roles to fill, as opposed to smaller companies where one person wears many hats. So both of these roles deal in analytics, deal in charts and graphs. We take our data out of the data warehouse and we rotate it around in our hands, we look at the top, we look at the bottom, in order to make human decisions. That's what makes these roles different than machine learning. Machine learning makes automated decisions. In other words, data analytics.
 analytics or business intelligence is for humans and machine learning is for robots. But we'll get to that in a bit. A data analyst is more of a technical role. A data analyst is actually likely to be involved in the entire data science pipeline up until this point. Everything we've already discussed, such as ingestion into a data lake feature engineering and working with a data warehouse is likely to also be tasks.
 performed by a data analyst, in addition to the actual analytics task of charting and graphing, looking at the data in order to come up with human decisions. Whereas a business intelligence person, a BI professional, is typically a less technical role. They're less likely to be involved in the entire data science pipeline up until this point. And instead is likely to be working with the data from the data warehouse.
 as a consumer, as a user. And the giveaway difference between these two roles is the tooling. BI people will be using BI specific tools, things like Tableau TA-B-L-E-A-U, and Power BI by Microsoft, and QuickSight on Amazon AWS. Whereas a data analyst is probably going to actually be doing their analytics themselves with code by way of Python in Jupyter Notebooks.
 with Matt Plotlib and Seaborn. We'll talk about those things in a future episode. Or during the data engineering phase of the pipeline by way of things like Amazon data Wrangler or Amazon Glue Data Brew. So an analyst is more of a technical role and a bi person is a less technical role. That's data analytics, data driven decision making for humans. So data analysts may take the tweets out of our
 data warehouse and then chart and graph some things, look at the optimal time to tweet during the day and what day of the week based on the engagement of those tweets, number of retweets and number of replies, and then go running to the boss and say, boss, boss, we should be tweeting at noon on Tuesday based on my research of the data. That's a data analyst. Machine learning. Now we go down the right side of the fork in the road in the branch of our pipes. Machine learning is automated prediction.
 It's predictions and decisions made by robots. Now when I say robots, of course, I'm being colloquial by machine learning models. So rather than charting and graphing data, as an analyst would do, a machine learning engineer will design a model using machine learning tooling, things like TensorFlow, Keras, PyTorch, Scikit learn, and XGBoost, maybe all hosted in the cloud using SageMaker, so that it will learn the patterns of the tweets, so that it...
 that can automate predictions or maybe even take actions. So rather than coming up with the conclusion that it is optimal to send tweets at noon on Tuesday, the machine learning engineer might build a model that literally sends tweets at noon on Tuesday. Hey, take that a step further and maybe they might write an NLP model whereby the marketing team can feed it something they wanna say. The machine learning model will construct the optimal way to say that.
 using an LLP, put that into a tweet, send it out at noon on Tuesday. So that's the data science pipeline. It is the flow from left to right of data at ingestion, put that into a lake, apply some stuff upon that lake, whether it's permissioning or a unified schema, pull it out of the lake and feature engineer it, clean up the data, store it in a feature store, veer left towards the analysts, first we hit the day.
 warehouse which does some further transformation of our data so that it can be a real-time proxy to the analysts for fast querying. The analysts have more of a technical role in doing analytics, charting and graphing on data for human decisions. And the BI people have a less technical role for the same. Via right to the machine learning engineers who build automotive predictive models. Now what is a data scientist then? Well that's a little...
 bit nebulous and the reason is, and the reason is it depends on the size of the company you work for. If you work for a mom in pop shop and you are their only data person, then as a data scientist, your role is everything. Everything I just listed, you're going to handle each phase in the data pipeline. But if you apply to a larger organization, like a FANG corporation, Facebook, Amazon, Apple, Netflix, Google, then it is likely the case that you will be applying,
 as a role for any one phase of this pipeline. So for example, Google might have an entire data science team, and that team is broken down into the data engineering team, the analytics team, and the machine learning team, and then you might apply to the machine learning team, or the future engineering team. In other words, depending on the size of the company, any one phase in the hierarchical breakdown of the data science stack is a-
 candidate for a role as a data scientist. And so what you might find is that people may market themselves either as specific roles like data analysts, business intelligence, machine learning, engineering, or data engineering, or they may market themselves as a data scientist, quote unquote, either implying that they can handle everything in the stack or that they're looking to be more generally applicable. Maybe they're targeting smaller companies and want to wear multiple hats. And then a company might be looking...
 for a data scientist, typically meaning they want somebody who can handle any and all of the moving parts in the data science pipeline, or they may specify which specific role they want, a machine learning engineer, a data analyst, or a feature engineer. And that's why it can be confusing sometimes the difference between machine learning and data science. Now which of these roles should you as a budding engineer in the space target, machine learning,
 engineering or the whole data science stack. Well, taking analogy from web development, there's front end server, database, and DevOps, as an example, and somebody who tackles the entire stack calls themselves a full stack engineer. You could either specialize in front end and get really good at it, and there's a lot to know about just front end, for example. Similarly, machine learning is enough to tackle all on its own and keep you busy for a lifetime.
 There's ML ops or machine learning operations, deploying your machine learning models to the cloud, developing and training your models on various tool kits like PyTorch and TensorFlow, deploying, optimizing, and monitoring your models. All these things could keep you busy for a lifetime, and you could dedicate yourself exclusively to machine learning. That would be the equivalent of somebody dedicating themselves exclusively to front end engineering, where other roles in the data science stack are analogous to other roles in a web development stack.
 So, it depends on if you want to tackle the entire stack at large or if anyone portion of the stack sounds more appealing than another. What about machine learning versus AI? Now, I did mention that machine learning is subsuming the various subdomains of AI, but that's not necessarily or completely the case, at least not yet. And so, machine learning is more of an industry practice, a professional field. If you want to get into industry, I would choose machine learning.
 over artificial intelligence. You might get a master's in machine learning or data science, whereas targeting artificial intelligence is kind of that dreamy scientific chasing the rainbow to the pot of gold of the future. That's more of an academic endeavor. So if you want to stay in academia and push the scientific frontier, then targeting AI conceptually would be more aligned than targeting machine learning. So that's data science, artificial intelligence and machine
 learning. Now I'm going to take the history of AI that I had originally recorded in 2017's episode two and I'm just going to insert it here because I don't want to re-record that stuff. So if you've already listened to episode two you can stop listening now if this is your first listen through keep on listening. Okay so we've covered the definition of artificial intelligence, the definition of machine learning and now finally let's go into the
 history of artificial intelligence. I'm going to recommend you some really good resources that I've picked up from the from around the web some common recommendations by people who are looking for good history information on artificial intelligence. So artificial intelligence goes way, way back, way back further than you imagine, starting with Greek mythology and then you know coming out in Jewish mythology with columns. And this is actually a very interesting point to me that artificial intelligence has sort of been conceived of in the dawn of you.
 It's almost like it's part of it was kind of our quest and we're gonna get back to that actually in the next episode Which is artificial intelligence inspiration or we'll try to inspire you around artificial intelligence the machine learning Right now just keep that in mind that we've been thinking about our artificial intelligence since at least Greek mythology The first attempt at actually implementing an automaton. I think this guy's name is Raymond Loll in the 13th century Leonardo da Vinci was working on what he made some walking animals Automata day car
 and liveness, they had a lot of philosophical musings and unconsciousness and liveness, co-created calculus, stuff that was used in the development of AI algorithms and mathematics. And both of them were thinking about AI. So this theory about AI has been around for a very long time. The real needed gritty stuff started happening around the 1700s and the 1800s, the period of statistics and mathematical decision making. One of the first most important figures is Thomas Bay.
 He worked on reasoning about the probability of events. So Thomas Bayes reasoning about the probability of events. Remember that name, he's going to come up again multiple times in the future. Bayesian inference is very important fundamental component of machine learning. So he's a very big figure. George Bull who was involved in logical reasoning and binary algebra, got lob frege with propositional logic. So these were components in the development of computers in general, but also in the development of machine learning.
 Charles Babbage and Ada Byron slash Lovelace in 1832 designed the analytical engine which is a programmable calculating machine and then it was in 1936 that we got the universal Turing machine and I think that a lot of people considered that kind of a kickoff point for computers Alan Turing Designed the concept of a programmable computer with his universal Turing machine It also just so happens Alan Turing was very interested in AI He talked about he has an article
 called Computing Machinery and Intelligence. So he was thinking about AI pretty early on as well. 1946 John von Neumann uses Alan Turing's Universal Turing machine in the development of his Universal Computing machine. If I'm not mistaken, I believe the Universal Turing machine was the theoretical and the Universal Computing machine. He actually made a computer, I think. John M. Neumann made the architecture for the first kind of universally programmable computer. And now we finally start to get into the actual machine learning stuff, machine learning and artificial intelligence. In 1943 Warren McLean.
 and Walter Pitts, they kind of built the computer representation of a neuron that was later refined by Frank Rosenblatt to create the perceptron. And that word, the perceptron, will come up later. That's the first artificial neuron. And of course, you stack those in an artificial neuron by way of what's called a multi-layer perceptron and you've got an artificial neural network. And that is deep learning. That's the fun stuff. So, McCulloch and Pitts and Rosenblatt, three very...
 important figures. It wasn't until 1956 that the word artificial intelligence was coined by John McCarthy at the Dartmouth workshop. So John McCarthy, Marvin Minsky, Arthur Samuel, Oliver Selfridge, Ray Solomonoff, Alan Nule, and Herbert Simon. They all came together at Dartmouth at this workshop in 1956, and their goal was to simulate all aspects of intelligence and, quote, and that is the definition of artificial
 intelligence at the beginning of this podcast. So these guys defined AI, they created a field of it, and then they set off working. So at the workshop, it was kind of like a hackathon, like a multi-day hackathon. They just cracked at it. Newell and Simon created heuristics, sort of the branch of heuristics in machine learning, created this thing called the general problem solver. Okay, here we see the creation of computer vision, natural language processing, shaky the robot. So this is between the 1950s and the 1970s. So they were working a little bit on this at the actual Dartmouth workshop, you just had a through.
 But then they all took their projects home to their own universities and they continued cracking at these things. We call this sort of the golden era of artificial intelligence. It's when it was like an explosion of interest and research and joy and light in the field. Fy-gun bomb creates expert systems. This is also sort of the creation of what we call good old-fashioned AI. Because later you'll see we kind of switched gears on the way we do AI in today's generation. Good old-fashioned AI or GoFot.
 G-O-F-A-I uses an approach called symbolism. These are logic-based approaches, knowledge-based expert systems, basically where the knowledge and the logic is kind of hard-coded into the system. It's a little bit, it's a little bit like that. Once and twice removed analogy, I used back when playing checkers, a little bit less magical, maybe then the connectionism approach of neural networks. But some of the experts, including Marvin Minsky, during this time, took the side of Gofi of symbolism and they said,
 that the connectionism approach of artificial neural networks wouldn't hold had some problems. So this was the golden era, the 50s and the 50s to the 70s. The golden era of an explosion of research and ideas and discoveries and developments by some of the most important and influential names in the history of artificial intelligence that you'll read in any book. But there was also a little bit of adversity between some of the players. And like I said, so connectionism began to be...
 criticized due to something what was called combinatorial explosion. Basically, neural networks were too hard computationally, too hard on the computer to be effective at generalizing solutions going forward. On the heel of that in the 1970s came this report by James Lighthill called the Lighthill Report and this did vast damage to the field of artificial intelligence. So like I said, this was between the 50s and 70s when this
 golden era was ensuing in artificial intelligence. When the world was just vibrant and excited about AI. And in that excitement, DARPA, you know, the military defense and lots of companies invested into these guys, these major players and created companies around these technologies. There was a lot of hype, a lot of excitement, and under delivery. It was almost like a Silicon Valley bubble bust. It was almost like they said we can do anything in the whole universe because we're simulating.
 intelligence. Anything you could imagine intellectually we can simulate that. But in fact, of course it's a little bit more difficult than that. We're getting closer every day, but they under-delivered in too long of a time. And so people started cutting funds and cutting grants and cutting contracts with contractors. And a lot of that was due to this Light Hill report, which was basically report on all these kind of negative aspects. And I'm saying, yes indeed, these guys are biting off more than they can chew and they're under-
 delivering. So that created what we call the AI winter. AI went underground and very few people were funding it anymore. The AI winter lasted from about the 70s until the 90s and it started to make a comeback and the reason it made a comeback was that AI finally started to have some practical applications. I'm sure it was kind of these die-hards that were hiding in the cave. They knew that it would that it had real application in industry and so this finally started making some practical use of AI without some lofty promises.
 So for example, advertising and recommender engines, which are two of the most commonly used applications of artificial intelligence in the modern era. The other thing is that the computers got better. So remember that one of the reasons for the AI winter under delivery was due to this combinatorial explosion that these generalizing algorithms could not perform on the computers of that modern time. But they could perform on the computers 20 years later. 30 years.
 later. It was the 90s and the 2000s that AI really started to pick up steam again. And finally, the last piece of the puzzle was data. One thing that you're going to find that I'm going to teach you in later episodes is that the accuracy of your machine learning algorithms improve the more data you have. And with the internet becoming so popular and data becoming so prevalent all over the internet that we could just scrape web pages and there was Excel spreadsheets and databases everywhere of this and that, these machine learning algorithms had basically...
 just a gold mine of information to work with. This is what the era of what's called big data. Now you can work with all the data at your fingertips as you can imagine. Big data. So AI finally made a comeback. Finally had this AI spring after the AI winter. Another reason I think that AI made a comeback, which I don't hear touted a lot, was actually optimizations to the algorithms. I know that they say that our computers got really fast, so the computation got more efficient, but so did the algorithms. So for example, in 2006, Jeffrey Hinton.
 optimized the back propagation algorithm, which made artificial neural networks substantially more tractable and put connectionism back on the map. In fact, I believe it was that this solidified connectionism as a really powerful, commonly used technique going forward, and that go fine symbolism started to go out of vogue. Finally we have the modern era. 2017 currently, Bloomberg says that 2015 was sort of a whopper for the AI industry. And I don't really...
 know why I think maybe what happened is just just this graph that's constantly beyond the rise since the 90s has just finally hit a peak that it's become really popular in the modern era. Now almost any technology company is adopting machine learning and artificial intelligence. They're hiring machine learning engineers and data scientists like never before. So we're in an explosion of AI. In fact, there's a little bit of concern that it's going to become another bubble bus, another AI winter. But there's a lot of people who say that that probably won't happen. That we're actually we're really doing again, design of your workshop.
 well with the algorithms and the data. The algorithms are very precise due to the amount of data we have, the computing machines. We can scale these algorithms horizontally across an AWS cluster and all these things. So we're in a very, very good time for machine learning, very interesting and good time to be alive and see what's happening. Now there's one company that I want to draw attention to and this will be my final point on the history of artificial intelligence. The company's name is DeepMind. And DeepMind was acquired by Google, I believe in 2014. DeepMind at first was kind of just...
 playing games. It was an artificial intelligence system using deep learning approaches like deep Q networks in order to play old classic games like Pong and then eventually old console games and more recently modern console games like from PlayStation and such. But Google saw potential in the types of things that they were doing with deep learning in reinforcement learning specifically and they acquired them and deep mind has been putting out a lot they have just been demolishing the field of machine learning and artificial intelligence. They're
 probably the most present figure in the news of artificial intelligence today. They put out papers all the time making big splashes in research and developments. Something I mentioned previously called a differentiable computer, which brought the subfield of knowledge representation and reasoning into the domain of machine learning, for example. Deep mind is something to keep a very close eye on. They're doing some really interesting work these days. Okay, so that's a history of artificial intelligence.
 intelligence. So we covered what is AI, a definition, what is machine learning compared to AI, what is machine learning compared to data science compared to statistics, why is ML the sort of essential subfield of AI, and what is the history of AI in brief. If you're interested in the history of AI and the definition of AI, I'm going to provide some resources. There's one book that's commonly recommended, and I'll point that out in the show notes. So go to the show notes, O-C-D-E-V-E-L.
 ocdevelop.com, and this will be episode two. And get those links if you're interested in the history of machine learning. So in the next episode, I'm going to try to, I hope, inspire you to be interested in the field of AI and specifically in machine learning. I think that this episode was probably rather boring. The next episode of all the episodes I have planned so far, the next episode I think will be the most fun episode. It's going to be inspiration around the field of AI. We're going to talk about the singularity.
 consciousness, automation of the entire industry, some really philosophical and crazy stuff. So I hope to see you then.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. W.A. 10th 2017 and this is episode three inspiration in this third
 I hope to inspire you to want to be involved in the space of machine learning and artificial intelligence or at least to keep a very close eye on it. AI, I think, is the most important thing happening in the world right now, period. And I hope that by the end of this podcast episode, you'll see where I'm coming from. This is completely a philosophical episode. It's a lot of fun and a lot of fluff. So, lest you lose credibility in me as a teacher of machine learning, just know that this is the last of its car.
 We're gonna get into some gory details of course starting in the next episode But this one's just for fun and like I said this is all philosophical There are four sort of philosophical Conversations that are happening around the space of AI right now and I'm gonna talk about those four Let's start with the most accessible which is Automation of the economy as I pointed out in the last episode the definition of artificial intelligence being able to simulate any Intellectual task you could imagine the types of jobs that are going to be displaced Once AI comes in for
 swing. In the dawn of AI, we were already kind of expected, maybe very menial digital data entry type jobs, tax preparation kind of jobs to go away. That's not, you know, we didn't really think of that as AI coming in and taking our jobs. We thought of that as the digitization and automation of jobs, of simple jobs, but more and more, very high skilled intellectual jobs are being replaced as well. AI is hitting the medical industry with vengeance. Medical diagnoses is a huge space where AI...
 sitting in radiology looking at x-rays and coming up with a prognosis. Well AI can do that very effectively by way of something called convolutional neural networks. Surgery is now being revolutionized by AI. Very high precision surgery robots are coming into the hospitals and they're right now they're kind of an aid for surgeons but we don't know how long that will last before they're a replacement at least theoretically. Of course cars that we're seeing right now already on the streets. Autonomous vehicles, Google Self-True.
 driving car is in many cities already. And it's in the news and people are discussing the implications. They're discussing the economic implications. They're discussing the philosophical and safety implications. But right away, you can imagine that taxi drivers and freight truck drivers are going to be impacted by this immediately. In fact, programmers, designers, and logo creators, programming, website programming. For example, there's a website called thegrid.io, which has the goal of generating a website by way out of all.
 Artificial Intelligence if you give it some specifications web design wicks has a module that you can have it design your page around your content By way of artificial intelligence and there's already a logo generator an AI logo generator out there I can't remember the name of it But as you can see even creativity is not exempt from AI and in fact talking about creativity Music and art is already being generated by AI and people are having trouble distinguishing between a human generated piece of music or art
 and a machine learning generated piece. So there are two natural responses to this economic takeover by AI. One is fear, are the robots gonna take my job? There's a web app on BBC.com that I'll point out in the show notes where you can look up your job and determine whether it's safe from the AI takeover. It's a lot of fun. I think nursing ranks up there as something that's safe for now. And I think it's because it involves a lot of highly unpredictable body motions where a robot on the road, a self-driving car, follows a very predictable set of rules.
 Well, a nurse running around and carrying boards and IVs and rushing to and fro and answering doctors, orders and all these things isn't something you can very reliably predict with robotics and artificial intelligence. But one general response of the fear to the AI economic takeover is that if you look at all the prior economic revolutions in human history, we've been OK. There was the agricultural revolution which switched us from hunter-gatherers to farmers. And people adapted for the better. There is the industrial...
 revolution that automated a lot of grunt work, physical labor, and the result of it was a stronger economy where people could focus on things that are less meanial. And then there's the information age of the invention of the internet, and everybody got on board with the internet, and new jobs are created even as others were displaced. So for example, previously you'd call a travel agent to book your flight to a destination well. Google flights can get you your flight and find the best price. No problem.
 those guys are out of a job. But new jobs were created in the information age, such as web developers and mobile app developers that didn't exist before that, and our economy is stronger than it was before. So every hump, every economic revolution, does have a small impact in its time, a potentially negative consequence on people who have the jobs that are being displaced, but as new jobs are created, people are able to adapt, and our economy comes out better. On the flip side, there's an entirely different...
 take to this economic takeover, which isn't fear, but excitement. People in this camp point to exactly what we're trying to achieve with economic automation, replacement of jobs so that we can live more comfortable lives. In the same way that the industrial revolution replaced very hard, menial labor, well, think about the more comfortable and safe lives that we're going to be living with autonomous vehicles driving us around and delivering freight. The money saved by all this automation has to go somewhere and it's not going to go into the pocket.
 of these robot CEOs who are conducting meetings and pointing at graphs that are going up into the sky and shaking hands with other robots and laughing and then having beers after work. No, it's gonna go to us. The robots are built to serve us. So the optimists say that it's not gonna destroy our economy. It's gonna make life a lot more livable for us and new jobs will be created that we can participate in. But we don't know what those might be yet. If you look at the result of any prior economic revolution, we will...
 lived more like kings after each revolution than ever before. And that's hopefully what's exactly about to come. We're gonna live even more kingly lives than we already live. And if you think about the way we live now in the information age with our couches and our TVs and our video games, then we can all just go out and eat. I mean, a person's net worth being negative in a bygone era that they lived on the streets. Persons net worth being negative today is all too common due to student loans. And that's just because we live in this age where so much economic.
 leniency is provided to us. And one way which the optimists think that this might pan out is something that's already being currently discussed in multiple governments, such as in Scandinavia, called universal basic income, UBI, where the government basically just gives us a minimum wage. So how about that? So look into that, UBI. So there you have it. Economic automation. The world is about to change under our feet by way of artificial intelligence, and I don't think that there's anybody who disagrees with that.
 This next section holds maybe a little bit less consensus than economic revolution, and this next section is about the Singularity. And this part is really fun. This is where I put on my tinfoil hat, and we start to go down that philosophical rabbit hole into the void. So let me define the Singularity. The Singularity was a term coined by Werner Vinge, and championed in recent times by guy named Ray Kurzweil. You're going to hear his name a lot, so just get familiar with that name. Ray Kurzweil. Come back to him in a bit. What is the Singularity? Mathematics.
 It's kind of like you call the singularity, maybe the point of no return. If you've got an exponential graph coming from the left, and it's going to the right and getting bigger and bigger and going up and up and up, and it goes up into the sky and in two infinity, the point at which it just clearly kind of made a break for it up into the sky is called the singularity. That's an exponential function. Now if you zoom into the graph and you try to figure out where that point is that it made a break for it, you're not gonna be able to find it because it doesn't look like any point is substantially greater than the next by comparison to the prior difference of...
 points. But if you kind of zoom back out and you let's say you take a toy car and you're trying to push it along the graph from left to right, there will come a point where kind of the front bumper of the car seems to hit a wall. And that's the point that we call the singularity. So some say that human technology follows along this type of graph. If you look at our economic revolutions from the prior section, the agricultural revolution, the industrial revolution and the information age, each revolution was closer to the, so the previous one, then that one was to its predecessor and higher up on the graph.
 And right now we're coming upon what's called the intelligence explosion where all mental work is potentially Automatable by artificial intelligence. Now you might be thinking well that doesn't sound like it's going to infinity That just sounds like a next big hump on the graph But here's where things change artificial intelligence is tailored towards specific applications medical diagnoses Autonomous vehicles and stuff like this the notion of artificial general intelligence where you have a intelligent agent which can perform
 all of these tasks, generally any mental task generally across the board. That's called artificial general intelligence or strong AI. If you give it the task of sort of automating our economy and making our lives cushy, fantastic. If you give it the task of making a version of itself that is even better at performing that job, and then that new child robot has the one goal of improving our lives and the other goal of making another version of itself that's even better.
 than it. You know, this is sort of evolution, robot evolution, what's called seed AI, seed artificial intelligence. And since it doesn't take 25 years for a robot to grow old and come to knowledge about the world, and it's going to be way faster at mental processing. And it's going to inherit the knowledge of its prior generations. You're going to get a spiral sort of out of control of technological progress. And that is a shoot up into the sky. And that, my friends, is the...
 singularity. Something that this is the next phase of evolution. Humans effectively stopped evolution when they invented such effective medical science. Well, the exponential graph maybe doesn't start and stop with technology. What if it goes all the way back into evolution starting at the Cambrian explosion and must necessarily continue all the way past humanity? And therefore it is almost our inevitable duty to create artificial intelligence so that evolution can continue. This is all a lot of fun. This is craziness, right?
 I promise I'm gonna talk about real technical details of machine learning and a future podcast I just I want to get you inspired first. There's some really really fun philosophy happening around Why you should care about artificial intelligence? Believe it or not you can take your side after you've heard the arguments all right for the next section Are you ready for this? Consciousness this one is near and dear to my heart. I love philosophy and I love the philosophy of consciousness An interesting thing is that consciousness might be the last bastion of unsolved
 scientific riddles. We found the alphabet of life in the genome, we explored outer space, and discovered the black hole in all these physical and mathematical phenomena that were previously magic. Everything was magic or religious. So for example, at one point sickness, illness, a cold, was what is now considered simply bacteria, was an infestation of evil spirits back in the day, it was magic and it was religion. Well, if you think about it, consciousness is still the domain of magic and religion.
 But surely our brains are the physical substrate of the mind and our brains follow biochemical laws, physical laws of the universe. Therefore, can't our brains be considered to be machines? And if that's the case, are our minds the result of a machine, the byproduct of the workings of a machine? That brings into question the notion of free will. Do we have free will or simply are we reactionary to an environment? If the human mind is the byproduct of a machine?
 And we're creating a computer brain, which is self as a machine, will the computer brain create as a byproduct a computer mind? This is a very deep, deep rabbit hole of debate in the space of cognitive science. Cognitive science incorporates philosophy, neuroscience, psychology, and artificial intelligence. All these people, these philosophers coming together and debating whether or not, in fact, a machine, artificial intelligence can be conscious. Let's talk.
 about the words intelligence and consciousness. When I say that a machine is artificially intelligent, it is intelligent if it is effective at performing its mental task at hand. In fact, maybe a calculator is intelligent by that definition. I think a lot of people will say, yes, well, it won't have intelligence like humans have intelligence. You can say, well, okay, so human intelligence is superior to machine intelligence at present, but you can compare them. They're not apples and oranges. Intelligence is intelligence. What you're thinking about when you have a debate in mind.
 is consciousness. Is the computer experiencing the phenomena that it's simulating or that it's seeing in the world in the case of a self-driving car or it's computing an algorithm? Is it experiencing these things? An experience is called qualia in the world of philosophy of consciousness. Is it experiencing this things? If you're religious then I'm sure you already have your own conclusions about consciousness where consciousness basically equates the soul. This is called dualism, the soul is separate from the brain.
 the physical substrate is separate from the byproduct in a substantial way. They're in different dimensions. That's called dualism, and one of the biggest proponents of the philosophy of dualism was Renee Descartes. While we've moved past dualism, you won't really find very many philosophers and cognitive scientists who believe in dualism, now they believe in most likely monism, which says that the brain is the mind in some important way. And so how does the brain create a mind? A very prominent subfield of consciousness philosophy is called function.
 that is accepted today. And I'm gonna boil it down to this and I may get in trouble for this, but if it walks like a duck and it talks like a duck, then internally it is experienced as being a duck. So if a human expresses intelligent action that appears to stem from consciousness and a robot expresses similar intelligent action, then question mark. This segway is effectively right into Alan Turing's test for consciousness, which is called the Turing test. If you're talking to a robot and you're talking to a human, they're both...
 behind curtains and you can't see which ones which and you can't tell the difference then I think we've done it. That's called the Turing Test. So this is so fun. The discussion of Can robots be conscious? Can artificial intelligence be conscious? And it warrants a whole podcast series of its own. I'm going to point out some good resources at the end of this episode and I highly recommend digging in. They're real fun. And finally the last big philosophical throwdown in the space of AI
 So the big scare will AI inevitably rise against us. One of the most prominent books in this space is called Super Intelligence by Nick Bostrom. And an example of his thoughts goes like this. It's called the Paperclip Maximization Algorithm. If you start utilizing AI at a paperclip corporation to maximize the production of paper clips, and you don't specify not to destroy the earth in the process, what might happen is that the AI might...
 find a way to achieve consuming all the resources of the planet in order to maximize paperclip production. The general idea goes that be careful what you wish for because you may not be able to specify all the constraints. Big figures like Elon Musk, Stephen Hawking, and Bill Gates got on board with this at one time and it became really big in the press. Me personally, I don't think that there's warrant for such a scare, but I'm going to leave it to you to come to your own conclusions by diving into the resources. So here are the resources. The book, the singularity.
 is near by Ray Kurzweil describes from A to Z the whole process of the singularity. And this is kind of a must read for anybody in AI. It's very fun, it's very fluffy, it's, you know, not rigorously scientific, but it's very, you should read it. In the space of consciousness and whether a machine can become conscious, Ray Kurzweil also wrote a book called How to Create a Mind. That's also a very good book. My favorite resource on consciousness is a video series by a company called The Great Courses and...
 The series is called Philosophy of Mind, Brain, Consciousness, and Thinking Machines. And this goes over all of the philosophy of consciousness from the beginning until now, including Renee Descartes and Dualism, and including artificial intelligence. It's really, really good. I strongly recommend it. And then there's another book that I'm currently reading by Daniel Dennett, who was a prominent modern philosopher in consciousness called Consciousness Explained. And when it comes to the fear stuff, there's a book, of course, I mentioned, called Super Intelligence by Nick Bostrom.
 And all of these resources I'll put on my show notes on OCDVEL.com forward slash podcasts, forward slash machine learning with a hyphen forward slash three. In the next episode, we're finally going to get into technical details. I'm going to describe machine learning from a high level overview and how it's broken down into the subfields of supervised unsupervised and reinforcement learning. Hope to see you then.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. February 12, 2017 and this is episode four algorithms intuition.
 In this fourth episode, we're finally going to get into the details of machine learning. So let's recall how machine learning fits into the overall artificial intelligence puzzle. Artificial intelligence has broken down into multiple subfields, such as reasoning and knowledge representation, search and planning, and then there's learning. And learning has its roots dug into all of the other subfields. In each subfield, you might perform an action, and then you might make a mistake in that learning from your mistake part.
 is learning. So machine learning is broken down into three steps. Infer or predict, error or loss, train or learn. When I say or it means that these are synonymous words. So infer or predict, error or loss, train or learn. So let's do an example where we're playing chess. In chess I might make a move, I might move the pawn, and then let's pretend that immediately I can know how bad that decision was. Obviously that's not the case, but let's just pretend.
 Immediately, I know how bad was that move. So the move itself was a prediction. I made a prediction to make a move, and then I made the move. And then determining how bad that was is called the error or loss function, which is step two of the machine learning process. Error or loss function, the error or loss function tells me how badly I did. And then I train myself, I learn from my mistakes. So with my error function said that was a really, really, really bad move.
 Then I'm going to update my model, what's called a model. I'll get back to that in a bit by substantially reducing some numbers. These numbers are called weights. But if I made a good move, then the error function might tell me to reinforce my weights. Good job. So infer or predict error or loss function and train or learn. So like I said, when I use the word or it means that one or the other of these words might be used when you're reading a textbook or watching some video series.
 I'm just going to start using one or the other word myself going forward. So I might only use the word error function instead of loss function. So here's how the process of machine learning usually works. Let's use an example that we're trying to predict the price of a house on the real estate market of Portland, Oregon. And this is kind of funny. In machine learning, you're going to see a lot of commonly used examples over and over. I call these the Hello Worlds of machine learning, where they reference a data set like a professor.
 or a textbook or a video series will reference some very common data sets such as the iris flower data set or the Portland Oregon real estate market data sets. So let's use the real estate example. What we're going to try to do is be able to predict the cost of a house in Portland Oregon given a number of circumstances. So for example, where's the house located? How close is it to downtown? What are the number of bedrooms, number of bathrooms?
 bathroom, square footage, et cetera. Now the way that the machine learning process works is that you typically pre-train your model by uploading a giant spreadsheet, let's say one million rows, one million houses in Portland, Oregon, where we already know the price of the house. We upload this to our algorithm. So in machine learning, there are many algorithms dedicated to specific tasks. So for example, in the next episode, you'll see linear rigs.
 is one of the best algorithms used for this exact type of scenario. You import your spreadsheet into your linear regression algorithm, for example, and the algorithm trains its model. It pre-trains sort of, it takes all these rows, row for row for row for row, and trains and trains and trains and trains, until it thinks that it has a very good sense of what it takes to determine the cost of a house in Portland or...
 organ. So that initial step, it's kind of like pre-training, but it's actually that's training, that's machine learning, that's part of that whole three-step process that we're talking about. And then going forward, now what you can do, now that you have a trained model, is that you can predict the price of a house that you've never seen before based on your model. And then let's say that you're the real estate agent, you sold this house to some young couple, they haggled you down, so you gave them the...
 price that came out of this machine learning algorithm, the predicted price, and they haggled you down to $30,000 less. You go back to your machine learning algorithm and you say, dude, what are you trying to embarrass me? It was $30,000 less. And so you tell it that and it goes back and it retrains its model. So every new example going forward is another piece of a training step. So that first phase of uploading your spreadsheet and training your model, that kind of pre-training phase. I mean, this
 just called training and then every new step, every one single example going forward where you retrain your model is called online learning. Okay, so we used a few words here. I'm going to tackle these words one by one. When I say algorithm, that's a piece of software written in Python and TensorFlow typically. We're going to talk about languages and frameworks in a future episode, but that's a very popular starting ground in machine learning these days. Python and TensorFlow. An algorithm is say 100 lines with Python.
 on Intensor Flow Code, which for example, spells out the linear regression mathematical algorithm. Now, when you pipe in your spreadsheet, a million examples of houses that came with their prices tied to them. Your algorithm runs over those rows and crunches some numbers to determine what it finds in common with all of these houses that really determines the cost. And it saves that information on disk or in a database.
 or in memory, but kind of tied to the algorithm itself. And the algorithm plus the saved data, sort of the stored pattern, the combination of those two things is called the model. So the model is the algorithm plus the pattern that it has learned. Now the spreadsheet that you import into your algorithm, it has rows and columns. The rows are the houses themselves, house one, house two, house three, house four, etc.
 The columns are called features. Features are pieces of the house that contribute to your prediction. So for example, number of bedrooms, number of bathrooms, square footage, and distance to downtown. Each one of those pieces of a house is called a feature. So the rows are the houses, and the columns are the features of each of those houses. And then typically at the end of the spreadsheet, the last column, the cost of the house. And the machine learning out.
 than what it will do is it will take out that last column and remove it, put it aside, and then consume the rest of the spreadsheet, which is each house and all of its features. Now features sort of deserve a podcast episode of its own. Features can be multiple types of information. So for example, number of bedrooms, number of square footage, those are numbers, so those are numeral features. There can be nominal features, which are sort of categorical. So is this house down to?
 or is it not rather than using distance to downtown? Yes or no, that's a nominal binary. Or you can say, accepts what types of animals, cats, dogs, or birds. That's a nominal feature that allows three classes. And it's features that are the most important part of the learning process. The algorithm determines what coefficient to put next to what feature and what you get is an equation. So that's the important part to realize.
 algorithm will look at your row, a single row at a time. This house has four bedrooms, two bathrooms, 800 square footage I don't know about houses, so I'm obviously getting this wrong. And it's one mile to downtown. Those are numbers. Those are numbers associated with each feature. And the algorithm, what it learns, is what number to multiply each of those features by. It's called in mathematics a coefficient in machine learning algorithm.
 it's called a weight. So let's say that the algorithm goes over all of your houses, row by row by row, looks at all the features, and determines that the best weight to determine the cost of the house is multiplying the number of bedrooms by point three, the number of bathrooms by point seven, the square footage by point five, and then what you get is a number. So the things that the algorithm learns is the weights or the co-efficient...
 that it puts next to the features. And the features are your variables x, y, and z in algebra. In fact, algebra is a primary math of machine learning. It's actually linear algebra that we're going to be using in machine learning. What linear algebra does is it allows you to take a bunch of equations and stack them on top of each other and then solve for all of them together. And that's exactly what you're doing when you pipe your spreadsheet
 your algorithm, each row is basically algebraic equation where the machine learning algorithm is learning the coefficients next to each feature. The whole combination of rows by columns, your entire spreadsheet is called a matrix in mathematics. So your machine learning algorithm is working with a matrix of features in order to compute a matrix of coefficients. Averges them all together, boils them down to a single formula that can then be used.
 to make a prediction for the cost of a house in Portland, Oregon. So that's very jam-packed, and we're going to be piecing this apart over coming episodes. So don't worry if you didn't understand it this time through, but let's take it from the top. Machine learning is three steps. In first slash predict, an error or loss function, and the train slash learn step. What we do is we will import a giant spreadsheet of a million houses from Portland, Oregon, or...
 All of their features, whether it be number of houses, square footage or distance to downtown, and the last column of the spreadsheet is the actual price of each of those houses. The machine learning algorithm will take it row by row and actually make a prediction, like a shot in the dark. Part of the algorithm will tell it how bad it did for that one row, how far were you off in that single instance? That's the error function. The error function will tell you...
 a number or a vector of numbers on how far off you were from your prediction. The machine learning algorithm will then update its coefficients or weights. That's the training process or the learning process. If the error function said you were off by a huge margin, then it will substantially change its numbers. If it was off by a small amount, it will change its numbers by a small amount. And then now going forward, we can make predictions on new houses on the market that don't already have a price assigned to them.
 The machine learning algorithm will use this algebraic equation that it has constructed. Make a prediction, a numeric prediction. We can give that to the buyers of the house. And if we are wrong about that prediction, we can tell the machine learning algorithm it was wrong by what amount using the error function and it will retrain its model and that step is called online learning. Okay, so that's the general approach. The machine learning is actually broken down into three categories of algorithm.
 Supervised learning, unsupervised learning, and reinforcement learning. Supervised learning is what I just described to you. Everything I just described to you up until this point is actually falls onto the category of supervised learning. What supervised learning does is you train it with a bunch of information that spreadsheet that you upload, you train it on data, and you can train it on data going forward. But the point is that you're training it. You are training it.
 So it's like you have a newspaper in your hand and you're spanking a good dog, a bad dog, based on whether it got a prediction correct or incorrect. Examples of this type of algorithm are linear regression and logistic regression. So those are two types again. One will give you, for example, a number output. So the price of a house, that's one type of supervised learning algorithm, what's called a continuous function. So based on the inputs, you get a number. And another type of supervised learning algorithm.
 them will predict for you a class. So is what I'm looking at a cat or a dog or a tree or what? That's supervised learning. Unsupervised learning is a little bit different. The machine will take your examples, the new rows that you give it, and it'll toss it into a bin. For example, this is called clustering, a sub-branch of unsupervised learning. It'll kind of put it in a bin, but that bin doesn't have a class. It's not like I'm putting this ball into the cat's category and putting this ball into the dogs cat.
 category. I'm looking at this ball and it's blue. I'm kind of turning around in my hand. It's blue and it's a little bit misshapen. I'm actually just going to put it in this bin over here. And then I pick up a new ball and this one's red and it's got spikes. I'm going to put it in that bin over there. But when the human asks me kind of what I'm calling these bins or why I made my decisions, I can't really explain it. It's just that these balls over here have some something in common, a lot more in common with each other than compared to these balls over in this bin.
 So unsupervised learning is the machine learning algorithm kind of figures out what is in common between this and that and segments portions of a data population. So an example of where unsupervised learning might be used is an user segmentation in advertising. So I want to determine based on user actions on Amazon.com, what they click and whatnot. What kind of add to show them? These types of users, I will show add one and these types of users I will show.
 So add two, and that's market segmentation. Supervised and unsupervised learning are obviously a little bit different than each other, but as a machine learning engineer or a data scientist in a professional setting, you're probably going to be using a mixture of both of these algorithms on a day-to-day basis for your job. They're different, but they generally use the same types of mathematical equations, and don't be afraid that you're going to have to kind of make a giant mind shift from one to the next. On the other hand, reinforcement learning is...
 is a totally different beast. Reinforcement learning to me is where most of the fun and the action is happening in machine learning right now. I consider reinforcement learning to be sort of the real AI in machine learning. The stuff that makes you think is this thing thinking. It's the real juice and meat. It's the stuff that deep mind is most heavily involved in. And it's also the stuff that sort of reaches its roots into the field of planning and searching and re-
 and knowledge representation in the other fields of artificial intelligence. Supervised learning is really interesting because it applies to fields like vision and speech. You can imagine, for example, where you are training a machine learning algorithm on a corpus of dialogue, and it learns kind of what makes sentence structure common from one sentence to the next so that it can actually construct a sentence on its own and have a conversation with you. Supervised learning, but reinforced.
 Learning is involved in taking actions, taking steps. So for example, robots taking a step in an environment or an artificial intelligent bad guy in a video game trying to figure out how best to kill you. The way reinforcement learning works is that you give it a goal. Let's pretend that we've got a mouse in a maze trying to find cheese at the end of the maze. The goal is the cheese at the end of the maze and then you give it a
 system of reward and punishment. That's the reinforcement step. So let's say that if it gets the cheese, it gets 100 points. And if it falls into a pit of doom, then it loses 100 points. The reinforcement machine learning algorithm will figure out on its own what are the rules of the game, what are the actions it can take, and the best strategy to achieving its goal. So unlike supervised learning, where you, the programmer, train the algorithm with a...
 a bunch of data and it learns the equation for what all this data has in common in order to predict a numeric value or a class, a reinforcement algorithm learns from its own behavior what actions to take in an environment. So reinforcement learning is sort of action oriented and it trains itself. So you can see why this is the really juicy, cool stuff. Now, unfortunately, I'm not very savvy on the world of reinforcement learning. I'm actually...
 learning a lot of those algorithms myself right now. And also it's not a field that presents itself to people who are just getting started in the world of machine learning algorithms early on. Reinforcement learning is an advanced topic in machine learning. So we're not going to cover reinforcement learning for a long while. Many, many, many episodes down the pike. But just to kind of give you a general idea of how they implement these algorithms. A common algorithm I see these days is called deep Q networks. So
 uses deep learning, machine learning algorithms, in order to determine the best course of actions. A course of actions is called a policy in reinforcement learning towards achieving a goal. You give it negative points for certain things in an environment and positive points for certain things in an environment. And kind of the key to the puzzle for making reinforcement learning algorithm solve its puzzle quickly and effectively is this thing called the floor is lava. Basically every step the mouse takes in the maze.
 is minus one. And so it wants to get to its goal as fast as possible. The floor is lava. Being alive hurts. So I need that cheese ASAP. So those are the general principles of machine learning. So starting from the top, again, this is all very hierarchical. Artificial intelligence, a subfield of artificial intelligence is machine learning. Machine learning is broken down into three subfields, supervised learning, unsupervised learning, and reinforcement learning. And then in each subfield of machine.
 learning, there are multiple algorithms. So for example, in supervised learning, there's linear regression, logistic regression, et cetera. And we're going to be in the next episode talking about linear regression, sort of the 101 Hello World machine learning algorithm. Now, I'm going to go on a tiny little break, maybe three weeks. I'm going to put this podcast series out on Reddit and some other places. And I just want to see how interested people are in this series. If I see a lot.
 of interest expressed, then of course I'm going to absolutely continue and I will do so in post-haste. If I don't see a lot of interest right away, then I will continue, but maybe I'll do it a little bit slower. I want some feedback. If you guys could go to ocdevelop.com forward slash podcasts, forward slash machine learning, and click on any episode and go into the comment section, or you can just email me at Tyler Renelli, T-Y-L-E-R-R-E-N-E-L-L-E at gmail.com with any feedback. I want to know are these podcast steps?
 Those are they too rushed, are they too basic? Again, my goal is to teach you machine learning, anybody who wants to know machine learning, who doesn't have to come from a programming background or a mathematics background. Just let me know overall kind of how this approach is working for you and if there's any suggestions or improvements that you have. As far as resources for this podcast episode, I'm gonna post an article by machinelearningmastery.com, which is a very good series of articles on introductory topics and machines.
 learning is sort of what this podcast aims to be, but in article format. And then I'm going to recommend the master algorithm. The master algorithm is a really good book. It's sort of like this episode and potentially the next four or five episodes where I'm going to be spelling out the basics of some very fundamental machine learning algorithms. And the reason I like it is that you can actually, it's very introductory, very accessible for newbies. And it's also an audio format. And I'm going to try to be right.
 recommending as often as possible audio format machine learning and mathematics resources. Because like I said in a prior episode, the best way to learn machine learning is through books and especially courses like the Coursera and Udacity courses that I'm going to recommend later. But having an audio supplementary backup resource while you're running or cleaning or commuting, there's a lot of time in the day that a lot of people could learn some really valuable information from audio.
 Audio resources that's wasted. So as often as possible, I'm going to recommend audio resources. So check out that book, The Master Algorithm, and I hope to see you in the next episode.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode five linear regression. In this episode we're finally going to get into the nitty gritty your
 first machine learning algorithm linear regression. Now let's recall about machine learning that it's a very hierarchical field. Starting at the top, something I call mathematical decision making. I don't actually know what they call this super field. It can be broken down into artificial intelligence, statistics, operations research, control theory, and some other fields. Artificial intelligence is broken down in a machine learning, that's where we are. And then ML is broken down into supervised learning, unsupervised learning, and reinforcement learning. So we're going to go down the supervised learning rabbit hole.
 You'll find the supervised learning is your bread and butter professionally and even maybe academically in machine learning. Of course, you're gonna be dabbling with unsupervised learning. It doesn't come up quite as often as supervised learning. Where reinforcement learning is a little bit more like level 99 territory. You're not gonna see that stuff until endgame. So here we are in supervised learning. A supervised learning can be broken down into two more subcategories. Classification and regression. So classifiers and regressors. A classifier.
 Classifier is a supervised learning algorithm that will tell you the class of a thing. So if I'm looking at a picture, am I looking at a dog or a cat or a tree, the classifier will tell me what I'm looking at, what class of things I'm looking at. A regressor will give you a number, a continuous variable. So the output of your regressor function of your supervised learning regressor algorithm will give you a number, one, two, three, four, five. We're going to talk about classification in the next episode on logistic regression. But in the...
 In this episode, we're gonna talk about linear regression, which is a regressor supervised learning algorithm to give you a continuous variable. It is the hello world of all machine learning. This is where everybody starts. If linear regression, everyone starts here, even before they go off and specialize in vision or robotics or control theory or something, that's kind of level 50 when you unlock the class system. Everyone starts at the beginning, no matter what your field is, linear regression. And the example that we're gonna be using.
 Using linear regression is the exact example from the last episode with Portland housing costs estimation. That's a linear regression example because you're guessing the cost, the numeric cost of a house given the features. So recall, we have a three step process in machine learning. Predict, figure out how bad you did, which is the cost or error function, and then learn from your mistake, which is the training or fit steps. So you're going to find in machine learning that there's like 10 words for any given word,
 synonym hell. So the predict phase could be predict or hypothesis or objective or Estimate the error can either be an error or a loss and the training step can either be called learning or training or Fitting or any number of things you're probably going to see Train most offense. So I'll use that myself But let's start at the first step which is to make a prediction the way you make a prediction is by way of a function called a hypothesis function or an objective function and it usually looks
 like either h with parentheses and x inside of it and the h stands for hypothesis. So it's your hypothesis function. The x is going to be your example. So you're going to put into it a house. The house has, remember, it has features such as number of bedrooms, square footage, number of bathrooms, distance to downtown. So x actually gets broken up into x1, x2, x3, and x4. The multiple features inside of the function, inside of the hypothesis function, we take that x, that 1 row, we break it up into its features.
 And then the body of the hypothesis function is multiplying each of those features by a coefficient. So it looks like an algebraic equation. So what you have is h of x, you know, h with parentheses in an x inside of the parentheses equals theta 1 times x1, theta 2 times x2, theta 3 times x3, et cetera. Those theta's sometimes you'll hear w for weight or a coefficient.
 If you're talking to a mathematician, a coefficient, those are the things that you're going to be multiplying by your features in order that the sum of that equation gives you your estimation, your prediction. So for example, let's say that we want to estimate the house cost of a house that has 800 square feet, one bedroom and one bathroom. What we want to learn, the whole process of learning is learning the coefficients, the parameters, the theta.
 the W's, W for weight, theta for, I don't know. The thing that goes in front of the x value, so what do we multiply the square footage by blank times square footage, blank times 800, plus blank times one for one bedroom, plus blank times one for one bathroom. So we're gonna learn those blanks, and going forward we're just gonna call them theta. Now what I'm gonna do in order to make this episode a little bit easier.
 easier to visualize, I'm going to lop off all those theta's except for one. We're only going to be working with one x, x one, which is the square footage of a house. So we're going to pretend that the cost of a house depends only on the square footage of a house. Obviously that's inaccurate. And in fact, if you are using very few features in your hypothesis function, then your outputs are going to be incorrect. Generally speaking up and to a point, the more features you have.
 the more accurate your output is going to be. But just for visualization's sake, we're going to pretend that the cost of a house depends only on the square footage of a house. So h with parentheses and x inside of it, the hypothesis function equals theta one times x one. Okay, so we're on the predict step. Remember, there's a three step process to the machine learning system. One is to make a prediction, one is to figure out how off you were and then one is to learn.
 So what we're going to do, we're going to make a prediction with our hypothesis function. We are going to import a spreadsheet of houses in Portland, Oregon, and their actual prices on the market. So remember every row is a house, and every column is a feature of each of those houses, but we've locked off all the features but one. So right now we just have one row, one column, one row, one column, one row. So it's going to be house one has 800 square foot, house two has 900 square foot.
 And then the second column is going to be the actual cost of the house, what's called a label. And before we go any further, let's try to visualize what we're looking at here. So we have an x value, which is the square footage of each house, and a y value, which is the label, which is the actual cost of the house. So we have an x axis and a y axis, the x axis being the feature, the y axis being the actual cost of the house. And what?
 What we're going to get is a scatter plot, a bunch of dots, kind of in the shape of a football or a skinny cloud, and it's pointing up and right, north-east. So that's all of our data. Those are all our houses with all of their square footage and their actual costs. It gives us a cloud pointing north-east. And what we're trying to learn is a hypothesis function that'll send a line right through the center of that football. So a bad hypothesis function. One with horrible...
 theta parameters will maybe be pointing down and right, southeast, would be the opposite of what we're trying to achieve. And the very good hypothesis function will shoot northeast right through the center of that football. Now, if you think about the function of this line, y equals theta times x, it looks similar to something you may have seen in algebra, y equals mx plus b, point slope formula, which is exactly the formula for a line sloping.
 which is exactly what we're doing here. We're trying to come up with a hypothesis function that's creating an angled line that goes through most of our data. So it looks very similar theta x, but there's no b. On the flip side, there's mx plus b, point slope formula. Okay, so we can replace m with theta. And let's talk about this b parameter. What is b in this point slope formula? If you do recall from your algebra class, what b is, it's called a bias. It kind of shifts line left or...
 right before it even starts sloping. So it's where does your line cross over the x-axis? Where does it begin? And then that mx bit tells it it's sloped. So let's pull that into our equation. We have h equals theta 1x plus theta zero. That's going to be our equivalent of mx plus b. Theta 1x plus theta zero, where theta zero is our bias parameter. What is a bias parameter? A bias parameter is kind of like an average.
 or a starting ground if you don't have any other information. So what would happen if we got a house that didn't have any data on the square footage? Well, then square foot would be zero, which would zero out that theta one parameter, leaving us only with the bias parameter. In other words, what it's saying is, what is the cost of a house if we don't know jack? The cost of a house, if we don't know anything in the Portland market, is going to be the average cost.
 cost of a house in Portland area. Maybe $250,000. I don't know what it is. But it's like somebody asking you, how much would it cost to live in Portland? And you're saying, hey, it depends, it depends on where you are, the number of square foot. No, no, no, just how much does it cost to live in Portland? You're like, no, it depends on the square foot. No, no, no. How much does it cost to live in Portland? Fine, $250,000. I don't know. That bias parameter is where do you start if you don't have any information? It's like the average and it's where you start.
 start moving from once you start applying your other parameters. So our new linear regression hypothesis function is h equals theta 1 times x1 plus theta 0. Okay, so now that we have a visualization in mind, let's return to our prediction step step one of the machine learning process. And linear regression is basically what it's going to do is it's going to put these labels, these y values on one side of a flash card, right? And it's going to look at the other side, the forward facing.
 seeing the side of the flash card that says the square footage of the house. And it's gonna make a random shot in the dark guess. It seems kind of weird, but you'll see why it does this in a bit. It's gonna look at 800 square foot house in Portland. It's gonna say, um, $10. And you're like, $10? Really? This is the house, you know? I'm sorry, hey, I don't know. I haven't looked at any costs yet. And I'm like, okay. It picks up the next card and it says, uh, $20. This one's $900 square feet. And on the back, it actually says $200,000.
 but I can't see these values yet. Goes through it, it makes a prediction for every house, $10, $5, $3, $2, $7. Once it gets to the end, now it's allowed to collect all the cards, flip them over, and it's all my gosh, it slaps its head and it says that one was way off. So what it does now, now is the cost step, step two. Figuring out how bad it did. It's called the cost function or the error function. And it's very simple functions, very simple formula.
 It's basically just the average of all of its mistakes. So the distance between its estimation, y hat, or h of x, and the actual value, the label, or y. So h minus y. But we want to average all these. So we're going to sum them all up, and we're going to divide them by the number of examples. Now it's a little bit more tricky than this. There's actually a twist, the puzzle. We're actually going to sum up the square differences of the actual and the prediction. So it's going to be prediction minus actual.
 quantity squared, and that squared doubles as an absolute value because you don't want positive and negative differences to be canceling each other out in this average. Quantity squared sum the whole thing up divided by 2m. So we have two twos, we have a square and we have a 2 in the denominator. These come to play in gradient descent, you'll see that in a bit. But the main reason there is so that we can take a derivative, you'll see that in a bit. Now, in order to visualize the cost function, we had our hypothesis function on a table, an x, y graph.
 Okay, imagine that that's on a table in a kitchen and it looks got that football cloud of dots, the scatter plot. Now we're going to move to a new table and on top of this table is a bowl. So this is now a three dimensional graph. We have the x and y plane and a bowl on top of it going up, which is the z-axis. And the way we're going to visualize our cost function is the x-axis might be theta zero or theta one. And Y.
 will be the other theta. So let's say the x axis is theta zero and the y axis is theta one. And the error is into the error. So at some value of theta zero and theta one, we have an error of 100, for example. When we plugged in our random shot in the dark guess in the initial pass when I was kind of going through the flash cards, I was going through all of the houses, the linear regression algorithm is going through all the houses and taking a guess and taking a guess and taking a guess.
 and it was way off, it had assumed some random values for theta zero and theta one just to start with so that it can know how bad it had done. How bad it had done is a value returned by the cost function and it is on that z-axis, that up and down axis. It's in the bowl somewhere. So it's a dot inside of a bowl. Now we move on to the learning phase. What we want to do is we want to take that dot and move it down in...
 into the very bottom of the bowl where the error is zero. We want, right now, the error is way high in the bowl, the dot that we have with our theta parameters, theta one and theta zero. The way they're set right now gives us a result way up here on the rim of the bowl, but we want to take that dot and we want to move it down the bowl. The way we do that is through a process called gradient descent. And at last you learn your first...
 learning algorithm gradient descent gradient descent uses calculus okay calculus to take the derivative of the cost function in order to take a step one step at a time take that dot and move it down the bowl until we found the place on the x y plane of theta zero and theta one where the z value the cost value is minimized at the bottom of the bowl so how gradient descent works is like
 this. You take the derivative of your cost function where you are in the graph. So you're a dot on a bowl, high up on the bowl. You take the derivative of that function with respect to that point. And if you know anything about calculus, what a derivative tells you is the slope of your point in that function. So if you can imagine taking a piece of paper and pushing it against the dot, so kind of pretend that the bowl is really, really thin paper.
 thin so the dot could be either on the inside or the outside of the bowl. It doesn't matter. And you push a piece of paper against that dot. The piece of paper has a certain slope, like it's pointing kind of downwards, right? Really steep. That is the tangent line or the derivative of your cost function at that point. And what it tells you is that you're doing really bad. If your slope is really steep, it means you're very far from zero. So what gradient descent does.
 as is this, it says, okay, guys, you're gonna need to take a big step. I'm gonna say 12 inches Southwest. 12 inches Southwest. So, South, let's say, is Theta 1 and West is Theta 2. So, those guys change their number. They change their number. So, you just updated your Theta parameters. That's the learning process. It's changing your Theta parameters, learning your weights so that your function is now...
 more accurate. Now you can imagine what just happened on our hypothesis function because we actually just changed the original theta parameters that are inside of our hypothesis function. You can imagine we remember that football graph, the scatter plot, that cloud, and we have an x and y axis. This is the other table to our left. And we when we made our first initial prediction, wild guess, shot in the dark that was really bad saying houses cost $10 and $20 are long.
 maybe was pointing south east, right? The exact opposite of what we want it to be, which is northeast and right down the center of the football. This step that gradient descent just took by taking a derivative of the cost function in order to minimize the value that the cost function outputs by changing the theta parameters. It's like grabbing our line and rotating it counterclockwise, one big step. So gradient descent.
 and now steps back and it's like, oh, okay, yeah, yeah, yeah, we're getting close. We're getting close. That was a good step. And scratches that's chin and it says, okay, I'm going to take another derivative here. So we got our new theta parameters in place, our new hypothesis function, our new cost function. Um, why don't you take in order to minimize your cost, another step southwest. Let's do a nine inches south and four inches west. So that's another iteration.
 of the gradient descent process. So it takes that step. It changes theta one and it changes theta two, smaller this time. Now we're closer to the bottom of the bowl in our cost function and our line and our hypothesis function just rotated even a little bit more counterclockwise. Now the line is actually touching some of the dots in our scatter plot from our original spreadsheet. And gradient descent says, okay, okay, we're really close guys, we're really close. Now I want you to take just one more baby step. I took the...
 derivative and I've determined that you just have to take one more baby step southwest. So our theta parameters are updated theta one gets a alteration theta two gets an alteration our hypothesis function has changed and we got a line going right down the center of the football. So gradient descent has learned for us the parameters that will give us the smallest error they call it minimizing the cost or minimizing the error traversing the bowl in our error graph all the way to the bottom.
 the ball. So that our hypothesis function is most accurate. So that's kind of cool. You actually see math in real life. You're using the derivative of a function in order to figure out how big of a step to take in which direction to change your coefficients so that you now have a more accurate function. This process of learning is also called function estimation because you're estimating the parameters of your function that will give you
 more accurate output. Now I want you to note, you're never really going to get a cost function of zero. In order for the cost function to be zero, all of our examples have to be exactly on our hypothesis function line. So it's like one, you know, x is one and y is one, x is two and y is two, x is three and y is three. That's, you know, you're never going to see that in the real world. Remember, we had like a cloud that looks kind of like a football and the line goes right down the center. The error is not zero.
 the error is the squared difference of the errors of all the points to the line that we created, which is some number, some positive number. But when it's right down the center and it looks like it's just it's fitting the graph just so, then the error is minimized and that's the ideal place for our line to be. By the way, the error function in this case where it's the squared difference and all
 summed up and divided by 2m. This is called the mean squared error. Now real quick, I'm going to tell you the equation for gradient descent. And remember that gradient descent is taking the derivative of the cost function. Okay, the derivative of the cost function. And the cost function one more time, the cost function is your hypothesis, your prediction minus the actual value. So y hat minus y is quantity squared. All.
 All of those, the sum of all of those in your spreadsheet, divided by two times M, divided by two M, and M is the number of examples in your training set. Now to take the derivative of a function, you don't need to know calculus right now. You can know some of the very basics. There's some rules. There's some rules where you don't actually have to go through this derivative process. And if there's a power in front of a function, you can just take that out and put it in front and subtract that by one. You do these little tricks.
 the trade. And that's the reason that we have these twos in there. The two at the bottom, the two m divided by two m, and the two at the top, the square. Well, when you take a derivative of the cost function, those, that two comes out in front and it kind of cancels itself out. What we get in the end is this is the gradient descent algorithm. Theta zero equals what it was before minus alpha times one.
 over m times the sum of the differences, y hat minus y. So you took a step, you had theta zero, the bias parameter, take a step in some direction by some amount. By the way, the alpha variable is called the learning rate and I'm not gonna talk about it. I'm gonna let you explore this in the details from the resources section. Theta one is theta one. So what it was before, minus alpha times one over m times.
 the sum of y hat minus y times x i, the feature. And you don't need to remember that. Let it go in one ear and out the other. This is more for people who are just mathematically inclined and curious. You're gonna learn all the details of this in the resources section that I'm gonna talk about later. Okay, so that was a lot of information. A lot of information. I'm going to basically start from the top. I like to do this a lot. I'm sure you've noticed by now. I like to start from the top and do it all over again. Now.
 that we have all the pieces in place, let's do it all over again. Machine learning is broken down into supervised unsupervised and reinforcement. Supervised is the case in which you are training your algorithm with a bunch of data. So Portland Oregon housing costs sounds like a supervised learning algorithm to me. In supervised we have classification and regression. Regression is coming up with a number. Classification is coming up with a class like cat dog or tree. In Portland Oregon we are trying to figure out the cost of a house so it sounds like...
 regression to me. Linear regression is the most fundamental machine learning algorithm whatsoever, but also of course it is a regression algorithm for estimating a number. So we're going to use linear regression. We have three steps. We make a prediction. We figure out how bad we did with our cost function and then we train. We learn how to fix that mistake. In the prediction step, we have our hypothesis function. Every machine learning algorithm has
 a different hypothesis function. The hypothesis function will change from algorithm to algorithm, as well as the cost function. The cost function will change because the hypothesis function is different. Remember that the cost function is based on the hypothesis function. And then of course the training step for every machine learning algorithm will change because taking the derivative of your cost function will be different depending on the function. In the linear regression model, your hypothesis function looks like this.
 Each of x equals theta zero, which is your bias parameter, which tells you the average that you're working with if you didn't have any other information to work with. Plus theta one times x. Theta one is your weight or your coefficient that you're trying to learn. You're trying to learn the bias parameters well. You're trying to learn the theta parameters. And x is the feature that's gonna come into this function for every row that we're looking at. We make a prediction.
 with the hypothesis function, we'll call that prediction Y hat. And in the initial batch of things, we will go through our spreadsheet one row at a time, make a prediction, make a prediction, make a prediction random shot in the dark. We're gonna use theta one and theta zero are gonna be random values, and we're gonna get random results. Step two, our cost function will tell us how bad we did, how far from the truth we were on average. It's called the mean squared error.
 in the case of linear regression, and it is the average of the differences between our estimations and their actual values. The equation looks like this. One divided by two m times the sum of all the differences squared. So h of x minus y squared for every row. Summ them all up, divided by two over m. Cost function. That tells us how bad we-
 did on average and it's a function which puts us as a dot on a bowl in 3d. We use gradient descent to move that dot down step by step by step to the bottom of the bowl to minimize the error and the error is a function of our theta parameters. So we find the point in 3d space where our dot is at the bottom of the bowl. We find what the values of theta 0 and theta 1 are.
 on the table such that that error is minimized. And now we have learned our hypothesis function, the theta one and theta zero parameters. We now have those handy and we can make predictions in the future. And the way we visualize gradient descent is as moving down that cost function, that bowl towards the bottom of the bowl, which is the same as changing the parameters of our hypothesis function so that we're like grabbing the hold.
 of the line, which was a bad random shot in the dark initially, and rotating it clockwise or counterclockwise until it fits that data set, most effectively, the point at which the error function is minimized. It'll never be perfect, but there is definitely a point where the error is minimal. All right, the savvy amongst you will recall that something is missing here. Something is missing. It is that I have removed.
 all the features but one. I have reduced our situation to something called univariate linear regression, one variable, one feature which is the square footage of the house. But that's not how linear regression works in the wild, of course. You have many features, the number of bedrooms, the number of bathrooms, the distance to downtown, whether it's in a safe or dangerous neighborhood, etc. We'll determine the cost of the house, all things considered. And then of course the bias parameter.
 which is basically the start zone or average that we're working with in this in this housing market. With multiple variables or multiple features, we are dealing with something called multi-variant linear regression. And it's basically the same as univariate linear regression, but it's more difficult to visualize in your mind when I'm describing it to you in audio. So I'm actually not going to go into the technical details of multivariate linear regression. It's so similar to univariate that making the transition
 when you see the details online, you'll understand right away. You'll be able to make that shift. But basically our hypothesis function is going to be h of x equals theta zero plus theta one times x one plus theta two times x two, et cetera. We're learning all the theta parameters through the gradient descent process. But I'm not going to describe the multi-variant linear regression model to you. Instead, I'm going to point you to the resource. The resource.
 section of this podcast boils down to one resource, one resource alone. It is the Andrew Eing Coursera course. Andrew NG. Eing Coursera is COURSERA. And if you've been around the machine learning community for longer than a month, that you will have heard this recommendation a million times over, stack overflow and Cora and Reddit and wherever you may hang out. This is the
 The most recommended resource for getting started in machine learning period. It is a course. So it's an online course. It's like a 12 week program or something. You can go really fast, it's self-paced. I finished mine in three weeks. But more than recommending it to you, I require a view to take this course. This is the most essential starting ground for picking up machine learning. If you start trying to learn machine learning from any other resources, you're not going
 have enough of the fundamentals in place. I read very many books, I started reading text books, I watched a lot of YouTube video series, I listened to audiobooks and podcasts. When I was first starting to learn machine learning, I had heard this Coursera course touted over and over and over, and I avoided it because I'm not, because I don't typically like to learn from MOOCs, M-O-O-C-S, I don't remember what it stands for, but these online courses, I'd rather learn from a book, so I avoided it for a while, which was a big mistake because when I finally...
 hunkered down and took the course. It just pulled the curtains and I saw everything for what it was and I slapped my forehead and I said, why did I wait so long to take this course? Androoing is the best teacher on the planet for machine learning. That course has you doing programming exercises in MATLAB and quizzes and tests and again it's self-paced and there's great visualizations in the videos. It's the course equivalent of what I'm trying to achieve with this podcast. So it's completely...
 Meet 101. It's going to be using a lot of math, but he teaches you the math along the way. But like I say, in the introduction of this podcast with audio being an inferior medium, Andrew Inks' course is far and away the superior medium to learning machine learning. So I'm not recommending it to you. I am assigning it to you. I'm requiring that you start there if you haven't already taken the Andrew Inks' era course. Don't fool yourself into thinking you don't have enough time or that you'll do it when you do have more time in your life.
 Just do 20 minutes a day, even 10 minutes a day. It's self-paced, but it's super fast. It's pretty easy. And you're gonna wanna start chipping away at it now, sooner than later, so that you're prepared for the good stuff once it starts rolling along, like deep learning and recurrent neural networks when I start talking about all that good stuff. Additionally, in cases like this episode, I'm not going to be posting show notes with the algorithms. I'm actually gonna be posting Andrew Eings show notes. I'm going to direct you to the Coursera page.
 with the notes from his courses. The links are gated, they're authenticated, so you're gonna have to create an account on Coursera first before you can access these links. But better today than tomorrow anyway, get that account set up, sign up for the class, and get started. So that's it for this episode. The next episode is gonna be on Logistic Regression, which is a classifier version of Superfie's learning. And if you haven't already yet, please do give me a rating on iTunes U or Stitcher or Google Play. I wanna thank everybody from the Reddit community.
 who came over and subscribed. So we got some subscribers and I'm definitely gonna be moving forward with this podcast. I hope to do an episode every Saturday or every other Saturday. Once I've got a good schedule down, I'll keep you guys in the loop. See you next episode.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode six certificates and degrees. So this podcast episode is going to be a lot.
 fluffier than the last. I realized that the last episode was really heavy, really, really heavy mathematics and algorithms stuff. And I wanna give you a break from that episode before we move on to logistic regression. I may do this from time to time in between very technical episodes, try to punctuate them with fluff. So you get a little bit of a breather. This episode I wanna explore, what are the options for you as somebody pursuing machine learning, whether you want to get into the industry professionally or you want to...
 go down the deep rabbit hole as a scientist and crack the mysteries of artificial intelligence and consciousness, or whether you're just doing this as a hobbyist who's curious about the whole thing, working nights and weekends maybe just on-site projects. And one reason that I want to address this right now is that in the previous episode, in the resources section, I recommended that you all start on the Andrew Eing Coursera course. And while that's not a very difficult course, it is a bit of a time commitment, it's something like a 12-week course. Like I said, self-paced, you could probably finish it.
 read four weeks if you dedicate yourself. But some people may be wondering if I may want to go back for a master's degree in machine learning or do one of these Udacity nano degrees, should I take the Andrew E. Coursera course now or would it be a waste of time since I'm going to be learning that stuff from scratch anyway? So let's approach the types of people learning machine learning so we can answer this question. The first type of person would just be doing this for self-advocation. They just are playing around with machine learning as a hobby for side projects and for fun. I think using my podcast at
 as a curriculum or a syllabus to help guide you in your own self-motivated education would be plenty sufficient for that kind of a person. In the resources section of every episode, I'm going to be providing you sequentially the best resources for learning the details. So early on in this series, like I said, I recommended Andrew Incoursera course. That's one that's gonna keep you busy for the next four to six weeks. Highly highly recommended resource. And then towards the end of my podcast series, I'm gonna be recommending things like artificial intelligence and modern approach.
 textbook and the deep learning book. I think for the self-learners coming up with your own curriculum or letting me guide your curriculum will be plenty sufficient. Then there's people who want to tap into the industry professionally. And what I'm going to actually speak to right now is a middle ground between those two types of people. On the one hand, we have people who are just doing this for fun. And then in the next run, we have people who wanted to tap into the industry and they want to have a, and they want to have a very strong resume. Somewhere right in the middle, we enter this world of what's called...
 M-O-O-C-S. It stands for Massive Open Online Courses. And there are some major contenders in this field. One is Coursera, another is Ed X, Khan Academy, and Udacity. So let's talk about these one by one. Khan Academy is for, actually, for high schoolers and kind of AP students or very early college. So the courses that are offered on Khan Academy are things like calculus, statistics, and linear algebra, would actually be very useful for machine learning education. But I don't think that-
 that they're gonna have anything specifically for machine learning. So it's all the prerequisite stuff. In a future episode that I'm gonna have on math, the various branches of mathematics that are recommended, prerequisites for learning machine learning, I'm going to make a shout out to Khan Academy. But they're not useful for the purposes of this discussion. So we move on to the next one, edX. Now, I see a lot of discussions on Reddit and hacker news when people are talking about whether these MOOCs are worth their salt. And unfortunately, edX is never one that kind of makes the cut. It's never really a contender in these conversations.
 And whether or not this is a good approach given that data, I'm going to use that as a telling bit of evidence that maybe edX is not one of the better platforms for learning data science or machine learning. So we're just going to discard edX. We're just going to put them out of the equation. The two contenders that I see in MOOCs most commonly are Coursera and Udacity. Coursera tends to be more recommended for one off courses. They do offer these nanodegraded competitors.
 certificates called specializations, Coursera specializations, such as the specialization in data science, in machine learning, et cetera. But I find that people don't gravitate towards those or recommend them a lot on social media. Coursera is fantastic for one-off courses like the Andrew Eing Coursera course. So you'll see Coursera courses recommended from time to time, but they tend to be one-off courses. I think in my own experience looking at Coursera, it's a little bit less professional, and has a little bit less in-
 industry backing and post production by comparison to Udacity. So that brings us to Udacity. Udacity has one off courses as well, free courses that you could take very good ones like the deep learning course that was put out by Google. But they also have this thing called nano degrees. There's certificates that you can earn online. You pay $200 a month. I think they tend to be about one to two year programs, these nano degrees. And there's some sort of stipulation like if you finish it and do time.
 than you get half back. These nanodegrees are very professionally put together video series with mentors assignments, programming assignments and class projects and solid peer-to-peer interaction. I absolutely think that Udacity is the future of online education and education in general. I really think that MOOCs are going to be revolutionary in the education space. I really hope that MOOCs eventually kind of overtake the requirement for...
 accredited university degrees because these are scalable, professionally made nano degrees and certificates put together by the best of the best in industry like Andrew Eing and Sebastian Thrun. But however, at present, these nano degrees are not widely accepted, recognized, or respected by employers. Now this is here to say this is my own opinion. You may want to get a second opinion about this and I do plan to revisit this podcast episode in the future.
 Once I've learned a little bit more about the space, I plan to come back and redo this episode. But from my own findings, from talking to recruiters personally, from reading posts by recruiters and hiring managers on various social media websites, it seems to me that Udacity nano-degrees are not respected in industry yet. Masters degrees are, of course, accredited degrees that are respected by corporations. But Udacity nano-degrees are, of course, accredited degrees that are respected by corporations.
 degrees are not. What they do do for the candidate who's applying to a job is they provide you with your portfolio of projects that you have built for that class and they prove that you have the motivation and self-drive to take an online course and the tenacity to finish an online course. So I have seen hiring managers and recruiters say that they very much respect the initiative shown by people who take these courses and the side projects that they've built.
 during the course of these nanodegris, but that the nanodegris itself is not respected. In fact, when it comes to that portfolio that I just mentioned, what I see hiring managers and recruiters say time and time again is that portfolio, portfolio, portfolio. Portfolio is the most essential ingredient to getting a job in industry. Having a strong portfolio of projects that you've built nights and weekends, not just little dinky things that can recognize a cat from an image, but some...
 something that scales, something that you've deployed to AWS, scaled it horizontally, and shows that you have skin in the game in developing large-scale machine learning projects. So that's why I put Udacity kind of in between somebody who's a hobbyist and somebody who wants to get a job. Because it appeals to the hobbyist because you're going to be learning all the good stuff that you want to learn from these nanodegries. And it helps you get a job in the sense that it shows that you're self-motivated.
 and you have a strong portfolio, but the Nanor degree itself doesn't help. So what I would recommend for somebody who doesn't have the money or for, and for people actually listening to my podcast, if you're already listening to my podcast, it shows that you have self-motivation enough that you could follow up with the resources that I give at the end of each episode, sort of a homemade curriculum or syllabus for you to go by. You can teach yourself everything you need to know and you can work on your own side projects. So I'm gonna leave it up to you whether or not you want to pursue.
 a nano degree by way of Udacity or if you just want to start working on side projects on your own and learning everything you need to know from these books and videos. Me personally, I don't plan on getting a Udacity nano degree. Okay, so now we're in that middle category of people who are doing this because they want a job. Like I said, the portfolio is the most important piece for getting a job and machine learning from talking to recruiters and hiring managers. This is sort of the way that web development is.
 in today's generation. In web development, you'll see a bachelor's of computer science listed as a minimum requirement on a job description. But applicants with a strong portfolio can kind of bypass that step. I often kind of view that bachelor's degree requirement on web development, job descriptions as a suggestion. I do have a bachelor's in computer science, so I don't know how much that helps, but my education is never talked about in an interview. Only my past projects. Similarly, I've heard from people that...
 the master's degree requirement on a job description for a machine learning job can sort of be bypassed if you have a very strong portfolio. So focus on that portfolio, but if you do want that piece of mind and extra edge then a master's degree is an actually a credited degree by comparison to Udacity NanoDegree, which is simply a certificate. And we'll get you in many doors that you wouldn't otherwise get into. Now a master's degree can be very expensive, $40 to $60,000.
 at some universities, but there is one master's degree out there that is online, completely online. It is by Georgia Tech. It is called OMSCS. Online masters of science in computer science, and it is $7,000, so it is substantially cheaper than your typical master's degree. The way I understand it works, I'm not entirely certain. I believe they use Udacity courses in lieu of certain classes. It may just be even a Udacity.
 nano-degree disguised as an accredited master's degree. I'm not sure, but I do know that they are partnered with Udacity and they're using at least some Udacity courses in their program. So I think they're able to drive down the cost of their degree because of because they're benefiting from the infrastructure that Udacity provides. Me personally, what I think I'm going to do is start working on some personal projects nights and weekends to build a very strong portfolio so that I can put in applications to companies. If I find that that's...
 not going very well than I plan to apply to this OMS CS program. Now a dirty little secret about me, I have worked professionally in machine learning for a time, but I was sort of grandfathered into the position so I didn't kind of earn my way there. That's why I say, this episode is a little bit opinion based based on things that I've read from conversations around the web and conversations that I've had with recruiters, but you may want to get a second opinion on some of this stuff. Okay, PhD. Why would you want a PhD?
 Now a PhD is not necessary for getting a job in machine learning. It is not necessary. A master's is plenty sufficient and in some cases you can bypass that master's requirement by way of a very strong portfolio. So why on earth would you want a PhD? Now a PhD will obviously get you into more doors like a machine learning engineer role at Google or Facebook, some of the more hardcore basement artificial intelligence development roles. But I find that most people who do go for their PhD, that's not why they're doing it. They're not.
 doing it to crack into industry, they're doing it for the sake of the PhD itself. The thing about industry is that your job is going to be prescribing to you the types of projects that you work on, and the nature of work in industry tends to be a little bit boring. I'm talking about recommender systems like Amazon's recommender system, or anomaly detection programs, or some very simple charts and graphs sort of data analysis kind of gigs. You're working in machine learning, yes, but you don't...
 really have the freedom to explore all the greatest nitty gritty rabbit hole science that's going on in the realm of artificial intelligence and the discussions that are happening around consciousness and the futureology of modern research in AI. Going for your PhD in machine learning or artificial intelligence affords you the time to dive deep into the stuff that makes AI fun. If you see yourself as a mad scientist who wants to solve these riddles...
 than a PhD is for you. Now a PhD program pays you, I don't know if you know this, you get your master's degree, you pay for your master's degree, $7,000 for the OMS CS program. But once you get into a PhD program, they pay you, now they pay you about $30,000. That's by comparison to $160,000 that you might be making in industry in machine learning. So it's kind of a compromise. You're gonna choose on one hand, do I get to work in all of the really...
 cutting edge research and all this fun stuff and make $30,000 and maybe be working 80 hours a week. Or do I work in industry where I have a decent work life balance and a very high pay scale, but the kinds of projects I'm working on are less likely to be very entertaining on average. That's that compromise that you're gonna have to come to terms with. Me personally, I'm going to try to crack into industry first by way of a strong portfolio. If that doesn't work, I'm going to get my math.
 degree and I'm going to reevaluate maybe three years down the line whether I want to go for my PhD. I definitely see value in the PhD program. Really those hard questions being tackled by the scientists is very appealing to me. So I'm going to leave the decision up to you. If you have any experience or any questions around this topic, please comment in the show notes. Maybe we can get some discussion going back and forth. Maybe some more concrete evidence. One way or another. I'm going to post some of the conversations up.
 I've seen from around the web in the resources section and let you come to your own conclusions. That's it for this episode and in the next episode we'll get back to the technical details with logistic regression.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash MLG. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash LLH. This is episode seven, logistic regression. In this episode we're going to talk about classifying.
 namely the logistic regression classifier algorithm. So remember where we are in the artificial intelligence tree. We've gone down to machine learning, down to supervised learning, and then supervised learning is broken down into two subfields, classification and regression. We've studied regression last episode with linear regression, which will give you a continuous variable output, a number. So the cost of a house in Portland, Oregon. And now we're going to talk about classific...
 which will give you the class of a thing, am I looking at a cat, a dog, a tree, a house, or in the case of binary classification? Is this a dog, yes or no? Zero or one? Now you may have noticed right off the bat, the word logistic regression. That is really confusing. Wait, I said that supervised learning's broken down to into regression and classification. So those are two separate categories. Now we're talking about logistic and linear regression. Those both sound like regression to me. How is logistic regression, class of the...
 Actually, the term logistic regression is historical. It was a mistake. I believe from what I've heard try to ignore it If you can the fact that it has regression in the word logistic regression. I Think what's going on there is that as you'll see in a bit we pipe our linear regression algorithm into Our logistic function. So the classification step is a function of our linear regression algorithm So I think that's why regression is in the time
 It's like we're doing the logistic thingy to linear regression, logistic regression. And what does logistic mean? Well what comes out of our classification function is what's called a logit, L-O-G-I-T. So I like to imagine this way. Linear regression is for guessing numbers, like the cost of a house in Oregon. And logistic regression is for guessing classes. This that or the other thing. And what you do, imagine logistic regression is like this machine, like this car-
 cartoon machine with conveyor belt going into it and a conveyor belt coming out of it. And so, it comes our linear regression algorithm and it goes down the conveyor belt, it goes inside of our logistic regression machine and it kind of does that cartoon like, being bang boom, it looks like there's a fight going inside of the logistic regression machine. And then, Outcomes, a number which tells us how confident the algorithm is that the thing we're looking at is a house or how confident it is that the thing we're looking at is a tree. So, Outcome, these numbers...
 associated with the class and these numbers are called logits. So 0.7% probability of this being a house, 0.5% it being a tree, 0.3% it being a dog, and then we pick the thing with the highest logit with the highest probability and we take that class and that function of picking the class associated with the highest logit is called argmax. You'll see this in various machine learning libraries argmax ARGMAX. It says find the thing with the highest number, take that.
 class. Now, a random aside, you'll notice that I said 0.7% likely hit of house, 0.5% tree, 0.3% dog, whatever. Those don't add up to one, right? They don't add up to 100%. So that's not a real probability distribution. That's not a proper probability. If your architecture needs a proper probability distribution, then you pipe those logits. So you go into another machine, you pipe those logits into something called softmax, softmax, s-o-f-t-m-
 AX, it takes your logits and it transforms them into a proper probability distribution, where all of your logits add up to one. We won't cover softmax in this episode. We'll cover that in the later episode. So, logistic regression takes into it linear regression, and then it does BingBang boom in the machinery, and then outcomes logits. One, two, three, four logits. Maybe we have a four-class system. Either this picture can be of a house, a tree, a dog, or a cat, and each logit-
 associated with each class comes out of the machinery and we pick the one with the highest value in this case house with .7 by way of a function called argmax. Now like I did in the last episode where I made the episode simpler to visualize by working with uni-variate linear regression rather than multivariate linear regression and I'm just assuming that you're going to take that andruine Coursera course where you will learn the details of multivariate linear regression. I'm going to make
 So this episode's simpler by working with binary classification. So is this picture a picture of a house or not? Yes or no? Zero or one. So in comes a picture, BingBangBOOM, Out comes one logic, and it's going to be a value between zero and one, where zero represents no, and one represents yes. So it might be point seven, which is the logistic regression algorithm telling us that it is 70% confident that this is a picture of a house. It's not 100% confident. Se-
 70% confident. And what we're going to do with logistic regression is say anything over 0.5 is yes and anything under 0.5 is no. So this is a yes. This is a picture of a house. We're just going to guess that it's a picture of a house. Okay. The example that we're going to be using actually for this episode is the same example for the last episode. We're piping in a spreadsheet of houses in Portland, Oregon. The rows are the houses themselves each column is a.
 feature. So square footage, number of bedrooms, number of bathrooms, distance to downtown, et cetera. The last label from the previous episode was the cost of the house, $200,000, $300,000. That's why or labels. So that last column, remember, is called the labels or the why values, the actual cost of the house. And we're going to be using this spreadsheet to train our model. We're going to use this spreadsheet to train the pattern that we recognize.
 so that in the future we can make predictions. Well again, logistic regression is not linear regression. We're not guessing a number, we're guessing a class. And so in this example, what we're gonna do is instead of saying the cost of a house, which is a continuous variable, that's not what we wanna work with. Let's say, do we consider this house expensive or not? Yes or no, expensive or not expensive? So zero will be not expensive and one will be expensive. And so we'll go through this spreadsheet ourselves manually. Anything, let's just say over 300.
 We'll consider that expensive and anything under $300,000. We'll consider it not expensive. So we're going to modify our spreadsheet. We're going to open it up in Microsoft Excel. One row at a time. We're going to say 0,1,1,1,1,0,0,1,1,1,1,1,1,0. Just replacing all these actual numbers with whether or not we consider it expensive. So we're working with classes here. In this case, binary classification. It could be one of two things. Now, remember how the machine learning system works. We have a three-
 step process. Predict or infer step one step two is our error or loss function and step three is train or learn. So we're going to pipe in our spreadsheet into our logistic regression function and it's going to go through all the rows row by row by row and it's going to make a whole bunch of predictions, a bunch of random shots in the dark that step one the predict phase and then step two remember we're going to use an error or loss function an error function in order
 determine how bad we did, how off were we. And then we're gonna do step three, which is to train our hypothesis function, we're going to train these theta parameters, the coefficients in our function. We're gonna update their values until we have a function that fits our data accurately, a line on a graph that fits our data accurately. Now it's not gonna be a line in the case of logistic regression. So let's dive in, let's open up that machine.
 cartoon machine and zoom in and let's look at these three steps in detail. So the hypothesis function. In linear regression, we remember we had kind of a scatterplot cloud of dots looking like a football pointing northeast and we wanted to shoot a line straight through the center of that football. That's called your regression line in linear regression. Well, we're not going to have numeric values in our case in logistic regression. We're not going to have numeric values, we're going to have ones and zeros. So on
 One side are things that are expensive, based on some combination of the features of the houses, and on the other side are things that are not expensive. So we need a function that somehow gives us zeros or ones or somewhere in between, and our linear function that's aligned going down the football cloud northeast, that does not give us one or zero that gives us a number, 200,000, 300,000. So the function we're going to use is a mathematical function in statistics, it's called a logistic.
 function, logistic regression. A logistic function or a sigmoid function. And the reason it's called a sigmoid function as an alternative to logistic function is that it looks like an S. Imagine if you take an S, you draw an S and with your fingertips, you grab the top right end of the S and the lower left end and you stretch it out. You stretch it out so that on the X equals zero on the X axis coming from negative affinity, coming from the left. You come from the left, from the left, and...
 And then once you get towards the y-axis, you start curving up really fast. You cross over the y-axis at x equals 0.5 at 1.5. And then when x is positive, you start leveling out and then you get to x equals 1 and you go to the right towards infinity. So it's an s on a graph. The bottom of the s is on x equals zero. The top is on x equals 1. Shutes off to the right towards infinity, towards the left towards infinity.
 crosses over the y-axis at 0.5. So we want to fit this S curve, this sigmoid function, or logistic function. We want to fit our data in the graph somehow to that function. What we want to do is create what's called a decision boundary that puts all the data on one side. If it's yes and all the data on the other side, if it's no, we want to learn what that decision boundary is, where we...
 cross over the yes, no, axis. And we're going to train our theta parameters. Remember, that's from the linear regression episode. We have these theta parameters, their numbers inside the function that we're going to learn. We want to train these theta parameters so that we get this good decision boundary. So that's our hypothesis function or our objective function. It is the sigmoid or logistic function. So remember, hypothesis or objective function is the name for the function that we're using.
 in the predict step, step one. And depending on the machine learning algorithm you're using for the task at hand, that function will be a specific function in math. So in this case, in logistic regression, it is the sigmoid or logistic function. In linear regression, I don't, I guess it's just a linear function. I guess that's all you call it. What you call it is just a linear function. Now let me just give you the formula for this function. The formula for the sigmoid function is one of...
 over one plus e to the negative linear regression. That's kind of weird, right? So one over one plus e to the negative, and then we say z, where z is your linear regression function, or specifically theta transpose x, where theta is the vector of parameters that we're gonna learn, where weights, and x is the matrix of examples, your spreadsheet. And if that transpose word through,
 you off. That's a technical detail of the multivariate linear regression step that I skipped in the last episode. But you're going to learn that in the Andrew E. Coursera course. So you'll learn this whole stuff with vectorization and matrix algebra and all that stuff in the Andrew E. Courses. So don't worry about that right now. But one more time, logistic regression function that gives you that S curve on a graph is 1 over 1 plus E to the negative theta transpose
 So linear regression is inside of that logistic regression function. Okay, so step one is we have our hypothesis function and we're going to pipe in our spreadsheet and we're going to map it all on our graph and we're going to make a bunch of random guesses. Remember that step one is to predict, predict randomly. So we're going to be like yes, no, no, yes, no, no, no, yes, yes, yes, yes, yes. When the actual values are no, no, yes, yes, yes, no, no, no, yes, yes, yes, yes, yes. And what we're going to do is now we're going to go to step...
 two, which is figure out how off we were, how bad we were. That's our error or loss function. And just like in step one, where our hypothesis or objective function is going to be a specific function, depending on the machine learning algorithm you're using, in our case, it's logistic or sigmoid function, in this step, our error function will be a specific function as well. R's is called the log likelihood function, because it uses a logarithm in the function. And here's how it works.
 We can't use our linear regression error function because we're not working with numbers. We're working with binary classifications zero or one when the actual value is one, but my guess was zero. How bad did I do? Or vice versa, my guess was zero, but the actual value is one. How bad did I do? Or if the actual value is one and the guess is one How bad did I do? If that's the case, the error should be zero. If I guessed correctly, the error should be zero. Now remember that we're using logits. A scale from zero to one where
 where anything below 0.5 is no, and anything above 0.5 is yes. And we may have guessed in our predict step. Point two, as in I am 20% confident that the answer to this particular case is no, where the actual answer was yes. In that case, we're less wrong than if I would have guessed zero, and the actual answer is one. So our error function is what it is, it's this log function, it starts at zero and it goes towards infinity. Why?
 equals infinity at x equals 1. So it goes up and up and up and up and up and up into infinity. So what we're looking at in our error function is a graph that starts at 0 and it goes right towards 1 and often to infinity, often to y equals infinity before it ever hits x equals 1. The closer my guess is to y equals 0, which is the correct value, the closer my guess is to 0, the closer to 0 is...
 the error. And the closer my guess is to one, even though the actual answer is zero, the closer to infinity is my error. Okay, this is very confusing and don't dwell on the details. You're going to learn this all in the Coursera course. I'm just describing it to you now for thoroughness. Now let's take the other example, flip the graph. In the cases where the house is considered expensive, then here's how the error function works. The closer my guess goes towards...
 In other words, I guess that the house is expensive, I guess, correctly. The closer I go towards one, the graph becomes zero. In other words, the error is zero. And the closer my guess goes towards zero, where I'm guessing that the house is not expensive, even though it is, the closer my error on the y-axis goes to infinity. So this one's like a sloping graph in the other direction. So it's like you're coming down from a ramp on the y-axis.
 and you hit zero where x equals one. Again, I know this doesn't come out well in audio format, so just dive into the details in the angioping course, but I just wanna step you through the process. Okay, so that's the visual representation of the cost function that we're constructing for our objective function, our sigmoid function. The cost function looks like two separate cases of a logarithm. We're gonna combine those two separate cases into one,
 function. And what happens is that one of these gets canceled out depending on whether we're dealing with a yes or a no. It's hard to describe. What the function looks like is y times the logarithm of your guess plus 1 minus y times the logarithm of 1 minus your guess. Okay. That's the error for one row of your spreadsheet, one guess. How bad did you do guessing for one particular row? Some all those up and up.
 divide them by the number of examples, so it's the average of errors. And in this case, it's a little bit complicated. We're working with logarithms, but just go to the Andruine course notes for week three. Okay, that was crazy. Step two was to figure out how bad we did with all of our yes-no, yes-yes-no, no-no guesses. How bad were we off? Now remember, the point of our cost function is to tell us how bad we did so that we can train our hypothesis function. We can train...
 the theta parameters to get a better graph that more accurately depicts the way things are with all of our data. And that's step three. Step three is to train our hypothesis function using our error function. Okay, so our hypothesis function goes into our error function. Our error is a function of our hypothesis and then our error function goes into our train function. Namely
 gradient descent. Remember, gradient descent. The function of gradient descent is to take the derivative of your loss function. The derivative tells you which direction you need to step with all of your theta parameters, which direction each theta parameter needs to change, maybe some negative value or some positive value up down left or right and by how much, so your derivative says how much each...
 of your theta parameters in your hypothesis function needs to change in order to reduce your error function. And we're going to keep doing that. One gradient step at a time, keep taking the derivative and changing your theta parameters until our error function is at a minimum. At the smallest point that it can be. So the hypothesis function goes into your error function and your error function.
 function goes into the derivative function. Remember that the derivative itself is a function. And you repeat the derivative step one step at a time until your error function gives you a small value, the smallest value that it can give you, which means your hypothesis function, going back one step now, is ideal. In our case, it means that our sigmoid function has a good decision boundary that can separate all the yeses on one side and all the noes on another side.
 And then in the future when you make a guess with a new house you've never seen and you don't have the label Is this house considered expensive by our relative definition of expensive? It will throw it on that graph and if our function gives us anything greater than 0.5 than the answer is yes, and if it gives us anything less than 0.5 the answer is no. Okay, so gradient descent trains your theta parameters By taking the derivative of your loss function which tells you
 how big of a step to take in which direction, over and over and over, until your error is small. And the gradient descent formula is for each of your theta parameters, you have your theta parameter, what it was before, minus alpha over m times the sum of all of your guess minus the actual value times that feature in that position. So theta j equals theta j minus alpha over m times the sum of all of your guess minus alpha.
 and to sum from i equals 1 to m of your hypothesis for that row minus the actual value for that row times feature j for that row. Again, you'll learn this in the Andrew E. Coursera course. Oh man, that was wild, huh? So let's run through this one more time. Remember, we have supervised learning broken down into classification where we're trying to guess the class of a thing. Is it a cat dog or tree? And regression, which is where we're trying to guess the value of a thing.
 the continuous variable or numeric value of a thing. And then inside of classification, you have any number of algorithms, such as the decision tree or a Bayesian classifier. And we're focusing in this episode on the 101 classifier, which is called logistic regression. Logistic regression takes a spreadsheet of data whose values or labels the y column is yeses and knows. 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1. In the case of binary class.
 classification. In the case of multi-class classification, it will be any number of classes, but we're not going to talk about that in this episode. We pipe that spreadsheet into our logistic regression algorithm. Our logistic regression algorithm goes over that spreadsheet and makes a whole bunch of guesses. That step one is predict. Step two is determine how bad you did with those guesses. And step three is to take your error function from step two and apply repeated applications of
 the derivative of that function to tell you how much to change your hypothesis function theta parameters so that you can get more accurate and more accurate over time until your error function finally reaches a minimum value. Now you have a hypothesis function that is trained on your data, on your spreadsheet or your matrix. And now when you get new samples in the future, you can pipe it into your hypothesis function and it will give you a guess and that
 guess will be more accurate. The details of each step is that the hypothesis function in our logistic regression algorithm is called a sigmoid function or a logistic function inside of that sigmoid function is actually linear regression. So logistic regression is a function of linear regression. On a graph our logistic function looks like an s, a stretched out s, and we're trying to find a decision boundary that puts all the yeses on one side and all the nose on.
 one side. Our error function in step two is called a log likelihood function and it tells us how off we were with our guess from the actual value. And the function is actually quite complex so I will just refer you to the angeline course to look at the equation and to watch his videos to understand the equation. But in summary, it just tells you how bad you did. And then of course, the training step just applies repeated applications of the derivative of the last function.
 and that loop, doing repeated applications, is called gradient descent. We're descending the error graph to the bottom of the graph where the error is the lowest. That's why it's called descent, you're descending. Now, let's sort of take a very big step back and remember what we're trying to accomplish. I mean, artificial intelligence in the very general sense of the term. Remember that artificial intelligence is being able to simulate any mental task. Now, we don't...
 down the details rabbit hole of linear and logistic regression talking about mathematical equations and graphs and charts and the training process or the learning process was like taking these lines or these S curves and altering them in some way that just you probably feel like you're very far removed from artificial intelligence by now. So let's take a big step back and let's remember the goal, simulating any mental task. Remember that artificial intelligence is broken down into multiple.
 fields one of which is machine learning. And that I said machine learning is sort of the most interesting and essential in my opinion, subfield of artificial intelligence, in that it affects all the other fields. It's almost like any mental task could be boiled down to learning, boiled down to storing a pattern about how the world works so that you can make a prediction in the future, an inference. Now in our
 examples were storing a pattern or a model of the costs of houses in Portland, Oregon that doesn't feel a lot like artificial intelligence yet, or whether a house is expensive or not, yes or no, logistic or linear regression. That's a pattern and then we can make a prediction with that pattern in the future. But if you step back a bit and think about other more high level sorts of machine learning tasks such as let's say you're on the African savannah and you're looking, you're looking
 in front of you, sort of like taking a picture, visual picture of what's in front of you. Oh, there's so happens to be a lion. Now you use classification in order to determine what class of objects is in front of you. Is this a lion, a tree, a house, or food? If it's food, I want to eat it. If it's a lion, I want to run. Okay, so my classification algorithm has determined by way of my stored model that this is indeed a lion. Now we go to another learning algorithm. What action should I take?
 given the circumstances. You may have learned, you know, in machine learning, you may have learned that lions will eat you, either verbally from your parents or maybe one took a bite out of your shoulder one day when you were on the hunt. So you have learned that lions will eat you and that the predicted course of action now, given that there is a lion in front of you, is to run away. So here we have vision turning into action. And if we wanna translate this into a machine learning situation, we might use a convolution.
 neural network in the case of classifying what you're looking at, okay, with vision, and we might use a deep Q network in order to determine what course of action or policy or plan to take given our determination. So everything in machine learning sort of boils down to this learn and predict cycle. But we have to start at the very bottom with linear and logistic regression, the building blocks, the Legos, in order to work our way up to the more advanced high-level topics of things like how to take F.
 in an environment given your state or advanced algorithms in vision and classification. Now I want to go on a little detour. I said that linear and logistic regression are like Legos are building blocks in the grand scheme and that you're learning the Legos are building blocks right now and that's why it's important. Machine learning you will find is a very composable branch of engineering, composable. If you come from a software engineering background or maybe web to them.
 or even mathematics. You might be familiar with this thing called functional programming, functional programming. It's a style of programming and it's used in languages like Haskell or Lisp where you have a function, function A, and it takes as its arguments, other functions, functions B and C, and let's say that function B takes as its arguments, D and E. Functional programming is like Russian dolls where you nest all these functions inside of each other.
 And then eventually at the very bottom, you have to sort of give it a number or a string or some constant, okay? And then you can like start the process and it's like opening these Russian dolls one at a time, you open the Russian doll and what's inside another Russian doll, you open that and what's inside another and you open that and what's inside. This is called Composibility. Composibility, your functions or your equations are composed of other functions or equations which are composed of other functions and so on so everything's nested.
 inside of each other. You already saw this in machine learning by way of logistic regression being composed of linear regression. It is a function of linear regression. So we took our linear regression algorithm and we put it inside of logistic regression. We also saw this in the steps one, two, and three process of machine learning. We have our hypothesis function and we put that into our error function. So our error function is a
 function of our hypothesis function. Our error function is composed of our hypothesis function. And then we put our error function, we put that into a derivative function. That's the gradient descent step that step three training. So in the case of logistic regression, here's how it all unwraps. We have our Russian dolls. The very outer Russian doll is our derivative. Pop that open inside is our loss. Pop that open inside is our logistic function. Pop that open.
 and inside is linear regression. And you will find that everything in machine learning is this way. Now that's kind of a thing in mathematics. It's kind of the mathematical nature of machine learning. Remember that machine learning is kind of like applied statistics really and calculus. Machine learning is highly mathematical and mathematics is highly composable. So it's like this by nature, but this is also a very useful and necessary attribute in order to scale machine learning once you're actually deploying.
 following these architectures in code, putting it on Amazon Web Services, AWS, and scaling them horizontally. If you know anything about functional programming as a software engineer or architect, you know that a proper horizontally scalable system needs to be functional by nature. And you will find that machine learning needs to scale indeed. A lot of these algorithms, especially once we get into deep learning, are very, very computationally expensive, very heavy algorithms and in order...
 to deploy a service that will be used by any number of people, you're going to need to be able to scale horizontally. And in order to do that, the nature of the architecture must be functional. Okay, that was a long-winded digression. One of the reasons I wanted to point out this composability aspect of machine learning is the following. You're probably chomping at the bit to learn about deep learning. That's all the rage in machine learning. And if you came to this podcast because you're excited, you've seen...
 all these articles and discussions on Hacker News about artificial neural networks and deep learning and all the stuff that's happening in that space. Well, patience, my friend, because we will get there and we'll get there sooner than you think. We'll get to deep learning. But, in order to understand deep learning, you have to understand logistic regression and linear regression because logistic regression is a neuron, a neuron in a neural network. So that composability paradigm is that...
 play here. A neural network in deep learning is a function of logistic regression, which itself is a function of linear regression. So everything's composed and nested inside of each other. So before we can get to deep learning and neural networks, we're going to need to learn all these little basics, these linear units and logistic units, because they're going to become neurons inside of our neural network. So that's kind of cool. Learning is a function of shallow learning, we call it.
 it. Shallow learning, these simple algorithms like linear and logistic regression. Okay, so that was a very technical, long-winded episode. I believe, don't quote me on this, but I believe that the next few episodes won't be nearly as technical. The next episode, specifically, we're going to be talking about mathematics. We're not going to go into math. We're going to talk about the branches of math that you need to know in order to succeed in machine learning and how much of these types of math that you need to know. What are the resources that you can learn these?
 things, et cetera, because that's a common question that comes up. What type of math do I need to know? How much of it do I need to know? Can I go into machine learning without knowing any math, et cetera? So we're going to do an episode on that. Sometimes soon, I'm going to do an episode on languages and frameworks. So Python versus R versus MATLAB, TensorFlow versus Theano versus Torch. And then we'll do a high level overview of deep learning and all these things before we finally get back into the technical details. So do not fear. My entire podcast series will not be like this and that linear
 regression episode which are super super technical. Okay what are the resources for this episode? No new resources. I'm going to point you once again to the Andrew InCoursera course. So like I said in the linear regression episode that course is not optional it is required you need to start on it. I'm gonna keep recommending it until we start getting into new territory but I want you to start working on that course. That's it for this episode and I'll see you guys next time.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode eight math. This time we're gonna talk about math mathematics. Not the f.
 equations, but the various branches of mathematics that you need to know to succeed in machine learning. Right away those branches are linear algebra, statistics, and calculus. Now before we go into the details, I don't want to scare you into thinking that you have to learn these things first. In fact, what I am going to recommend to you is not to learn the math first. I know that's going to ruffle some feathers, especially the mathematicians come into this podcast in machine learning.
 especially when you're taking these introductory courses like the Andrew In course or these 101 textbooks. They have in the appendix or the first or second chapter or the first or second lesson of the Andrew In course, a primer for all of the math you need to know to succeed for that level of machine learning, this introductory level of machine learning. And I'm a big believer in the top down educational approach. Learn how to build something first with your own two hands, and then you can learn the theory behind why you did what you did. So for example,
 In Web and mobile app development, you can go to these boot camps where you can learn how to create a website. The high-level essentials of React or Angular, JavaScript HTML and CSS, build out your portfolio enough that you can start applying to jobs in doing this day to day. They don't start with the fundamentals teaching you predicate calculus, discrete mathematics, and assembly language. Some would say that you don't even need that stuff at that high-level. Others would say that maybe an argument can be made that in order to truly become a master.
 of your craft, you want to learn those building blocks. I might agree with the latter, but I would say go back to it later. You'll be better equipped to appreciate why you are learning these fundamentals. And the fundamentals will then snap into place in your mind inside your machine learning algorithms where you were lacking knowledge. So that's my personal recommendation about how you should approach math and machine learning. Learn machine learning first. You're going to learn the essentials of math through machine learning in the first chapter.
 or the first course. Then either choose to learn the mathematics fundamentals after you've learned the basics of machine learning or maybe at the same time you're learning machine learning, some small percentage of your learning time allocated specifically to learning math, say 20% of your learning time or one day a week where the rest of the days are dedicated to learning machine learning. Okay, so let's dive right into the math. What math is used where? Let's start with linear algebra. Linear algebra is the easiest of these branches of mathematics.
 And it's also sort of the most commonly used branch in machine learning. Every step of your ML process uses linear algebra. Let's try to understand what linear algebra does. I'm going to make an analogy to cooking throughout this episode. Cooking. So machine learning is like cooking. And I think linear algebra is kind of like chopping your vegetables. So it's an essential step at every point in your machine learning process, but it's sort of an easy step. You'll learn it pretty quickly. And you'll give you a primer on the file.
 the mental's linear algebra and you'll be off to the races in no time. What does linear algebra do? Remember that when we imported a spreadsheet of rows and columns, okay, we call that a matrix in mathematics. Rows are your houses. Every row is a house, an example in machine learning lingo, and every column is some aspect of that house like the square footage, the number of bedrooms, the number of bathrooms. We call those features in machine learning lingo. And the last column is called the label and it's the actual price of...
 the house that we know so we can use that to compare to our own estimations and figure out how bad we're doing. So there you have a matrix. We call those x's. Every row is a lowercase x. The whole matrix of rows and columns is a capital x. Every column of an individual row is x sub i, i being the column number. So you have x3, x2, x1, and x0. That is square footage, number of bedrooms, number of bathrooms. And then what we want to do is multiple.
 those values, those features, those x features, by theta's, theta parameters. It's theta that we're learning in the machine learning process. We're coming up with these parameters, these coefficients, these weights, that we're multiplying through each of our rows, each of the x values, in order to get one final number, the hypothesis h, or y hat, whatever you want to use, that will be our predicted cost of the house. Okay, so we're multiplying theta parameters.
 We have theta 3, theta 2, theta 1, theta 0. X3, X2, X1, X0. Theta's are the things that we're learning. X's are the actual features of a specific house. So theta 3 times X3, theta 2 times X2, theta 1 times X1, and then theta 0, which remembers our bias parameter, times X0, which we're always just gonna basically make one, because theta 0 stand alone. Remember, MX plus B, Y equals MX plus B.
 The bias parameter is the starting point. If you don't have any data about the house, you're trying to make a prediction. If you don't have any information about the square footage, number of bedrooms, et cetera, then you're just gonna use the average cost of houses in Portland, Oregon. That's our bias parameter. We're multiplying a whole list of theta's by a whole list of features. We're multiplying our theta vector by our example. A vector times a vector. What's a vector? A vector's a list. Theta three, theta two, theta one, theta zero. That's a vector. a
 numbers. We're multiplying one vector by another vector. Now our ultimate goal is to multiply our theta vector by the whole spreadsheet. For every row in the spreadsheet, we want to multiply our theta vector into that row. So the spreadsheet is called a matrix. Now let's think about how we might do this in Python. What we'd have to do is a triple nested for loop. For every row in our spreadsheet. For every column in that row.
 row for every theta in our theta vector, multiply it. And then outside that loop is to add those together. And then outside that loop is to reduce them all together. A triple nested for loop. Okay, so that's fine. We could do that. But what if our spreadsheet had a million rows and 50 columns? Well, that's going to be very slow. And as you'll see once we get into deep learning, you're going to be multiplying matrices and vectors at
 every neuron and you may have 100 or 1000 neurons. So using for loops is simply not an option. That's where linear algebra comes in. Linear algebra is basically doing that exact same thing, but with a single operation. Linear algebra is the study of matrix algebra. It is how can you multiply a matrix, your spreadsheet of rows and columns, your houses, by a vector of theta's all at once. Fam, you just multiply.
 The process is executing this matrix algebra on your CPU or your GPU, as you'll find in a later episode, is done by way of something called SIMD, SIMD, single instruction multiple data. It allows you to multiply a matrix by a matrix or a matrix by a vector with one fell swoop. So obviously that saves you tons and tons of time. And that, my friends, is linear algebra. It is simply matrix multiplication. Okay, let me introduce you to one
 new word, it is tensor, tensor, T-E-N-S-O-R. That is the general word for any dimension list of things. So we had a vector, that's our theta parameters, theta, 3, 2, 1, and 0. That's a vector. So it is a one-dimensional tensor. A matrix, which is our spreadsheet, rows and columns, x3, x2, x1, x0, and then the next row is x2, x3, x1, and 0. That's a two-dimensional tensor.
 or a matrix. You might have a cube. In the case of images, you have rows and columns of pixels. And any individual pixel has RGB values, red, green, and blue values anywhere between 0 and 255. So you'll have a new list kind of depth wise, like looking forward of three items. So that's a three dimensional tensor. And we'll just call it a cube. I think they just call it a 3D tensor. So a tensor is the general word for any dimensional list of things. And in fact,
 The most popular machine learning framework put out by Google that we're going to discuss in the languages and frameworks episode. So linear algebra is simply tensor math, which you could do with or without linear algebra, but linear algebra helps to make it fast and vectorized and easy to learn. And the reason I bring that up is to keep an eye out for that because you're going to see that in the namesake of tensor flow, the most popular machine learning framework put out by Google that we're going to discuss in the languages and frameworks episode. So this is the most popular machine learning framework put out by Google that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that we're going to discuss in the languages and frameworks that
 to reason about. So I like to think of in my cooking analogy, I like to think of linear algebra as chopping your vegetables. It's essentially you got to do it, it's easy. Really somebody will sit down with you, show you how to chop vegetables and you get it. But it's kind of a necessary step that you had to take it every point in the machine learning process. The next step is statistics. Statistics is the hard part, the very hard part, the hardest math of the machine learning triumvirate. And statistics in our cooking analogy is
 cookbook. It is the recipes. All the algorithms that we use in machine learning come from statistics. It's like statistics is saying, Hey, I came up with this first linear regression. That's a statistics formula, logistic regression statistics. Those are in our prediction step, our hypothesis functions. So our hypothesis functions are statistics equations straight out of the stats textbook. Now we go to our error functions or loss functions mean squared error stats log.
 likelihood function stats. So we grab our recipe book and we slam it down on the table. We open it to page one and it says Ratatouille and it says Chop some vegetables. So the recipe itself is a statistics equation. Sure enough, these equations are nothing to shake a stick at. I mean, if you look at that log likelihood error function for logistic regression, it was something like one over 2m times the sum of i to m of y times the log of our hypothesis minus 1 minus y times the log of 1 minus
 hypothesis and then you add in regularization which we haven't talked about yet. It's just ugly and hairy. If you're looking at an equation in machine learning that looks wild and crazy, it's statistics. Statistics is the hard part. Statistics is what makes cooking, cooking. What makes a good chef a good chef is having good recipes, having a good cookbook. Knowing how to put the right ingredients with the right other ingredients, that's the essential piece of cooking. It's the essential piece of machine.
 machine learning. So this is why I said basically machine learning could be considered applied statistics. And finally, we have our learning step train or fit or learn. Whatever you want to call it step three in the machine learning process is calculus calculus calculus takes the derivative of our loss function in order to know how big of a step each theta parameter needs to take to fix itself. And this is all part of the loop called
 gradient descent. So our loss function, which is a statistics equation, our loss function could be graphed in 3D space or 40 space or whatever. In the linear regression episode we talked about the loss function looking like a bowl in 3D. But the loss function can be any number of things. Sometimes it looks like a mountain range. And what you're trying to do is traverse the space. Walk around in your graph until you find the smallest valley or sometimes the
 highest peak depending on your algorithm. So it's like you're a hiker and there's snow and you've got your boots and your hiking stick, there's snow everywhere and the visibility's really poor. But you're trying to get to the bottom of the valley. That's where the error is lowest. That's where your hypothesis function is optimal. What calculus does by taking the derivative of your function with respect to your little guy in the function, so taking the derivative of your mountain range, taking a derivative of some grass.
 with respect to your little guy in the mountain range. It's like a video game tutorial. He can't see well enough in front of him due to poor visibility. And the derivative creates this sort of semi-transparent yellow arrow pointing down the mountain slope. And if it's really long, it's telling the guy you need to walk all the way to the end of this really long arrow. A big step, a big gradient descent. Remember, this process is called gradient descent, we're descending the mountains. Once you get to...
 At the end of this yellow arrow, you can stop, and I'll take another derivative with respect to where you are now, and I'll make a new yellow arrow pointing you down, but you're gonna go left a little bit this time. So gradient descent is the learning step of our equation using derivatives. That's calculus. And the cool thing about this is that calculus is pretty easy conceptually. I mean, if you understood the way I describe it to you there, then you understand the intuition. Taking the derivative of a function proves actually be...
 quite easy in machine learning, at least as far as you're concerned, as far as implementing these algorithms is concerned. A lot of the times when it comes to taking the derivative of a loss function, you can do it pretty easily by way of some trick of calculus, these rules, like the power rule or the chain rule. You just memorize these sort of spark notes. Calculus tricks a flick of the wrist in order to get a derivative of a loss function so you transform your loss function into a new function, that's the...
 and that tells you how much of a step to take in which direction. Now this fits into our cooking analogy very effectively actually. It's like putting the tray into the oven, setting the heat to 465 and pressing start. Okay? It's the final step. It's like cooking your theta parameters. All your theta parameters in your hypothesis function in the initial step, step one, predict. They're all set to zero, or sometimes they're all set to some random small number, remember.
 Remember that I said initially you take a random shot in the dark so that the learning phase can tell you how bad you were, step by step by step through gradient descent until all your theta parameters are just right. So this step is like cooking your theta parameters. They all start out raw. You can't eat your hypothesis function yet. It's raw. So you put it in the oven and now they all start to cook and smell delicious and brown. And finally your egg timer dings and it's ready to come out of the oven and all your theta parameters are just right.
 Machine learning, we have this final step of learn by way of calculus. And that is the namesake of machine learning. Learn machine learn. So in a way, this is the linchpin of our machine learning puzzle. This is what differentiates machine learning from other fields like statistics. In our cooking analogy, we compare this step to cooking. Cooking a dish is the namesake for the field of cooking. If you're a cook, then there's a lot of stuff that goes into your...
 craft, your chopping vegetables, you're putting together a recipe, and then the final step is putting your tray in the oven, hitting start, and that final step is actually cooking the dish. So that step is the namesake for cooking, the field of cooking, just like the learn step of calculus by way of gradient descent machine learning is the namesake of machine learning. And by the way, we've been talking about calculus as a means for descending the error graph by way of gradient descent, this application...
 of calculus towards minimizing some function is a branch of mathematics called optimization. Or in our case convex optimization. Convex means that the graph sort of looks like a cup and we're trying to get to the bottom of that cup. Convex is kind of the shape of a graph and we're trying to traverse the graph by way of calculus to the bottom of the cup. This is a spin-off of calculus called convex optimization. In the same way that physics is a spin-off of calculus, optimization is a spin-off.
 off of calculus. So you may see when you ask somebody, what math do I need to learn from machine learning? They'll say, linear algebra, statistics, calculus, and optimization. Well, kind of optimization sort of goes hand in hand with calculus stuff, and you'll learn them together. The specific application of calculus, you're learning in the case of machine learning is called optimization. It's not something you need to go off and learn independently in the beginning. This is just something to be aware of. Just keep an eye out for that word. So I want to go over one more time sort of, what each branch of mathematics is f-.
 for independent of machine learning. So linear algebra is all about matrix math or tensor math. So you use that in machine learning at every step where you're combining theta's and x's or any other sort of tensor math you might be doing in machine learning, which is very frequent. Statistics is the math of data, populations of things. When we're looking at the Portland housing market, we have a whole bunch of data in our hand. We want to come up with some sort of probability.
 distribution of the Portland housing market, okay? So that's a sub-branch of statistics called probability. You're gonna be learning two branches of stats, probability and then inference. Inference is the step of making a prediction about a new house as it fits into the market. So statistics is all about data. And then calculus, the field of calculus is all about motion of objects in a physical world sort of. Physics comes directly from calculus and physics.
 you might put in a video game is all about objects dropping and bouncing and you running into walls and you're walking up and down mountains and stuff like that. So that's kind of how in machine learning the learning step is our little dot descending the error slope to the bottom of the valley. It's the motion of an object in a physical world. That physical world is a graph put there by statistics. It's the distribution of our data or at least the distribution of our errors given the theta.
 parameters used in our hypothesis function. So linear algebra is tensor math. Stats is data. Probability stats is distributions of data. Infrance stats is making predictions on data. And calculus is motion in a physical world, namely motion of our little error dot to the bottom of the valley. Now, like I said in the beginning of the episode, don't learn math first. Learn math through machine learning. Learn the essential math.
 that you need for machine learning through machine learning. It's like learning the essentials of cooking in general by cooking some dishes. You don't go off and take a course on cooking and then start making some dishes. You do it the other way around. You start cooking dishes and you learn the principles of cooking through the act of cooking. Now let's say you end up at some steakhouse and everybody thinks you're a great cook and all and you're making some money. But you want to scale the latter. You want to be the best chef.
 there ever was. You want to work at some fancy restaurant or maybe open your own in Paris. Well now you're going to go back and you're going to find some books on how to chop vegetables perfect. How thin does everything need to be for certain dishes? Why do certain ingredients pair effectively? What's the theory of ingredient matching? What about temperature? What's the optimal temperature for specific ingredients and how do we find that to be the case and why is that the case? So similarly with machine learning you can go back now and you
 can start picking up the details of linear algebra, start learning some of these more esoteric concepts like like eigenvectors and coefficient matrices. Understand why the statisticians chose these hypothesis and loss functions for specific models. How did they come to these equations? So when you learn math after you learn math through machine learning, you've got an eye for these things. If you were to learn math first, your eyes would glaze over because you would.
 have an appreciation of where these equations are being applied. So you don't know what you're looking at. And what I see so common as a result of that is that people burn out on math before they get back to machine learning and then they go with their legs tucked between their tails six months later back to their prior field like web development. But if you learn math after, then you start to pay attention to the details of the equations. And it's sort of like Gandalf when he's pouring over all those books he's looking for something and he catches something that people didn't catch before
 Okay, so enough of my opinions, I'm now going to give you the resources for learning the math, either after machine learning or some small percentage while you're learning machine learning or hey, if you want to just ignore me and just learn it before a machine learning, reference this resources section. I'm going to break it down into a few categories. One is MOOCs by way of a company called Khan Academy. You remember MOOCs like Udacity, Coursera and all those things, online courses, videos and lessons, quizzes and all that stuff. Khan Academy.
 me is sort of high school level or AP level courses like US history and includes calculus one, two, and three statistics and linear algebra. So you can learn all of your math basics from Khan Academy. I will post links to that in the show notes. The next category is textbooks. So if you prefer to learn by textbooks instead of MOOCs, I'll post what I've seen to be commonly recommended from course curricula or from recommendations on course.
 overflow, etc. Textbooks for statistics, linear algebra, and calculus. I'll also post PDFs for primers on the basics of these mathematics branches that you need to succeed in machine learning. So maybe three or four-page PDFs that are primers on just the essentials, maybe in calculus, just taking derivatives of functions. In statistics, it would be some basic probability influence. So those are three clarisedorns.
 categories, different approaches that you could take to math. And now I'm gonna give you one final category. And this is a little bit hardcore, but I really personally like this. It's a course series online called The Great Courses, what used to be called the teaching company. They put out these 30 video series on every topic under the sun, history, cooking, art, and they do indeed have math. Calculus one, two, and three, and statistics, two series on statistics.
 I don't think they have anything on linear algebra, but if they do, I'll post it. There's one course I want to draw particular attention to. It's called mathematical decision making. And it's actually very similar to this podcast series. Obviously, much more professionally done. They have a lot of money. It covers a lot of the machine learning topics with a special focus on the mathematics. It's actually a study of a field called operations research, which is very similar to machine learning or artificial intelligence. It's like math for managers trying to decide scheduling a...
 of employees or trained departure times or factory settings. It turns out the math is very similar to what you're going to be learning and machine learning. So the course is mathematical decision making, and I'll post that in the show notes. Now here's the catch with these series. They're video series, but I wouldn't necessarily recommend them as a primary source of education for these topics. Go to Khan Academy or go to one of those textbooks instead. But what I use these series for is I convert them to audio and I listen to it on my iPod when I'm actually...
 exercising or cooking or cleaning, commuting, et cetera. So obviously you appreciate audio supplementary education because you're listening to this podcast. So that's how I use these series as audio supplementary education, video converted to audio. Now that sounds really hardcore because obviously we're talking math here. These professors are gonna be referencing graphs and charts and equations. They're gonna be pointing to things and asking you to look at things as they explain them. But one nice thing about these course series is that the instructors...
 are very good at narrating their actions. Verbally talking you through everything they're doing, step by step, okay, I'm drawing a line horizontally, this is the x-axis, I'm drawing a vertical line, this is the y-axis. I'm making a squiggly line now, it goes up, then it goes down, then it goes up, and there's a dot on the valley, so they're very good about narrating the whole process. It's definitely a bit of a brain exercise. You're gonna wanna be well-caffeinated if you're gonna do what I do and convert these video series to audio. You can just put them on your iPod.
 as video and simply listen to the audio, or I actually have a script that I run, a bash script that converts videos to MP3 files. I'll post that in the show notes, but I can't recommend the great courses enough, not even just for math. So I'm doing the calculus one, two, and three courses, the statistics, I did the statistics. They have a whole thing about philosophy of mind, whether AI can achieve consciousness, the thought of our minds as machines, they have a whole series on neuroscience. So lots of supplementary...
 fringely related to machine learning. By the way, the great courses can be quite expensive, maybe $100 per course. If something has an audio format option like the philosophy of mind one does, you can get it from audible by way of Amazon for cheaper. But the math ones that I'm going to reference in the show notes are video only. So you'll have to buy them as video and then convert them to audio. Okay, so that's it for this episode on math. The next episode will be about deep learning of.
 basic overview of neural networks. Please don't forget to give me a rating on iTunes, Stitcher, or Google Play, whatever you use. If you have any friends trying to learn machine learning, please point them to this podcast. As always you can find the resources on ocdevelop.com forward slash podcasts, forward slash machine hyphen learning. That's ocdevl.com. Thanks for listening and I'll see you in the next episode.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode nine deep learning. Hello and welcome back to the machine learning guide.
 And this is the episode that you've all been waiting for, even if you didn't know you've been waiting for it, deep learning and artificial neural networks. This is the very exciting stuff that's happening in the world of machine learning in 2017. When you see on hacker news or just the actual news, artificial intelligence and machine learning being applied to new and innovative spaces, it's almost always deep learning they're talking about, neural networks. If you have any sort of background...
 interest in machine learning prior to embarking on this podcast, then you've already had your ears perked to deep learning and maybe wondering what neural networks are all about. If this podcast is maybe your first introduction to machine learning, well then just keep your ears perked because this is the stuff that's the most interesting in the space right now. Now I keep saying deep learning and neural networks. They're slightly different. First off, remember everything in machine learning is hierarchical. We start with AI.
 that's broken down into machine learning and some other subfields. Machine learning is broken down and is supervised, unsupervised, and reinforcement learning. Supervised is broken down into various categories, one of which is called deep learning. The other, which is shallow learning. Shallow learning is the stuff that we've been learning so far, linear regression, logistic regression, and a handful of other algorithms that we will be talking about in subsequent episodes. We're going to skip over them now and jump right into deep learning. The reason I'm doing that is because I want to wet your appetite.
 And I also want to appease the itch that I know a lot of you have who are very curious about deep learning. If I were to go the standard educational route, it would take me many, many, many episodes until we landed on deep learning. So I just want to give you a hint of what it is now. So you can at least understand what news articles are talking about before we dive back into the details of shallow learning algorithms. So we have deep learning and shallow learning and then deep learning is broken down into various other spaces, one which is called neural net.
 Artificial Neural Networks or ANNs. As far as you're concerned, deep learning is neural networks. The other models within deep learning, I haven't really seen them in the wild, whether it's professionally or academically. Really, essential deep learning can be boiled down as neural networks. And then neural networks themselves are broken down into different types of neural network models, which we'll talk about in a bit. For example, multi-layer perceptrons, recurrent neural networks, convolution.
 neural networks, etc. So this episode is all about the subspace of supervised learning called deep learning, and we're going to talk about one of its branches called neural networks, the only branch as far as you're concerned, and some of its sub-branches. Why are neural networks so interesting and exciting? I'm going to give three reasons. One is that they may or may not, we'll get to this later, represent the human brain, which of course takes us a lot closer to artificial intelligence.
 that's our goal. They're a little bit magical. The way they work internally is like a black box. We don't know what's going on in the little brain of these neural networks, unlike the shallow learning algorithms where we know exactly what's happening. Deep learning does a lot of its learning inside the box and we can't peek inside. It's very magical. Another reason is that it is fast subsuming the other spaces of artificial intelligence. Remember how I said in the past we had natural language processing and vision and all.
 all these other subspaces which have been consumed by machine learning. Now, vision is almost entirely the domain of convolutional neural networks. Language modeling is almost the entire domain of recurrent neural networks. These are all deep learning, machine learning algorithms. So it is specifically by way of deep learning within the machine learning world that machine learning has come to subsume the other spaces of artificial intelligence. And in that way, it is almost like deep learning is the master algorithm.
 of intelligence. So there's a lot of magic behind deep learning and a lot of automation as well, consuming the other spaces of artificial intelligence. And speaking of black box, the third reason I'll give that mixed deep learning special is that it removes machine learning one step away from the programmer. Remember that I said that the closer your program is to the programmer, the less it feels like artificial intelligence. Artificial intelligence, remember, is simply defined
 as automating any mental task. If you could build a perfect rule-based system, like the symbolists were trying to do in the early days of artificial intelligence, that would still be artificial intelligence, a bunch of if-else rules. But it doesn't feel very much like artificial intelligence, and furthermore, it feels like an impossible task. How could you possibly enumerate all the if-else rules of the universe? So we came up with statistical approaches to machine learning, like we've seen with linear and logistic regression.
 We have these theta parameters that we need to learn. We boil down aspects of the universe into these theta parameters, and then we can use them to make estimations. So that's one step removed. The machine learns these parameters on its own. While deep learning does an even extra step, which we're going to discuss in a bit, it's called feature learning. It has to do with the features that we've been looking at. In linear regression, for example, you might have x3, x2, x1, and x0. Squirr.
 The number of bedrooms, the number of bathrooms, and distance to downtown. Those are all the features. Well if the model is impossible to represent linearly, as is the case in many things in the real world, then that x3x2x1x0 breakdown does not work. You can't represent your data as a line on a graph, maybe it's a squiggle on a graph. So you have to learn how to combine features in a specific way, maybe squaring the square footage or combining the square footage with the distance to downtown.
 in some way that creates a new graph. Knowing how to do that, how to combine specific features is a programmer's task in shallow learning. But in deep learning, the neural network learns how to combine parameters in a way that is effective. So it's another layer removed from the programmer, which makes it a little bit more magical, a little bit closer to the end goal of artificial intelligence. OK, let's take a little step back and differentiate shallow learning from deep.
 learning. Shallow learning is everything that we've been learning so far, linear and logistic regression. I don't really have a concrete way to represent shallow learning. I think of it as the pistol of machine learning. Really small algorithms or mathematical equations. You pipe in an input, it does a quickie inside a box and outcomes an output by comparison to deep learning, which I think of like a bazooka. You pipe in an input to a factory, a castle, and outcomes an output. Let's use that castle analogy. What makes deep learning deep?
 is that you're stacking shallow learning algorithms deeply. Okay, so we've already been talking about linear and logistic regression. Logistic regression is a Lego, a single Lego, in the Lego castle that is a deep neural network. So a neural network is composed of little Legos, and those Legos can be shallow learning algorithms, specifically in a multi-layer perceptron, the vanilla form of an artificial neural network, which you'll talk about in a bit.
 your logistic regression unit, logistic regression Lego, is the primary piece of your neural network. So take logistic regression that we've already learned from a prior episode, combine a bunch of those together in a web, and now you have a neural network. So shallow learning is your basic algorithms, and deep learning is taking those things and combining them deeply into a network, an artificial neural network. Why do we call it a neural network? It's because the units of this network that we're constructing out of...
 our little Legos? The units themselves are called neurons. Let's talk a little bit about the history of this unit. We've learned it as logistic regression. It's a classifier. A thing for classifying data is this house expensive or isn't it? Is this a picture of a dog or isn't it? It actually is a little bit deeper than that. There's a history involving characters by the name of Warren McCulloch and Walter Pitts who proposed the mathematical rep...
 presentation of a human biological neuron. These were both computer people, but McCulloch was a neurophysiologist, and pits was a computational neuroscientist. So these guys were no newbies to the space of the actual human brain. It was Frank Rosenblatt that originally came up with the idea of a perceptron, and it was these two guys that sort of formalized it as an artificial neuron. A perceptron is a stepwise function. Similar to logistic...
 regression, but a little bit different. We're going to ignore that fact for now. Think of a perceptron as logistic regression. Wrap your logistic regression into a unit, we will call it a unit, and now you have what we call an artificial neuron. According to McCulloch and Pitts, the mathematical representation of the human neuron. It's very interesting, so I think a lot of misconception about neural networks is that they were inspired by the brain in a very fuzzy way by computer scientists who don't know one thing or other.
 about the brain, absolutely not. It was inspired by the brain in a very real way. That each of these units is believed to be the mathematical representation of a biological neuron. So let's hit that from the top one more time. We have McCulloch and Pitts, a neurophysiologist and a computational neuroscientist, looking into the human brain and formalizing what they believe to be a mathematical representation of the biological neuron. They call this an artificial neuron. It's a very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very simple, very
 string a bunch of these together and we have what's called a neural network. Now as far as we're concerned for this episode, a neuron is logistic regression. You wrap up your logistic regression into a unit, you string all those together and you have a neural network. But a neuron doesn't have to be logistic regression and in the case with Frank Rosenblatt, he used something called a perceptron, which is a stepwise function, very similar. So we're just going to continue calling it logistic regression, a logistic or sigmoid
 function, unit, and the first artificial neural network to come to be is called a multi-layer perceptron. So, it's a lot of verbiage here, lots of words. So, let's start from the top. Again, we have deep learning, the field of connecting shallow learning algorithms together in a deep way, stacking them. We have deep learnings broken down into neural networks, artificial neural networks. And then artificial neural networks are broken down into various types of artificial neural networks.
 One is called a convolutional neural network. Another is called a recurrent neural network. These are used for various different spins off of deep learning applications, such as language modeling, image recognition, et cetera. But the vanilla neural network, sort of the poster child of neural networks, is this thing that we call a multi-layer perceptron, or a feed-forward network, a multi-layer perceptron, one of the earliest versions of a neural network created before they started specializing neural networks in various ways for different applications.
 So that's what we're going to be talking about in this episode, a multi layer perceptron. What does a neural network look like? Let me try to paint a picture in your head, and then we're going to talk about an example of why you would use a neural network. The way you might envision a neural network is coming from the left and going to the right. On the left you have your input that comes in. So for example, our spreadsheet. Every row, one row at a time, comes in from the left.
 Take our row and we flip it on its side so it looks like a column. Feature feature feature feature feature. So all the features are stacked. Square footage, number of bedrooms, number of bathrooms, etc. So that's the first layer. It's called the input layer. Imagine these as little circles. So each of our x's is a circle. All of those then feed to the right into new circles. Each input feature, each input feature feeds into the circle.
 A layer of neurons, let's say that there's five neurons, five logistic regression units. Each feature feeds into each neuron in this new layer. So we're going from left to right, we're taking our inputs and feeding them into each of the neurons. And then let's go right one more layer, and we have our output function, our objective function, or hypothesis function. It's our final logistic regression unit that is...
 going to tell us whether or not a house is expensive, for example, in the case of a classification neural network. So the architecture goes from left to right. Imagine a column of circles. Those are your inputs. One row of your data, where each circle is one feature of that row. They all feed into a new layer of circles. Those are your neurons. Those are called your activation functions or activation units. They're all just logistic regression.
 and they all pipe into your final single neuron, the third layer, and that's your final classifier, your objective or hypothesis function. So we have an input layer on the far left. That's your data. It pipes into a hidden layer, it's called, we'll see why it's called a hidden layer in a bit. Those are your neurons, your activation functions, and those pipe into a third layer called your output layer. So that is...
 is the architecture of a neural network. That sounds a little bit wild and hard to understand. Let's use an example to help you understand this. When it comes to health care, there are many features about an individual which might determine the medical costs of that individual. Let's say that an insurance agency is interested in how much to charge a person based on various aspects about their life. Things like age, whether they're a smoker.
 whether they're obese, etc. Well, your first impression is to use linear regression. Those are all features. You pipe them into linear regression, and out comes why, which is the cost per individual of medical bills. But that doesn't work that way. It turns out, with health, these features don't apply in a linear fashion. It turns out that age gets more and more and more important the older you get. It's not...
 linear. It's a little bit more like x squared. So it's more likely the case that you're 16 years old, you hit the doctor maybe once a year. You're 40 years old, you're hitting the doctor maybe once every six months. But once you start getting in the 90s, you're seeing the doctor maybe weekly. So it's a little bit more like a polynomial function x squared. So we already cannot use linear regression here. But there may be other combinations of features in our example. For example,
 For example, we know that smoking an obesity combine to be greater than the sum of its parts. It turns out that an obese person and a smoker separately have their own health issues, but if both of those are combined, it causes even worse issues than those separately. They combine non-linearly. Now if we knew a little bit about this already, as I'm explaining it to you right now, we do know we could construct a polynomial linear...
 regression algorithm out of combinations of features like age, squared, obesity times smoking, et cetera, manually. But again, when you're doing things manually, that's not very machine learning, is it? And most of the time, we don't know enough about the puzzle to construct combinations of features like that ourselves. That's what we want the machine learning algorithm to be able to do. Figure out how features combine in a specific way, most effectively, to be able to determine the output.
 So let's see how a neural network might handle this situation. Let's use that multi-layer perceptron architecture that I just described. We will start from the left. We will input one row at a time from our training data set. Remember from the Portland housing market, we import a spreadsheet of training data, data for which we already know the answer. What's the cost of the house? So every row in our spreadsheet is a house, and every column is that it's...
 on that house such as square footage, number of bedrooms, number of bathrooms. And the final column is the label or the known cost for the house. Or in the case of logistic regression where we were just trying to classify whether a house was expensive or not, then it's just gonna be a zero or a one, yes or no. In the case of regression, our labels will be numbers and in the case of classification, our labels will be a yes or a no. So for our neural network, we're going to do the same thing. We're gonna upload a spreadsheet. It's gonna have rows of people. And the columns are features of those people.
 such as age, BMI, whether or not they smoke, and then various other things like location, do they exercise, do they have a healthy diet, et cetera. And the final column is, in the case of regression, maybe the yearly medical costs of the individual, or if we want to do classification, maybe we'll say, is this an expensive person or not? We're gonna do a regression example in this case. So our neural network will take one row, row by row. Pipe it in as the first layer. That is...
 the first layer of our neural network. Each circle of that layer in a column is a feature of the first row. And each of those features pipes to each of the neurons of the second layer. We call each of these neurons a unit or an activation function. In our case with the multi-layer perceptron, we're using logistic regression as the activation function. So our activation function is the sigmoid or logistic regression.
 Let's pretend that we have five neurons in that second layer. Then each feature of the first row of data will be fed into each neuron of that layer. Remember this layer is called the hidden layer. So each input feeds into each of the neurons of the hidden layer. And then each of those neurons feeds into the last neuron, which is the objective or hypothesis function. Neurons of the hidden layers, the layers in the...
 in between the input layer and the output layer are called activation functions. And the activation function depends on what function you use in the neuron, which shallow learning algorithm you use in our case, it's logistic regression. So we're using the sigmoid function or logistic function. So the activation functions of our hidden layer is the sigmoid or logistic function of logistic regression. And those all feed into the final neuron, which is our hypothesis.
 or objective function. What is going on in that hidden layer? Here's what's going on. Every feature of our row that's feeding into the neural network is being combined with every neuron of the hidden layer. What the hidden layer is doing is learning how to combine the features optimally and then it sends all those out to the final
 neural, neural, neural tell you the final result. Function in our case may be linear regression if we want to estimate the predicted cost of the individual at the end of the year, or it may be logistic regression if you're trying to classify something. But the point is that the purpose of that hidden layer is to try every which way combination of features from our data in order to learn the best way to combine features. So let's say that it might figure out that age comes.
 combines best with itself. In other words, age squared, like we said before, or it might figure out that smoking and weight combine. So the purpose of that hidden layer is to find optimal combinations of features in order that the neural network can properly predict values. And by the way, I apologize if you hear rain in the background. I have to do this episode outside, unfortunately, it's a long story. So you may hear some background in nature noises. So remember, the three-
 steps of machine learning are predict. So we're going to make a bunch of predictions with all the rows of our spreadsheet. Figure out how bad we did. That's the loss function. And then train. Train on that loss function. And it's this training step or the learn step. This final step we're using an algorithm called back propagation. It's the stacked application of gradient descent. So it's the way we would make gradient descent deep.
 talk about that in a bit, it's this final training step that has each of the nodes of the hidden layer learn how optimally to combine features. Okay, so there you see a single layer neural network learning how to best combine parameters for a situation which is non-linear. And then of course our final output function, our objective function, in this case is linear regression because we want a number. And in the case of classification.
 is logistic regression. Okay, now we learned one superpower of neural networks. Feature learning, learning how to combine features in a way to construct a non-linear function. But neural networks have another secret power, hierarchical representation of data. Breaking your data down hierarchically by way of this feature learning paradigm deeply stacked deep.
 Okay, so let's switch to a new example. We're gonna move away from this health situation and to face recognition in images. So what we're gonna do is we're gonna upload a bunch of images of faces and non-faces. So this is gonna be a classification example. Non-faces might be a picture of a dog or a picture of a house. Now real quick I'm not gonna get too deep into this, but we've been uploading spreadsheets of rows and columns. Rows are individual examples and columns are their features.
 The way we would turn an image into a spreadsheet like that is we would take its pixels. Let's pretend that it's a 5x5 image, flatten it, so it's now a 25x1 row of columns, where each column, where each feature is a pixel. And then we will take those pixels which are currently RGB values in color, and transform it into gray scale so that each pixel could be represented as a single number.
 And then we take all our images and they're now rows and we put them into our spreadsheet. So there you go, you just took a bunch of pictures and you turned them into a spreadsheet. You'll see a little bit more of that when you get into the details like the Andrew InCorsair of course. And then of course our final column is whether or not the thing is a picture of a face. So here we go. We pipe our spreadsheet into our neural network. Our neural network takes it one row at a time. We're going to train on each row.
 First, we're going to do a feed forward pass. Feed forward. That's the prediction step of our 123 machine learning stepwise process. Feed forward pass will give us a prediction as to whether or not this is a picture of a face. Then we will use our loss function to figure out how bad the neural network did. And then we will send that back through the network in a backwards pass called back propagation, which will use gradient.
 descent at each neuron to update the weights of fatas in each neuron so that the whole neural network will get more accurate by the end of the spreadsheet after it has seen all the examples. But here's the twists to this neural network. We're going to have two hidden layers. So there's four layers now total. The first layer is called the input layer. It is simply all the features of an individual row and that will happen once.
 for every row of our spreadsheet. So the Neural Networks input layer has a size of 25 pixels for a 5x5 pixel image. And this Neural Network will be used over and over and over for each row of the spreadsheet. The next layer is our first hidden layer. So all our pixels of an individual row from the input layer connects to each neuron of the first hidden layer. So what is that hidden layer?
 trying to do now. We said that the purpose of a layer in a neural network is to combine all the features of the input in a way that the whole is different than the sum of its parts. So what are the things we're combining here? Well, we're combining all the pixels of the image. We're trying our hand at a bunch of combinations of pixels in order to give us something new. So that first layer might be combining...
 all the pixels to figure out if there's lines or edges or contours in the image. It will combine all those pixels into lines and those lines are now new features. Those are now the features of the first hidden layer. Those features get sent to the second hidden layer which tries a bunch of combinations of those features in order to figure out
 combinations are important. So the second layer might be combining lines and edges and contours into eyes and ears and mouth and nose. And then it will take all those, combine them into one, which is our final output, the output layer, the objective function, the hypothesis function, a logistic regression unit that tells us whether or not we're looking at a face. So a neural network does two things. One,
 It breaks a thing down into chunks. And two, it combines things at that level in order to find important combinations to make a determination. So for our image, the first layer is the input layer. The second layer is the first hidden layer of neurons. Each neuron is called an activation function. In our case, the activation function is the sigmoid or logistic function from logistic regression.
 In other words, each neuron is logistic regression. And all the neurons of the first hidden layer are trying to combine all the pixels of the image in every which way possible in order to find important combinations of pixels. Well, in the learning process of back propagation, the final step, we're going to be training these neurons. We're going to be telling these neurons whether or not some combination of pixels.
 it found was important or was not. That's the training step. So this first hidden layer is finding combinations of pixels that makes important things. Those things, in our case, are likely to be lines, edges, and contours. Well, it's not up to us. The neural network itself will figure it out. It'll figure out what combinations of pixels combine in a certain way that it finds important in order to increase its prediction accuracy. Now, that first hidden layer is now...
 acting as the input layer into the second hidden layer. All the neurons of the first hidden layer, all the combinations of features from the picture, are now new features. And those features go into the second hidden layer. It is the purpose of the second hidden layer to combine those new features into, again, new features. So the things that the second hidden layer might learn are eyes and ears and other things.
 the face. Then it will combine all those together into the last neuron, the objective function, the logistic regression hypothesis function, which will tell us, yay or nay, is this a face. So I think that's really cool. We have logistic regression, which we learned in a prior episode for classification of a very simple pattern. Well, a face in a picture is not a simple pattern. It is not a linear function. You can't possibly make a line on some.
 scatter plot of pictures of faces. There's nothing, it doesn't even make sense. You can't visualize that in your brain. It doesn't make sense. What you can do is look at a face and cut it up into parts, cut out the eyes of the face, cut out the nose, cut out the mouth, and then cut those up into further parts until we have lines. Work backwards from there really is what we're doing in neural networks in order to combine things into a hierarchy where the final root of the hierarchy is a yay or nay. So I like to...
 think of it like an org chart of a company. Let's say that we have a boss, the head honcho of a company, the CEO. He is the objective function. He's our hypothesis function, our last neuron, our output layer. Under him, he has supervisors. They are the second hidden layer. The supervisors each have subordinates, employees who work under them. Now unlike a typical organization chart, which is purely hierarchical.
 where a supervisor has their own employees. Each of our supervisors is sharing the employees of the company. So it's not purely hierarchical. So what happens? Well, this organization all resides inside of a building. That building is called a neural network. We don't personally know what's going on inside. The company manages itself. But we knock on the door and the door opens and we hand an employee a picture. And the employee taps on his nose and gives you a look and says, OK, I'll come right.
 and I'll tell you whether or not this is a face. He goes inside and all the employees huddle together around the picture and they're all pointing at various pixels and some other employees pointing at this other pixel. Well, this one's black, no, yeah, but this one's white and they're all trying to combine all the pixels of the picture in their minds. There's 10 employees huddled around the picture and they've got magnifying glasses out. Well, 10 employees means 10 neurons, meaning there's some sort of combinations of pixels that could be boiled down into 10 principle components. Maybe.
 a dark thick line and a short skinny line and stuff like this. Okay, they all nod their head and they think they have their solutions. They walk down the hall to the room with their supervisors and each one of them reports to each of the supervisors. Each supervisor is looking for a specific object. We've got left eye supervisor, right eye supervisor, nose supervisor, mouth supervisor, left ear and right ear. There's mouth and nose, so that's six supervisors.
 six neurons in the second hidden layer, meaning it is six specific objects that we're trying to detect at this layer of the hierarchy. So left eye supervisors listening to the report from all of his subordinates, all 10 employees are all clamoring over themselves. And he says, hush, hush, okay, everybody raise your hand if you saw a line or an edge and they all raise their hand. And so he nods his head and says, yes, okay, we definitely have a left eye. But they all run over.
 to the nose supervisor and he says, okay, everybody who has a thick vertical edge, raise your hand, and three raise their hand and he says, okay, now I need a circle-ish line. Anybody in one race is their hand and he says, okay, how about a shadow kind of figure? And nobody raises their hand and he's kind of nodding to himself. He's like, hmm, he's got his clipboard in his pen. He's checked off seven out of the 10 features he's looking for and he's kind of scratching his chin. He's like, you know I'm gonna call it, it's a nose. We're missing three of the features, but it's a nose.
 7 is enough for me. Weighted sum. Remember that logistic regression works on a weighted sum such that if we get a 70% out of 100%, then it's a yes. And if it's anything under 50%, it's a no. All the hidden layer, two supervisor neurons, now have all their answers. Whether or not they have the object they're looking for, eyes ears, mouth and nose. And they all rush down the hall with their clipboards to the boss's room. The CEO is at the top floor with these windows overlooking the city. And he turns around.
 in his chair and he's a big fat man, he's got a cigar in his mouth and a fancy suit. He says, well boys, do we see eyes ears mouth a nose? And both eyes supervisors raise their hand. Yes, we both saw eyes. Nose supervisor raises his hand. I saw a nose, but mouth doesn't race his hand. So the boss is scratching his chin and he's looking at his clipboard, shifting his cigar from left to right of his mouth and he's like, hmm, maybe there was a beard or maybe something was obstructing the mouth. Who knows? I'm calling it a face. Yes, we got a face. 70% probability that we're looking at a face. So he...
 comes down the stairs and he opens the door, we're waiting patiently outside the company's building. And he says, Tyler, I'm gonna say it's a face, 70% probability. And I look at him with a sad look on my face because I knew from the spreadsheet. I doped him. I gave him a picture where I already knew the answer. It's not a face, it's a dog. The answer is zero and he gave me 0.7. So I use my loss function. Remember, it's called cross entropy. And in the case of neural networks, it's a little bit different.
 than a typical logistic regression cross entropy loss function. But it's fundamentally the same, and you'll learn the details in the Andrew Inge course. So I use my little function, and I calculate how off he was, and I tell him, I say, you are off by this amount. And he slaps his forehead, and he is mad, so he runs back to his supervisors, and he says, guys, you're yelling at them, and they're all shifting uncomfortably and looking down at the ground. And in his mind, he's changing some numbers, some theta parameters. Remember, each neuron...
 including the hypothesis function has a set of theta parameters just like in logistic regression. So he's adjusting all the theta parameters in his head as he's barking orders to his supervisors. Now each of the supervisors adjusts their theta parameters in their own heads as they run down the hall to their employees and they start yelling at their employees. And each of the employees starts adjusting theta parameters in their heads as they're looking miserably at the ground and taking a lashing and they turn
 around and they look at the picture and they open their mouth to yell at the picture, but but the picture doesn't have theta parameters. The picture can't fix itself. The picture is the picture. So of course, the input layer doesn't change. And that last learning step of yelling down the tree is called back propagation. It is running gradient descent down the org chart of the neural network. So there you have it, a neural network or deep learning. What it does is it's just like any other supervised learning.
 system except that it learns how to combine features in important ways and if necessary hierarchically, how to break down your data into a hierarchy of features and a hierarchy of combinations of features. So when we say deep, we mean the number of layers, the number of hierarchical layers in the breakdown. And when we say wide, we mean the number of neurons in each layer.
 A wide layer has a lot of neurons. A deep network has a lot of layers. Neural networks are sort of a silver bullet. They can handle any linear situation, just like a linear algorithm could, and handle stuff that's non-linear, which linear algorithms cannot. They can handle housing market estimations, linear, or they can handle face predictions in images, non-linear. So they're a silver bullet. But I want...
 Why don't you take that statement with a grain of salt because you should not treat them like a silver bullet? Can you use neural networks in everything? Yes. Should you use neural networks in everything? No. Why? Why shouldn't you use neural networks for everything? Well, it turns out that many problems are linear. And many problems don't need to combine features. Maybe it's not some line on a graph like linear regression, but you could use an algorithm like...
 Bayesian inference, which doesn't depend on the combination of its features in order to succeed. There are many other shallow algorithms that we're going to be going over in future podcast episodes. Bayesian inference, decision trees, support vector machines, K nearest neighbor, K means algorithms, all these things. They're shallow, they're quick, and they can handle many real world problems. Well, if deep learning can handle most, if not all real world problems, then shallow learning can handle.
 some real world problems, why shouldn't I use deep learning for everything? The reason is that deep learning is expensive, very, very expensive on your computer. To run a typical shallow learning algorithm like linear regression or Bayesian inference, you could probably do it on a laptop. For deep learning, you're going to want to rent space on an AWS cluster of Titan X GPU machines. The difference in performance and scalability of deep learning versus
 shallow learning is significant and can cost you an arm and a leg if you're running an online service. I like to compare it to our org chart analogy. If your situation is linear, then here's what would happen. You would knock on the door of the company and you would hand a row of your data to the employees. They would all take that row, they would all look at it, scratch their heads, and they'd determine that there is no sort of combination of features that needs to happen. So they all look at each other and they shake their head and they pass it on. That layer just gets passed on as...
 to the second hidden layer, your supervisors. They all do the same thing. They scratch their head, they look at the data, and they determine that there's no sort of combinations of features that's necessary, nor hierarchical breakdown of that data. They all look at each other, they shake their head, and they pass that data onto the boss. And this time, the boss isn't some fat cigar smoker. He does all the work. He's linear regression at the end of your chain, and he starts hammering, chiseling at this thing, and outcomes your solution. So you just pay...
 a whole company of employees to do the work of one person, linear regression. That's why you don't want to use deep learning for everything because you don't have to. And it costs more money and time to compute a deep learning solution than a shallow learning solution. So it still is in your interest to learn these various shallow learning algorithms and where they apply. And we're going to be going over various algorithms in future episodes. Okay, so that was a neural.
 network, my friends, that's called a multi-layer perceptron, a feed forward, multi-layer perceptron. And a perceptron because the type of neuron we're dealing with is, in most examples, that you'll see what's called a perceptron or a stepwise function. But in our case, we use logistic regression. It's basically the same. So it still counts. Multi-layer because we have one or two hidden layers. And feed forward, you'll see this word feed forward. There's the feed forward path that you do in the initial predictions.
 step. But when they say feed forward network, like a feed forward, multi layer perceptron, what they're referring to is by comparison to other types of architecture. For example, a thing called a recurrent neural network. It doesn't exactly feed its data forward. It feeds it like back into itself. We're going to go over recurrent neural networks in a future episode. Most neural network architectures are feed forward, but there are some snazzy little twists of an architecture that can do recurrent neural networks.
 version and other types of feeding. One final technical aside, that is that in the back propagation step of neural networks, you may or may not be using gradient descent. The type of thing that is used for training is called an optimizer. So gradient descent that we've been talking about in all these episodes for the learning step of machine learning is an optimizer. And there are various other types of optimizers that you can use.
 called adagrad, one called atom. But I just wanted you to be aware of that. If you just jump into the deep end and you start working with neural networks and you start seeing these optimizers used that have weird names like that, adagrad, atom, what they're doing there is they're replacing gradient descent with maybe some more specialized or optimized optimizer for that particular architecture. Okay, I want to compare deep learning to the brain. We did this a little bit in the beginning of the episode, but now we're going to come back to it. A neuron.
 in a human brain looks a little like this. We've got a cell body, a little blob, and into it come inputs by way of a structure called dendrites, so it has these little lines coming into it. And out comes the output of a neuron, that output line looks like a tail, it's called an axon. So physically, a human neuron looks a little bit like an artificial neuron. An artificial neuron takes inputs from all of the data or the layer before it. It does some computation in the middle.
 That's kind of like the cell body or the soma of the neuron and out of it comes the output. There's a lot of debate as to whether the artificial neuron created by McCulloch and pits really does represent a biological human neuron. I think the argument might be missing the point. From a functionalist perspective, they do the same thing. There's a common point of comparison you'll see made between birds and planes. In order to achieve flight in our era, we didn't have to create.
 flatten feathered wings, instead we use the laws of thermodynamics to create a giant metal bus with stationary wings. The point is to achieve the same effect. Well, our artificial neurons may or may not represent the human neuron in a fundamental way, but functionally they achieve the same effect. And in that way, I think it is safe to say that neural networks are our big chance towards solving intelligence. I bring up the brain again for another reason. In the human
 We have different centers dedicated towards solving different tasks, speech, image recognition, planning, etc. Each of those centers of the brain is what's called a nucleus. It's not like a cell body in biology. It's the same word in neuroscience. They call a center of the brain that handles a specific task, a nucleus. And a nucleus is nothing more than, as far as we're concerned, a neural network. So a neural network in artificial...
 neural network land of machine learning is like a nucleus of the brain. Now, as different nuclei of the human brain are tailored towards handling different tasks, they have slightly different physical architectures. Let's say that primary visual within the human brain has maybe shorter axons or some different combinations of neurons in a specific way that makes it very good at handling vision, where Broca's area for speech might be physically structured in a different way.
 The human brain doesn't use the exact same nucleus architecture all throughout the brain. Instead, it specializes various nuclei to be better at particular tasks. It still uses the master algorithm of the neuron and combinations of the neuron, but it does so with a twist. Different floor plans. We're still dealing with blueprints within a house, but just different floor plans for different specializations. You will find this to be the case in artificial-
 neural networks as well. Within deep learning, you will rarely see the multi-layer perceptron used in the wild. The multi-layer perceptron is sort of the trainer's neural network. Neural network 101, the thing that I taught you in this episode is like neural network 101, but it's not really what you're going to be using most of the time. In vision, for example, recognizing images, what we did actually in this episode. The most common neural network you'll see here is called...
 a convolutional neural network. And it has additional types of layers inserted in the neural network, different types of neurons. So you may not be using logistic regression units, for example. For language modeling or anything in the domain of natural language processing, you're going to use a thing called a recurrent neural network, an RNN. And this has a special tweak of the architecture, like I mentioned earlier, that neurons can feed back into themselves. It's pretty clever. And for planning, we're...
 will use something called a deep Q network or a DQN. So variations of the general neural network architecture with a twist to make architectures specifically suitable for specific applications. Now, like I said, each of these different architectures might use a different type of neuron or activation function or hypothesis function. Remember a neuron inside the hidden layers, the neurons in the black box are called activation functions.
 and the final neuron or final neurons of the output layer is called the hypothesis function or objective function. Activation functions in the hidden layers? Hypothesis function in the output layer. You may use logistic regression or softmax or linear regression in the final output neuron. Or any number of things. In the hidden layers, you're more likely to see a thing called Ray Lou, R-E-L-U rectified linear unit.
 That's one of the more common types of activation functions. One of the more common neurons in neural networks. Another type you might see is tan H. And they're all quite similar to sigmoid functions. They pretty much look the same too. A lot of them look like an S just shaped a different way. In the case of Ray-Lew, it's flat before x equals 0. All that stuff isn't so important. It's easy to understand a neural network as stacked sigmoid functions, since you've already learned the sigmoid function. But you may be dealing with different types of activation functions, different types of neurons. And you...
 Usually the determiner of why you would use a different neuron under different circumstances is based on the way the math works out for the particular architecture, the particular neural network you're using. It may learn better using calculus in the back propagation step, the gradient descent step, the calculus may work better for certain neurons under certain architectures. Me personally, I just take whatever neuron or architecture is popular in the space and I just roll.
 from there, I don't even question it. So that's it, my friends, that's deep learning, that's neural networks. Like I said, we're going to come back to shallow learning algorithms, like support vector machines, decision trees, can yours neighbors, K means, Bayesian inference, et cetera. And then eventually we'll get back into deep learning and talk about recurrent neural networks, convolutional neural networks, deep Q networks, and all those things. For the resources of this episode, I'm going to recommend a mini series on YouTube.
 tube, which will give you a lay of the land of deep learning architectures, really short videos providing the visuals, representing various deep learning models like RNN's, CNN's, etc. You can plow through that one pretty fast. Then, as usual, I want you to finish the Andrew Inkorsera course. He has a whole week or two dedicated to neural networks where you're going to be learning the multi-layer perceptron. And then, I'm going to recommend you a book. The book is by Ian Goodfellow.
 Yashua Benjo and Aaron Corvill, and it is simply called deep learning. And it's probably the most popular resource for learning deep learning. It's a textbook, and it's been a long time in the works. It's rather newly published, but it's a great resource for learning deep learning. Now again, you don't want to start this book until after you finished the Andrew E. Coursera course. But once you finish that course, it's safe to begin right away reading the deep learning book because deep learning is the immediate next step after shallow learning.
 So finish the Andrew Eankors and then start on this textbook that I'll put in the show notes. Again, as usual, the show notes are at ocdevel.com forward slash podcasts, forward slash machine learning. This is episode nine. Again, ocdevol.com. There's also a contact button in the top right of that web page where you can get my contact information, my email address, Twitter, LinkedIn, etc. If you want to reach out to me, the next...
 episode is going to be about languages and frameworks. We're going to talk about Python, versus R, versus Java. We're going to talk TensorFlow, versus Theano, versus Torch, all those things. See you then.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 10 languages and frameworks. In this episode we're going to talk about languages and frame.
 Languages like CNC++, math languages like R, MATLAB, Octave, and Julia, the JVM, like Java and Scala, and of course Python. Frameworks like Theano, Torch, and TensorFlow. I'm going to give you a little bit of a podcast spoiler and that is that at the end of this episode I'm going to recommend to you Python and TensorFlow. Python and TensorFlow without knowing anything about you, if you're looking into machine learning, then my guess for you is the language Python
 and the framework tensor flow. Do you remember in a previous episode on linear regression when I was talking about a bias parameter theta zero? What that bias parameter does is it says, if I don't know anything about your situation, I can still make a prediction. So if you give me a house in Portland, Oregon, and I don't have any information on its square footage, number of bedrooms, number of bathrooms, et cetera, I can still give you at least the average cost of houses in Portland, Oregon. That's all that.
 bias parameter is, it's like an educated guess if you don't have any information and that's what I'm doing here. My bias parameter is tensorflow and Python, not knowing anything else about your situation. Now of course, the details of your situation are going to change that so we're going to get into the details of why you might use one language or framework over another in this podcast episode. Let's start right away with C and C++. C and C++ are as close to the metal as you can get in computer programming before you go to assembly language.
 and below that binary language. And there's no way that you're going to be writing in binary assembly. So to milk the most performance that you possibly can in computer programming, you'll end up using C or C++. What just so happens that machine learning really needs performance. Machine learning is heavy, heavy, heavy on the computer. A deep neural network costs a lot of computational resources and time. And so in order to get the best performance out of your neural network, C and C++.
 are the best options for you. You'll often see this to be the case in larger corporations that specialize in very computationally expensive things like video games and cryptography. You'll also see it in machine learning companies, very large-scale organizations like Google and Facebook. They might use CNC++ for their very hardcore, very highly optimized machine learning algorithms. There's one more nuance about CNC++ is that machine learning algorithms.
 are best performed on the GPU, the graphical processing unit. The reason for this is that the GPU is specialized in mathematical operations, floating point arithmetic, especially matrix algebra. Remember linear algebra, but really any sort of mathematical operations, whether it's statistics, calculus, or linear algebra, are best suited to the GPU and not the CPU. And C and C plus plus give you direct access to running your operations on...
 the GPU. They'll let you manipulate the math directly on the GPU, which makes CNC++ the best bet for highly-performing machine learning programming. Now I'm going to throw a curveball at you and say I do not recommend CNC++. As you'll see in a bit when we get into the Python frameworks, these things called computational graphs or symbolic graphs, let you write your code in high-level Python and TensorFlow, and it will actually be converted into C and run directly on the GPUs.
 So you get the best of both worlds. Your code will run in C, but you get to write it in a high-level language like Python. And you'll appreciate that because C and C++ are very difficult programming languages. Very difficult to write. Very verbose. Many, many lines of code to accomplish the same thing that much fewer lines of code and higher level programming language would achieve. Let's say 100 lines of C++ code becomes boiled down to 50 lines of Python code. And you have to handle things like memory management and stuff that you wouldn't...
 have to normally handle in a high level language. So I highly do not recommend C and C++ for machine learning programming unless you already know the languages and you feel confident in which case, power to you, you've actually got a lot of runway knowing those languages as far as job applications. Or if you're working a little bit closer to the metal on some really screaming performance characteristics of machine learning models. Basically, my recommendation is that you will know when and why you need C or C++. If you don't,
 feel like you need it right now, skip it. Now let's dive into a category of languages I'm gonna call the math languages, mathematics languages. R, MATLAB, Octave, and Julia. These languages are very different from typical procedural programming languages, object-oriented languages or the like, like Python. The looks of R is substantially different than the looks of Python code. R, for example, is highly optimized for mathematics operations.
 and linear algebra and matrix operations are directly supported in the language itself. It's hard to describe, but you'll see things like colons and commas between braces of multidimensional arrays, what's called data frames or matrices in R, that looks a little bit weird coming from a traditional procedural programming background. And what these allow you to do is slice and dice matrices directly, perform matrix algebra directly, and then statistics functions like the standard.
 interdiviation and correlation of vectors or scalers or matrices are directly built into the standard library of the R programming language. So math is a first class citizen that really shines in the syntax of the language. R also supports machine learning very robustly. There are third party libraries around neural networks that give you a lot of leverage. Something that would take you many lines of code in a traditional programming language that has to do with math would really be slimmed down in all.
 So R is a fantastic language for the math petitions. And as we know, machine learning is nothing more than math, which makes R also a fantastic language for machine learning. Then there's two other languages, MATLAB and OCTAVE. These are direct competitors. MATLAB is a commercial licensed programming language. It's proprietary, so if you want to use it, you actually have to buy a license. It's also actually the language of choice used in the Andrew Inck Coursera.
 courses, unfortunately. Luckily, you don't have to pay for a license to take that course, Andrew Eing, has a code you could use for a temporary license for the duration of the course era course. Well, MATLAB is proprietary and commercial. Octave is a direct response to MATLAB. It's everything MATLAB is, but open source and free. So it's kind of a middle finger by the free and open source community to the MATLAB product. It's kind of like what Gimp is to Photoshop, if you're familiar. And.
 And from what I understand, it's got a little bit of the same thing going on that Gimp has with Photoshop. Sure, it's an open source competitor, but it seems like MATLAB still is a little bit more polished. MATLAB shines a little bit more on the linear algebra side of things than on the statistics things from what I understand. I don't really think that you would be seeing MATLAB in a professional capacity. It's much more commonly used in academia, in university courses, or maybe in research,
 some simple puzzles in research before you migrate your model to a more robust solution on Java, Python, etc. So I wouldn't recommend picking up MATLAB or Octave on your own for your own professional purposes. Instead, just take MATLAB as it comes as necessary in your learning curriculum. Like I said, Andrewings Coursera course uses MATLAB, so you will be too. Don't worry, it's a pretty minimal language, it's pretty quick to learn and he teaches you all the basics in his course. And then...
 Then there's a programming language called Julia. Now, R, like I said, specializes in linear algebra and statistics and calculus. It has very strong mathematics foundations. Julia is very similar to R, but newer, and slicker, and sexier. That has pros and cons, pros being syntactic sugar and more powerful machine learning models. Cons being that you really won't see a lot of people using it in industry yet. So it's kind of a Johnny come lately, and up and coming.
 language that I would maybe keep an eye on, but hold off for now on trying to pick that language up. So as far as the math languages are concerned, my preferred language of this whole set is R. R gives you a lot of power, flexibility, it's open source, and it's actually quite common in industry. So knowing R would be professionally advantageous. Now I see R used pretty commonly on the data analytics side of of data.
 Remember from a previous episode that I said that data science is sort of an umbrella term for various professions or fields of study. One which is data mining, one which is data analysis or analytics, and another which is machine learning. So while R supports machine learning in a very robust way, it's a lot less common. I find when I'm looking at job postings or conversations online, then Python, which is a language much more common in the machine learning space, where R is more common on the data.
 that analysis side of things. I'll get back to that shortly. Those are the math languages, R, MATLAB, and Octave, and Julia. Next we enter the JVM Java Virtual Machine languages. Java, of course, being the namesake for the Java Virtual Machine. So Java is the most popular of the JVM languages. Scala, being a recent contender on the space, which is picking up steam at a very rapid clip. Scala is kind of the dynamic function.
 version of Java with syntactic sugar. So I like to think of that as the kind of hip new startup contender on the JVM. Java is one of those 50-year-old middle manager fancy suit languages and Scala is the big bearded coffee drinking startup hipster. Scala is a very fun programming language to program in, especially its attention to functional programming. Functional programming is a very powerful tool in the programmer.
 tool kit. If you're not familiar with functional programming, you will be eventually trust me. It's becoming more and more of a necessity in modern programming paradigms. But it is especially essential for distributed computing. So distributed computing is the name of the game for the JVM. The JVM languages Java and Scala are meant for very, very highly scalable data pipelines or data architectures by way of distributed computing.
 Okay, so there's two frameworks that you'll use. If you're using Java, then the framework of choice is Hadoop, H-A-D-O-O-P. And if you're using Scala, then the framework of choice is Spark. So what do these frameworks do? You take in a bunch of data from the internet, and you pipe it into these distributed computing frameworks, Hadoop or Spark, and now you are inside what's called a data pipeline or a data architecture. Let's say that you're trying to consider...
 Zoom every tweet on Twitter so that you could do some sort of sentiment analysis on what is popular, what is making people happy, what is making people enraged on Twitter. Well consuming all the tweets from Twitter would be done by way of a data architecture or a data pipeline through distributed computing. So these would be your guys, the JVM languages and frameworks. Hadoop or Spark. You would pipe all the tweets into Spark or Hadoop and you would write various nodes in this distribution.
 graph for processing the data. So this is data mining. This is the data mining piece of data science. Let's think of a mining analogy just so. If you're a miner and you're going out and you're chiseling away and you've got a bunch of ore, you bring it into a truck and that truck brings it to a factory and the factory dumps it at the receiving bay and it all goes onto a conveyor belt and the ore goes through some sort of processing mechanism that gets dumped into some cauldron and mixes some liquid and out comes some shiny object.
 on the other side of the conveyor belt. And then that goes through some packaging system where it gets wrapped up into a gift box, and then the gift box gets delivered to your machine learning algorithm. So the data mining side, or the data pipeline, or the data architecture, that is what the JVM is all about. And that is one of the three main components of data science, data mining. You need your data for machine learning. Your machine learning algorithms need data to function.
 But you don't want raw data, you need pre-processed data. And if you're working with lots and lots and lots of data, as in the example I just used, then you're going to need to use a scalable distributed computing system like Hadoop or Spark. So this data mining step is all about boiling all your data down into just the basics, cleaning it up, slicing and dicing, packaging it up for either your data analysts, the all.
 people or your machine learning people in Python. So that's what these frameworks do. They take in raw data, they slice it into chunks, send those chunks off down different conveyor belts, those conveyor belts, run the data through multiple processing steps in order to clean up your data and package it all up. And now you have the best of the best clean data for your data and analysts or your machine learning experts. Three subfields of data science, data mining, data analyst.
 analytics and machine learning. Now, just to help you understand that differentiation, let's go through one more example. I'm gonna use the Google ecosystem. We have Google Analytics and Google AdSense. Analytics is charts and graphs. It's for helping your growth hackers or your biz devs make business decisions. It's called business intelligence. You're looking at charts and graphs to determine what sort of advertising strategies are bringing in new customers.chodzi.
 Maybe we need to A, B test our website in order to determine whether the button should go on the top right or the top left. I think the search engine optimization is not working for the website under this regime. Maybe we need to change the wording under certain pages to drive in more traffic. Just a bunch of charts and graphs in order for your business people to make better business decisions. That's the data analysis or data analytics side of the data science package. It's jobs like those that you're mostly going to see like.
 languages like R, R specializes in data analysis. Now R can be used for machine learning and R can also be used for data mining. As you'll see in a bit, every language can be used for all three prongs of the data science umbrella. But if you've determined that you want to specialize in one capacity or another, it's in your best interest to choose the popular language of that ecosystem. So there's data analytics. On the machine learning side, let's talk about Google AdSense. AdSense.
 determines what types of ads to show which users, based on users browsing history, their prior search queries, things they've clicked on in the past, maybe they've sat on some Amazon product page for like 10 minutes, really weighing the pros and cons of purchasing the Oculus Rift. What Google does is it tracks as much of this as it possibly can in order to determine what are the best ads to show the user in the future. That's what machine learning does.
 It learns some patterns by way of data in order to make predictions, predicted courses of actions, predicted ads, classification numbers, etc. So Google Analytics is like data analytics within data science. Google AdSense, the ad-serving piece of Google, is like machine learning within data science. And finally, we need all this data both on the analytics and the machine learning side to work with. And that's what data mining does. Data mining pulls data from some source.
 Of course, slices and dices, cleans it up, and then hands it off as a beautiful package to your analytics or your machine learning. So in this Google analogy, there's some data pipeline at Google that's collecting every click, every action a user takes, how long a user is sitting on some web page, every search query, all that stuff is being collected in terabytes and terabytes through the scalable data pipeline, the data architecture by way of very likely, Hadoop or Spark. Now if your data...
 collection phase is very minimal. If you're not collecting megabytes or terabytes of data, then you don't need a data pipeline. Maybe you're just collecting data on a one-time pass or maybe you're collecting clicks and actions, but it doesn't happen really that frequently. So you don't need this major scalable architecture. You can just take the data as it comes, slice and dice it and package it all in the language that you're using, whether it be Python for machine learning or R for data analytics. So you don't need a data architecture. You don't need a data architecture. You don't need a data
 don't need a data pipeline. You'll know if you need it because you're trying to process lots and lots of data. But if you're not trying to process gobs of data, you don't really need this step. Data mining is a field of its own. People specialize in data mining. They know all the tricks of the trade for web scraping, breaking down data, cleaning it up, normalizing numerical input, and all that stuff. Okay, so that's data mining. That's what we're talking about on the JVM. So these languages and these frameworks, the language Java.
 It's an older language. It's more enterprisey. It's more robust and powerful, but it's also very verbose. It's very chatty. It takes a lot to do little. So Scala comes in and cleans a lot of that up with syntactic sugar. Scala is a fresher take on Java. Hadoop is the data pipeline framework that would go on top of Java if you're using Java. And Spark is the data pipeline framework you would use on top of Scala. Now I believe that you can use
 use Hadoop with Scala and Spark with Java. So you can mix and match them. But I also believe it's probably in your best interest to go with the language that's chosen as a first class citizen. So here would be my recommendation. If this is a Greenfield project where you get to call all the shots, in other words, you get to choose the technology, then choose Spark on Scala. It's simply a cleaner, slicker, faster, sexier, Hadoop on Java. And it's where a lot of the mind share is and future growth in this space. But you may not have a choice.
 You may be applying to a company, maybe a larger organization or a little bit of an older company, or even a new company whose developers are very well versed in Hadoop and Java. So Hadoop on Java, Spark on Scala. Now one more note, I said that any of these languages and their frameworks can be used for any of the prongs of data science. Our specializes on the analytics side of things, but it can certainly be used for machine learning. It's actually very good for machine learning. And sure...
 It can be used for data mining, maybe not large-scale distributed data mining, so if you're working with terabytes, R might not be the best solution, but R can be used for any of these three pieces of data science. Similarly, Java can be used of course for data mining, of course on the analytic side, and in machine learning, by way of some modules that you would plug into your pipeline. So if you're using Hadoop on Java, the legacy stack, then you would use a module called Mahout M-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A
 H O U T, Mahout. That is the machine learning framework built into Hadoop. So if you are mining data from the web, let's say you're scraping some information off of web pages and you pipe it into your factory and it gets cut up into pieces and it gets massaged and cleaned up, normalized, packaged up, and then sent to the last module or node of your data pipeline, which is the machine learning piece. You would use Mahout. So you would use Java, Hadoop.
 and Mahout. On the Schala stack, you would use Schala, Spark, Spark for the data pipeline framework, Schala is the programming language, and then your final note of the pipeline for machine learning is called Spark ML, ML for machine learning. So Spark ML. So if your specialty is data mining, with maybe a little bit of machine learning sprinkled in, then you can go this route. But if your specialty is machine learning, with maybe some analytics and data mining.
 sprinkled in, then that brings us to Python. Python. Python is the most highly recommended and most popular language of choice for machine learning engineers. If you know anything about Python, then you may be thinking why the hell is Python the most popular language for machine learning? I said way back in C and C++, we need to the metal screaming performance for our machine learning models. We need it to run on the GPU. We want to scale our
 data mining pipeline distributed across multiple computers. Python doesn't seem like a good fit for any of this. Python is a dynamic language that is, if I dare say rather slow, it's not functional, so it doesn't distribute nearly as well as Scala. Here's what Python has going for it. Python can handle any of these tasks with ease. By way of either a domain specific language or a framework. Okay, so let's start with the math languages are MATLAB and oct.
 and Julia. How does Python stack up to those languages for supporting mathematics as a first class field within the language? Well, the language doesn't support mathematics that well. Out of the box, instead, you use these libraries called NumPy and Pandas, N-U-M-P-Y, and Pandas, you know, like a plural panda. NumPy is sort of Python's answer to MATLAB. So NumPy gives you out of the box, including the syntax, the colon's and
 the commas that let you slice across matrices. All the operations that MATLAB supports. So boom, using Python, we just crossed out one entire language. Pandas gives us all the same functionality that our gives us, not all the way, not completely. Of course, our is still a little bit more robust, a full community backing behind it. But most of the way there, and as far as we're concerned, as machine learning engineers, all that we could possibly need. So boom, we just cross out our. Now, of course.
 Python might give us lots of other things that we might need in our entire solution of a startup or some sort of web service such as the server, whether you use Flask or Django and connectivity to your database through SQL Alchemy. So it fits into the rest of the architecture of your company without having to know other languages and frameworks. It just fits right in. How about data pipelining? That seemed like something that was very well suited to Java. Well, guess what? Spark supports Python.
 You don't need Scala for Spark. It has a Python API. You can use Python for Spark. So boom, we just cross out the whole JVM section. We have data pipelining through PyS Spark. We have mathematical operations through NumPy and Pandas. Okay, and there's one thing left, performance. When we're talking about how it compares to CNC++ and you need your operations to run on the GPU and not the CPU, that's where frameworks come in. Theano, Torch, and TensorFlow.
 to Python is a paradigm called a computational graph or a symbolic graph. What you do is you write your neural networks in the framework like TensorFlow in Python and TensorFlow packages up what you just wrote. It walks down the stairs to the basement and gives it to C. It says here C, I want you to rewrite this in an optimal way for you to execute in a C environment.
 C looks at what you wrote in TensorFlow and Python, and it kind of turns it around into tans, and it kind of chuckles to itself. And it says, newbie, it rewrites it because it knows a better way to run things. And then it turns around, it opens the oven, pops it in the oven, that's your GPU. So TensorFlow takes your Python code, converts it to C, and runs it directly on the GPU. And then when the oven's finished, it pulls it out, C gives it back to Python, and Python gives it back to you. So there you have it, boom, we just blew C out of the equation.
 Python handles everything. That's why Python is so popular in the machine learning community, is not because the language itself stacks up to any of these other languages. In fact, it doesn't. However, libraries and frameworks written in the Python ecosystem can replace all these other paradigms very effectively. So Python gives you your full data science stack from data mining to data analytics to machine learning. Now I will say that if you apply to a job...
 as a data analyst, or if you apply to a job as a data minor, you will still find the JVM or maybe R used predominantly in those particular verticals. But for a company which supports the whole process A to Z and which you might be spanning the full stack of data science, Python is certainly the most popular and common language and ecosystem that you will see in the space. Most of the jobs that I see when I'm scanning the job market and machine learning are certainly Python jobs. And again, it's nothing in
 In the language itself, it's all in the ecosystem. The libraries and modules that are available in the Python ecosystem that make data science completely spanable from A to Z by way of a single language. So let's talk about these frameworks. First off, like I said, we have NumPy and Pandas. NumPy gives you a little bit of the quick and dirty matrix operations. It's sort of the answer to MATLAB. And Pandas gives you all sorts of statistics operations, summaries, analytics, plots and graphs over...
 data, a bunch of operations on a data structure called a data frame. So these are two libraries that you'll commonly see in the Python tool kit. Another library is called Scikit Learn, SCI, KIT, like a scientific kit. And this is a bunch of shallow learning algorithms available for Python. Shallow learning algorithms like linear regression, logistic regression, support vector machines, and Bayesian inference, driving cars. Anything you could possibly name in the machine learning universe.
 I think you'll find that the majority of your machine learning work professionally and academically will tend towards Deep learning if not in the beginning definitely by the end So most of the work that you're going to be doing in machine learning will be done in deep learning Neural networks from the prior episode. So once we graduate to deep learning you can no longer use Scikit learn instead you move on to one of these frameworks These frameworks I mentioned previously that are kind of categorized under the umbrella of comp
 mutational graphs or symbolic graphs and one more time. Let me explain what these frameworks do You write your code in Python within the frameworks architecture So you use the frameworks API. Let's say to build out a neural network Let's say that you create a new object that's an instance of a neural network and you call a few methods off of that object adding new layers and new neurons and you add a final operation, which is back propagation Which will perform gradient descent and then
 you feed it a bunch of data to train the network on, and then you say object.go or session.run or some other incarnation like that. And what happens now is Python by way of TensorFlow or whatever framework you use will hand off what you've written to see. See will assess what you've given it and rewrite it if necessary if it finds that it can optimize it, execute the thing on the GPU and then give you your result.
 Now notice that step where I say that C may decide to optimize what you have written. This whole setup may sound familiar to you. If you've ever worked with React Native as a mobile app developer, React Native lets you write your mobile app in JavaScript. The language being JavaScript and the framework being React and the framework on compilation will then pass what you've written down to Android by way of Java and iPhone by way
 Objective C or Swift and those respective languages will make optimizations if necessary and then compile your code Down to native mobile code. Okay, so that's on the mobile space if you've ever made a video game with Unity It's very similar you write your game in a high-level framework and it gets compiled down Or if you've ever used an object relational mapping system an ORM for databases maybe using Ruby on Rails or Django what you do is you write your SQL
 query by way of the framework. You're not actually writing any sql. Instead, you're running a bunch of functions passing in parameters and then you hit go and the ORM will do some analysis of what you've written. It'll optimize it. It'll gut out some stuff that's unnecessary. It'll add in some new stuff that might make things a little bit faster. And then it will turn it into the sql language. So it'll transform your Python or your JavaScript code into sql and then sql will execute the code. So that's exactly what's happening.
 with these computational graphs in Python, you write your Python with any of these frameworks, theano, torch or TensorFlow. The frameworks optimize it and compile it down to C, execute it on the GPU, and therefore you get screaming performance with a high level expressive language. Very slick indeed, highly recommended approach to machine learning. It's also by way of these frameworks that most of the mind share and research and progress is happening, especially TensorFlow.
 Before we get into TensorFlow, let's kind of take a historical approach to these frameworks. We'll start with theano, THENO. As far as I know, theano is the oldest of these computational graph frameworks. It was built out at the University of Montreal where a lot of my share was going on, especially in the early days of deep learning, making a comeback with Jeffrey Hinton, Yashua Benji all those guys. So that was sort of the hotbed physically in the universe of machine learning research was in Montreal.
 So a framework came out of this called theano and it supported sending mathematical operations down to C and from C to the GPU and all the way back up That's the computational graph and while it was used almost Exclusively for machine learning it didn't initially support machine learning directly It was more on the math side of things so the community built third-party modules ones called blocks and another one's called lasagna So it's like another layer on top of theano for
 common machine learning tasks like linear regression and logistic regression. So that stuff was built out on the shallow learning side, but we still needed a high level expressive language for the deep learning side of things. And so another layer on top was built called Keras, K-E-R-A-S. So theano is more of an ecosystem than an individual framework. If you use the theano ecosystem, you will be using the theano framework indeed, and any number of other modules on top. Hi.
 find personally and this might get me into trouble that Theano looks to be going out of vogue to its competition, namely Torch and TensorFlow. So I think Theano's got a little bit more history, a little bit older. Well, over time, it became more common to use very research heavy, experimental, deep learning architectures like a CUMV-Net, C-O-N-V-E-T, or CNN, or convolutional neural network. You've already seen this in the prior episode about vision.
 CNNs are very good at classifying images, convnet. Well, turned out that Theano wasn't great maybe at handling something a little bit more obscure like that. So Facebook created a competitor symbolic graph-based framework called Torch, and Torch particularly specialized in image recognition by way of conv nets. And of course it handled any other sort of incarnation of neural networks and other machine learning paradigms, and the math at the lower level, etc. You can see why Facebook would-
 be so interested in Convnets. Image recognition is their specialty. Please tag your friends in this photo. That's what Facebook's all about. So Facebook is the king of the machine learning universe when it comes to image recognition specifically. So that's something that torch from the get-go was very, very good at accomplishing. Incidentally, the framework torch was written on Lua, Lua. I have never heard of that language up until torch. I've never seen
 anywhere else. And I'm sure that they experienced that kind of reaction in their community as well because they recently released a Python API. So now you can do Torch in Python. You're starting to see a trend here. If you want to play with the big dogs and machine learning, you have to support Python. That's what we saw with Spark. Spark released a Python wrapper because when they were stuck in the JVM, they realized they were really missing out on a major market of developers. So, the piano and then Torch.
 and then finally a new player has entered to Hensor Flow, T-E-N-S-O-R-F-L-O-W, Tensor Flow by Google. If Facebook has an active interest in image recognition and classification, Google has an active interest in machine learning period. In fact, if you haven't made this realization yet, you will. Google is the God of machine learning of the universe. Everything that could possibly be a-
 machine learning problem, Google needs to solve it. Language modeling in natural language processing for processing your search queries. Knowledge representation for coming up with a response to your search queries. Image recognition by way of Google images, voice recognition with all of their voice services on mobile, reinforcement learning in their self-driving cars, anything you could possibly name in the machine learning universe, and especially the patience in linear algebra statistics and calculus.
 In a middle layer, you can work into nominator amongst their various teams trying to solve these problems. The framework they use and built TensorFlow on top of Python. I don't know if it was that people realized who Google is in the machine learning equation or if it's that TensorFlow itself is a superior framework. But when TensorFlow is released, people flocked in droves. TensorFlow is bar none. The most...
 popular machine learning framework on the face of the planet. The most talked about, the most commonly used in courses, in GitHub projects, boilerplates, new and interesting research and projects. Tenser Flow is a little bit similar to the Theano ecosystem in that it supports various layers. On the lowest layer, you have basic math operations in linear algebra, statistics and calculus. In a middle layer, you can work with raw machine learning algorithms and constructs, and then on a high layer,
 level, you can write neural networks with various architectures in a very expressive manner. There's another option to go even higher than that with Keras. Remember the top level of the piano stack? You can put that hat on TensorFlow as well. And you'll get a little bit even more high level expressive neural network capabilities. TensorFlow has a little bit of a history. It actually had some major performance issues in the early days.
 If you look up TensorFlow Performance and you find any articles from two years ago or one year ago even, you'll find performance comparisons between TensorFlow, Torch, and Theano, and you'll see that TensorFlow always loses. However, because TensorFlow uses that computational graph system, the code that you write is not the code that's executed. And so the TensorFlow team was able to optimize the in between, the Python goes down to C step, and of course the GPU stuff, and they milked out that performance and now TensorFlow is
 one of the most, if not the most, performant machine learning framework on the market. TensorFlow also has built into it the capability for running distributed computation. In other words, you wouldn't even have to use Spark if you use TensorFlow, because it's built into TensorFlow by way of something called TensorFlow serving. And finally, TensorFlow goes beyond just compiling down from Python to C into the GPU on your computer, you can run TensorFlow on mobile.
 on an Android or iPhone device. So if you're building a mobile app that does machine learning and there's some way that we can do some of the medium to minimal, level, compute resources models on the user's device directly, let's say for example, image detection, you're taking an image and it will maybe highlight a person's, a silhouette separating them from the background or something like that, can be run directly on the mobile device and then you can kick off your more computationally expensive operations directly to the TensorFlow server. So TensorFlow.
 really is powerful. It runs everywhere, it runs any sort of machine learning model under the sun, including recurrent neural networks, which are something a lot of these other frameworks struggled with for a while, and reinforcement learning, which is really getting us into the artificial intelligence territory. Reinforcement learning, remember I said, is that gateway that takes you from machine learning and into AI. Reinforcement learning, and TensorFlow supports reinforcement learning models directly. Okay, so if you decide on TensorFlow and Python, which I
 suggest you do, you will have the option to compile TensorFlow to run only on your CPU or on your GPU. So if you're doing TensorFlow on your laptop for work and you don't have a very powerful GPU, you can just compile it to run on your CPU and it will. It's slower of course, and for very, very heavy architectures, it's going to be very slow. But for your server, on an AWS cluster of Titan X GPUs, etc, you of course will compile it to run on the GPU. TensorFlow supports out of the box and video jeep.
 GPUs and video is sort of king in the GPU space of handling machine learning and deep learning computations on GPU. They own it. They know that machine learning frameworks use GPU and so they've actually written drivers for their GPUs so that machine learning models can run more seamlessly on their devices. They do this by way of something called CUDA and I don't really know what that stands for but if you look up CUDA it says parallel distributed computing platform application.
 Anyway, it's for running heavy math on your GPU and then an additional layer called QDNN, CUDNN and that is specifically for running neural networks on your GPU. So they have an interface, a driver, for machine learning frameworks like TensorFlow to interface with directly their GPUs. So incidentally, if you actually are looking for hardware, you may want to target NVIDIA instead of AMD because they have these interfaces. Okay, so that's a...
 the land of the Python machine learning frameworks. We have theano, torch, and tensorflow. Now there are plenty of other machine learning frameworks out there, many, many machine learning frameworks. I'm going to give you a handful of some of the more commonly mentioned ones online. One is called cafe. It's written in CNC++ and its interface is C in C++. So you're not gonna be using cafe in the Python ecosystem. As far as I know, it's a little bit old and dying. So I don't know that I recommend cafe.
 for anyone really. I may be wrong there. Take that with a grain of salt. Do some research. CNTK is a framework put out by Microsoft. MXNet, put out by Amazon. Okay, I haven't really seen CNTK or MXNet in the wild on job postings, online tutorials or any of that. So, and then there's deep learning for J. If you're using Java, if you're using the JVM stack, particularly if you're coming from a Hadoop or Spark stack, then you may...
 want to use Java for your machine learning framework as well. And so there's a framework called deep learning for J. And this is actually a very popular framework. It's very powerful. And if it weren't for it being in Java, I would put it into the frameworks comparison. So it's on par as far as popularity goes to maybe the likes of theano or torch, but it's in the Java ecosystem. So very powerful and popular framework called deep learning for J. And then there's another framework out there that you'll see a lot called open CV. CV stands for computer.
 and this is really just for computer vision. So you'll know if you need this framework, don't seek it out for its own right. Computer vision can be handled with convolutional neural networks, which are an architecture that come along with TensorFlow, Theano, Torch, et cetera. So that's languages and frameworks, and my recommendation to you is Python and TensorFlow. Bam, I'm going to post a handful of articles that you can see people comparing languages and frameworks for machine learning purposes.
 where I got a lot of this information. I've also got a lot of my information from job hunting and from talking to colleagues. And for the resources section, okay, you need to know Python to get into machine learning. You're going to unfortunately learn MATLAB first if you're starting with the Andrew Eankorsera course and it'll kind of be a throwaway programming language, but it's okay because that language isn't very difficult to wrap your head around. Python is a little bit more difficult because it's a little bit more robust. There's a lot more to know in the Python ecosystem. So if you don't already.
 already know Python, then I would recommend picking up a Python book. I'll do a little bit of research myself on Amazon and I'll pop one into the show notes. So learn Python first and then learn TensorFlow. Now there's books and there's online courses and there's video series on TensorFlow. And I've consumed a handful of various media for TensorFlow myself. But the thing that I actually found to be the best resource for learning TensorFlow was actually just the documentation on the website. The tutorials are very...
 very thorough and of course they're always updated which is something that one of these other media can't achieve. But they have very thorough tutorials to go along with code in an examples folder of the TensorFlow project itself that you can reference. So I highly recommend the TensorFlow tutorials and that is it for this episode friends. See you next time.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. You're listening to machine learning applied. In this episode we're going to talk about practical clustering tools.
 per the usual difference between machine learning guide and machine learning applied. I won't be talking about any theory about clustering or how these clustering algorithms work. I'm just going to be talking about some of the scikit-learn packages and some of the tips and tricks that I've found useful in actually applying clustering techniques in Nothe. And hopefully in a future machine learning guide episode, I'll talk about the theory behind some of these tools and clustering in general, etc. Now the first
 tool I'm going to talk about is scikit learn k means. Everyone knows k means k means is just the most popular clustering algorithm ever just ever. It's everyone uses it 99% of the time if they're clustering they're using k means. And in fact, I just recommend trying k means first. If you need cluster your vectors try k means if it works great. If not, then you'll move on to basically the other tools world.
 talk about in this episode. How does k-means work kind of generally in practice? We're not going to talk about theory. I don't want to talk about how these centroids move around and then the points shift in all these things. I want to talk about in practice, you will be using the scikit-learn clustering.k-means package and you will specify upfront a number of clusters. So let's say you have 1,000 vectors. That's your matrix. 1,000 by 10. 10 being the number of-
 mentions per vector in your matrix. Well, you would need to know in advance how many clusters you're going to be clustering your matrix into. And then what will happen is you'll say k-means, parentheses, n underscore clusters equals number of clusters. And in the case of 1,000 vectors, maybe let's say 10, 10 clusters. That sounds pretty good. And parentheses. There are other hyperparameters in the k-means package that I won't be discussing. And then you'll say dot fit or dot.
 predict your parentheses, your matrix, and it will return to you the labels, the cluster assignment, which cluster each row got assigned to. So if you have a thousand rows, you will get a list of a thousand numbers between one and ten. Now I mentioned in a recent Machine Learning Guide episode that Euclidean Distance does not perform well in high dimensions, and K means is effectively using...
 the Euclidean distance metric under the hood in order to move its points around in order to find those centroids. In Nothe, the journal entry embeddings using the sentence Transformers Embedding Tool are 768 dimension vectors. That is too high for k means to work effectively. It is too high. As far as I understand it from poking around online, when we say too high of dimensions for k
 means to cluster effectively for Euclidean distances to work well. We're talking like 100 dimensions is like push-in things and we really want something like 10 to 50 dimensions. That's really our sweet spot. So 768-dimension vectors is just right out. It's way too high for k-means to work fantastically. Now I have used k-means in no-thee in practice.
 And it's worked okay. It's not terrible. And in fact, if you go to notheai.com now, sign up, create a few journal entries and then go to the insights page where you can generate themes of your entries, of your journal entries, themes in nothear clusters. They're clusters. They are clusters of your journal entries. In other words, if I talk about relationships a lot and I talk about politics a lot, what may start out as a thousand journal entries becomes two clusters in...
 in the themes section of Noether. It will show you the key words, the common key words that appear in cluster one. In this case, it would be politics, Trump elections blah, blah, blah, as well as a summary of the core of that theme using the hugging-face transformers summarization pipeline. And then theme two, a.k.a cluster two, because we are using clustering, that's all themes are, are just the clustering feature in practice. Theme.
 two might have the words, relationship and marriage and all these things and then a summarization of those entries. You can on the website, click the Advanced Gear icon and select K means to be your clustering algorithm behind the scene. It uses a glomerative clustering which I'll talk about in a bit. And you can see the result. I have found that K means for this purpose, for natural language processing embedded documents. Now it's 768-dimension vector, very high-dimension vector.
 Clustering those does not work fantastically with camines. It works okay. It's not terrible but I have personally found that a glomerate of clustering works better and we'll talk about that in a bit But before we talk about that, you know, camines is the most popular clustering algorithm on the planet So let's dwell here a little bit more. There are other implementations of camines You don't have to use the scikit-learn package a very popular other implementation of camines is the FICE F-A-I-S-S is a Facebook
 machine learning package. And it has its own implementation of K means, which is actually faster, it's more performant. And it actually is more accurate evidently after you train it on very large data. So the scikit-learn K means package works effectively for low to medium data. I'm talking about numbers of rows, not dimensions. And the FICE K means implementation by Facebook works better for large data, number of rows. Both computationally performance wise.
 and accuracy performance wise. And what this package FICE does, I will talk about this concept in another episode because it is highly related to Nothi. I'm not actually using FICE or any of these related packages, but I will eventually. And I think it will be of interest to you listeners. FICE, they call this approximate nearest neighbor search algorithms or packages. And there's a handful out there. One is FICE, one is ANOE, A-N-N-O-Y, yeah, exactly ANOE.
 And another one is HNSWlib. And you'll find, if you want to dig into these now, because I won't be talking too much about them currently. If you want to find out a little bit more about these, actually go to the UKP Lab Sentence Transformers package, which is the tool I use in Nothe for embedding your documents. And in the examples directory, there's code that uses these packages and discusses what they do and why you would use them. So if you want to get a quick start.
 and dive into these things, I recommend going to the UKP lab sentence transformers examples directory doing a little poking around. But in essence, what these packages do, these approximate nearest neighbors algorithms is they are, they're used for semantic search for looking things up. Just like you would use cosine similarity and I am using cosine similarity in nothe, but it's for very, very, very large data sets, extremely large data sets, millions of rows. What the, what these packages do
 And let's just take a FICE for an example. FICE creates an index, basically your database of document embeddings, an index of these documents. So you instantiate an index and Python code, index equals FICE.index, parentheses. And then you feed in all of your document embeddings. These are all 766-date-dimension vectors into the index. And the index creates a database. It's basically effectively like a Postgres database or my SQL database. It's a database of vectors that can...
 and be swapped in and out of disk and memory very computationally effectively, and which indexes the location in vector space of these documents to each other, where you get to specify what is that distance metric by which these are compared to each other for look up on the index. So if you're using document embeddings, you care about cosine similarity, as we discussed in the distance metric episode. Document embeddings care about cosine similarity to each other.
 So you specify in the FICE index. Here are my documents, a million of them, and the similarity metric I want is cosine similarity. Now you have an index that is used for extremely efficient lookup and search. And the next time you have a document, so for example, if I have a million books being my FICE index and Now I have a new journal entry that I put on Noathy. I could say find me the top 10 books matching this journal entry and you just throw it into the FICE index and it returns to you.
 10 results, which using the similarity metric that you specified, in this case being cosine, it's able to just project your journal entry into vector space and these points, these books are represented computationally efficiently, can be just pulled out really fast and easy. And the way this relates to K means is that if you don't specify any distance metric, it's going to be using the Euclidean distance metric. Everything always uses Euclidean by default. Euclidean is just like the default distance metric of everything. And the default.
 Plusturer is the K-means algorithm, but FICE is implementation of the K-means algorithm. So the FICE package, primary utility is this approximate nearest neighbor search, which allows you to do semantic search between vectors. You can vary computational efficiently, look up similar vectors to one that you pass it for very large data sets. But one of the side benefits of FICE is that it uses its own implementation of the K-means algorithm under the hood in order to perform this process. It also exposes that...
 k-means implementation to you as a developer, which you can then use for your own, just normal clustering application for very large data sets. So that's k-means, the psychic learned package for small to medium data sets, the vice package for large data sets. And k-means sort of uses Euclidean distances as part of its algorithm under the hood. But like I said, Euclidean distance doesn't bode well in high dimensions, and indeed with document embeddings as we're using in Nothi, we're dealing with high dimensions. So what do you do?
 You have a couple of options. One thing you could do is dimensionality reduce your large vectors. And that will be a very near upcoming episode in machine learning applied as dimensionality reduction. There's all sorts of dimensionality reduction algorithms out there. One is PCA, principal component analysis. Another is SVD. Another is UMAP and TSNEE. Now TSNEE is a little bit more commonly used for visualization purposes. But UMAP and TSNEE both are...
 are what's called manifold embedders, and they can be used for dimensionality reducing your large vectors to very small dimension vectors. In the case of T-SNE2 to three components, down from 768, and in the case of UMAP, you get to specify however many components. We won't talk about dimensionality reduction here, that'll be an upcoming episode. And I'm not using dimensionality reduction in Noether. Instead, I'm using a different clustering algorithm,
 which supports different similarity metrics for determining the clusters of your rows. And namely, as I mentioned in the similarity metrics episode, in NLP, cosine is almost always preferred over Euclidean distance anyway. You know, it's not just that high dimensions don't perform as well. That's not the only reason we're not using k-means here. It's that in the case of NLP, Euclidean distance, which is sort of the driving force behind k-means, doesn't really make sense.
 a distance metric in semantic searching. Cosine is preferred. Cosine is preferred. So K means is pretty much kicked to the curb in this particular case of NLP. High dimensions, check, Euclidean check, move on, and the place that I ended up landing on is a glomerative clustering. A glomerative clustering. I'm not going to spell it. It's a long word, a glomerotive. You can look it up in the show notes. A glomerative clustering is a type of hierarchical clustering. It's a different, it's a totally different style of clustering algorithm. A glomerative clustering is a type of hierarchical clustering. A glomerative clustering is a type of hierarchical clustering. A glomerative clustering is a type of hierarchical clustering. A glomerative clustering is a type of hierarchical clustering. A glomerative clustering is a type of hierarchical clustering. A glomerative clustering is a type of hierarchical clustering.
 the way K-means works. These hierarchical clustering algorithms, there's all sorts of different hierarchical clustering algorithms, and there are a lot of other clustering algorithms that aren't hierarchical necessarily as well. A few out there that I've seen commonly used are spectral clustering, mean shift, affinity propagation, and some other clustering algorithms that I won't discuss in this episode. I won't discuss the three that I just mentioned, but hierarchical clustering
 works totally different than the way K-means works. And hierarchical clustering algorithms are more friendly to using different similarity metrics under the hood for clustering your data. In our case, like I said, we want cosine, K-means doesn't use cosine. In fact, it doesn't even support cosine. And so in Nothi, we are using a type of hierarchical clustering algorithm called a glomerative clustering. I won't talk about how a glomerative clustering is
 of clustering works mostly because I actually don't understand it that well. It just so happens to really work well for me in practice. And I've tried a handful of other hierarchical clustering algorithms. So, Glamourd have worked the best for me. So it's the one I'm going to go with. But I will say technically that the way it allows you to use the cosine metric is that it allows you to pass in a metric by name. So if you're to say scikit-learned clustering to agglomerative parentheses and number of
 clusters. You always got to say number of clusters in the site, learn clustering packages. So if we have a thousand entries, maybe 10 clusters sounds good, comma, and then metric equals, and then you can specify word, one which I believe supported is the word cosine. I may or may not be wrong about that. I'm not actually using that. I'm using a different one. The metric string that I'm using is called pre computed, and what that does is that it
 expects you not only to pass in the data, the rows, that you're going to be clustering, but also another matrix. And that matrix is the distance of every row to every other row. Now because we're comparing every row to every other row, what we end up having is what we call a square matrix. A square matrix is the same number of rows by columns.
 all of B. So there's a lot of repeated data obviously. But that's what these hierarchical algorithms which allow you to pass in a pre-computed distance matrix, expect is a matrix, not a list or a vector, m by m matrix, m being the number of rows in your data set. So you pre-compute the cosine similarity of every entry embedding by every other entry embedding. And what you get back is a-
 a square matrix of all the cosine similarities, and you pass that into the agglomerative clustering constructor function, and then you call dot fit transform on your entry embeddings. And what you get back is, again, a list of labels, the list length being the length of your list of entries, and the label values being any number in the range of the number of clusters that you specified, be 10 in this example. How do I compute the square matrix of the cosine distances from every investment performance?
 entry embedding to every other entry embedding. You'll just have to go see that in the code. I'm using PyTorch to normalize all the vectors first and then compute the dot products, right? I mentioned that in the similarity metrics episode. The cosine similarity function can be approximated as the normalized dot products of two vectors. And the reason I do it that way, as PyTorch norm then dot product is that it's actually really fast. You can do this on the GPU.
 normalization and dot product are both too extremely fast very first-class implementation obvious linear algebra on a GPU kind of stuff. Whereas if you were to use one of the scikit-learn cosine distance functions like the p-dist or c-dist methods it would be performed on the CPU and it would be slower. Okay so that's the k-means algorithm which uses Euclidean distance under the hood compared to the agglomerative clustering algorithm which is
 a type of hierarchical clustering algorithms, which allows you to specify whatever distance metric you want. And in our case, we're using the cosine similarity metric. We don't just tell it to use the cosine similarity metric for technical reasons using the string metric equals cosine. Instead, we pre-compute the cosine similarity into a square matrix of a by b cosine distances. And we do that by calculating the norm of every vector and they're...
 dot products against each other. Normed dot product is effectively cosine. Next, we're gonna talk about DB scan and HDB scan, but before we do, I want to talk about finding the number of clusters to use in your clustering algorithm, because like I said, both K means and a glomerate of clustering expect you to pass in and underscore clusters parameter, being the number of clusters you wanna cluster your large data set down in.
 But how do you know in advance? You might know, you might actually know that there are three topics on this website. Three news topics, politics, sports, and beauty, right? Those are the only three topics we talk about on this website. So no matter how many news articles we publish, if we're going to be using a clustering algorithm, we're going to cluster down to three and clusters equal three. But it's very rarely the case that you know in advance the number of clusters you need for this clustering application so there are automatic...
 ways for finding the number of clusters. The first way is through what we call the elbow approach. Now I can't show you this, we're in podcast format, it's a very visual thing, but what happens is there's a point at which you start getting diminishing returns in what we call inertia. We'll talk about that in a bit and you can graph that on a graph and you'll see that point. It's very crystal clear. What happens is you try every number of clusters up until you start getting diminishing returns. We try one cl-
 thousand rows, let's try clustering this into one. Okay, we get back what's called inertia. And we plot that point on a graph. And then we say, how about two clusters? We cluster those thousand rows into two clusters and then we get back the inertia. We put that on a graph and we keep going three clusters, four clusters and so on. And what the graph ends up looking like is it's like this really steep fall and then like a elbow, a point where there's just like an angle, it really stops falling and then it kind of level.
 out and goes to the right much more level. Still decreasing, but much less rapidly. We call that the elbow. And that's the point at which you decide that's your optimal number of clusters. Now you could do that by actually creating a graph. And most of these tutorials teach you that approach. Literally iterate through 1, 2, 40 or half the number of rows in your data set. And generate clusters for that number of clusters.
 get the inertia back, point it on the graph, make the graph, look at the graph. I ball the elbow, what seems to be a good elbow, and then manually you, the programmer, choose that to be your end clusters. I don't like that approach. I don't like the human sort of being in the mix here in that way. I believe in visualization of your data sets and distributions and making judgment calls on how to normalize your data and stuff like this, but I don't believe that the human should be in the mix in this particular way. I think this should be automated and there is a path.
 package out there called need, K-N-E-E-D, like knee instead of elbow, because if the kink in a graph is pointing concave up, we call that an elbow, and if it's concave down, we call that a knee. So they decided to go with knee instead of elbow, maybe elbow was already used as a package. Knee with a D, K-N-E-E-D. And what the kneeed package does is allows you to just automatically find the elbow or the knee. A handful of hyper...
 parameters and which direction are we supposed to be pointing concave up convex down blah blah blah you specify those things and Then you give it the graph that would have otherwise been generated in visual output You just give it those points and it finds the knee for you automatically very handy utility So need dot knee locator parentheses your points and some hyper parameters and it will find that elbow for you But those points on the graph inertia inertia let's discuss this
 a little bit. The inertia of the K-means model is the intra-cluster distance between clusters. It's the distance between clusters, between clouds of dots. Okay? So what you do is you instantiate the K-means model with a number of clusters that you specify, two, for example. And then you fit, transform it on your rows. And it remembers, it's a variable now on the K-means model. It's a public variable you can access.
 called inertia, it is the distance between the two clusters, the two clouds of dots, the mean squared distance between these two dots. What would those distance be? Euclidean distance, okay? Little trivia from our distance metrics episode. So it's the mean distance between clusters. You add a third cluster, now it's distances between all three clusters from each other. So that's inertia and it's stored on the instantiated k-means model, which is a variable in your Python code. You can just access it as model dot inertia
 It's just a number and you just throw it into a list and then you can pop that into your into your knee locator code, which will then find the elbow for you. But there's an alternative method of finding the optimal number of clusters that's not inertia. It's called the silhouette score, silhouette score. It takes into consideration both inter and intra cluster distances. So it takes into consideration the distance of points in a cluster to their cluster centroid. So it takes into consideration the distance of points in a cluster to their cluster.
 and the distance from cluster to cluster. So it's a little bit more informed. It has a little bit more information packed into what it provides you back and in addition, it does something extra, which is that the maximum silhouette score is the winner, is the optimal number of clusters to choose, which is better than using this knee locator approach because finding the elbow of a graph
 can be a little bit fuzzy. And there are hyperparameters that you have to pass into this knee locator algorithm so that you find the right elbow, given the types of graphs that you're getting back. In particular, there's this one hyperparameter called S, which is the sensitivity, how sensitive are you to these elbows? If you have a too smooth of a graph, we need to up the sensitivity so that it's a little bit more hardcore. And if it's a very jagged graph, et cetera.
 So finding the elbow of a graph is usually not all very cut and dry. It can be subjective and it can be fuzzy. And it might elude automated techniques like the knee locator, whereas the silo-S score has two benefits over using inertia for finding the optimal number of clusters for k-means or a glomerative clustering. The first benefit is that the information that is packed into this process namely inter-and-intra-cluster distances.
 is more informative when it comes to deciding the optimal number of clusters. You have more information to help you make a decision on the optimal number of clusters. And the second benefit is that you can simply select the silhouette score that is maximum. As that represents your optimal number of clusters. And there's no complicated fuzzy knee locator logic you have to do to find the elbow. So nine times out of ten, you see people prefer to use this...
 silhouette score and there's a psychic learn silhouette score function that just makes this whole process super easy for you and there's one third Huge benefit is that it allows you to pass in a Pre-computed square matrix of distances So you can use the silhouette score with cosine distances for deciding the number of optimal clusters in a Glamour of clustering. So let's take a little step back. K means model. You
 is Euclidean distance under the hood. That has problems and high dimension. It has problems because you can't use cosine. So in NLP applications, K-means isn't the best algorithm to use here. In most other applications, K-mean is the default and it is a great clustering algorithm to use. But specifically in this case, document embeddings and cosine similarities, high dimension, and we prefer cosine over Euclidean. So, can't really use K-means and it is the K-means model that generates this inertia?
 score when you run the model on some number of clusters. So we don't even have the inertia score. We can't even use the inertia score. It's not available to us. We're using the agglomerative clustering algorithm, which does not come with an inertia score. So what do we do? We run the agglomerative clusterer for every number of clusters from some minimum to a maximum. And then we actually compute the silhouette score for the same square matrix.
 of cosine distances between documents that we used as the data to train the agglomerate of clustering algorithm on, we used that as the input for the pre-computed distances for the silhouette score. So the silhouette score method takes the labels that are output from the clustering and the distances themselves being in this case the pre-computed matrix cosine similarities. So the silhouette score is fantastic, the phenomenal it is preferred in most
 cases from what I understand for finding the optimal number of clusters, either for k-means or for a glomerative clustering, hierarchical clustering, whatever you have you. And so for most applications, what I recommend is you go k-means and you go silhouette score. And take the max silhouette score, that's your number of clusters. That's how you find the optimal number of clusters. K-means from scikit-learn, if your data is small to medium, k-means from feists, if your data is large, and you will use the silhouette score.
 to find the optimal number of clusters for your k-means. That's most applications, but we're not talking about most applications here. We're talking about natural language processing. So in our case, we are using the Glamourative Clustering Algorithm, which works well with pre-computed squared distance matrices, whatever that may be. In our case, it's cosine. And then you can use the silhouette score on the Glamourative Clustering Output in combination with the pre-computed cosine matrix. So k-means and a-
 of course, there's so many algorithms out there, and there are different popular ones used by different circles, but in my opinion, I like those two clustering algorithms the best. And finally now, we'll talk about DB scan and HDB scan. DB scan is another type of hierarchical clustering algorithm, similar to a glomerative clustering being a type of hierarchical clustering algorithm. So DB scan is another hierarchical clustering, and in fact, it seems to be, and this is just my observed.
 A lot more popular on the internet just used a lot more commonly in various applications. Even it seems to be the case that a lot of the community is moving away from k-means and towards DB scan. It seems that DB, whatever DB scan has as part of its special sauce under the hood offers a lot of versatility that lends itself to multiple applications and sort of auto-learns some aspects as part of the process. One aspect being the number of clusters.
 So, big benefit, if you use DB scan, you don't need to specify the number of clusters. It learns it. It finds the optimal number of clusters for you on its own. That is enormous. That's a huge reason to choose DB scan over either a glomerate of clustering or K means. Now DB scan is an algorithm. It's a concept and there's a psychic learn implementation.
 called, scikit-learn, clustering, DB scan, probably. Well, there's a more popular implementation out there called HDB scan, and it's just a non-psychic learn implementation. And it's just more popular, it's more powerful, it has a lot of bells and whistles, it does a lot more for you out of the box. A lot of great documentation, a huge community behind it. So if you're going to use DB scan, I recommend using the HDB scan package instead, that is not...
 part of the sci-kate learn package. And yeah, so like I said, one of the big benefits of DB scanner, HTTP scans that automatically learns the number of clusters optimal for your clustering situation. And another big benefit is that it's sort of, there's a lot of automaticness to it. It sort of does some stuff under the hood that I don't understand. I haven't spent much time with it. That it lends itself generally to various specific use cases where you would specifically be
 using a specialized algorithm. So me, personally in Nothe, I am specifically using a glomerative clustering because it specifically lends well to the pre-computed cosine similarity matrix being passed in and working off that. And I find it to work very well. Whereas the K means algorithm would be used over here for some other stuff, stuff that's you clit in and not language-ish. Well, HDB scan from what I understand, one of its great strengths is that it's...
 a little bit more general and multiply applicable. You can use it across the board under various circumstances and it can handle multiple of these very circumstances very well. I would recommend exploring HDB scan if you're doing clustering in your application and seeing how well it works for you. It's very popular and people swear by it. People love HDB scan. Me personally, I tried it in Nothe for the themes feature and for clustering your journal entries before...
 running those through the semantic similarity search to books. It didn't work at all. For me, I fiddled with it for hours and hours. It never found any clusters. It was always negative one, which means it didn't find any. The only cluster it found was negative one, which means it was an outlier. I don't know if it's just the data I'm using, you know, the document embeddings, these 768-dimension vectors. I don't know if I need to normalize those in advance or
 or if that's too high of dimensions of vectors, as far as I understand, another claim of HDB scan is, it doesn't matter the dimension of vectors you're using. I could be wrong about that one, but it's supposed to be one of these, like, catch all clustering algorithms that can basically handle anything. But it was not able to handle my data, it was not able to handle these document embeddings, whatsoever, neither DB scan from Cykit, learn nor HDB scan the standalone package, with all the fiddling of hyper parameters, and...
 dimensionality reducing my vectors and all these things I couldn't get it working whatsoever So I just moved on I had a glomerative clustering was working just fine for me so I stuck to my guns So I'm not using it, but I do recommend you try it because people swear by it So K means is kind of the the default HDB scan is the catch all and a Glomerative clustering is nice because it allows you to do cosine and any other Pre-computed metric you might be using that is not Euclidean based. That's it for today Next
 next time we'll talk about Docker actually, a little change in pace from the types of stuff we've been talking about lately. See you then.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 12, shall we learning algorithms part one. In this episode I'm going to discuss.
 various shallow learning algorithms, shallow learning. So remember in a previous episode when we talked about deep learning, we talked about using neural networks, which is kind of the quintessential deep learning concept. As sort of a silver bullet approach, you can use neural networks for classification, for regression, you can use them for linear situations where the features don't need to combine, and you can use them for non-linear situations where features may need to combine in some special way. Maybe the system will learn x-
 three squared plus x2 times x1. So that's feature learning. And the other piece of deep learning is the hierarchical representation of the data it's learning. So breaking down a face into its subparts, eyes becomes lines and angles and those become pixels. But I also said that while we may treat deep learning as sort of a silver bullet, it isn't necessarily so. And in fact, if you talk to a machine learning expert in the field, it will very much aggravate them to see the level two which new machine learning engineers are treating.
 deep learning as a silver bullet. There's two problems with thinking of deep learning as a silver bullet. One is that it certainly is not a silver bullet. You can't use neural networks for everything. There are still many situations for which deep learning cannot be applied. And two, if you have a problem that fits the bill of a shallow learning algorithm just so, then you're going to save immensely on time and resources. Deep learning is extraordinarily, computationally expensive. So always using neural networks to solve.
 All of everything means time and money when something could have been used in your problem for much cheaper. So having a basic knowledge of the fundamental shallow learning machine learning algorithms most commonly used in the space is very essential. When you embark on your machine learning journey, learning machine learning, you're going to be learning these shallow learning approaches first to understand a baseline of how machine learning works before you dive into the deep end of deep learning. And like I said in a prior episode.
 So a lot of shallow learning algorithms can be used as a neuron in a deep learning architecture, not all of them, but some of them can be. So we are going to discuss the high level overview of various shallow learning algorithms, some of the most common ones that I personally see. We're going to do so because having an understanding of all these different approaches is essential for your development as a machine learning engineer, both in being able to apply the right.
 tool to the right job, especially in situations where deep learning cannot be used, and also so that you'll be equipped with more computationally efficient algorithms for situations where a particular algorithm fits the bill just so. Now I want to warn you, there are hundreds of machine learning algorithms out there. Hundreds. Machine learning for the longest time was this space of exploration of mathematical equations, statistical formulae. I'm going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to be going to discover which algorithm is going to be going to be going to be going to discover which algorithm is going to be going to be going to discover which algorithm is going to be going to be going to discover which algorithm is going to be going to be going to discover which algorithm is going to be going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to discover which algorithm is going to be going to
 algorithm would fit what situation? If your problem was probabilistic, you might use something like Bayesian inference. If you were data mining, association rules out of a market basket approach, you might use something like a priori. We're gonna get into all these in a bit. There are all sorts of different strokes for different folks, different algorithms for different situations, and so much so that almost every particular type of situation warranted a particular type of algorithm. This created the quest for the master algorithm. This is like the
 quest for the Grand Unification Theory in physics. I mentioned the master algorithm book in a prior episode of resources. It's all about that quest. The quest to find an algorithm that can handle any situation under the sun, or at least most situations under the sun. And that's where I think a lot of the hype that's surrounding deep learning is these days because neural networks can handle a surprising amount of situations. But for a while there, a particular algorithm was used for a particular situation. I think over time, a lot of these algorithms became a little bit more sane.
 Maybe one algorithm could handle multiple approaches at least within a domain. And it became increasingly obvious which situations were most common. So for example, in industry, you're gonna see recommender systems all over the place. Amazon recommending the next book or product for you to buy. Netflix recommending the next movie for you to watch. Google recommending an ad based on your search queries, clicks, and other things. Recommender systems, that's a very common situation. Another common situation is something I just mentioned called a market basket.
 It's determining if you buy this and that, what might you also buy? It's to suggest for you another product based on things that are currently in your shopping cart. So that's another common situation. So we've learned over the years in industry which situations are the most common and which algorithms can be applied to the situations. So of the hundreds and hundreds of machine learning algorithms out there, there are a handful of algorithms that you're going to see most commonly. And me personally, what I've seen most commonly I have...
 boil down into this and the next episode, I'm splitting this episode in half because it would be so long otherwise. Into the algorithms that I personally think are the most fundamental for you to learn. So in this episode, we're going to talk about K nearest neighbors, K means, a priori, principal component analysis, and decision trees. And in the next episode, we're going to talk about support vector machines, naive bays, and then a handful of other miscellaneous algorithms, like anomaly detection, rec-
 Commander Systems and Markov chains. Now, understanding these basic algorithms, like I said, will be fundamental for your success in machine learning and you will encounter so many other machine learning algorithms out there in the wild. But you'll quickly get a feel for which algorithms are most common and you'll also need to determine for whatever project you're working on, whether it's professionally or a hobby, which algorithm is the algorithm to use for your specific project? So that's gonna be a key point is, you are going to get a job in machine learning or you're gonna be working.
 on personal projects and machine learning. There are hundreds of machine learning algorithms out there. We can boil them down to the most used algorithms. And so what you're going to need to do is determine which algorithm best suits your specific project. So at the end of the next episode, and in the show notes, I'm going to post a link that's basically a decision tree for deciding which algorithm to use for your project based on various attributes about your project. And then you can dive into the details of that specific learning algorithm. Now as is the nature of my podcast, I'm not going to...
 go too deep into any of these machine learning algorithms. I'm going to do a very high level approach to how they work conceptually. So apologies for that. You'll need to dive into the details later on your own by way of the resources that I recommend to you per usual. Okay, let's start from the top. Let's remember how machine learning is hierarchically broken down. AI becomes machine learning machine learning splits into supervised unsupervised and reinforcement learning. The
 three subcategories of machine learning. There is a fourth category called semi-supervised. It's kind of a middle. You can think of it as like 3.5 categories of machine learning. I'm not going to really talk much about semi-supervised learning. But let's understand these three categories. We've talked about supervised learning in the past with linear and logistic regression. Actually, everything we've talked about in the past is supervised learning. Linear and logistic regression and neural networks. Those are all supervised. Neural networks can be used actually for unsupervised and...
 for reinforcements. That's another reason why neural networks are so popular is they can be applied in any space of machine learning. Supervised gets broken down into classification and regression. Remember classification is categorizing a thing as this, that or the other thing, cat dog or tree. And regression is coming up with a continuous value output, a number. So either a classification or a number. So I think we all understand supervised learning just dandy by now. So data is letting over the awake range. Create areas for fears, that are ever stored in a box for a novel and are ever stored in a box. Feed areas
 a spreadsheet with all of the labels in place. And it learns these theta parameters by looking at each row one at a time, making a guess, and figuring out how bad it did, and correcting that error. The algorithm is sort of supervising itself. I mean, you feed in the spreadsheet, the matrix of data, with the labels, and the model sort of consumes it, iterates over it, and trains itself. But we don't think of it that way, really. We think of it as we are supervising the training of the model.
 think of it like this. Think of it like training a dog to sit or to do some trick. And you've got your hands behind your back. In one hand you have a treat and in the other hand you have a newspaper and you say sit and the dog makes a guess. So it tries something, it stands up. And you spank it on the nose with the newspaper. So that was the error function telling it kind of it made a mistake. So the dog thinks in its head and it tries something else. And it tries rolling over and you spank it with a newspaper and you say bad dog. And so the dog stands.
 back up and it thinks and it thinks and then it sits and then you give it a treat and you say, good dog. So supervised learning is like supervising the training of your algorithm, just like training a dog with treats or a newspaper. Good dog, bad dog. Reinforcement learning skipping over unsupervised for now. Reinforcement learning is an interesting twist. We're not going to see reinforcement learning algorithms until much later in this series. And reinforcement learning is sort of the gateway to artificial intelligence. The way by which...
 you exit machine learning as a professional field and enter the world of artificial intelligence proper. The way reinforcement learning works is you give your model a goal. Okay, so you give the dog a goal. You want the dog to run through the woods and retrieve the fesent that you just shot down. But more than that, I like to think of reinforcement learning is even deeper than that. It's like putting a backpack on your dog, giving it a sword and a shield and saying you're going to find treasure at the...
 end of your quest. If you find yourself in a cave of sorrow, that is bad. Leave the cave. If you find yourself amongst the blue people, they are good. Stay with them. They'll give you points. So you give your model a system of positive and negative reinforcement and a purpose, which is maybe to find the treasure at the end of the maze. And it learns how to navigate the system. It learns the rules of the game. What actions it can take. It's kind of a coming of age.
 algorithm. You send it on its journey, you say go now for time is up the essence and you push the dog away It's got its backpack and it's sword and it's a neo fight right now It doesn't know left from right but eventually at the end of its journey It's strong and it's level 99 and it's figured everything out. It's figured out the rules of the game How to play the game pits are bad dog food is good and finally learns how to get to the treasure at the end That's reinforcement learning reinforcement learning is sending a model on its way giving it some positive and negative
 reinforcement guidelines and the algorithm learns the rest of the system on its own. Okay, so supervised, unsupervised, and reinforcement, so we skipped unsupervised. Unsupervised, I think of it as sort of the red-headed stepchild, the everything in between. Unsupervised, I mean, it has a very simple definition. It's that you are not supervising its learning process, whether by reinforcement rules in the case of reinforcement learning, nor by pre-labeled data in the case of supervised learning. The algorithm is left to its
 own devices to figure things out. And you're not telling it whether it did good or bad. You're just saying, okay, I'll take what you gave me. So for example, you give it a bunch of unlabeled data. Let's pretend that you have a bunch of humans, cats, and fish. And you hand them all to your unsupervised algorithm. And you say, hey, can you kind of sort these out for me? You as the human don't know. Let's pretend that you don't know the difference between a human of fish and a cat. And the machine doesn't know either. You didn't give it labels.
 to come with the data. You just give it a bunch of stuff. And you're like, can you maybe sift these into three piles? I don't know. You figure it out. I'm sure you'll do just fine. And so the algorithm kind of scratches its chin and it looks at this pile of objects. K, they're not sorted and they're not in three piles and it needs to sift them out into three piles. So it does its best. And it's sure enough, you know, good algorithms sift these things out such that all the humans are in one pile, all the cats are in one pile and all the fish are in another pile. That's unsupervised learning. Because you didn't super.
 the algorithms training, it learned on its own, what characteristics separate things, and the output doesn't come with any labels. Okay, so supervised unsupervised and reinforcement learning. So, okay, so I think you understand supervised learning, maybe reinforcement learning sounds a little bit mysterious and interesting. We're not gonna get into that. Maybe unsupervised a little bit fuzzy for you. Well, I think you'll understand it a little bit more once we get into an example. But first, let's start with a supervised learning algorithm, okay?
 So again, we're going to be covering many algorithms in this and the next episode. Just a lay of the land of some of the most popular algorithms used in the machine learning world. We've already covered linear and logistic regression and neural networks, three supervised learning algorithms. So we're going to cover a handful of other algorithms. Linear and logistic regression are examples of shallow learning algorithms. Neural networks, of course, are deep learning algorithms and we're focusing on shallow learning algorithms.
 in this episode. So your next shallow learning algorithm is called K nearest neighbors. K n n. K nearest neighbors is an interesting algorithm in that it doesn't learn. So you may be thinking, okay, why are you putting this in an episode about machine learning if it's an algorithm that doesn't learn? Well, it's commonly used by machine learning engineers. K is commonly applied to machine learning situations. Imagine it is
 basically a brainless machine learning algorithm. That's all. It doesn't learn. It doesn't update theta parameters. There are no theta parameters in the system. And the reason that we're starting with this algorithm in this episode is it's the easiest to understand. In fact, it is the easiest machine learning algorithm to understand of all the machine learning algorithms in my opinion. Much more so than linear and logistic regression. So this will be a breath of fresh air. Canierist neighbors, how does it work? Imagine our example of here.
 humans, cats and fish. I'm gonna use that example for this episode. Imagine taking a giant pile of humans, cats and fish, and dropping them into a vet, into an aquarium, okay? And all the humans are kind of positioned in one cloud, one cluster over here, and then all the fish are in one cluster over here, okay? So we have x, y, and z axes. These are going to be features, of course, when you're working in space, in machine...
 learning, 1D or 2D or 3D, 3 dimensional space, 4D, 5D, etc. D or dimension is your features. So the position of a human in this aquarium would be maybe three features here. Let's say that our features here are number of legs, lives in the sea, and is a mammal. So based on those three features, we are going to position our animals in very...
 locations in our aquarium. Now our fish will all cluster around one area, our humans will all cluster around another area, and our cats will all cluster around another area. Now this is a very simple example with three features, three dimensions, and more complex examples we may be dealing with 50 features or even more, having any range of numerical values whose values may be a little bit less obvious. The way K nearest neighbors works is you drop a
 new object into the aquarium. Plop, you drop in a cat and the cat gravitates automatically to the cluster of cats. Simple as that. K nearest neighbor means when you add a new object into the that, when you add a new object into your model, it figures out which group of things it has the most in common with. So of all my neighbors, which ones am I nearest to based on my features, my number of legs, MIMM.
 and do I live in the C? K in K nearest neighbors means the number of classes. You're actually going to see this a lot in machine learning. K representing the number of classes. Think of it like saying class with a K. I don't know why it's actually K. But so what the algorithm does is you specify up front how many classes are in the system. In our case three we have cats humans and fish so three classes you put all your data into place pre labeled from the
 spreadsheet. So the last column of your spreadsheet is the label, whether it's cat, human, or fish, all of them come pre-labeled and so everybody's in place in three-dimensional space. And then you add a new input to the system and it figures out what class or cluster that input has the most in common with based on its features. Who of all its neighbors is it nearest to? So I said this is not a learning algorithm. This is what's called an instance-based algorithm. The
 way it works is you have all of your rows in memory. And when you input a new row into your model, it compares that input to every single row that exists in your training data in memory. So we loop over our spreadsheet every time we add a new row. It's looping over every row. We call those rows instances. So it's looping over every instance. Okay, so that's it. That's K nearest neighbor. Basically, I like to think of it as.
 magnets suspended in a vat is you have three clusters or K clusters all centered around a magnet and you drop a new object into the vat and it gets automatically pulled to the right magnet based on its features. So that's a supervised learning algorithm. The dead giveaway for whether something is a supervised learning algorithm is does it come with the labels? Did you give the algorithm a spreadsheet with labels as the last...
 column and indeed we did with K nearest neighbors. So that's K N N, that's a very simple, supervised learning algorithm. But it's going to segue us very effectively into your first unsupervised learning algorithm called K means. K means, K again being the number of classes. So first off unsupervised learning, like I said, is that you have your data, but it is not labeled. And your algorithm is going to try to figure something out about the data.
 It's gonna work with the data a bit, but it's gonna work with it on its own. It's not gonna try to label data, it's not gonna learn to predict a value or a categorization. Instead, it may restructure data or organize them in some way, okay? So, three types of unsupervised learning algorithms are called clustering, association, and dimensionality reduction. Let's start with clustering. So clustering looks very similar to K nearest neighbors. We have a that, a aquarium.
 with clusters of objects, okay? We've got our humans, our cats, and our fish, and they're kind of separated from each other in space based on their features. In K&N, we basically had magnets put into place in the middle of those clusters. We had a human magnet kind of put into the cluster of the humans, a fish magnet in between all the fish and a cat's magnet in between all the cats. And it doesn't really work that way under the hood. If you look at the algorithm, you may not understand what I'm trying to get at there, but kind of conceptually this.
 the way it works. You're kind of creating a magnet out of your cluster of objects, a cat magnet. In K means, you're learning the magnets. You are learning. It is machine learning. You're updating some parameters. You're learning where these magnets go. Now here's the thing. Like I said, in supervised learning, the dead giveaways always whether or not comes with labeled data in clustering. You don't have labeled data. Instead, you just have a pile of stuff. You just dump a box of...
 of humans cats and fish into the aquarium. We don't know if they're humans cats or fish. The machine doesn't know if they're humans cats or fish. It's just stuff. It's just a bunch of data points. It's dots, okay? As far as you and I are concerned and the model in the computer, it's just a bunch of dots in the aquarium. And what the model's trying to do is figure out what are some sensible clusters of all those dots. How can we partition them in a certain way? It seems...
 that there's a sphere of dots over here, and a sphere of dots over here, and another one over there, it seems like there's space between them. They're not touching each other. There's not overlap. Or if there is overlap, it still seems like this is a sphere, and that's a sphere, kind of like a venn diagram that's overlapping in the middle. It still seems to me that there's something, there's like a difference between this cloud and that cloud. I don't know what, I don't know what I'm gonna call them, but I'm gonna learn the line that separates.
 them. That's what clustering does, that subcategory in unsupervised learning. And k means is one of the most popular specific algorithms used in clustering. So k means, k means is interesting. It's very similar to knn, k nearest neighbors, all mean already, you're probably trying to figure out what could possibly be the difference the way I'm describing it. The difference is that it learns where these magnets go. These magnets are called centroids, centroid.
 And once again, you specify up front how many you need. So if I have a bunch of dots and I know that I need three classes out of this, then you specify three, k equals three. You dump all your dots into the vat. And so here's conceptually how I think of the algorithm. Imagine you're playing some sports game. With a bunch of kids, you drop all the kids onto the field. And then you drop three team leaders onto the field. Randomly, you just drop them from the air and they land onto the field. Different.
 They practically look around and they try to figure out who their teams are. You tell them up front, there are three teams and I'm pretty sure that, I mean, there's an answer to this equation. I don't really know as the human, but I'm quite sure that there's three kind of real concrete teams here. You've got to figure them out. So your team leaders look around and they all scatter. They run in some direction. They run in the direction that it looks like to them is a cluster of people, a separate
 cluster of players. And so they stop and all the players realize, okay, this is my team captain. So they assign themselves to that team captain. The players don't move. These dots do not move in the that. But the team captains do. And then the team captains all kind of look around again and they reassess and they're like, is that right? And then no, no, so they all scatter again. They run a little bit closer and they do this over and over and over. And every time they do it, all of the team members sort of keep in their mind. They're like, okay, I'm that's my captain. That's my captain until eventually.
 All the team leaders have figured out what really is the natural clustering of these teams. I didn't really explain it very well. I don't have a very good way of explaining this, but that's how it works. Basically, you drop all these examples into a vat. They each have features, but they don't have a label. So that last column of your spreadsheet is non-existent. The features are the dimensions. Those dimensions determine where in space these objects are. And then...
 you drop three magnets into the vat. And those magnets figure out on their own how to position themselves such that they're in a position that separates the three clouds of dots most effectively. And then of course the purpose of any machine learning algorithm is for making future predictions. So those magnets are now in place so that when you drop a future example into the vat, it automatically gets assigned to the right cluster. k mean.
 k being the number of classes. Now you'll note if we dumped a whole bunch of data into a vat with three natural classes of humans, fish, and cats, and we then said that there are two classes, k equals two, then it might not learn a very effective delineation, or if we said k equals four. There may be sort of a natural segregation of data in the population that is not a parent
 to us, nor is it apparent to the machine, and giving it the wrong number of classes means sort of dooming the algorithm. So with k means and clustering in general, you sort of have to have an intuition about how many classes might kind of exist in the system. There's a good example in a book I'm going to recommend at the end of this episode called Machine Learning with R, where the author takes a bunch of social network data for a bunch of high school students.
 Okay, so things they might say on their Facebook wall or things that they like or dislike various interests And it naturally categorizes these students into five categories now As is the case and unsupervised learning it doesn't give you any labels But if you look at the way it categorizes these things you can totally tell what it's doing It has the jocks the princesses the nerds the criminals and the basket cases So these are kind of five
 natural high school stereotypes that we kind of think of as natural in our minds and it turns out the machine learning algorithm Discovers with ease is actually the case as long as you tell it that there are five stereotypes If you made it four then it might remove that last stereotype I mean that was a little bit of a fuzzy one anyway the basket case Do I don't know what that is if you made it six then it might come up with a new class of people and maybe it might dilute our stereotypes a little bit spread them thin
 make them make a little bit less sense. So having a natural sort of understanding of the problems that you're working with in order to determine the number of classes in the system, the number K in the system is kind of important. But there are actually algorithms that can naturally learn the best K for you. There are some algorithms out there that will help you to determine which K is the best for your situation. So I'm gonna leave it to, of course, as usual, the Andrew E. Coursera course to teach you with this video and see how the class follows tips.
 of k means and clustering. Okay, so we covered k and n, a supervised learning algorithm for sort of finding what object fits where. With a label, pre-labeled data, makes it a supervised learning algorithm, and in putting a new object and getting a labeled back, is also part of the supervised piece of it being a supervised learning algorithm. Now we delved into the world of unsupervised learning, the second of the triumvirate of machine learning algorithms. We broke that down into cluster.
 and dimensionality reduction, and there are other subfields of unsupervised learning. But clustering, I think, is probably what you'll end up seeing the most common in the world of unsupervised learning. And we discussed a specific algorithm of clustering called K means, very similar to KNN. Okay, so let's enter a new subcategory of unsupervised learning called association, or association rules. Again, you don't give the machine learning model...
 pre-labeled data so that last column of the spreadsheet with labels does not exist. Association rule learning as an interesting type of machine learning algorithms. Another name is sometimes a market basket analysis. The idea of association rule learning is you're going to learn if somebody buys this and that, what other things might they buy? So this obviously has massive value in e-commerce, in regular commerce. Okay. So I'm going to.
 example is if you are designing a store layout, if someone's gonna buy marshmallows and graham crackers, what might you also put near those two things? Obviously chocolate, Hershey's chocolate bars. So association learning is all about putting more stuff in the customer's basket. It's learning what commonly goes with what other things. If you have a set of some stuff, A, B, and C, what often goes with those? D, E, what if you-
 We only have a, does B go with A? So that's association rule learning, market basket analysis. Now here's a little bit of interesting info about association rule learning. It's often associated with data mining. Remember when we were talking about data science being broken down into various subfields. So one field of data science is data analysis and visualization, charts and graphs kind of stuff. Another field is machine learning, of course.
 So another field, which I didn't mention in a prior episode, is just database science, being really good at writing database queries, being a MySQL or Postgres expert. Okay, so that's a subfield of data science. And then of course, another subfield of data science is called data mining. It is mining information from the web or from some other data source case of scraping the web, maybe crawling the web, putting it into your database, piping it in through a data pipeline by way of Spark.
 or Hadoop. Remember those two technologies, Spark and Hadoop. Those are your data, pipeline, frameworks for very large amounts of data. Big data, they call it big data. And then finally, putting it all into your database and maybe coming up with some sort of conclusion. And that conclusion may be by way of association rule learning. Now association rule learning is clearly machine learning. We're learning to predict something based on a pattern that we
 recognized. But it's kind of also data mining. So this is where you start to see all the subfields of data science really, they really all get muddled together. It's really tough to tell them apart sometimes. There's a lot of overlap. Association rule learning is clearly a machine learning type of algorithm, but it's a lot of times categorized within the field of data mining. And I think it's because data mining sort of predates machine learning as a very practical
 field in industry where people were using association rule learning in e-commerce really early on. And so they just kind of categorized anything of machine learning that went along with the data that was super practical in the early days as simply part of the data mining process. So market basket analysis or association rule learning is learning what typically goes with what other things. And a very common algorithm used here is just called a priori. The odd priori.
 algorithm. If you know what that word means, you're probably like, a priori, what the hell? So a priori means like it comes before, where you know something already. So in philosophy, if you have some a priori knowledge before embarking on an argument, that piece of information is naturally part of the argument. So a priori knowledge is stuff that you already knew. And so the way a priori works in the market basket is like, so if I know that A is common, you're not going to be able to do that. So a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori, a priori
 And I know that B is common, and I know that C is common, okay? So that's marshmallows, graham crackers, and Hershey's chocolate. But I know that D is super uncommon, nail polish. Who knows? And I want to know what goes with A and B? Well, then a natural conclusion is C. Not because I know that C goes with A and B, but because I know that C is simply common and D is not. So it's kind of using these like you, you, you,
 build up a small database of common things, and then you group those together in twos, and figure out which of those is common, and then you group them in threes, and figure out which of those is common, and you sort of chop things off along the way to sort of reduce the problem set. So, association rule learning really works with big data most effectively, and so the more that you can reduce your data down in this process, the better. So that's the operarity algorithm. And finally, one last.
 subcategory of unsupervised learning I'm going to mention is called dimensionality reduction. Dimensionality reduction is very easy to understand conceptually. The idea is that if you have a lot of features, many, many, many, many, many features for all of your rows and your data for your machine learning algorithms, it is in your best interest to reduce the amount of features you have. And you can do that, actually, you can remove features.
 But what what dimensionality reduction does is doesn't just delete features it figures out what sorts of combinations of features can make new features how can you boil to into one and Keep doing that over and over until you've really slimmed down all your features to the minimum amount of features So the best example I've heard is students GPA the GPA is one feature that sort of represents how
 well the student did academically in college. Now there's lots of features that you could consider. You can consider their test scores, their homework assignments, their attendance. If you were trying to decide if you wanted to hire a student, for example, you could have them send you their transcripts with all the information that you could possibly gather from that university. Their test scores, their attendance, all these things, or you could just ask for their GPA. And the GPA is basically all those things, boil down.
 into one number. So that's what dimensionality reduction does. Dimensionality reduction algorithms figure out how to assess all the features that you've given it in order to boil those down into a smaller amount of features. And one of the most common algorithms here is called principle component analysis. And the name of the algorithm is basically, I mean it makes sense. If it's everything I just described, we're figuring out of all the components of a hundred component.
 which few are the principal ones, which of these are the most important. Or if all of them are important, how can we boil them all down into say five components, features, components equals features. So principal component analysis. And I'm not going to get into the nitty-gritty of the algorithm. Of course, it's an Andruine Coursera course. It looks a lot like linear regression in my mind. So this is another case where knowing the fundamentals of
 these basics and statistics, linear regression, logistic regression and stuff, they come into play elsewhere into machine learning. So that's why I taught you those first. Okay, so that's unsupervised learning. Unsupervised learning. I kind of think of unsupervised learning as your miscellaneous drawer that you might have in the kitchen. Okay, you have your silverware drawer and you have your towels drawer and then you have a drawer of a bunch of stuff that doesn't really fit anywhere. That kind of tends to be the unsupervised learning category.
 of machine learning. They're all very useful algorithms. Don't get me wrong. I'm not calling them junk. They just kind of, I don't see what keeps them all together. It's stuff that's not reinforcement, stuff that's not supervised. It's kind of all the rest. There, it's just a bunch of like utility algorithms. Unsupervised learning. So unsupervised learning is clustering, association rule learning, dimensionality reduction, and a handful of other stuff that I didn't mention. K means is a clustering algorithm. Op-priori is an associ-
 Rural Learning Algorithm and Principle Component Analysis or PCA is a Dimensionality Reduction Algorithm. You'll learn a lot of these from the Andrew Inkourse. And for the final algorithm of this episode, we're going to talk about decision trees. Decision trees are a very, very important algorithm to know. Very important. As I mentioned previously, machine learning engineers who have been in the space for a very long time are extremely skeptical and critical.
 of deep learning. They think that people are jumping the shark with deep learning, taking it too far, using it as a silver bullet where it shouldn't be used as a silver bullet. And a lot of times it's decision trees that they'll use as an example of something that performs just as well. They'll say, see, decision trees, which has been around the block since the 50s, work just as well as your neural networks. And when I say, work just as well, I'm talking about accuracy, performance, things that we're going to talk about in a future episode about performance evaluation.
 all those things. But what's special about decision trees, something that decision trees can do that deep learning cannot do is explain to the viewer what was learned, what was learned. Okay. So when you use a decision tree model to learn your data, the result is something you can read. It's on your computer. Visually you can read the steps taken to come up with the decision. So that's what makes decision trees particularly special. But
 They're also very high performance. They work very effectively. So what is a decision tree? A decision tree is exactly what it sounds like. It's just like when you come up with a decision tree in real life, you say, okay, if my mom arrives at three, then maybe we'll go out and do this. But if she arrives at five, we'll go eat, okay? If she's not hungry, then we'll go drinks first. But if she is hungry, then we've got three restaurants to choose from. If it's raining, we're gonna go to this restaurant, blah, blah, blah. So basically it's a whole...
 bunch of if else statements in a hierarchical structure. Now they call them decision trees but they always draw them from the top. So imagine if you will a circle at the top that branches downward left and right into two circles and each of those branch downward left and right into two circles. Now with a decision tree learning algorithm will learn is these nodes and their branches. It will learn what things to check first and what things to check.
 and what things to check, third, et cetera. It will learn what goes at the very top. What is the first thing that I should try? When we're talking about our fish, humans, and cats scenario, what is the first thing you could check to rule out the majority of data? Remember, we have three features. Number of legs is a mammal and lives in the sea. Well, humans and cats are mammals. So if our root node checked is a mammal, then we would have to break that down even.
 more based on the number of legs, for example. But if we first checked number of legs or lives in the sea, either of those features would tell us right away that it's either a fish or something else. And then, if it's something else, we could break it down by number of legs. So the top node of our decision tree of our learned decision tree would be lives in the sea question mark. If yes, it's a fish.
 If no, then you go down a new branch and then that branch asks the question number of legs. If two, then it's a human, if four, then it's a cat. So that's a decision tree. Very simple. Our model will learn the optimal placement of questions and number of branches in this tree. And then like I said before, the nice part is when it's done, it will present to you visually a tree.
 and you can actually look at it and read it as the engineer, which is not something you could do with almost any other algorithm under the sun. So if a customer comes into you and you have a linear regression or logistic regression, learning algorithm that is meant to learn whether or not somebody should be eligible for a bank loan. Okay, you are a banker, a financier, and a customer comes in and asks, can I please have a bank loan? They fill out an application and all the questions of the application are the features
 that go into the learning model. Things like, have you ever defaulted on a loan? Have you ever been bankrupt? What's your age? What do you do professionally, et cetera? Well, if you pipe that into logistic regression or a neural network, then it'll spit out the answer, no, ineligible for bank loan. And you'll turn around in your swively chair and you'll say, I'm sorry, sir, you were ineligible for the bank loan. And they'll say, what? Why? And you'll turn around, you'll look at your computer, and it'll be a whole bunch of green, ones and zeros, trickling down a black screen like the matrix.
 and you're like, uh, and you turn off your monitor and you turn to your customer and say, I don't know, but you are ineligible and they storm out of there in a gruff. You cannot visually interpret most machine learning algorithms. They're just numbers. But if it was a decision tree, the customer would come in and say, can I have a bank loan and you pipe in the application, punch it into your decision tree algorithm, and it spits out a visual tree for you to take your finger, or you point at the top of your local careless...
 see, so I said no, and you go down to the right and you go down to the left and you go down to the right and you say, ah, this is where you failed, sir, it's because you've defaulted on a previous loan. I'm so sorry. And so he says, okay, I understand and he walks out. It just so happens that in the financial industry, you are required to tell your customers why they are ineligible for a bank loan. So in this particular case that I used, a decision tree is legally required. So for many applications and...
 learning decision trees are very good. They're very readable, they're very obvious, they're very easy to implement by way of any sort of machine learning library out there, like scikit learn or any of the R packages. You can actually use decision trees not only for classification like I just used in this example, but also for regression for actual numerical outputs. It's a little bit tough to explain, but you call a classifying decision tree, a classification tree and a numerical output tree you call a regression tree.
 naturally. Now the algorithms I'm not going to explain how they work. The actual learning algorithms, the process of learning the tree is figuring out what questions to ask first, second, third, and so on. The algorithm for learning those things, there's one by the name of ID3, one is c4.5, another is c5.0, another is cart. Okay, so they got these funky names. I don't think you really need to understand how the algorithms work even when you're
 learning machine learning, you kind of can just pipe them in with your machine learning libraries or packages in R or psychic learn, whatever you're using. And I'm throwing a bunch of words at you. I just, I want to prepare you for it. You're going to see when you're learning the details so that you're not overwhelmed with a bunch of words that you see in the wild, okay? When you're starting to explore the world of machine learning and you see millions and millions of words, I want to just tell you what they are, even without getting into the details, just so you know kind of where everything falls into place. Along those lines.
 there's two kind of popular spins off of decision trees. The vanilla decision tree is just like I described to you. It has a problem with it called overfitting, which is something I'm gonna get into in a future episode. Overfitting. And the way you alleviate this problem is by way of making many, many, many, many trees, what's called a forest, or a random forest sometimes, you figure out the average best amongst these
 trees. So who did best of all you with all you trees? Who are the best trees amongst you? And figure out what they all have in common in order to construct the ultimate tree. Okay, so that's a random forest approach. You make a bunch of trees, boil them down into one. And something very similar is called gradient boosting. So I don't I don't really understand how gradient boosting works. I think they cut off the tree at a certain point at an early stage in the trees development.
 and then they kind of do the same thing as random forests, don't quote me on it. But basically I just wanted to heads up you to those two words. You're gonna see random forests and gradient boosting very commonly thrown around all over the internet. They're basically optimizations on decision trees. Okay, how about that whirlwind tour? This is only part one of a two part series on a whole tour of all these algorithms. Part two is going to cover support vector machines.
 naive bays, anomaly detection, recommender systems, and Markov chains. And again, this may seem very overwhelming. I just want to give you a lay of the land of many of the popular machine learning algorithms out there. There's hundreds and hundreds of machine learning algorithms. Every algorithm can be applied to a different setting. We're getting closer to the master algorithm, potentially by way of deep learning and neural networks. But it is in your best interest.
 to at least have an appreciation and understanding of the most popular shallow learning algorithms so that you can both save time and money when it comes to computational resources and for situations for which neural networks are not suited at present. And there are many such situations, there are many situations where shallow learning algorithms are your best bet. What I mean by that is we're talking apples to oranges. It's not a performance comparison, it's just something that neural networks may or may not be able to handle. So...
 It is in your interest to get at least a lay of the land of the shallow learning algorithms And I would recommend trying to understand the details of them offline by way of the resources So let's talk about the resources. I'm going to post an article called a tour of machine learning algorithms It's basically just like this episode. It's by machine learning mastery.com Which is a website that I've recommended before he puts out very good stuff very very similar to this podcast But an article format it'll be a visual representation of every
 thing that I'm discussing in this episode. So it'll give you something to visually latch on to. Since I know that audio can be a little bit too much. There is an image online put out by Psychit Learn. Remember Psychit Learn is the Python library of shallow learning algorithms. Basically everything that you're going to hear about in this and the next episode are going to be things that you can use by way of Psychit Learn. These are not algorithms that are present in TensorFlow. TensorFlow is built primarily for De PL-
 learning. You can build your own shallow learning algorithms and tensor flow. If you understand how the algorithms work, you just use the low level tensor flow math libraries. But I wouldn't recommend it. I'd recommend sticking to scikit learn for the shallow learning algorithms and then moving on to tensor flow for the deep learning algorithms. So scikit learn put out an image that is a decision tree for which algorithm to use where how about that decision trees, very meta. So it's a decision.
 entry for deciding which algorithm of the world of machine learning algorithms you should use given your predicament. Okay, am I working with text? Okay, go this way. How many training samples do I have? Okay, go that way. Very, very handy. So look at that. Look at that before we get into the next episode, because in the next episode we're going to do a couple more algorithms. And then I'm basically just going to copy and paste the resources section in the next episode. These are the same resources that are going to be useful for the next episode. The next resource.
 going to recommend is called Machine Learning with R. This is a Tyler recommendation. Most of my recommendations are sort of averages from around the internet. Things that I see people recommending over and over and over kind of the de facto resources. This is a resource. Obviously I picked it up somewhere so somebody recommended it somewhere but I've never seen it recommended otherwise and I loved this book. Machine Learning with R. It's a fantastic explanation of the machine learning concepts in a way that I thought was
 lacking sometimes in other courses or books. So he does a very good job at thoroughly explaining the details of machine learning concepts. Additionally, he covers a lot of algorithms that Andrew Eing does not cover, but which are essential. So for example, naive bays, market basket analysis and our priori, association rule learning, and canyorse neighbors. And he also covers decision trees, which is not covered by Andrew Eing. Decision trees are one of those things that you're gonna see in almost any other machine learning resource you read. So it'll all.
 teach you are along the way, which Python is the main language that you're going to want to be using in your machine learning journey. But like I said in a prior episode, R and Java are two very strong run reps that might be handy to have in your tool belt. Okay, now I'm going to mention two textbooks. They're very heavy and they're very commonly recommended. They are there to help you understand how machine learning works at a fundamental level. And they cover a lot of these algorithms that I've mentioned in this.
 next episode. Remember my analogy of the machine learning process to cooking? I said that linear algebra is kind of like chalkmating things up and statistics is your cookbook, your recipe book, and then putting it in the oven is calculus. That's the learning process. Well, most of the resources that I've been giving you up and to this point are basically cookbooks, like a list of recipes to use in your machine learning endeavors. These textbooks that I'm going to recommend to you are for learning the theory of food.
 It's basically learning why the recipes are what they are. If you have some understanding of statistics, how can you glue a bunch of concepts together in order to form a machine learning recipe? And these, so these textbooks, one is called elements of statistical learning. And another is called pattern recognition and machine learning. So I recommend these textbooks eventually. You should eventually read these textbooks. They're very, very commonly...
 recommended and essential for you to have a thorough understanding of how machine learning works at a fundamental level. I don't think I would necessarily recommend them yet. Now, I try to tie my resource recommendations to the topic, the episode topic, even if you're not necessarily ready for those resources at that point in time. So these are kind of, these are textbooks that I would recommend you come back to when you really want to have a more thorough understanding.
 about how machine learning works at a fundamental level. And they will cover a lot of these machine learning algorithms that I've mentioned, but they'll go even deeper. They'll help you understand not just what algorithms to use under which circumstances and why, but how did we even come to that conclusion? How did we come up with a new machine learning algorithm for this situation? How do we come up with these recipes? Learn how to invent recipes so that you're not confounded by the recipe book. It's not.
 just memorization. So again, these are books that I would recommend you return to in the future once you have a more solid understanding and foundation in machine learning. Elements of statistical learning and pattern recognition and machine learning. Two textbooks. Okay, that's it for this episode and I'll see you in part two.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 13. Shallow Learning Algorithms part two support vector machines and naive.
 in this episode I'm going to be talking about support vector machines and the naive base classifier. These are two very powerful machine learning techniques shallow learning algorithms. These are kind of those power machine learning algorithms power tools. I consider decision trees support vector machines and naive bays. I consider them all three to be sort of power tools. A lot of the other shallow learning algorithms that you'll learn are sort of dedicated to particular tasks or even if they're multi-purpose they shine under specific
 circumstances, but decision trees support vector machines and naive bays are sort of these power tools that could be applied across a very wide spectrum of machine learning applications. They're primarily built for classification. All three of these algorithms are primarily built for classification, but can be used for regression. Before we get too far into this episode, I want to talk about the fact that we're talking about so many machine learning algorithms. In the last episode, I dropped a bunch on you. In this episode, we're going to be talking about two more. And in the next episode, I actually decided to stretch the two.
 two-parter into a three-parter. In the next episode, I'm going to be talking about even more machine learning algorithms. If there's so many machine learning algorithms, how are you supposed to decide what to use when? Well, there's a multi-part approach to deciding which machine learning algorithm to use given your circumstances. We've talked in the past that sort of deep learning can be seen as a silver bullet that can be used across a wide spectrum of machine learning problems. But right now, we're taking a diversion. We're talking about shallow learning algorithms. And with so many shallow learning algorithms, we have to know.
 specifically which algorithms are supposed to be used under which circumstances. So this kind of this multi-tier approach to deciding which algorithm to use under your circumstance. At a top level, certain machine learning algorithms only handle specific tasks. So for example, in the next episode, I'm gonna talk about an anomaly detection algorithm. Well, you only use one or a handful of algorithms for anomaly detection. You don't use things like linear regression or logistic regression or any of the algorithms that we're gonna be talking about in this episode.
 So there is a level of domain knowledge that will filter down the types of algorithms that you're going to be using for your specific purposes. It's apples to oranges. This situation only calls for a handful of machine learning algorithms that can be applied whatsoever. And that just takes familiarity with a lot of the machine learning algorithms and what they're specifically built for. I think listening to this podcast series and taking the angiouing course and some other follow up material you'll
 to get a feel for what algorithms are built for what purposes. But then we go down a level. We've decided that we're going to be working in supervised learning, classification, and so that only includes a handful of specific algorithms. We've excluded a whole bunch of algorithms, like all the unsupervised learning algorithms, any regression algorithms, etc. We're not going to use linear regression for this. We're not going to use k-means. We're going to do classification. But now we have a whole bunch of classification algorithms to work with. So in the last episode, I talked about decision trees. In this episode, I'm going to talk about support.
 vector machines and naive bays. All of these are classifier algorithms. We also have logistic regression. We have neural networks. How the heck am I going to choose from amongst all these classifiers? Well, at this point, what a lot of people in machine learning do is they look at their data and they look at their environment, their system, the computer, how much RAM does it have, and how flexible are we with time as far as running the algorithms, making the inferences and predictions? Certain algorithms work better with certain types of data. Maybe, logistic regression works really well with...
 numerical input, but it's also very sensitive to missing features and such like this. Now you've Bayes works better with categorical input, but is not sensitive at all to missing data. Now you've Bayes classifiers are very fast to run and very memory efficient, but maybe a little bit less precise than something like a neural network. A neural network takes a lot longer to run and train, but it's very precise and can represent very complex models. So once we know what category of machine learning algorithms we're going to be using for our...
 purposes, then we consider the situation at hand, memory restrictions, time restrictions, what does our data look like? How many samples do we have? Certain machine learning algorithms work really well with only a handful of samples, while other machine learning algorithms require tons and tons of examples, your training data set. Neural networks, for example, are vexed by the fact that they need lots and lots and lots of data, whereas a naive-based classifier, for example, doesn't need that much...
 data to get up and running. So you look at your data, you plot it, you chart it, you graph it, you decide if you're missing any information. Is it numerical? Is it categorical? How much memory do I have on my machine? How much time do I have to work with? How many examples do I have? And then you decide. So that part is a little bit tough. It's tough to explain in podcast format. I'm going to link in the resources section to a table of pros and cons for specific algorithms, given situations like memory constraints, time constraints, etc. There's also a
 decision tree put out by Scikit Learn, a Python library for shallow learning machine learning algorithms. They have a decision tree, a picture, like a flow chart for helping you decide which algorithm to use given the circumstances. So it'll ask you some yes, no questions. Things like, do I have greater than 50,000 examples in my training data? Yes, go this way. If not, go that way. Okay. Is this text-based? Go this way. Am I missing any data? Go that way. And it'll help narrow down what machine learning algorithm you're supposed to use. And that the reason I'm exposed-
 planning all this is that it can seem so overwhelming at first when I just throw a million machine learning algorithms at you. And you're thinking, I have all these algorithms. How am I supposed to know what to use when and where? And there's a system. There's a system to deciding what to use. And the final approach, finally, once we've decided a handful of algorithms that we can use given the circumstances is to actually just try them all. You'll often see a lot of machine learning engineers, what they'll do is they'll import from scikit learn or tensor flow, just all the algorithms that can possibly be applied to their circumstance.
 They'll clean up the data, they'll visualize the data, they'll do some stuff with the training data, they'll split it into training, validation, and test set will get into that stuff in another episode. And then they'll just throw all their data through 10 machine learning algorithms. Run them all, parallel or serial, whatever. And then at the very end of the file, you'll see they'll evaluate the performance of all the machine learning algorithms. They'll write some code that determines how well each algorithm did. Compare them all to each other and find the champion or champions, throw away the losers and maybe...
 We keep the top three on hand as they continue in their programming. Eventually, they'll sort of come to a conclusion that one is clearly the winner. This is the right algorithm for the job. We're going to roll with that. So it's not necessarily very clear what algorithm to use when. It's like a three-part approach. We start at the top where we decide what algorithms are even grossly applicable. Okay, is this a supervise or an unsupervised learning situation? Do I have the labels? Okay, if it supervises it regression or classifications.
 Okay, it's classification. Now we have maybe 30 algorithms that we can choose from. Let's plot our data. Let's look at the situation. Let's look at the environment. What kind of constraints are we up against? And at this point, it's a little bit difficult to memorize which algorithms work best, given constraints and data. What you usually do then is you look at this reference table or that psychic learn flow chart to help you pick a handful of algorithms to try. And then you just throw them all against the wall. You just shock an approach, all these algorithms.
 and the evaluation metric at the end of your script will tell you which one did best. So you're gonna learn a lot of machine learning algorithms and use this approach to determine which algorithm to use given your circumstance. With that out of the way, let's jump into the first of these two algorithms called support vector machines, SVM. It's a very weird word. I'll tell you why it's called that in a bit, but let's just try to understand an intuition of what it does. So like I said, these power tool machine learning algorithms, like decision.
 entry, support vector machines and naive bays, they can all be used for both classification and regression. So they're supervised learning, machine learning algorithms that can be used both for classification and regression. Also, neural networks can be used for classification and regression as well. You'll find that the primary use case of all of these algorithms is classification. I don't know if this is true or not, but it seems to me from my experience that classification is kind of the majority use case of machine learning that you'll see.
 in the wild. I don't know if this is true. Don't quote me on it. But you'll see that these machine learning algorithms, these power tools are primarily built for classification, but can be used for regression. But because their primary use case is classification, you'll see the examples or the tutorials. They'll all be showing you how to use them for classification. And that's what I will be doing in this episode. And then you'll have to look up how to use them for regression on your own. So support vector machines can be used for classification and regression when you use a support vector machine.
 for classification, it's called a support vector classifier, SVC. And if you use it for regression, it's called a support vector regressor, SVR. And the broad category of these is called support vector machines. So we're going to go with the classification examples, like I mentioned. How it works is it determines a decision boundary, a decision boundary between your things over here and your things over there. That sounds a lot like logistic regression. It's very similar to logistic regression.
 But it's got some it's got some perks over logistic regression that we'll give you to in a minute But let's remember what a decision boundary is let's say that you have all the cats on the left and all the dogs on the right You have a graph of cats and dogs. They're just dots on a graph, right? Imagine blue dots and red dots. These are your data points. These are your training examples You have all the cats on the left and all the dogs on the right and what you want to do is come up with a line You're gonna draw a line in the sand between the cats and the dogs So no dogs allowed over here on the left say the cats
 So that line that separates your cats from your dogs is called your decision boundary. And now if you add a new animal into the mix, based on some features about the animal, whether it has whiskers, does it bark, how many lives does it have, etc. These are all the features. We'll be used to determine where, locationally, the object gets placed in 3D or 4D space. And if it's on the right side of the line, then it's a dog. And if it's on the left side of the line, then it's a cat. That's your decision boundary. And that looks a lot like it.
 logistic regression situation. Now, what makes a support vector machine different from logistic regression in categorizing things over here and over there is this decision boundary specifically. A support vector machine, it doesn't use a line. It doesn't draw a hair thin line between the two sets like logistic regression does. Instead, it tries to make that line as fat as it possibly can. It makes a wall. It doesn't use a one point line. It used it.
 This is a 16 point brush stroke. How fat is this wall? Well, the borders of the wall bump up against the innermost cats and dogs. So the rightmost edge of this decision boundary is going to bump up against the leftmost dogs and the leftmost edge of the decision boundary will bump up against the rightmost cats. Okay, so that makes sense. It's just, you're just trying to fill a river or make a wall.
 between the two things, these over here and those over there, as wide as you can before you touch them. So you do that with your training set of examples. Now, why did we do that? Why was logistic regression insufficient? Why wasn't that line sufficient? Why did we need a fat line? Well, the reason is because of a problem that we're going to get into in a future episode called overfitting. Overfitting is basically if I were to draw the line between the cats and the dogs, I could draw it wrong, actually.
 Let's say that I had a cat closer to the middle. Well, if I wasn't smart, I might draw the line to accommodate that cat, what's called an outlier, something that doesn't really fit the bill of the majority of the data. I might skew the line, maybe I'll tilt the line counterclockwise or clockwise a little bit to accommodate for that one outlying cat. In other words, I didn't make the most ideal line possible. You and me.
 We're humans, we look at a cluster of dots over here and a cluster of dots over there. And in our minds, we can draw a vertical line right down the center. Even if there is an outlying dot, we still have an intuition of where that line goes. Where's the best line that separates the two classes so that in the future, if I were to add a new object into the mix, it will go on the correct side. But logistic regression is a little bit sensitive to outliers and things like this. And this can cause a line that gets improperly drawn across the kidney block, because you can find religion in another sense. Because more powerful modern disorders prefer to other phenomenon factors, was that time that we were able to avoid public representation of how the body transmitted or that financial know- these scientists
 is called overfitting. In an extreme example of overfitting, imagine a line that goes right up vertical and then it squiggles out like a half circle to include that outlying cat and then keeps going. Imagine that we created some wild function of polynomials that allowed for that little squiggle out. That's a wild example of overfitting but in our particular situation where we're using logistic regression which is a linear function, we only can work with a line, not a polynomial.
 function. Just tilting the line, maybe counterclockwise or clockwise, might cause some overfitting. So what support vector machines do different than logistic regression in the case of coming up with a decision boundary between the classes on the left and the class on the right is it makes that decision boundary as fat as possible so that we can deal with these outliers no problem. Now, the thickness of our line, it's called the margin. We want this fat.
 a line is possible. We call this a large margin classifier, large margin. So that's the word for the thickness of the line is margin. And then the word for the dots that are being bumped up against by this line that we're drawing, they're called support vectors. That's why this thing is called a support vector machine. Support vectors, it's kind of a weird word. I don't know why we don't call this. I think we should call this algorithm, the fat line algorithm. And we should call
 these dots that the fat line bumps up against, we should call them bumping dots. But no, we call the algorithm a support vector machine or a large margin classifier and that these dots that the fat line bump up against is, they're called support vectors. A vector, so a dot on a Euclidean graph, x, y, plane, this is what we're looking at, there's a bunch of dots on a graph. You can think of them as a dot or a point or you can think of them as an arrow pointing from the origin to the...
 that dot. You can graph that arrow with a function mathematically. So you can represent these dots in another way and we call that a vector. A vector is a line that points from the origin to a dot. And that's why they're called support vectors. They're the vectors that support drawing a fat line. Okay, all finding good support vector machines seem pretty simple. It's like logistic regression with a fat line instead of a skinny line. That's the only difference.
 Right? Well, it's got one more little twist. And this is where things start to get wild. Really weird if you ask me. Support vector machines only handle linear classification. So does logistic regression. Now you can throw some polynomials into the function and make the situation nonlinear, but that's a little bit less than ideal. Typically, if our situation is nonlinear, we move away from the linear classifiers into something more complex, like a neural network, for example. So both logistic regression and...
 and support vector machines are linear classifiers. But there's a trick, a trick that can transform a support vector machine into a non-linear classifier. And this trick is called the kernel trick, kernel, K-E-R-N-E-L. You'll see kernels used quite commonly in machine learning. They're very weird. They're very hard to understand. I still haven't quite wrapped my head around them. But what I think of as a kernel...
 It teleports you into another dimension or these rose colored goggles that you put on and they change the way everything looks. Okay, so that sounds very strange. Let me give you an example. An example used from the machine learning with R, book that I'll post in the resources section, is that if you're looking at a graph of dots, summer, blue, and summer red, okay, and this is what it looks like. You have a blue circle of dots in the center and surrounding that is a
 red circle of dots. It's like a blue circle with a red border, but they're all dots, okay? It's not drawn onto the graph. It's the bunch of dots. Well, that is clearly a non-linear situation. This isn't a bunch of cats on the left and dogs on the right, which is linearly separable by our decision boundary. No, this is a circle and a circle. Those are not separable by lines. However, if those dots represented something conceptual...
 for example, if we were looking at latitude and longitude, and those dots represented say snow on peaks of mountains versus non-snow, latitude and longitude, well that wouldn't really make sense as a way of looking at this, would it? What we really care about is altitude, how high up the mountain peak is, and latitude, how far north and south we are. Those are the two characteristics that are more important rather than longitude.
 Longitude, Longitude doesn't help us at all. So if we think about the problem different, we can actually transform our situation into a new graph where dots are indeed linearly separable. All the blue dots suddenly have become sort of a rectangle on the left or the right or top or bottom. Some sort of situation where we can actually draw a line between the two classes of dots. OK, so that's a little bit weird. Let me think of another way of representing this.
 If you have two circles, blue and red dots, maybe you can think of them instead of in a Euclidean space of x and y, you can think of them in a radian way. So this is what a kernel does. A kernel, what it does is it takes your data, the stuff you're looking at right now, which is non-linearly separable. And it transforms it into a new set of dimensions. So you're looking at the latitude and longitude representation.
 of mountains in the world and trying to decide whether or not they have snow on the peaks. And you're scratching your chin and you're like, how am I going to separate this? But I grab your hand and I'm like, no, no, no, come over here, come over here. And I pull you around so you're looking at it from a different angle and you go, ah, okay, looking at it from this angle things seem a little bit different. So a kernel is something that you multiply your data by in order to transform it into a new dimension. So I think of it as...
 It's looking at the problem from a different angle. I think of it as like in Zelda a link to the past I can't remember you blow you blow on a flute or you do some mirror trick and it goes And you're now in the dark world you do it again And you're in the light world you're in the same place the whole world is really the same place everything is the same But you're looking at it different and it helps you to solve different puzzles So you can transform a circle world into a
 line world. There's a whole bunch of kernels out there. There's like radial basis function kernel, polynomial kernel, sigmoid kernel. So there's a whole bunch of kernels, a whole bunch of of colored goggles that you can put on. Imagine a drawer full of colored goggles that you could put on at any time. But you have to know a little bit about your situation. You have to know whether the data that you're dealing with is sort of could be transformed into a different world so that it's easier to work with.
 with so that it is now linearly separable. So support vector machines, very strange machine learning algorithm. I really took me a while to kind of wrap my head around it and I still don't know exactly when it's preferred to be used under certain circumstances or not. So let's hit it from the top one more time. Let's reference prior algorithms that we've used. Remember linear regression is a regression algorithm for coming up with a number output. Okay, if we want to classify something in the past, we've piped linear regression into a...
 new function called logistic regression. Logistic regression is like using linear regression to classify things. Is it a cat or a dog? Is it going the left of the line or the right of the line? Now, conceptually, logistic regression draws a line down the middle. We call this the decision boundary, this line that separates the cats from the dogs. Now, with logistic regression, unfortunately, this line may be prone to overfitting based on outliers. If there's a lot of data.
 that's bad, bad data or just noise or anything like this, it could kind of screw up our line. It might tilt it, tilt it down counterclockwise or clockwise. It may not be the best fit, the ideal line straight down the center, separating the cats from the dogs. So we have this new algorithm for classifying things called a support vector machine. And it uses a decision boundary as well, but it makes that decision boundary as fat as...
 a large margin. It bumps up against the innermost dots on the left and the right classes. We call those innermost dots support vectors. And that large margin helps us prevent overfitting future examples. That's step one of a support vector machine. It is simply maybe a little bit more accurate, a little bit more efficient version of logistic regression you might consider it. Step two is this strange trick of the trade called the kernel.
 trick and the kernel trick lets you take your data, which may be represented non-linearly if you look at it like this, transform it by putting on some goggles into a new dimension. And now suddenly it is linear. So you take a non-linear data set, look at it a different way, and now it's linearly separable. Cool. So a support vector machine is a classifier or can be used for regression, and it has the ability to represent non-linearly.
 linear circumstances. Now the problem is, like I said, you have this drawer of kernels, goggles that help you look at situations from different angles. Well, there's only so many of these. These, you know, like circle world or radial basis world. There's only so many ways to represent a non-linear data set in a linear fashion. And you have to know which one to use given the circumstance. Unlike a neural network, which is able to represent non-linear situations completely on itself.
 It will learn the way to represent them non-linearly. It can represent any number of complex situations. So support vector machines and neural networks are often compared to each other because they're both these black box methods and they can both handle non-linear situations. But the difference is that a neural network in deep learning is more powerful. It can represent more non-linear circumstances. And you as the developer don't have to know in one...
 What way is this situation non-linear? The neural network will learn that mapping for you. Whereas with a support vector machine, you have to know sort of in what way is this circumstance non-linear? You don't necessarily have to know in advance. You could just try throwing at it all the kernels in your drawer. But if you don't have that sort of upfront information, it might be better to use a neural network anyway. So why wouldn't you use a neural network? Well, if your situation...
 and can be handled with a linear support vector machine, okay, vanilla support vector machine, or you do know about the situation and you can pop in one of those kernels into your support vector machine. Then support vector machines are a lot faster than neural networks. They're faster and they take up less memory. And in fact, you're gonna see that this is a very common recurring theme in machine learning. Like I said previously, machine learning engineers, they look at people who use deep learning as a silver bullet for...
 every situation and they say, you could do this faster with a dedicated shallow learning algorithm. For specific situations that call for the shallow learning algorithms. So if your situation supports using a support vector machine, then you will get a lot more speed and memory savings using that over a neural network. But you'll have to know a little bit about your data set or your circumstances in advance to help you determine whether using a support vector machine is for you or not. So I kind of like to think of.
 Machine learning algorithms as you have this backpack, like an enrol playing game You have this backpack of tools that you can use you have a grappling hook for certain circumstances You have your sword and shield you have a magic wand and so if you're presented with a puzzle Okay, so you need to kill a bad guy you all you use the sword and shield you need to open the entrance to a cave You use a bomb well neural networks and deep learning they're kind of like a bazooka You can almost solve any situation with the bazooka
 But maybe it's overkill and expensive and can cause collateral damage. So kill a bad guy, bazooka. Open a treasure chest, bazooka. Open a cave entrance, bazooka. But why not use the cheap bomb in the case of the cave entrance? Why not just use your hands and a key when it comes to opening a treasure chest? The way I think of support vector machines is like a gun. And it kind of looks like a space gun, like a plastic ray gun. And it's kind of, for me, it's a little bit tough to know when to use this thing. And you're trying to figure out what's the best...
 for opening a door. That's locked. You can use a key, you can use a bomb, you can use your bazooka, or you can use this weird plastic ray gun. And the proper approach is to try all of them. Try all of them and evaluate the performance of all of them at the end of your script, determine which did the best, which took the least amount of memory, the least amount of time, was the most accurate model, etc. And it just so happens that it turns out a key in this particular case opened the door the best. Logistic regression.
 and handled situation A the best. But I always think of support vector machines as this weird ray gun and you point it at the door and you shoot and the laser comes out and nothing happens. And you're like, huh, you turn it around in your hands and somebody behind you says, oh, well, you're not using the radial basis kernel. Of course, that's why it's an open. So he hands you this little module and you look at it, it's the radial basis kernel. And you're like, uh, and you clip it into your ray gun and you point it at the door and you shoot and outcome these sonar circles, woo, woo, woo, woo, and the door opens. And he's like,
 You see, it was obvious. And you're like, was it? Support vector machines. Now let's move on to naive Bayes classifiers. naive Bayes. Bayesian inference is a very interesting and important component of machine learning in general. In fact, Bayesian inference really is a rung of the ladder of statistics. And like I told you in a previous episode, statistics is the god math of machine learning. Statistics is everything in machine learning.nie Bay rung of another inverse is that a learns this.
 basic principles of statistics like probability, joint probability, conditional probability, etc. are used everywhere in machine learning, even if you don't know it. Many of the machine learning algorithms that we've been discussing so far, there are algorithms that come straight out of a statistics textbook. Linear and logistic regression, that's statistics. Statistics is really essentially boiled down into probability, probability and inference, and inference is based off of...
 probability. And probability is sort of raw statistics. So the algorithms that we've been learning so far are probability and raw statistics deep down inside, deep down under the hood. They're just statistics. But at the high level, the way that we've been looking at them, they kind of look like machine learning algorithms that kind of look like computer algorithms or complex mathematical equations. Yes, they are. They are indeed, but they're truly fundamentally based on probability.
 Now, I'm not going to teach you statistics in this podcast. I'm not going to teach you probability, but I am going to really quickly run you through the basics of probability here in order to help you understand how naive base classifiers work because to understand how Bayesian inference that is naive base classifiers, how they work, you have to understand the very basics of statistics. So like I said, all the algorithms that we've been using thus far, they use statistics.
 but they use them under the hood. They use them conceptually in principle. Well, Bayesian inference, which is a classifier, supervised learning algorithm, but also can be used for regression, Bayesian inference is like raw statistics. It's like using statistics in the raw to handle machine learning circumstances. Statistics in the raw. So Bayesian inference is really just raw, true, pure statistics in order to make an inference or an estimate.
 about whether something is classified as this or the other thing. So let's try to understand probability a little bit. Probability. Probability is very simple. It's the chances of something, the likelihood of something, of an event we call it, an event. What is the probability of getting heads when I flip a coin? Well, 50%, 50%, 50, 50, right? It's one half of the time it is heads and one half of the time it is tails.
 So the probability of this event of flipping a coin is 50%. Okay, so that's step one, basic probability. Step two, joint probability. What are the chances of me getting heads first? And then flipping the coin again and getting heads again. What are the chances? What's the probability of A and B? Well, it is simply the probability of A.
 times the probability of B. 50% times 50%. That is 0.25. So the probability of heads and then heads again is the multiplication of the two, which is 0.25. That is called joint probability, the probability of these things joined. Step two, joint probability. Step three, conditional probability. And now we get into right proper statistics. The good stuff, the meat, can.
 conditional probability. What is the probability that my second flip gives me heads? If the first flip was heads That that's an interesting question. I don't see how the first flip has anything to do with the second flip Exactly if I flip a coin once heads tails, okay 50% and I flip the coin again and get heads or tails That second flip has nothing to do
 with the first flip. The result of the first flip does not affect the result of the second flip. Those are what's called conditionally independent events. Independent because they do not depend on each other. They do not affect each other. Independent events. Well, there are some situations out there which are not independent. They are dependent. So for example, what is the likelihood of it raining today?
 given it is cloudy outside. Ah, now there is an interesting question. If it is cloudy outside, then it is, let's say, 40% likely to rain. The probability of it raining depends on the probability of it being cloudy. We call these conditionally dependent events, and this is all called conditional probability. Conditional probability. Conditional probability.
 is an interesting thing. It is very useful and widely applicable in machine learning. And it has a mathematical formula. Okay, probability, raw probability, step one was just probability. Joint probability, step two is probability times probability. Just multiply the two. Conditional probability, step three, is this mathematical equation. The probability of B given A, that is the probability of rain, given that it is cloudy outside.
 is the probability of A and B, the joint probability, A times B, over the probability of A. Okay, so the probability that it is rainy, given that it is cloudy, is equal to the probability that it is rainy and cloudy, over the probability that it is cloudy. Very strange, very strange. This seems kind of non-intuitive. I mean, first off, it's a mathematical formula.
 and it's a little bit tough to kind of tease what everything is in this puzzle. But let's talk a little bit more about probability, just general probability. Imagine we have a big giant circle that represents weather. And inside that circle are a bunch of little circles. We have cloudy, and we have sunny, we have rainy. These are built based on observations of the past. The number of times that a day is rainy is the number of times that we've seen it rain in the last five years, for example. Over.
 the total amount of times we've seen weather at all. Okay? So the way that we build up probabilities, the way that we build up like what are the chances of it being cloudy at all, is just that we look at days, day after day after day, and count the number of cloudy days. And then we divide that by the total number of days we've observed. So that makes sense. Don't overthink it. It's the number of times we've observed something over the total number of observations. So we have cloudy days, we have rainy days, we have sunny days. Now, the joint probability of two events is...
 the number of times they overlap. That is in the case of non-independent events. The number of times they overlap. So it's like a Venn diagram. We have cloudy days and rainy days and sunny days. Let's say that it's cloudy 40% of the time and it's rainy 30% of the time. And there's a little sliver of overlap between the two. Actually not a little sliver, a very large chunk of the time. They kind of both fall in the same day. We have both a cloudy and a rainy day at the same time. That's joint probability. That's A times B. Joint probability. The number of...
 the amount of overlap between the two. It's a vendiogram. And then conditional probability is a very interesting formula. It's very non-intuitive. It doesn't make a whole lot of sense when we're trying to visualize this as a bunch of circles and vendiograms. The conditional probability that, remember, the question that we're asking is, what are the chances that it's going to rain today if I know that it is cloudy today? And the formula says that the answer is the joint probability, the amount of overlap between the two, the two.
 the number of times it rains and is cloudy over the probability that being cloudy at all. So the probability of A and B over the probability of A. So again, we have three steps so far. We have basic probability and we build that up just by observing things over time. Okay, coin flips, flip them a million times and you build up a database of 50-50. We have joint probability, which is the probability of two things.
 Co-occurring and then we have conditional probability and that is the probability of something if we know something else And that my friends sounds a lot like fundamental machine learning, right? What is the probability of it raining given it is cloudy? Well, it is cloudy is a feature a feature in our spreadsheet X X1 and what we're trying to determine is why whether or not it will rain today that looks a lot like
 just a regression or linear regression or any other algorithm, any basic fundamental machine learning algorithm that we've seen. This is kind of the skeleton form of machine learning. So conditional probability is really core machine learning. So that's kind of the raw statistical formulation of a machine learning algorithm, conditional probability. Now, the next and final step is called Bayes theorem, Bayes. Bayes. That is the namesake.
 for our algorithm here called a naive Bayes classifier. There was a man a long time ago named Reverend Thomas Bayes, who is a statistician. And he learned a little trick of the trade when it comes to conditional probability. Specifically, if you know the other thing than the thing you want to know, you can do a little reversey on our conditional probability formula. That's it, that's all Bayes theorem is. It is using some statistics out.
 algebra, some probability algebra, and flipping stuff to the other side of the equation. So if what we want to know is it cloudy and we do know that it is raining, so the opposite, the opposite of what we were asking before. Well they're not the same thing. Very obviously they're not the same thing. How likely is it to rain if I know that it is cloudy? Well it is very likely to rain, okay, maybe let's say 40%, maybe not that likely, but
 to rain if it is cloudy outside. Well, how likely is it to be cloudy if it is raining? Oh, oh, totally different number. Now we're talking like 90, 95%. Have you seen rain on a sunny day? Yes, so have I. On a blue moon. It is substantially more likely to be cloudy if it is raining than it is to be raining if it is cloudy. So conditional probabilities don't reverse. They're not the same thing, but they're reversible. There is a way to re-
 to them and that's called Bayes theorem. And Bayes theorem looks like this. The probability of A given B, okay, so I want to know the opposite order. Equals the probability of B given A times the probability of A all over the probability of B. What the heck? I'm not going to explain where this comes from. You're going to have to learn Bayes theorem and you're going to learn all this in statistics anyway. Bayes theorem is a very fundamental component of statistics proper.
 You'll learn base theorem in one of the early chapters of your statistics textbook or the Khan Academy course. So it's not specific to machine learning. It's a very raw fundamental core component of statistics in general. And all it does is it gives you the ability to ask the question the other way around. So why is it so fundamental then? It sounds like step three, we talked about regular probability, step one, joint probability, step two. The joint depends on rate, the null.
 of probability. And then conditional probability, which is step three, that depends on two and one. So they all, you learn them in sequence because they depend on each other. Seems like conditional probability is the crux of what we need to use statistics in the raw to solve probabilistic machine learning situations. Yes, that's true. But very often the question isn't asked the way you wanted it to be asked. The question is the other way around. So
 So base theorem is using conditional probability and doing a little reversey on the equation so that you can ask the right question. Okay, so that was a little bit crazy. Let's talk about an example using email spam. It's whether and spam are the two most commonly used examples in understanding Bayesian inference. And in fact, whether and spam classification are two of the most common applications of naive bays in the wild. Released up until now into recent times.
 when I think deep learning principles are used a little bit more commonly in these spaces. Naive Bayes was the champion of weather, prediction, and spam classification for emails. The way it works for emails is you break up your emails into all of the words of an email. Let's say that we build up a dictionary of English words and we throw out all the very dumb words and basic words like the is and we call these stop words. They are of course important in grammar and understanding sentences, but they may not be as important.
 in just classifying an email as spam. So we throw out these stop words and we keep the essential words. We start to learn that certain words are commonly co-occurring with spam emails versus non-spam emails. So for example, the word Viagra is very often seen in spam emails. But let's not get ahead of ourselves. First off, what we wanna do is just build up a database of how common every word is in an e.
 email in general and how common spam is in general. So we build up a probability of the word Viagra. We build up a probability of the word friend, Saturday, weekend, every word under the sun and a probability of whether or not an email is spam. Let's say it's high, let's say it's like 60% of email is spam. Well, that's that. Okay, so we have a bunch of probabilities. That's step one, regular old probabilities. Step two, we're gonna skip.
 because we use joint probability in the equation of conditional probability, but we don't really use it directly. So step three is conditional probability. If I've got all these probabilities, words and spam, what is the probability of an email that I'm looking at right now being spam? Just straight up. Okay, well that's 60%. We've already said that. Well, what is the probability of that email being spam given it has the following words because it does in the circumstance.
 It has the following words, Viagra, free, act, now, etc. Okay, well, we will use the conditional probability formula. It'll give us a number. It'll give us the probability of the thing being spam. And if we have to ask the question a different way based on the information that we were provided, which is usually the case, then we will use Bayes theorem to do a little reversey on the conditional probability formula, and that'll give us our answer. Bayes theorem. Now, the specific algorithm for class of a k-
 is actually called naive bays, naive bays classifier. Why is it called naive? Well, there's a level to which all the probabilities in our formula actually depend on each other. I kind of think of it as like a Mexican standoff. It's like, what is the probability of this given this guy, this guy, and the other guy? Well, they all depend on each other. So you can think of like three guys pointing guns at each other and they're all looking at each other. Well, what's the probability of this given that guy? Well, probably this guy depends on the probability of that guy and the other guy. Well, the probability of the other guy could have the same giving the other guy.
 depends on this guy and this guy. So they're all kind of mutually codependent. The naivety part of naivbeys cuts off the dependence of events from each other. It makes things not dependent on each other. And this makes the algorithm tractable, able to be computed within a reasonable amount of time. Without that naivety part, they call it the naivve assumption, the algorithm would be too computationally difficult for modern machines to perform. And so in order to use Bayes theory.
 and conditional probability in the wild for machine learning applications. You had to introduce this naivety assumption, which severs the dependence of events from on each other. It assumes that they were all independent events. Now, as we did with support vector machines, let's compare naivv-base to deep learning. Naivv-base is commonly used in text-based applications. Like I said, spam classification of emails. It's going to be based off of words in the email. We call this a-
 bag of words approach. It's called a bag of words because you're not assessing grammar or how words relate to each other. You cannot with naive bays. The way words relate to each other remember that would be dependent events that would not be independent. Therefore, we would not be using the naive assumption. If words related to each other in a grammatical structure, they would depend on each other and our approach could not be naive. So we're going to
 to assume that they don't depend on each other. Instead, we're just gonna pull out all the words of the email and we're just gonna kind of keep our eye on trigger words like Viagra. What is the probability of an email is spam given the existence of the word Viagra? So that's why it's called a bag of words. It's just all the words just throw them all in a bag and hand the bag to Naive Bayes. A recurrent neural network which is an algorithm that will get into in a future episode and it's just a simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple,
 is a type of neural network, so it's a type of deep learning algorithm that is very good at handling text-based applications as well. So naive bays and recurrent neural networks are commonly pitted up against each other. But unlike naive bays, which uses a bag of words, and the naive assumption that there are no relations between the words, recurrent neural networks literally read the email from left to right, top to bottom, and they keep grammar in mind, negating words.
 modify the words they negate. I mean, I think of a recurrent neural network as taking an email, printing it out, and it's a classy English gentleman who sits in his leather sofa and he has a pipe. He's a nervous man. Well, I see the existence of Iagra, but let's not be too hasty because the use of some amount of Antonins in this particular structural here, and I do find that they use abbreviations more often than real words. Why would they use abbreviations? It's either they're uneducated, less versed in formal grammar, or that they're trying to save precious space.
 so that they can get it a word edgewise. I believe through formal analysis of the documented hand, we are indeed dealing with spam. But it took the guy almost a day to come to this conclusion by comparison to naive bays who sit in their folded his arms, he's got a cigar in his mouth, he's like a bug. It says Viagra. You don't need any other information. Recurrent neural network looks up from the paper and he says, oh yes, of course it has Viagra, maybe, increases the probability of the thing being spam, of course, but that's not a behavior, he's still always causing... And naive bays snatches the paper out of Recurrent neural networks' hands
 it up. It says that God damn things spam. It has Viagra. I don't need to know anything else. So if time and memory are crucial to your application, if things need to be fast and not consume a lot of memory, the naive base is a preferable machine learning algorithm to a more complex algorithm like a recurrent neural network. But if you need more accuracy and complexity in the analysis of the situation, then a recurrent neural network is more likely to be your guy. If time and memory are less of an issue for your particular situation and you'd...
 rather have higher accuracy and a more complex modeling of the situation than deep learning is preferable to naive base. But let's think about email spam classification. You don't have all day to determine if an email is spam. When somebody sends an email, the recipient expects to receive the email in very short order, let's say no more than one minute. Well very powerful recurrent neural networks on very powerful machines could probably do that in a minute. I'm not so sure. Whereas...
 an naive-based classifier could snap its fingers and make a judgment, the blink of an eye. So in the case of email spam classification, it is very likely the case indeed that an naive-based classifier is preferred to a recurrent neural network. And this is a prime example where we see a shallow learning algorithm may be better for a particular purpose than deep learning. Even though deep learning is more accurate and complex and magical. In fact in this particular situation of using recurrent neural network.
 for email classification. They call this field natural language processing, but using recurrent neural networks with what's called word vectors, we're going to get into in another episode, the way that it represents these documents is as a point in vector space that can be compared to other documents. It's actually very magical. So much so that the spin of natural language application using this type of technology is called natural language understanding, which indicates if you might stretch your mind so far that the machine may be under
 understanding in a fundamental way, the meaning behind what classifies a document as spam or not spam. Very interesting indeed. So there you have it. Support vector machines and naive bays and I do want to admit I don't understand these algorithms as much as the algorithms that I have presented to you thus far. So this is probably one of my worst episodes. I would encourage you to go learn these algorithms offline which brings us to the resources section of course the Andrew Inck
 Sarah of course he has a week on support vector machines. I will link to that in the show notes. Andrew Ng does not cover naive Bayes classifiers. I found that very interesting actually because naive Bayes classifiers that's one of the fundamental algorithms of machine learning that you see brought up over and over and over compared to more complex models like like neural networks and used in the wild today with great success. You'll see it in most introductory machine learning textbooks and all these things.
 So why didn't Andrew Ink cover the Naive Bayes? I actually found a video by Andrew Ink on YouTube when I was trying to learn Naive Bayes later on Naive Bayes and it clearly came from his course. He took it out at some point. I think that he didn't want to bog down newcomers to machine learning with statistics. Because like I said, to understand Naive Bayes classifiers, you have to understand Bayes' theorem. To understand Bayes' theorem, you have to understand conditional probability. To understand Bayes' theorem, you have to understand Bayes' theorem.
 conditionally, you have to understand statistics. So the whole world of Bayesian methods, it's the world of statistics, raw statistics, and stats is hard stuff, my friends. So it's important, it's essential, but I have a hunch that Andrew Ng decided they'll get to that later. I don't want to scare them away from the field yet, because you don't need it to succeed right away. You can start doing linear and logistic regression, and you can deep dive right into deep learning and neural networks and skip.
 I've asked all this statistics stuff, but it is essential for you to know. So I would encourage you to learn naive based classifiers, the machine learning with our book that I'll put in the show notes, is it has a great chapter on naive based classifiers. It also has a great chapter on support vector machines as well. And the mathematical decision making great courses series that I've referenced from time to time also has a whole episode, audio episode dedicated.
 to naive bays. So I'll post that in the show notes and I would encourage you to try to learn the basics of these two algorithms offline because like I said, I don't think I did a very good job of presenting them unfortunately. I prepared and I prepared, but I was a little bit out of my element for this episode. And finally, like I mentioned before, how do you choose which algorithm to use? When you know your situation, whether it calls for supervised learning or classification, regression, etc, you'll be able to narrow down grossly.
 which algorithms to throw out, and now you have in your hands 20 algorithms that you could possibly use for classification. And in order to decide which of these algorithms you should use, you assess your data. So for example, naive bays works well with categorical data and missing data, something many other machine learning algorithms do not work well with is missing data. naive bays works a okay with missing data. How many examples do you have in your training data set? Are you working with text or numbers, et cetera? Using these types of questions will help you narrow it down even for...
 And in order to do that, I'm going to link to a table of pros and cons of various algorithms under various situations and a decision tree put out by the Psychit Learn project for choosing an algorithm giving various circumstances in your problem. And from there, once you've got five algorithms to use in hand and you still don't know which of these five is best to use, you just try them all. You try them all and you see which one has the highest performance based on some evaluation.
 metrics. In the next episode, I'm going to be talking about some more miscellaneous machine learning algorithms, things that are very dedicated. So the last three algorithms that I talked about, decision trees, support vector machines, and naive based classifiers, these are all very general purpose, power tool machine learning algorithms. All three of these could basically be swapped with each other and knowing which one goes where, a little bit difficult. But in the next episode, the machine learning algorithms I'm going to be presenting to you are very specifically tied.
 to very specific use cases. So it'll be a little bit easier. It'll be one of those apples to orange as bits. It'll make it easy for you to decide that yes, you should use this algorithm because the situation is A or B. I'm going to be doing these episodes now every other weekend. I've become quite busy recently. I apologize. So rather than every weekend, I'll do every other weekend. So, so I will see you two weekends from now.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 14, shallow learning algorithms part three. This is the third and final of the shot.
 learning algorithms episode and this episode will actually be quite easy by comparison to the last two. The algorithms that we're going to cover in this episode are recommender systems anomaly detection systems and mark-off chains. Recommender systems and anomaly detection systems are extremely easy to understand so we'll start with those. They're also very specifically applied so you're not going to use these algorithms maybe all over the place like we did with the last few algorithms namely support vector machines.
 naive, bays, and decision trees have a lot of applications, and it's difficult to know when to use which where these algorithms, recommender systems and anomaly detection, are used specifically in those applications, recommending things and detecting anomalies. Let's actually start with anomaly detection because that's even the easiest to understand. So first off, what would you use an anomaly detection for? For example, fraud detection in credit cards. I just want to figure out when something went...
 with your credit card. They want to find some sort of transaction that didn't look typical. It looked a miss. It's not your typical spending habits or it's overseas when you made most of your purchases today in America. The way credit card fraud programs work are by using anomaly detection systems. They may indeed use the algorithm that I'm going to be explaining in this episode or they've probably in my opinion moved on to deep learning. I think a lot of big corporations.
 have moved on to deep learning for much of their machine learning process. Another application of anomaly detection is figuring out, for example, when a server is acting up, is it spiking in CPU and RAM usage, swap, swap space, networking, et cetera. It could be the case that maybe it got a very big load. It got asked to perform a task that is very heavy indeed, and this is not anomalous. And so implementing an anomaly detection system can see through that and figuring out when something is indeed...
 a miss so that a systems administrator could pay attention to this anomalous server activity. So anomaly detections have wide application and real quick I can explain to you how they work without getting into the details and that is simply to find an outlier on a bell curve. That's it. It is so simple. You have a bell curve of typical behavior, whether it's credit card transactions or server activity in a cluster and if something acts...
 as an outlier, something way over to the right over here on the bell curve or way over to the left over there, then you have an anomaly. That's it. But let's break this down a bit. First off, let's describe what a bell curve is. Even if you haven't begun learning statistics on your quests to learning machine learning, and you will at some point because statistics is the god math of machine learning, I'm still certain that you have seen the bell curve. It is the most fundamental component of statistics, probably one of the most fundamental like...
 equations of the universe if I am so bold. I may ruffle a lot of mathematicians feathers here, but in my humble opinion, I think that the bell curve should be the symbol of mathematics. It's very likely already the symbol of statistics, but I think it should be the symbol of mathematics proper. So for example, if you were to represent various branches of science with different pictures, for chemistry you might use a beaker or a little potion symbol filled with green liquid and bubbles.
 For physics, you'd use the atom with all the electrons swirling around it. You see this image used for like, atom the text editor, for example, or react. The web development framework. You know the symbol I'm talking about. It's the atom symbol it's used to represent physics in many places. Maybe biology might be a cell, but math, when you think of math, you were like, okay, what kind of picture would I use? You'd probably think of E equals MC squared. My friends, I propose that the symbol of mathematics be the bell curve. The bell.
 curve or also known as the Gaussian distribution or a normal distribution. I believe it's called normal because it's the most common distribution of all. And let me explain what a distribution is. A distribution is how your data falls onto a graph. So we've been working with data in this podcast series. For example, we've been talking about housing costs in Portland and Oregon. If you were to throw your spreadsheet of houses, just throw them all onto a graph.
 by cost, you would get a bell curve. You would see that there is sort of an average cost of houses right in the center and then things start to move away from it either on the high or on the low. So you have fewer very cheap houses and fewer very expensive houses and most houses are somewhere in the middle. So the way that this falls on a graph is you have your average. The average cost of all your houses is the center of this bell. It's called a bell curve because it's shaped like a bell coming in from the left.
 It slopes up and it peaks at the top, rounds out, comes back down, slopes to the right, and goes off to infinity. Looks like a bell standing up. So in the center is your average. This is called the mean. Your average of all the prices of houses in Portland, Oregon. And the reason it's so tall, the reason that's the peak of your graph on the y-axis, that is the number of samples taken from this distribution. That's the number of houses that are the average price.
 So naturally it's the highest. As you go off towards the left, you slope down, you start to reach the very cheap outliers, houses that cost a lot less than the average. And if you slope to the right, you start to reach the very expensive outliers, houses that cost a lot more than the mean. So the mean determines the central location of this bell curve and what's called the standard deviation determines the width of the bell curve. So if your data is wide.
 disperse if there is a huge range in housing costs you'll have a very wide bell curve wide and short we will have a very large standard deviation if you have a small range of costs so let's say the average is $200,000 and nothing goes above 250 nor under 150 then it'll be a skinny bell curve that's tall and that means you have a small standard deviation so that's understanding the bell curve it's a very simple distribution
 And again, so a distribution means how your data all falls onto the graph. That's what a distribution is. And taking a sample of your distribution is taking one little house out of your data and looking at it and looking at the price. Okay. So I sampled the data and this house is $172,000. So that's taking a sample of your data, the house that I took, it's called a random variable. You'll learn all these words when you're working with statistics. They use, they have all these terms for stuff when you're...
 working with distributions. So a probability distribution. And there's all sorts of probability distributions out there. Like I said, the most common is the Gaussian distribution or the normal distribution, aka the bell curve. There's also things like the Bernoulli distribution or the multi-noulli distribution or the normal distribution, et cetera. Different graphs, they look different depending on how the data is formed. And like I said, I think this is one of the most magical.
 and common functions of the universe that you'll see. It's actually a mathematical function. You can actually say like f of x equals, and then this big mathematical equation on the right. I'm not gonna list it out for you. It's actually quite complex. You look at this bell curve, and you're like, okay, the most common distribution in the universe, it's gotta have a very simple formula. It's actually pretty complex formula. You'll see it in your dealings with statistics. But anything that falls outside of the norm, okay, that's why we have this word the norm, falls outside of our normal,
 distribution over here on the left or over here on the right. Very, very, very expensive houses. I'm talking a million, two million dollar house or over here on the left, a house that costs one dollar. Okay, those are outliers, big outliers. Really, once you get past your standard deviation, your typical widths on the left and right, you're getting into outlier territory. But when it comes to the anomaly detection algorithm, we're more concerned with mega outliers, things that fall really far on the right or the left. So how does this system work?
 Well, you just build up a graph, your normal distribution, representing your data. So how do servers typically act? Here's CPU, here's common network usage, here's common disk usage. And you build up a normal distribution, now a normal distribution that I've been explaining so far is in two-dimensional. You got an X and a Y axis. But you're typically going to be working with multi-dimensional data. And it's hard to envision a multi-dimensional normal distribution. So if I say in 3D, it looks a little bit like a hill.
 And once you get into 4D, it's a little bit, you can't really envision that. But you're gonna be working with multi-dimensional data and you're just simply gonna find outliers. Now, the learning bit of anomaly detection is this little variable you're gonna learn called Epsilon. And Epsilon basically is the threshold that you're going to learn. Like if you have a small threshold or a large threshold, it basically determines how sensitive your system is when working with outliers.
 And what you'll do is you'll train your algorithm on normal stuff, stuff that's not anomaly, and then you will test it against anomalies. So for example, they say that your training data will consist almost entirely of your negative examples, and then your positive examples will be split mostly between your validation and your test set. And the reason for this is we want the system to be able to learn what's typical of your data, and then you're going to basically be using your out...
 liars to fine tune the hyper parameter of the anomaly detection algorithm, which is called your epsilon variable. And that's it. It's so simple. Honestly, I feel like I've talked too long because I don't want you to overthink this. It's so simple. All it is is coming up with a bell curve in one or two or three dimensions, however many and figuring out what is a mega outlier. That's it. And the learning process is learning. Where is your threshold? How sensitive are you to outliers? So for example, let's think about.
 a classroom setting where you have a bell curve of grades where the average grade is 80, a B. And so 80 is your mean and so that's the number where on your graph on the x-axis, the peak of your bell curve is. So the very top of your bell curve, you put a vertical line right down the center and that number is 80. You go off to the right and you've got a bunch of B plus A minus A A plus. So people who are getting A plus are on the far extreme to the right for
 example, and then there's a smooth curve connecting to our mean and coming down to the left and going towards people who are getting B minus C plus C, C minus and D. Now let's say that that's basically our bell curve. And we have one student who got an F. That student falls very far outside the typical population of students. Therefore, that student is more than just an outlier that student is an anomaly. As far as the teacher is concerned, and so the teacher needs to go talk to them.
 that student see, you know, what's going wrong and how can the teacher help the student et cetera. So something that falls basically doesn't look anything like the rest of our population. It falls outside that bell curve in a significant way. That's the anomaly detection system. It is simple statistics, Gaussian distribution. So let's move on to recommender systems. Recommender systems are one of the most common applications of machine learning in industry. And I think they were one of the earliest applications of machine learning
 especially in e-commerce and online companies. And recommender systems are exactly what you think they are. So for example, Amazon, Pandora, Spotify, Netflix, they all recommend to you a product or a song or a movie based on things that you've liked in the past. They're basically ads if you think about it. It's just an ad system, but we call them recommender systems. And there's two very common flavors of recommender systems. One is called content filtering.
 and another is called collaborative filtering. So two common recommender system algorithms and they're very, very related. They use the same underlying principles which we'll get into in a bit. In order to show you how these things are different and similar, I'm going to use two popular internet radio products. One is called Pandora and another is called Spotify. Now by the way, I believe that Spotify has gotten a little bit more complex with the way they
 work so I may be misrepresenting the way that Spotify works. This is just how I understand Spotify to work. So pretend that it works this way. Bear with me. Let's start with Pandora. Pandora internet radio. Pandora is a website where you can listen to some music and you can like or dislike some songs. And it will play songs that are similar to things that you've liked and dissimilar to things that you don't like. When you...
 go on to Pandora and when he first start it says, seed a song or an artist. Like give me something to start with. What do you like? And I say I like Ryan Adams. He's my personal favorite musician. And it says, okay, I'll give you a few Ryan Adams songs. Listen to that, three minutes, another three minutes. And then the second or third song that comes on is a new song. It says, you know, based on the fact that you like Ryan Adams, I think you will like over the Ryan or Ava at Brothers. So it plays a new song for me. And I can choose to either thumbs up it or thumbs down.
 it. And as you thumb your way through these songs, Pandora learns your musical preferences. It's got this thing called the Music Genome Project. The idea being that all these workers at Pandora have determined for every single song in the Pandora database. Various attributes about these songs. Things like male versus female vocals. Is it instrumental? Is there some bass? Is there guitar? What genre of music is this? What sub-genre of music is this?
 And they've tagged all the songs in the database with various tags. When you like or dislike a song, it sort of throws in these tags to the dislike pool. And these tags into the like pool, it comes up with the sort of venn diagram, conceptually, of the types of songs that you like to listen to. But that's not how it works underneath the hood. The way it functionally works is by way of this thing called content filtering. And content filtering, you're going to love this, is very easy to understand, because you already know the algorithm that you would use for content filtering.
 It's one of the algorithms we learned in the very early days of this series. Let me give you a hint. What we're trying to do here is determine a score for a song in the whole database of songs based on your personal user preferences. A score is a number, usually when you're working with recommender systems like on Netflix, you might give a movie anywhere between a one and a five star rating. So a score between one and five. So it's a number, it's numerical, it is regression.
 In the case of Pandora, it's a thumbs up or a thumbs down. So it looks a little bit more like classification, but I imagine under the hood Pandora probably does some sort of conversion of your thumb to a score, for example. So we're working with numbers. We're trying to predict the number that would go along with this song in the database, given your user preferences that you have built up over time, given your theta parameters. This, my friends, is linear regression. It's just like trying to predict...
 the cost of a house. We've looked at hundreds of houses and we've come up with a bunch of theta parameters for predicting the numerical output. So that when you throw in a new house to the mix that it's never seen before, but it can look at the number of bedrooms, the number of bathrooms, a square footage, this is downtown, et cetera. I'm going to guess $175,000 using my theta parameters of this linear regression model. So content filtering, parameter systems work exactly the same as linear regression.
 We throw in a new song to the mix and I've already learned all these theta parameters by Thumbning my way through these songs this new song gets thrown into the linear regression Algorithm and outcomes a score on a scale from one to five or Zero to one or whatever a guess as to whether the user will like this song or not So it is linear regression There are some very subtle differences that you'll see when you take the andruine course between linear regression and Content filtering, but it's
 So minor. So there you have it. The first algorithm you ever learned from machine learning, linear regression is used for recommender systems also. Content filtering. Now there is a problem here. The problem is this thing called the Music Genome Project. The Music Genome Project is this process where workers at Pandora were tagging every single song in the database with various tags. Hundreds of tags. I think even thousands of tags. Tags that represent various features. Remember features of what you were.
 working with when we're talking about machine learning on these songs. That is very time consuming and that is often very difficult to do, even if you have the time on your hands, you would have to listen to each song one at a time and you'd have to have a lot of musical knowledge. You have to know how to apply these features to the songs, what you're looking out for. Usually, we don't have this kind of time nor the expertise for tagging every piece of content in the database and especially as new content is
 added to the database constantly, constantly. In fact, I do not think that this music genome project at Pandora is being applied manually anymore at all. I believe that Pandora Internet Radio is using a deep learning, audio analysis machine learning algorithm to determine what features are associated with new songs as they come in, and auto tag them with these features. I don't think there's any human listeners there anymore. I believe there are robot listeners.
 deep learning algorithms. But let's pretend that you can't even do that. Or that's too difficult for you given your circumstances. So for example, for example, Amazon as new products are being constantly added to the database, you don't really tag them with features why you're interested in or what a user might be interested in is what other users have looked at or liked or purchased. So this is to my understanding how Spotify works, the competitor to...
 Pandora internet radio. And like I said, don't quote me on it. They may have changed their algorithm since. But the way I understand Spotify to work is what's called collaborative filtering. It is users also liked. So not similar songs based on various features about the songs, but users also liked. And Amazon and Netflix, and in fact, I would say the majority of recommender systems about their use collaborative filtering.
 rather than content filtering. Users also liked rather than similar items. Now the goal of users also liked is to determine similarity between products or songs or movies. It is indeed trying to figure out similarity between products to recommend to you, but it is not by way of attributes of the product itself, but rather by finding users who are similar to your purchasing preferences.
 figuring out what they also liked. So users also liked is the collaborative filtering algorithm. And I'm gonna kind of hand wave my way through this algorithm. You'll learn the details in the Andrew In course, but the way it works is really kind of nifty. It is linear regression yet again, but you don't have features of the product to work with in building up your Theta vector for each user. You don't have the features, you don't have those numbers. So you're gonna have a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a
 So you're going to build up those features yourself. So now I'm gonna switch examples to movies, Netflix, for example, to make this a little bit clearer to describe. Let's say that user A is interested in fantasy movies, maybe some sci-fi stuff, he doesn't like romance, he loves action, and user B loves all the romance and the thinkers, doesn't like action, hates fantasy and sci-fi. Okay, so we got basically two opposite users. And in the database, if we were gonna use the content filtering, recommender out-
 algorithm, well the database would be pre-populated with how much fantasy and how much romance and how much Thriller is associated with each movie, but since we're moving on to collaborative filtering Recommender systems we don't have those features every movie is a blank slate. It is an empty vector of Features now we don't even know what to call these features We we don't know that we're necessarily going to be learning the amount of sci-fi or the amount of thriller We're just going to allocate let's say
 10 features for every movie. And when user A rates movie A, and then user A rates movie B, and movie C, and movie D, we start to figure out the common theme of what this user likes. So we can start to build up a numerical representation for each movie vector. It is actually linear regression in the reverse. We are using basically the movies are learning their theta parameters from user scoring system. Then we're going to use those learned features for each movie vector. .
 guess what users will like. So then we switch again the other direction. What we have is linear regression flipping sides over and over. Users rate movies. From that movies learn their features. From that it will recommend movies. Users rate those recommendations. From that the movie learns its features, etc. So we have this infinite loop of linear regression between the movie.
 and the users. And it's really pretty magical. It's really cool. Now it's an infinite loop of linear regression. So you might imagine just constantly, you know, in code you have this running loop of two linear regression models. But that's not so actually what you can do is you can put both linear regression models into the same equation. There's a little trick where you can put two linear regressions into one into one mathematical formula. And now you have one
 model and that is your collaborative filtering recommender system. And there you have it. That's recommender systems. So two spins of recommender systems. One is called content filtering and that's the preferred one to use when you can and do have features tied to your content in the database. Do you have a bunch of information associated with each book or movie or product? Yes, use content filtering. If you don't have that information and you want to learn that information.
 by way of your users rating or purchasing or even just viewing products. You can use this twists to the system called collaborative filtering, which is basically like an infinite loop linear regression between the content and the users and is best thought of in my opinion as users also liked. So content filtering is similar items. Collaborative filtering is users also liked. Now all of the above, both of these are intended to be similar.
 items, they just work off of different principles. Now a little bit about the name, the reason they call it content filtering is because you have features on the content itself. The content itself has features and you're using that to filter down things that users will like content filtering. Whereas collaborative filtering is your content is collaborating with the users and the users are collaborating with each other to help make recommendations. Your system is very simple. Now I want to make it
 distinction clear. We talked about an algorithm in a prior episode called a market basket algorithm or an a prior algorithm for determining what items go typically with what other items. So for example, if you're in a store and you buy marshmallows and graham crackers, what should you also buy with that? Well, the answer is Hershey's chocolate. This is different from recommender systems. A recommender system finds similar items. In other words, if I like
 Kirchys chocolate, I might also like a Snickers bar or a Kit Kat bar. It's basically something to replace what you have in your hand. I ate what I had in my hand. I liked what I have in my hand. Give me another one of those, something similar to that. Whereas market, basket, or op-priori algorithms are for finding things to go with what you have. I bought diapers. I also want beer. And finally, I'm going to explain an algorithm called Markov Chain.
 M-A-R-K-O-V. And this may seem a little bit weird and out of place if you are a machine learning engineer and you're familiar with all the educational material and you're just kind of listening to this for fun. Including Markov chains here may seem a little bit of a weird one. It's not one of those 101 machine learning algorithms that you typically get exposed to. It's actually one of those algorithms that you get exposed to later down the pike. Especially when you're starting to dive into reinforcement learning.
 territory. Now, that's not necessarily so. There you can use Markov chains in various aspects of mathematical decision making. Maybe it's a little bit more common in like operations research, for example, or control theory. But it is a machine learning algorithm. You may see it from time to time, but you're not very likely to see it as a newcomer to machine learning. It is not a 101 algorithm. So why am I explaining it now? The reason I'm explaining it is because it is brought up a lot.
 From time to time you're going to see Markov chains and Monte Carlo or Monte Carlo Markov chains MCMC. You're gonna see them these words thrown around all over the place and for me personally when I first started getting into machine learning and I never saw these algorithms Described in my books or my videos or Andrew Eing or anything. I felt like I was missing something. I was very curious what Markov chains are at least in principle So I'm explaining Markov chains really just to kind of ease your mind explain something
 that you're going to see from time to time, but tell you now that you're not going to be using Markov chains any time soon, you're going to probably see this stuff once you get into reinforcement learning. It is the core algorithm of artificial intelligence. It's the base, the foundation of reinforcement learning, upon which you will use deep learning principles and stuff to go deeper and deeper, but Markov chains are the linear regression of reinforcement learning. So let's explain how
 they work. I'm gonna get a little bit dramatic here bear with me. Imagine you're playing a video game and you're a barbarian and you have just stumbled on the last boss in his layer, the Dark Lord and there's flames and darkness everywhere and the Dark Lord turns to face you with his flaming demonic eyes and he says, come here, and you grin darkly and you buckle down and you start running and he throws fireballs and dark spells at you and you don't left and right casting gas leaks.
 shadows in your wake and you make one giant lunge and you bound off the ground leaping into the air, unsheathing a great sword from your back, twirling it in your hand, locking it into place, coming down for a blow. The Dark Lord yells, awww, and he's springing up his arm for defense and freeze. I'm the narrator of this game, and I step into the scene and I say, barbarian, Dark Lord come with me, and they both turn to me, and they follow me into the game room, the game room. Where I have them both sit down. Dark Lord,
 Barbarian, please, they sit down. And I use my laser pointer to point to the scene that they were just in. The Barbarian is suspended in the air with his great sword coming down for a blow. And the Dark Lord is bringing his arm in from the left to block, deflect the blow. And I say, Dark Lord, what do you think is gonna happen here? And he says, oh, I don't know. It's complicated. Yes, yes, yes, but what's the probability of the Barbarian landing a blow? I mean, you have your arm coming in from the left, you're gonna block. He's over here with his trajectory.
 and his velocity in all this. The Dark Lord scratches his chin, he looks at the barbarian, the barbarian looks at him and shrugs, and he looks back at the scene, and he starts to assess the history of this scene. Well, he jumped through the air after he was dodging left and right from my magic spells. And I cut him off and I say, Dark Lord, Dark Lord. The magic spells don't matter. That's all bygones. At this point, he's in the air with his sword coming down at you. Does it matter that he dodged your fireballs and your dark spells?
 In fact, Dark Lord does anything in history matter up into this point. And the Dark Lord scratching his chin and he's looking at the picture and there's this like blue trajectory showing the path that the barbarian had taken up into this point. It arcs through the air in his jumped trajectory. Comes off of the ground, zigzags where he's dodged those spells and stops where he had entered the room. Surely that trajectory is important. The Dark Lord said...
 but if I look at this blue line, I consider his coming to it and I say no no no dark lord. Never mind the blue line. History doesn't matter up into this point. All that matters is now. All that matters is now. This is called the Markov Principle. Allow me to explain. The player's history up into this point that has made the player a barbarian is Absolutely essential for this battle. The player has leveled up gained in strength and agility. Fought various bad guys.
 gained experience both as a player and as a role-played barbarian. He's learned some spells, he's earned some weapons and armor, so the player's history certainly should make a difference in the outcome of this battle. But we can boil all that history down into a handful of numbers, level, strength, agility, experience, etc. In other words, we can erase history if we know the present, or rather we can...
 boil all of history down into the present. This is the Markov principle at play. The Markov principle says history is irrelevant for determining the future. And in certain models, in certain circumstances, and this may be just one. Now that all seems fine and dandy, the players history is condensed down into their level strength and agility. But what about this trajectory that is sending the player down onto the Dark Lord to deal a strong blow?
 that depend on this sort of blue line tracing them through this dark lords layer, leaping through the air and onto the dark lords. Don't these steps in history matter? Yes, they do, but they can all be boiled down into a handful of variables of the present as well. So imagine this blue line going, to get sucked into the user, a flash of blue light. And now you have an arrow pointing downward toward the dark lords coming out of the sword, after a passing maybe.
 Trejectory and velocity. And of course, we also have those variables strength, agility, level, et cetera. So you can boil all of history in this video game, including the last few milliseconds down into the present, only the present. Trejectory, velocity, acceleration, strength, agility, et cetera, a handful of variables that represents the present state, we call it state. And the dark lord has some state, himself.
 He has the trajectory of his own arm and how strong is he? Can he make it the defense? There's other things going on in the environment. Maybe the Dark Lord has some minions that are currently shooting arrows at the hero. So the history only matters in so far as it can be boiled down into the present. And that is the Markov principle at play. The Markov principle says that the next, the Markov principle says that the next state only depends on the current state. So the next state depends on the current state.
 And the next state in a Markov chain is a series of probabilities, a series of probabilities chaining their way through an action sequence. So for example, in our current state, the state being comprised of the user's level where he is acceleration, velocity, trajectory, etc. Combined that with the dark lord's position, his arms trajectory and velocity, the configuration of minions and all those things. That's the current state. The next state can be determined by the...
 current state and some probabilities. Let's say that given all of that information, the probability that he delivers a blow is 80%. The probability that he is blocked by the dark lord is 10% and the probability that he misses is 5% and the probability that he gets clipped by a minion's arrow is another 5%. So we can use these probabilities, these percentages, to determine how likely it is for each possible next state. We're in a current state. Imagine drawing a circle around...
 our current state and drawing four arrows out of that current state. The number next to the arrow is the probability of it happening and the circle in that next state where the arrow points is the state itself. So we have one next state being delivers a blow. Another next state being clipped by an arrow, another being blocked by the dark lord and another being miss. A mark-off chain then is a sequence of actions in time. You start in the beginning of the video game. You have some new clock bell-pulling here. Now this would be a
 of maybe going left, going right, going forward. You have a probability if you went forward into going forward again, or waiting there. If you go forward again, you have a probability of picking up the sword, or you can ignore the sword. A sequence of actions all pointing to each other and leading eventually to the end. These actions can loop back on each other, what's called a cycle, which makes a Markov chain a graph. A graph in computer science, you're going to see these a lot. A graph is a sequence of things that can loop back on themself.
 If there is no such loop, it's called a tree, just like a decision tree from a prior episode. Remember I explained a decision tree similarly, you have a circle, and out of that comes some arrows pointing to next circles. This decision tree goes like this. If my mother's in town, we go over here, okay? If she's hungry, we go to this restaurant. If she's not, and she wants some drinks, we go to these bars. Okay, if she's in the mood for this, we go to that bar. If she's in the mood for this, we go to that bar. So a decision tree looks very similar to a mark-off chain. There's some differences in mark-off chain.
 as a graph so you can have some cycles. And importantly, a Markov chain is basically a sequence of probabilistic actions in time. It's a process, a system, a running loop of action sequences, if you will. A decision tree looks like a process or a system, but it's not really, you input some inputs and outcomes and output. It's a regression algorithm or a classification algorithm. It comes your input, snap your fingers through the decision tree and outcomes a number.
 or a classification. Whereas a Markov chain or a Markov decision process, as you'll see in reinforcement learning, is a sort of running loop, a sequence of actions. It's something that may determine the actions to take by an artificially intelligent agent, for example, in our video game analogy, or it may describe the action sequence taken by customers or components in a system. So it's more of like a time-based system or process, sequence of actions.
 Whereas the decision tree is a simple algorithm for computing an output. So now I ask the dark lord again, so what happens next? And he says, oh, now I know. Given this current state with the trajectory and velocity of a sword, he's going to deliver the blow with probability of 80%. That is my final answer. And I say, yes, dark lord. Good job. Now, that's a mark off chain. I'm going to do one more example. This example comes out of the mathematical decision making great course.
 a series that I recommend over and over. And that goes like this. Let's say that a restaurant is modeled by a Markov chain, a group or an individual or whatever can enter the restaurant. Okay, so that's the first state. The next state is they are waiting for a table. You have a 50% probability of moving on from there to being seated by the hostess or a 50% probability of looping back onto yourself. In other words, you are still waiting. Okay. This Markov chain and
 we may be going back and back and back and back into our current state, which is simply waiting. And then eventually we leave that state with probability 50%. So basically it's kind of like the time it takes to leave a state. So like the time it takes to stop waiting and to get seated kind of is based on the probability. If it's let's say 99% probability of staying in the waiting period, then you're going to wait for a very long time with a 1% chance of exiting that and being seated. But it's 50-50 then you won't wait for very long.
 You get seated, you're at your table, you're in a new state, state three, you can order or you still need time to decide. So you have a loop back to your current state. You loop back, you loop back, you loop back, you're trying to decide. And finally you decide and so you call over the waiter and you order. That brings you into a new state of waiting for your food, etc. So you've got this graph of circles representing states that you could possibly be in at a restaurant with probabilities of exiting your state into a new state. That's a mark off chain. Mark off.
 chain is a little bit different than the other machine learning algorithms that we've seen so far in that it represents a sequence of actions. That's what a Markov chain does. Now, we've seen linear regression, logistic regression. These are like snapping your fingers and coming up with an estimate. Somebody says, what's this house going to cost? And you snap your fingers and you say $175,000 next. Those are different than a Markov chain, which is you enter a system, a system or a sequence of actions. And you take...
 various actions depending on the probability of taking that action in a system. And you navigate your way through a system, through a Markov chain. Now a Markov chain can be represented in a matrix form, a Markov matrix. And that's just a mathematical convenience for representing the chain, the chain that I was describing with circles and arrows. That's for visualization purposes. It makes sense to look at on a piece of paper for us. But the way you'd actually rip.
 this mathematically or in a computer is with a matrix, just like almost everything with machine learning. The way we represent most things in machine learning mathematically is in matrix form. That's why linear algebra is so important. Now, you might use a Markov chain in operations research or mathematical decision making, sort of to come up with an assessment of how long it takes for a user to maybe leave a restaurant. Or where is the bottleneck in your restaurant system? Where do we...
 we find users staying the most when maybe we want them to leave. Or where do we find a state where they haven't yet paid for their meal and the probability of leaving, so the next state being leave, is higher than we want. We can use a Markov chain to sort of model a system and kind of assess it, figuring out what's going wrong, figuring out where bottlenecks are. Now how do we come up with these probabilities in the first place? Well, we do that just through...
 observing data. We have years and years and years of users coming in and doing various things. Now let's pretend over there in the corner is some statistician and he's writing on a clipboard. Every time a user leaves a table or every time a user orders food. And so eventually can come up with some sort of probability of a thing happening next, given the current state. And again, the thing that makes this Markovian, the thing that makes this a Markov system, is that no current state depends on the...
 the prior states. We have all prior state information is encapsulated in the current state. The fact that I'm sitting at a table must necessarily mean that I waited at some point for a table and that I entered the restaurant. All prior states, all the history, can be boiled down into the present. So that's a mark off chain. Now let's go back to the game room with the Dark Lord. And we ask the Dark Lord, is this player going to win? The Dark Lord looks at you and he looks at the barbarian.
 and he looks back at you and he's like, I don't know. That's too loaded of a question. There's too many variables at play. You cannot analytically use this Markov chain to come up with a simple numerical output, probability of the user winning. When there's too many variables at play in a Markov chain, instead of using some mathematical formula to snap our fingers and come up with a judgment, we use what's called a Monte Carlo simulation, a Monte Carlo Markov chain. Monte Carlo is basically just...
 running a series of simulations over and over and over until we get sort of a what's called an expectation and expected value. You'll see a lot of this when you get into reinforcement learning, running simulations over and over and over until you get what you think to be the most probable answer. They call this in mathematics when you are solving an equation or a system of equations, snapping your fingers, coming up with a judgment, they call this solving an equation analytically, analytically. So when you can basically come up with a mathematical formula...
 that takes an input and outcomes output and you can just snap your fingers and boom, you're doing something analytically. Well, when something is too complex to do analytically, then we might try different approaches to it. We might try like iterations for loops, for example. And in this case, in the case of a Monte Carlo simulation, we're going to basically run simulations of this player playing the video game over and over and over and over and over until we basically start to convert.
 converge, they call it when you start to see a pattern, onto what we think is going to be the expected value, the expected value being the final score. What is the average final score that I'm starting to see over time when I'm simulating running the user through this video game, okay? So a Monte Carlo Markov chain, let's take it from the top, here's how it happens. The user enters the Dark Lord's layer and the Dark Lord turns around and says,
 Comhero and the barbarian buckles down with a dark grin Unsheathes his great sword turtles it around in his hand locks it into place and then 500,000 ghost images of the barbarian run in different directions One goes left one goes right one goes forward another is already dodging left dodging right dodging left gets hit by a fireball and dies Disappaints into the air the other one is jumping into the air yet another one is blocking a fireball with his
 All 500,000 simulations ghostly heroes are running through the scene, killing the Dark Lord, being killed by the Dark Lord missing, getting clipped by an arrow, kill, kill, die, die, kill, kill, kill, kill, and finally we start to see a score of 7, 9, 3, 2, 6, 8. That seems to be the converged expected value of this Monte Carlo Markov chain simulation. So the chain, the Markov chain is if I'm in this state.
 What happens next? Well a probability of 0.9 this and 0.1 that the Monte Carlo part is running through a bunch of simulations Now it's a little bit complex the way basically one of these ghost heroes Runs to state 2 and now he has to decide is he gonna run to state 3 4 or 5 Well, we pick the thing with the highest probability with that probability and sometimes we pick the things with lower probabilities This is kind of that Monte Carlo process and there's this whole system to this there's this algorithm called the
 of a Trapless Hastings algorithm, a random walk, et cetera. But basically, that's MC, MC, Monte Carlo, Markov Chains in a nutshell, is that you have a chain or a graph of states and next states. They're all determined, probably, holistically, given your current state and running through that whole series with simulations is the Monte Carlo bit. Now, like I said, you're going to be seeing this Markov chain in reinforcement learning and artificial intelligence.
 We're going to see that there a lot. There it is called a Markov decision process, M-D-P. And this might be, for example, the algorithm that sits in the core of the brain of the Dark Lord. The Dark Lord is the artificial intelligence that is fighting you. You are the player. The Dark Lord is the AI that's fighting you. It's not the game that's fighting you. It's the Dark Lord. The game is basically the physics system, the rules, and the Markov chain.
 The thing that says given state one, what is the probability of being and what is the probability of transitioning to two, three and four? And then the dark lord needs to act upon this decision given probabilities in order to determine what is the best next action to take. So there you have it, MCMC. And in the resources section of this episode, it's just gonna be a link to week nine of Andrew Eings Coursera course on anomaly detection and recommender systems, very simple. And that ends this.
 series on shallow learning algorithms. The next episode is going to be the most boring episode of all. It is going to be basically the parts of machine learning algorithms, things like performance evaluation, under fitting and over fitting, regularization and all that really boring stuff. It's all unfortunately essential in machine learning. You're gonna learn this stuff actually really early on elsewhere, like in the Android.
 in series. I've decided to punt on it for a while because it's just so boring. I want you guys to have dessert first and vegetables later, but we'll get that episode over with and then we'll move on to the fun stuff. I'll see you guys then.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 15 performance. This episode is going to be the last of them.
 basics of machine learning, my friends, at this point you have now gone through all the very fundamental introductory components of machine learning, shallow learning, the basics of deep learning, a little bit of the math, the technology. And finally, this episode where we're gonna talk about performance evaluation and performance improvement. You may have heard about things like high bias and high variance, overfitting and underfitting, regularization, and then accuracy, confusion matrices, all these things.
 This is going to be a very boring episode. Unfortunately, I apologize. You're gonna want a coffee for this episode. Let's just get it over with. And at the end of this episode, you will be an official machine learning Patawan, my friends. You have come far applaud yourself. So let's do this episode so you can call yourself a machine learner. In this episode, we're gonna be talking about performance, performance evaluation and performance improvement. Now, performance is basically grading your machine.
 learning algorithms. How well did your machine learning do at the task it was given? So for example, classifying spam, guessing, housing costs of Portland houses, calling things, cats, dogs, and humans, all of the machine learning applications that we've talked about in prior episodes, we want a measure to determine how well machine learning algorithms are doing at performing their job. Now, you may be thinking, wait a minute, we already have a performance measure. Don't...
 And we had this three part machine learning sequence. We have predict error functions, okay? And then learning. Now we learn based on these error functions. And the error functions tell us how poorly we are predicting so that we can improve upon those predictions by using the learning step, by using calculus. That's true, this error function or this loss function that exists in every machine learning algorithm can be used to measure the performance of the algorithm.
 so that it can improve itself. However, the loss or error function of a machine learning algorithm differs from machine learning algorithm to algorithm. Each error function is different for these different algorithms. And that's because these algorithms function differently. The learning step of each algorithm, the calculus component is different, depending on the structure of the algorithm, or when we're dealing with neural networks, we have this back propagation process that uses gradient descent.
 at each neuron in a backwards pass through the network, which is totally different than decision trees, which uses the thing called entropy, how chaotic is all the information at any given level of the tree, which is also different from logistic regression, which uses cross entropy. So every algorithm has a different error function, and these error functions are like subjective personal error functions. It's basically a personal gauge on how well you're doing as
 a algorithm and by which you can improve yourself. Whereas the performance metrics that we're going to be talking about in this episode are more of a universal grade, like an A, B, C, D, F grade, that you can compare machine learning algorithms against each other. So this metric that we're going to be talking about is called accuracy. We'll talk about that later. We're going to get into the analogy first. This metric accuracy is very similar to like a final grade at the end of a semester.
 by taking a final test and getting back a percent score from zero to 100 percent, and you can use that grade to compare algorithms against each other for a particular task, or you can use that grade to basically inform a single machine learning algorithm as to how well it's doing overall. So let's use this analogy, taking a class. Let's pretend that this class, this course at a university is the spam detection course, okay? And remember how we said that various machine learning algorithms.
 can be used for various purposes. Some are very specific, such as anomaly detection algorithms and recommender systems. But some are a little bit more universal, such as naive bays, decision trees, neural networks, and support vector machines, these power algorithms. So we may have a situation, such as spam classification, for example, where we can use any number of algorithms. Well, we're gonna put all these algorithms into a classroom, a class that is the detecting spam class.
 at a university. And we're going to give them textbooks and assignments and quizzes and tests. And at the end of the course, they're going to all get a final grade which tells them how well they did. And we're going to use that grade ourselves to determine which of these machine learning algorithms we will select for the purpose. Who got the highest grade in the class? The learning phase in this class is these students, these individual machine learning algorithms trying to learn what it means to be spam when we're looking at email.
 What makes an email ham that is not spam versus spam? So they're going home and they take home these assignments and they're reading the textbook and they're crunching and crunching and crunching. This is the training phase of machine learning. And each of these algorithms has its own metric that it uses to figure out how well it thinks it's doing in the class. So we've got one learning algorithm, naive bays, and he's a father of three kids. And he has a really taxing job and he comes home and
 every night and he studies and he studies and he gets so little sleep and he listens to lectures on his iPod on the way to work and he studies in the break room and he's just beating himself up. He's like, I don't have enough time to study. I'm not going to pass this class. He's beating himself up and he takes a test and he gets a beat. He's so surprised. Oh my gosh, I'm doing pretty well. But I could improve. It looks like I maybe need to focus on some of these and not focus on some of these other things. So he's got this metric that he kind of compares himself against. That's his loss function, his error function. So he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that he's got this metric that
 It's basically his own subjective personal error function for how well he's doing in the class. And then we have another algorithm over here, algorithm B, logistic regression, and he's a gamer, and he's playing games. He's like, I'll get this class, I'm fine. And one day he actually studies an hour out of a textbook and he's congratulating himself, oh my gosh, I'm doing so much better than I usually do. I am going to nail this class. So he's got his own performance metric that he's kind of comparing himself against. Well, the end of the class rolls around, they take the final test, they get the...
 their scores and guess who did better the father of three kids. So they each have their own loss or error function, their own personal subjective performance metric that they use to tune their learning process. And then there is a final grade which is this performance evaluation metric that we're going to talk about things like accuracy and an F2 score and all these things that will basically inform objectively or universally how well algorithms are performing against each other or how well the algorithm did in general.
 at this class, at this particular tasks, which in this case is spam classification. This performance measure, the subjective performance measure, that measures algorithms against each other. We call this accuracy. And it is simply how many questions did you get right over all, over all the questions available? Very simple, accuracy. In the case of classification, spam classification, how many emails did you correctly classify as spam or correctly classify as ham over how many total? Accuracy.
 Very simple. It's not always that simple. Sometimes the task is a little bit more complicated. For example, let's say the task is classifying cancer. Not spam, cancer. Well, cancer is, let's say, 1% of the population. Whereas spam may be, well, it's not actually 50-50. Bless pretend it's 50-50. Well, classifying spam versus not spam takes a little bit of know-how if it's going to be 50-50-50 breakdown. But classifying cancer.
 is non-cancer can be done with a very simple learning technique. Learning to always classify not cancer. Why? Because if cancer is only 1% of the population, then you will be 99% accurate if you always classify a patient as not having cancer. So the accuracy measure of number of corrects over number total will not do in this particular case. So for the spam classification course at University.
 that are machine learning students we're all taking and then we measure them all against each other. The key to the final test that is used is going to be a different key than in the oncology class where we're learning to classify cancer. There's going to be a different key that the professor uses to grade the final performance evaluation of the machine learning algorithms. So we don't use accuracy in these edge cases. So what we do varies on a case by case basis based on the machine learning task.
 at hand. And what we have are these two measures that we can balance against each other given the circumstance. Okay. And these two measures are called Precision and Recall. Precision and Recall. Precision is when you're making a guess, do you always make a perfect guess? And then Recall is kind of like how wide do you cast your net? So I think of the definitions of Precision and Recall by way of an analogy to a video game I play called Robo.
 recall where recall is in the very name of the game and in fact you do have these little measures precision and recall at the end of your gameplay. Precision is like how precise you are. That's the definition of precision. How precise you are. How much of a straight shooter you are. Do you never miss a shot you take? Is every shot you take a perfect headshot? Do you never miss a shot? Then you have perfect precision. The higher the precision, the better
 shots that you take. You never miss a shot you take. High precision. You are a precise shooter. You are a straight shooter. Now the purpose of the game, the story is that there's all these robots and there was a virus and they're all turned evil and you have to kill all the robots. It's called Robo Recall because you work for the company that made these robots and so it's your job to recall the defective products, right? You're trying to recall the products, which means you're going out into the field and you're killing all the robots. So the amount of robots you...
 kill is the amount of robots you recalled, the amount of defective products you recalled back to the factory. So your precision is how sharp a shooter you were. You always make the headshots and the recall is how many robots you kill in one level and they're balanced against each other. I don't care how good a shooter you are. You can't possibly have perfect precision, never miss a shot and perfect recall kill all the robots. So there's some sort of balance. You want maybe a
 a decent amount of precision where you kind of don't miss too many. And a decent amount of recall where balanced against the precision, you're killing a lot of robots. So that's what we're getting at here. Precision and recall is you're trying to balance some level of how precise you are when you make your shots versus how many things you get right. Overall, no, let's bring this back to the spam and cancer analogy. In the case of cancer, you want high recall. You want to throw a wide net. Why is
 that because you are okay maybe falsely classifying some people with cancer because you never want to miss the people who do have cancer. You would rather make false positives than miss any shots. So you want high recall in the case of cancer. You want to cast a wider net. You're okay accidentally classifying people who don't have cancer as long as you always classify the people with cancer. In the case with spam you want every shot you take to
 to hit a spam email and you'll let some spam come into the inbox at the expense of casting a wide net. Why? Because if you cast too wide of a net, you might accidentally send ham legitimate email to the spam box. That would be very unacceptable. You would rather the user get a few spam emails per day, let some through low recall rather than accidentally send non spam to the spam box. So every shot you take you want to...
 headshot your spam emails, high precision. So different edge case machine learning situations call for a different performance measures. The general kind of catch all measure we use is called accuracy. And if your situation is a little bit edge case, then now we have the sliding scale. Let's say we have a slider with a knob, and we can go all the way to the left with precision or all the way to the right with recall. And there's some maybe sweet spot for this particular.
 algorithm where you slide that knob until it's just so and it's really good for your situation. And let's say that that standard performance measure that we call accuracy is where the slider is right in the middle. So accuracy, precision, and recall. And then there's this other metric called an F score and then there's different types of F scores, F1, F2, etc. A common one that's used is F2. So an F2 score, which sort of measures the balance of precision versus recall. It's kind of a mathematical equation.
 gives you the location of that knob on this slider that I'm talking about. When you slide it left, you go Precision, you slide it right, you go Recall. The F2 score kind of tells you where that knob is on the slider. And so you'll use that for different situations. Other things you'll see in this space of performance evaluation, when you're reading around one thing is called a confusion matrix. Okay, when you classify spam and ham, you can make true and false negatives and positive.
 Okay, four options. You can make a true positive, a false positive, a true negative, a false negative. When you're making your guesses, when you're classifying spam. So a true positive is when you correctly classified spam. A false positive is when you said it was spam, but it wasn't. A true negative is when you said it was ham, and it was ham, and a false negative is when you said it was ham, but it was spam. So four options, and you put them together on a grid.
 a 2 by 2 matrix, and we call this little matrix a confusion matrix. It doesn't have to be 2 by 2 if there are multiple classes that we're trying to identify, such as cat dog and human, then you'd have a 3 by 3, etc. And this is kind of like the key that your professor of the spam class will use at the end of the class to grade your final test. This confusion matrix is how your professor will sort of visualize the things that you got wrong and the things that you...
 got right. Now, you don't use this performance evaluation metric, whether it be accuracy or an F2 score or whatnot. You don't use it to train your algorithm. You use it to grade your algorithm. You use your algorithm's error function to train the algorithm. So, your algorithm has taken the class and he's adjusting himself based on the class assignment grades and the quiz grades and all these things and his life decisions.
 and does he need more sleep, et cetera? Improves himself, improves himself. And at the end of the class, we get a final grade. And that tells us how well our algorithm did done. So the performance metric is not for training, it is for grading. Now, how does this work? What we'll do is we have data. We have our spreadsheet of houses in Portland where we're trying to estimate a cost of a house given its features. We have this spreadsheet, let's say, to 100,000 rows. We'll do
 is we'll take 80, we'll put it aside and that will be used for training the algorithm. In other words, 80% of this spreadsheet is our algorithms study material. It's its textbook, it's homework assignments. 80% is called the training data, the training set. So we just took our spreadsheet and we cut it. 80% goes to the student and 20% goes to us, the professor. We call this the test set. And this is this test set that we're going to use to grade the final.
 performance of the algorithm. This is the final test. The final that you give at the end of the semester is our test set. So we take our test set of features and we have the the Y values, the actual prices of these houses, but we hold them behind our back sort of. This is the test key. So we put all those rows as questions on the final test and we hand that to the machine learning algorithm and we say go and it answers all the questions and it gives us the test back.
 and we look at it, we compare it against the key that we have, and we grade it. Now, there's a little something called hyperparameters. When a machine learning algorithm learns, remember in linear regression and logistic regression episodes, we talked about these things called parameters, theta parameters specifically. A parameter is a coefficient that a machine learning algorithm learns. It's the number that goes in front of the x. It's this multiplication.
 piece that the machine learning algorithm learns in order to construct an equation that fits a line to the data parameters. That is the stuff that the machine learning algorithm learns. But there are other components to a machine learning algorithm stuff that it does not learn that can be tweaked to improve an algorithm's performance. So for example, when we were talking about linear regression, we said that the data may not optimally be fit with a line. It could be just so that maybe a little
 squiggle or it's a exponential function, a little curve up or something like that. So the amount of polynomials in a linear regression algorithm, that's called a hyperparameter. It's not a parameter that machine learning algorithm learns. It's something that we tweak as a human. A hyperparameter is a thing that the human tweaks. So it's kind of like a level up above the algorithm. It controls the algorithm.
 but then the algorithm learns its theta parameters under the restriction of the hyper parameters that we impose upon the algorithm. I think the best way to visualize this is with the hyper parameters of a neural network. The hyper parameters of a neural network are the number of neurons in any layer and the number of layers in the network. So how wide and how deep is our network? You might think, well, it seems like the optimal neural network would just be...
 the biggest brain ever. Let's just have a bazillion layers deep and a bazillion neurons wide at each layer. Wouldn't the biggest brain ever be the best neural network? Not necessarily. The size of the network is particular to the circumstance. If the situation is simple, then having too many layers or too many neurons actually causes something called overfitting. That we're going to be getting into towards the end of this episode. Another problem with two complex...
 of a neural network is that the more complex the network, the more training data the network needs to eat to understand the situation. If you've got a spreadsheet of 100,000 rows, but you have a gazillion neurons, that's not enough information. And you could probably never get enough information with that complex of a network. So you slim your network down to accommodate the amount of data you have available. Furthermore, the more complex your neural network...
 the more computational resources required. Too many neurons means your computer just can't handle it. So you actually want to have as slim down a neural network as possible, as small a brain as possible, that can optimally handle the situation at hand. So the number of neurons and the number of layers are your hyper parameters, and you, as a human, choose the hyper parameters that best fit your circumstances based on
 your understanding of the situation. So it takes actually understanding on the human part in order to optimize these hyper parameters. Now that takes away a little bit of the magic, doesn't it? We have machine learning algorithms that learn the information that you're giving it. But there's some stuff about the machine learning algorithm that it doesn't learn. And you have to tweak. Yes. So it's so hyper parameters. But there are, there are libraries out there that automate this process. So it's not actually machine learning. It's more like.
 iterating through a handful of different combinations of hyper parameters given these restrictions you give it and finding the thing that achieves the best performance metric using that evaluation stuff that we talked about in the earlier part of this episode. So it's not machine learning proper. It's just that there are libraries out there that can cycle through hyper parameters, run your machine learning algorithm against it against the performance evaluation and tweak the hyper parameters until it gets the best score possible. So these hyper parameters are things...
 that we tweak. And so what we do is we add a additional step to this train and test process. Okay, so training a machine learning algorithm has the algorithm tweaking its parameters, its theta parameters in the case of linear and logistic regression. And then the final test just tells us the overall score of the machine learning algorithm. How did it perform? We add this third piece called the validation step that allows us to kind of like stop, look at how the out.
 algorithm is performing before we get our final grade. And if we sort of have a hunch on how we can make some changes to these hyper parameters, we can tweak them. And then we can click go again on the training phase. And the algorithm can retrain given the adjusted hyper parameters. So we call this step the validation step. So now we have, and I think of this in our university course analogy as the midterm. So it's this second test we insert in the middle of the semester that.
 measures the performance of the machine learning algorithm, but allows us to kind of step in and change the hyper parameters and then step back out and then let it go again. It's kind of like we have our machine learning algorithms are taking this course and they've got their internal measure of their own performance. They're like, I think I'm doing good. I think I'm doing good. They're making some adjustments to the way they study. Okay, they start to realize that they fully understand concept A, but they maybe need to focus a little bit more on concept B. So they have their loss function cross.
 entropy, for example. And they adjust their theta parameters, adjust their theta parameters, they learn and learn and learn the course material. And then we give them a midterm. And they get this grade back and they're like, whoa, a C. I thought I was doing really well. And so we pull them aside as a human. Okay, we turn them off. There's a off switch on their back because these are learning machines after all their robots, right? And we open up their skull and they have a little robot brain inside and we take our wrench and we take our screwdriver in turn the
 week two tweak, turn, turn, turn. We make some adjustments to the hyper parameters, whether it be number of polynomials in a linear regression algorithm or number of layers or neurons in a neural network. And then we close the lid on the head and we put it back into its class and it starts to learn and learn and learn. And then we can actually maybe retake the midterm. And it gets an A this time. Great, it's actually improved. So I think we're ready to take that final test and the machine learning algorithm takes a test and gets an A. So training is learning.
 We have a midterm, which lets us peek into how well we're doing and make any necessary changes to our hyper parameters. This is called the validation step. And then we have our final test, which tells us our grade for the semester. So we split our spreadsheet into three chunks. Let's say it's 60% training data. That's the stuff that the machine learning algorithm is going to learn from. And then 20% validation set. That's the stuff that we're going to test the machine.
 learning algorithm on midterm to see if there's any sort of hyper parameter adjustments we should make to improve the algorithms overall performance. And then we have our final test with the test set and that tells us the overall score. So this is called, this process is called cross validation. We split our spreadsheet, our data into three chunks, training set, validation set and test set. And we use that data to train, validate and see if we need to make any hyper parameter adjustments and test.
 final evaluation. Okay, so we have a way of measuring algorithm performance, both against itself with a loss function and against each other with a performance metric like accuracy. Then we can use this grade to decide which algorithm to use for our particular purposes. But what could go wrong in an algorithm that could mock up its performance? What could cause an algorithm to be less performant than another algorithm? It could be that one algorithm is just...
 the better fit given the data. But it could be a handful of other things, things that can improve. We already talked about hyper parameter adjustments. Those are things that we could adjust to improve our algorithms performance. Some algorithms require more data to learn from than other algorithms. So one thing we can do to improve performance is just collect more data. So for example, when we split up our data into training, validation, and test sets, we just do...
 the amount of data that an algorithm has to work with when it's studying, when it's learning. Some algorithms work fantastic with very little data. Nive bays, in particular, works very well even with little data, whereas neural networks need lots and lots of data to train with. So one way to improve performance is to collect more data. Another way to improve performance is sometimes the data actually needs massaging. There's some missing fields, some...
 times, maybe we just need to fill in those missing slots with maybe maybe an average across the data or something else. Basically, filling in missing fields in your data set is there's this whole science to it. It depends on how the rest of your data is structured and what that particular field is. But choosing how to fill in that missing slot is a very advanced topic. I'm not going to talk about it. There's this other thing you can do called
 normalizing data. Sometimes numbers are wildly different. Let's say that one feature. For example, is number of bedrooms when we're talking about housing costs can be two or three or maybe four, whereas distance to downtown measured in feet is going to be wildly higher. Well, those two numbers being on totally different scales can actually hurt the performance of certain machine learning algorithms. So you want to bring them down to the same kind of...
 scale and we call that normalizing. So you normalize your features. So these are all things that you can do to improve your performance. But there is one thing that drastically improves the performance of machine learning algorithms. And that is called regularization. And you are going to learn about regularization probably in the first couple weeks of Andrew Eing or the first couple chapters of any other book. I apologize that it took me so long to get to this.
 in my podcast, it's just so boring and it's so technical and detailed. It's like, I just wanted to kind of cover the high-level stuff before I got into the nitty-gritty. It's not hard to understand. It's just technical. So it's super, super important to machine learning understanding. You can't go on without it. And so I would estimate you probably know a little bit about regularization already. And if not, you will learn about it when you start diving into the details of machine learning. Regularization is a step you take to reduce overfitting.
 and underfitting. Overfitting and underfitting, or sometimes called high variance and high bias, respectively. In machine learning, we have variance and bias. A machine learning algorithm has any level of variance and bias. The level of variance determines the amount of overfitting, and the level of bias determines the amount of underfitting. And this is what these things mean. The best way to...
 think of this is judging a book by its cover. Now, you're a machine learning algorithm. You may be support vector machine or neural network. And you are trying to learn whether or not you will enjoy a book by its cover. Well, you've read a hundred thousand books and you've seen all their covers. And you know which books you've liked and which ones you haven't liked. And I think you have a hunch as to what books would be good just looking at its cover. Now, a naive machine learner will come up to you and say, you can't judge a book by its cover. That's so full.
 So pa, everybody knows you can't judge a book by its cover. And you tap your nose and you say, can't I? The other algorithm says, no, of course you can't. Every single book is unique in its own special way. Every single book cover represents a specifically different book. Okay? Every book has its cover and it's unique. There is nothing that connects these things together. That algorithm suffers from high variance.
 fitting. Okay, so that's a problem that you can have in machine learning. You on the other hand, let's say that you have gotten a little bit overconfident and you pick up a book and you look at it and you're like fantasy, I'm gonna love this book and the high variance algorithm looks at it and it has a bare-chested man sitting on a horse and a girl behind him swooning with her hand on her head and he says come on that's romance. Would you say that's fantasy? Well maybe you were tipped off by the horse and the strong man sitting on it. Maybe it looks like he's riding...
 into battle, I don't know. But the fact of the matter is you guessed too fast. Two few variables tipped you off. You didn't think hard enough. You pick up another book and you're like fantasy. You pick up another book and you're like fantasy. Fantasy, fantasy, fantasy, fantasy, fantasy. You, my friend, suffer from high bias. That is underfitting. Underfitting or high bias is when you don't use enough data to make a decision. Usually this is a result of not having enough data to begin with in the training phase over.
 or a high variance means you're too specific. You're not generalizing enough. You don't believe in generalization. You think it's harmful. So if we're looking at linear regression, for example, linear regression, remember, was fitting a line to a cloud of data points, dots in the shape of a football. Let's say that the data is not shaped like a football. It's a little bit curvy. And we need ideally to come up with a polynomial function, maybe x.
 squared or some sort of curve that goes up, noun and up again. That's what we're hoping for. That's an ideal fit. Good generalization. High bias would not have any polynomials at all, which would just be a line, goes right through the center. It's not using complex enough features. It's not using, or it's not using enough features in general to make its predictions. It's too simple. Its predictions are too simple. High variance or overfitting might fit the line with too many polynomials.
 You might have too many x-squares and x-cubes and x-to-the-forth and to the fifth. And what you get is a line that goes through every single point on the graph. That's not right either. We want something that generalizes something that reduces the overall error, but is as simple as we can make it without being biased. I think of this as Occam's razor. I'm sure most of you know what Occam's razor is. I'm not going to define it really. I'm just going to cut.
 of summarize it as simplest solution wins. Now you don't want to go too simple, otherwise you have high bias underfitting. But you don't want to go too complex because then you have overfitting high variance. What you want is the simplest equation possible that will give you a generally good algorithm, a generally good graph, something that squiggles up and down and up really cleanly and simply doesn't touch on all the dots. Just kind of goes through the middle of them all. And the reason for that is...
 If you add a new dot, if you add a new data point, it's more likely to get that right or at least close to right than an overfitted graph. So the problem with bias and variance, the problem with underfitting and overfitting, is that they may allow you to make somewhat accurate predictions in the training phase, but when you come to the validation step or the test step, you're going to be wrong. So these are both evils, variance and bias. And we want to reduce both of these if...
 possible. We want to come up with a good generalization strategy that is neither too complex nor too simple. And that is this thing called regularization. Regularization is a little chunk that you add to the end of any machine learning algorithm that modifies the equation by reducing underfitting and overfitting. It's interesting, but what it does, for example, linear regression is it might reduce the effect of polynomials. It's hard to...
 explain. I'm going to leave it to the Andrew Inks stuff for you to learn the details of regularization. But it's basically a little add-on that you add to any of your machine learning algorithms that will reduce biased invariance. So once again, underfitting is jumping the gun basically. You don't have enough data to come up with a good generalization strategy. And so you come up with an insufficient generalization strategy. Maybe a line when what should be used is a polynomial. You pick up a book and you make a judgment call.
 based on two few features of the book in your hand. Now I do believe that a book can be judged by its cover. At least the genre of a book could be judged by its cover. But overfitting algorithm over here comes along and says no book could be judged by its cover. Every single book is independent. This algorithm has does not come up with any sort of generalization strategy. They wouldn't be able to tell what genre even a book is when looking at it because it's too specific. So the optimal strategy is
 some sort of middle ground where you can judge some characteristics of a book by its cover, not too specific, not too simplistic. Okay, so that is performance. We talked about performance evaluation and performance improvement, things that can hurt performance. Things that can hurt performance would be high bias and high variance, that is underfitting and overfitting respectively. Non-normalized features, missing data, too little data, poorly tuned hyperparameters, such as number...
 of neurons or number of layers in a neural network. And there are a number of ways to improve on any of these issues. You would tune your hyper parameters or you would fill in your missing data. You would normalize your numeric data and you would regularize your algorithm by adding a little extra algorithm chunk to the end, regularization, and that would decrease overfitting and underfitting. Now the way we measure our algorithms performance, well, each algorithm has its error or loss function. And those are used to measure.
 its own performance while it's learning its own subjective personal performance metric. And then at the end of any session, we will finally test our algorithm with the test set. This performance evaluation, we might use something called accuracy, which is a very general, simple catch-all for measuring performance. But in certain edge case scenarios, we might try to balance precision versus recall. Precision is how much of a straight shooter you are. You nail every...
 re-estimate you make versus recall, which is how why do you cast your net? Do you not let any estimates escape? You can use an F2 score to measure the balance of your precision and recall. And finally, we don't just break our data set into training and test sets. No, we add a third set, we break it into three, training set, validation set, and test set. We use our training set to train to learn the theta parameters.
 We use our validation set to determine how well our algorithm is doing. If it's not doing too well, then we might adjust hyper parameters, human adjustable parameters of the algorithm. And then finally, at the very end, we will measure the algorithms overall score by way of the performance evaluation. Boring, huh? That was not fun. But my friends, as I said in the beginning of the episode, you are now done with the basics of machine learning. Of course, per usual, I encourage you to finish the angel in curiosity.
 course, and then from there move on to the deep learning book. Start coding with Python and TensorFlow. There are no resources for this episode. Performance, evaluation, and improvement is kind of tied to learning machine learning algorithms. You're going to be exposed to the stuff that I mentioned in this episode, not as an independent module in any learning series, but alongside any individual algorithm as you go. And starting now, I'm going to...
 going to be moving on into deep learning. I might break this podcast now up into seasons where the next season is going to be deep learning, which means that I may end up pausing creating new episodes for this podcast in the short term, just to catch up on the deep learning essential so I can, so I have it in my mind, good enough to teach to you guys and then create a new season on deep learning. But that's what we're going to be covering in the...
 next sequence of episodes as far as the eye can see we're going to be moving completely into deep learning. So it's going to be fun. We're going to be talking about recurrent neural networks, convolutional neural networks, and more. Eventually, once I've covered all of the deep learning material, I will move to reinforcement learning. I hope to sort of ease into artificial intelligence proper with this podcast series, but it's going to take some time. I actually don't know.
 enforcement learning myself yet. So I'm going to have to learn that stuff in the background before I can teach it. In order to pave the way for deep learning and re-inspire you to learning this stuff because it is beautiful, it is magical, fascinating algorithms. I'm going to do my next episode on consciousness. I'm going to talk about what people are talking about and thinking about in philosophy and cognitive science and neuroscience.
 along the lines of consciousness and how it may relate to neural networks, artificial intelligence and all those things. It's going to be a little bit pseudo-siancy, definitively subjective, and I know it could ruffle some feathers, but I do encourage my rigorously empirical listeners do listen to the episode. It'll just be some fun, it'll just be a little bit of inspiration. I'm not going to try to make any definitive claims one way or another, I'm just going to introduce the topic and inspire our listeners.
 because there is certainly some potential correlates between artificial neural networks and biological neural networks. If not entirely in an architectural way, potentially in a functional way. I think this will be a very fun episode and at the end of that episode, I will get back to you guys as to whether I'm going to break this up into seasons one and two or if I'm just going to continue from there right into deep learning. I'll let you know. See you guys next time.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 16 consciousness. This is going to be my favorite episode to make this.
 This is a topic near and dear to my heart, the concept of artificial consciousness can and will artificial intelligence when done right be conscious, an extremely controversial topic to be sure, and one which will require a definition of consciousness, we'll get into that in a bit. But before we get into consciousness, let's talk about the inspirations that drive people into the field of artificial intelligence. We've talked about this before in a very early episode.
 the various inspirations bringing people into the field. In the past, people who were developing an artificial intelligence or probably more specifically in machine learning got into the space because they just wanted to solve interesting and challenging problems. Maybe they had a statistics or theoretical computer science PhD and they quickly found out what were the best applications of their skills to make big money and to solve challenging problems. These are people like the Quants who use machine learning on Wall Street for high frequency trading in the life.
 Things have changed recently. The graduating class of 2017 machine learning engineers are brought to the field not so much to solve new and interesting problems for its own right, but by way of inspiration. We see major things happening in the world right now. Self-driving cars, music, poetry, and art synthesizing. Things that we considered impossible by way of artificial intelligence in a prior generation, and we're seeing happen day by day new incredible
 Breakthroughs on a monthly basis usually by Google also by Facebook, by do IBM, OpenAI and all these other companies But big stuff is happening So that's one of the main inspirational drivers that are bringing people into the field of machine learning is they're seeing this incredible amount of Economic automation that we're achieving by way of this technology self-driving cars even scientific discoveries drug discovery and creation Medical robots x-ray analysis legal proceedings
 done by machine learning, like robot judges, all this stuff. So people are seeing that the new wave of jobs is by way of automation and the smart move as a professional is to be at the top an automation engineer. One, they don't wanna get left in the dust so they wanna make the smart professional move but two, these problems that we're solving by way of AI are incredibly inspiring and powerful things. So that's the first inspiration in my mind that's bringing people into the field, trying to stay on top of the economy and being inspired by.
 of things that are being created. Inspiration number two, the Singularity. Many have come to this podcast after being inspired from media. Books like the Singularity is near and how to create a mind by Ray Kurzweil. TV shows like Westworld and Black Mirror and just the overall zeitgeist of this concept called the Singularity. The Singularity is this idea that technology is progressing at an exponential or at least at a polynomial.
 pace, not a linear pace as should be expected of technological growth. The idea was proposed long ago, but championed by a guy named Ray Kurzweil, who wrote a seminal book called The Singularity Is Near. Now, my 15% of expert machine learning engineer or PhD listeners are rolling their eyes and groaning at the mere mention of the name Ray Kurzweil, and my 75% of bright item Bushy Tailed.
 inspired listeners are sitting up in their chairs in anticipation. For both of you, I'm actually not going to take a stance one way or another on the credibility of the singularity. I'm just going to describe it my purpose in this podcast episode is consciousness. So the idea of the singularity is that if we look at the rate at which technology has progressed in human history, it appears to fall on something of an exponential or maybe polynomial graph. Rather than a line, we've got an upward facing slope.
 all the way back to tools, then to the agricultural revolution, then the industrial revolution, then the information age, which we presently live in, and possibly what comes next being the intelligence explosion by way of artificial intelligence. Each one of those technological revolutions was more substantial than the prior and closer to the prior than the prior to its prior, thus looking like an exponential or at least polynomial graph. The idea goes that at...
 At some point on an exponential graph, there appears to be a hard elbow, a point at which the graph sort of rockets into space, trying infinitely to reach what's called an asymptote. A lot of the graph prior to that point appears to be increasing linearly on a very normal pace, but at some point there's an elbow, a hard shoot up into the sky, and we call this the singularity. Well, if our technology appears to show the trend of a polynomial or exponential graph, then we have...
 haven't hit that elbow yet, but we do appear to be increasing at a rapid clip. What might cause such an elbow? Well, artificial intelligence. Now, here's the thinking. AI is the automation of our technology. Period. All of our technology can be automated potentially by artificial intelligence, which is simply defined as automating any mental task. That's what artificial intelligence is. Of course, there's robotics. So, according to the singularity, at some point, we may not even be participants in the development.
 of technology will all be automated. Well, what if an AI not only could handle automation of a particular mental and physical task, mental by way of AI and physical by way of robotics, but also influence its successor by improving upon its own algorithm. So it can either influence itself or it can either update its own algorithm to improve the algorithm that we gave it in order to achieve its automation or improve the next generation of such algorithms so that they're better to task.
 whatever it is that may be their task. We have a self-improving machine learning algorithm, a self-improving AI, and that's what's called seed AI, AI that can improve itself. Once that happens, we don't even know what comes next. Pff, the sky's the limit. So that's a possible candidate for what kicks off the singularity. Like I said, the singularity is highly controversial, it's debatable, it's lots of fun though. You can read all about it and rake herswiles book, The Singularity is Near, and most of the episodes of Black Mirror TV series.
 are based on concepts of the singularity. So that's another inspirational driver bringing people into the field of artificial intelligence. And by the way, I keep talking about AI, but this is a podcast series about machine learning. Now my listeners remember from a prior episode that I made the distinction between AI and machine learning, but I'm going to make this distinction one more time because we may have new listeners here specifically for this episode on consciousness. So let me just define AI, which I already said, is automatable mental processing of it.
 any sort. Anything that is an automated mental process is theoretically considered artificial intelligence. Then there's this concept of artificial general intelligence and artificially intelligent agent which can perform all mental tasks or at least all mental tasks that humans are capable of and to the level which humans are capable of performing those tasks. In other words, an AGI agent, artificial general intelligence is an agent which can do everything humans can do mentally, at least as good as humans, if not better. When it's better, we call it super-
 AI when it says good as humans we call it a GI and when it's less than humans we call it weak AI when it's a specific task in artificial intelligence such as image recognition, speech synthesis, etc. If it's a specific artificial intelligence task not intended to be applied to everything across the board We call that weak AI and if we can apply it across the board we call it strong AI or a GI Now where does machine learning fit into this picture? Machine learning is a subfield within AI AI
 There are many fields within AI, there's robotics, there's perception, there's planning, knowledge representation, all these things. And the reason that I'm focusing on machine learning is for two reasons. Machine learning is the most accessible field within AI if you wanna crack into the industry professionally. There's a lot of machine learning jobs that are opening up all over the place. It's becoming wildly popular. It has a lot of professional application that allow you to crack into the industry. Things like fraud, detection, image recognition, increase speech percentage,
 you're starting to see a lot of chat bots popping up left and right. Any sort of actionable insights based on any data collected by companies. So machine learning is the accessible part of AI. It's the way you crack into AI professionally. And also it is increasingly a core component, if not the core component of AI. Lots of dedicated spaces within AI are quickly becoming subsumed or at least majorly contributed to bots.
 learning. We're finding that learning is more than just an aspect of intelligence. It may be one of the most important, if not the most important, aspect of intelligence. So if you're interested in AI and you want to get involved, start with this podcast series. It's very introductory. It starts from the very beginning and works its way up. Okay, so two inspirations of three driving people into the space of artificial intelligence by way of machine learning. The first being economic automation.
 It is in their best interest professionally to be at the top when this major economic revolution lands down hard in the near future. I will affine myself with this position. I do think being an automation engineer is a wise career choice. The second inspiration, a little bit more controversial, being the singularity. If you believe in the singularity, then you can be a participant in this exponential explosion of technological advancement by being a machine learning engineer. I'm not going to state.
 whether or not I believe in the singularity, because I want to save my credibility destruction for the next inspiration, the third inspiration, driving people into the space of artificial intelligence. And that is consciousness. Artificial consciousness can robots be conscious. This is a very important question because if robots can be conscious, if we can say that they are conscious, then we say they have a mind. If they have a mind, they have a soul.
 soul. All three things are synonymous, consciousness, soul, and mind. And that is a very, very important assertion indeed that has major implications with religion, with cognitive science, and everything. If we can be convinced that a robot is conscious, then my friends, everything changes. Life, as we know it, changes. This is indeed the inspirational driving force that led me to studying machine learning.
 It's what brought me into the field. Now in this podcast episode, I'm going to do my best to not make an opinion and to not affine myself with any theories that are presented in the space of consciousness. I just want to present a lay of the land and I want you to explore making your own decision. You're not going to be able to make an opinion just from this podcast episode, but in the resources that I give you, you'll be able to continue exploring what I present in this episode and eventually come up with your own conclusion. One thing I do want to say.
 is a lot of times when this topic is brought up in the space, these curmudgeonly jerks come around and they say, we don't know anything about consciousness. We can't even talk about it. Nobody in the space can even agree on a definition. And I hate it when that happens. That's just not true. Honestly, when people come at me with that retort, it shows that they lack understanding about consciousness and therefore there is no understanding about consciousness. So you may want to be careful bringing up the topic, some of the stuff you learned in this episode.
 because you will find people get really upset, really angry. They just want to shoot down the conversation and say, stop talking about it, shut up, shut up. They close their ears and they just do not want to talk about consciousness. There's two reasons that you'll see people respond this way. One is that they believe that the topic of consciousness as we'll discover in a bit called the hard problem of consciousness is not something you can even talk about because it is definitively subjective. Another reason people get upset is that they think that we as machine learning engineers, artificial intelligence people, et cetera.
 are not neuroscientists, we're not neuroscientists, et cetera. And therefore, we cannot participate in such a controversial scientific topic. You'll see that this is not true. Artificial intelligence is a subspace of cognitive science, and machine learning is a subspace of artificial intelligence, and therefore we are indeed participants at the table of the conversation of consciousness. We are board members of this company. Additionally, this stuff is inspirational. If we dain't call the conversations of consciousness, science fiction.
 or pseudoscience, which I believe they're not. I think they're legitimate conversations. That doesn't affect my desire to explore the topic at all. It's science fiction and pseudoscience that inspires some of the greatest minds like Elon Musk to explore and achieve the impossible. We as humans are capable of achieving so much greatness if we believe in the impossible and we apply ourselves. And a lot of times that inspiration comes these science fictiony pseudoscience topics like sp-
 Exploration, sending people to Mars, and yes, artificial consciousness. So don't listen to these naysayers. For one, we can prove them wrong by achieving the impossible. And for two, they're simply wrong about statements like nobody knows anything about consciousness. Nobody in the space can agree upon a definition of the thing. So how could we ever hope to talk about it in such? That is wrong. That is pure wrong. There's a lot about consciousness, about its definition, that is disagreed upon.
 from one expert to the next, but there's a lot that is agreed upon. There's a lot of agreement in cognitive science. So let's talk about some definitions. Let's talk about cognitive science first. Cognitive science, CogSci, is an umbrella term for all of the sort of brain sciences. That includes psychology, neurophysiology, neuroscience, computational neuroscience, neurobiology. So those are all those hard sciences about the brain. It also includes philosophy.
 At least the branch of philosophy concerned with the mind. Now, it doesn't quite concern itself with the brain as such because that stuff is covered by the brain sciences. But there are branches of philosophy which concern themselves with the mind. And as I mentioned already, it also includes artificial intelligence. Very importantly, it includes artificial intelligence. Because, my friends, artificial intelligence was not computer science first. We didn't start by making a bunch of algorithms and...
 and then saying, holy cow, this kind of looks like a brain. Let's call it a neural network. That would be disingenuous indeed, if that was how we went about it. No. Major names in the origination of things in neural networks, such as the creation of the perceptron, are Frank Rosenblatt, Warren McCulloch, and Walter Pitts. These guys, respectively, were neurobiologist, neurophysiologist, and computational neuroscientists. So these were brain guys, and they wanted to...
 come up with a mathematical representation of what they were seeing in the brain, and they came up with the perceptron later to become the artificial neural network. The ANN came from the BNN, the biological neural network. So artificial intelligence was a spin-off of brain science. Therefore we are allowed a seat in the cognitive science board of directors. Within cognitive science, these various fields, there is agreement and disagreement about what constitutes consciousness.
 So what kinds of things do they agree with and disagree with? Well, for one, we now approach this definition of consciousness, consciousness by comparison to intelligence. So we have two things that come out of the brain. Intelligence, which is simply the capacity to compute, to perform a mental task. Every human is prospectively intelligent, maybe intelligence comes in scale. Some humans are smarter than other humans. Humans, generally, are more intelligent than the lower species.
 down to a fish, all the way down to a snail. So intelligence seems to come in scale. Snails are intelligent, just less intelligent than humans. Intelligence is the capacity to compute. Information processing capacity, maybe we'll call it. As such, we have no problem calling artificial intelligence, artificial intelligence. It is replicated intelligence in computer form, and it is indeed intelligent. It is simply computing information, processing information, doing math, thinking in a TED. Then...
 separate from intelligence is the only other thing which is consciousness, the mind, the soul. So we agree upon that distinction amongst the various fields in cognitive science. Now what of this consciousness? Well, that's where things get a little bit dicey. Consciousness is many things to many people. Let's go through some of the aspects of consciousness which may or may not be controversial within the fields of cognitive science. Perception. That appears to be an essential component of consciousness. We'll start there just to work our way up. You're thinking what?
 Perception, that's not consciousness. Well, it's maybe an aspect of consciousness, maybe a prerequisite for consciousness. We'll see. Perception is your ability to see things, hear things, touch things, et cetera. Your five senses in humans, other biological species have fewer senses. Maybe others have more. Maybe robots could have more than humans. There is thought that perception is a requirement for thinking, okay? Some people say, no, you don't need perception to think you can live without perception. And then I'll write back.
 that they call it. You can think math. We have the architecture in our brains to deduce, to come up with mathematical equations or philosophical syllogisms. All these in general are called deduction by contrast to induction, which is learning by experience, learning that touching the hot countertop burns your hand. Some philosophers think that while we have the architecture for deduction built into our brains naturally, we cannot kick it into process without first...
 inducing something without first experiencing something by way of induction on which to deduce. So the idea goes that if you see one cup and another cup and another cup you can come up with the concept three and you can come up with the concept two plus one and two minus one and all those things. But if you've never seen items in the world in some countable capacity before would you ever be able to perform arithmetic in your mind? It's controversial. Some say yes, some say no. Let's move on.
 exception may or may not be an essential characteristic of consciousness. How about self-identity? Ha, there's a more controversial topic. I think you will find when you're talking to your friends and family about consciousness. And you say, can you define it for me? This is probably the most common definition you'll get. Oh, the me, the I, in the equation. Consciousness is me being able to self-reflect. Self-reflection. Self-identity. Hmm. I'm not so convinced that.
 that equals consciousness personally at the very least. And I may even say that it's not even an essential characteristic of consciousness. We'll get to that in a bit. But you can imagine, for the sake of argument, that self-identity, self-awareness, may just be almost an evolutionary add-on to consciousness. Here's how the theory goes. If you have a theory of self-identity, you can adjust your actions in accordance with your theory of self.
 Your theory of self may be a simulation of how others see you. So an intelligent mind will learn what things to do and not to do in an environment. Don't touch the hot stove. Do eat the food. We'll take that up a notch and learn about ourselves, about who we are in an environment. And we can learn how to adjust properties of ourselves, so as to better get on with those around us. This theory...
 self-identity says basically, the self-identity is nothing more than a running theory of how others see you so that you can make proper adjustments to get along with people. Not having a solid self-identity means making enemies and maybe getting killed. Having a strong self-identity enough to improve upon that algorithm means making friends, allies, and having sex. Of course not everybody agrees upon this, some people think indeed that self-identity is a core component of...
 of consciousness. But one more retort to that concept is the idea that certain lower species seem to lack self identity. It is unclear that dogs are aware of themselves, for example. Certain higher species appear to have self identity like chimps. We've got certain tests for testing whether they exhibit self-reflection. Things like they can put a dot on an animal's head and then have the animal look at itself in a mirror. If an elephant reaches its trunk towards the mirror versus towards its own head, then we think maybe it does.
 exhibit self identity. Certain animals pass this test certain animals fail. We have other such tests for self identity. The tests themselves are even controversial. But the point I'm trying to make is some animals have self identity and some don't. Now let's say your dog doesn't. Would you say your dog is not conscious? Do you indeed personally think that your dog has nothing rattling up there? Simply a mechanical computing device and it's just this reactionary
 biological organism that isn't experiencing the world around it. I don't think you do. I think if you sat with this for a while, you would come to the conclusion that most, if not all, biological organisms are conscious in a fundamental way, even lacking self-identity. So self-identity appears to be a very controversial component contributor to the definition of consciousness. But indeed, an important modifier, we do hope that if we can achieve artificial consciousness eventually that it will...
 have self identity. So the very least self identity is a powerful module. At the very least it's a powerful contributor to high scale consciousness, but it is unclear whether it is a necessary component of consciousness definitively. By the way, my own personal opinion is this. If we go with the theory that self identity is your running simulation of self so that you can improve yourself in an environment, does that sound familiar to something we've mentioned in the
 episode to me that sounds like seed AI. In my personal opinion, the moment we build self-identity into machine learning algorithms effectively and powerfully is the moment everything begins. seed AI recall is an artificial intelligent agent able to improve upon its own algorithm. Another word for this is actually metal learning and it is indeed a hot topic in the space of machine learning going on right now. Lots of companies are...
 exploring machine learning algorithms capability to improve on themselves. Okay, a few other aspects of consciousness when we're talking about it are memory and learning. Now these go hand in hand. You can't learn without memory. And by the way, you can't have self identity without learning and memory. Now nobody would say that learning and memory are consciousness. But these are things we may agree upon as essential to the grand picture. We need these things in place to get there. So we've talked about
 perception, self-reflection, memory, and learning. But we're not quite there yet. There's something missing. Something feels not quite right about the way we've been talking about consciousness. Like these are all components, but they don't really get all the way there. There's one thing we haven't mentioned. One thing that, in my opinion, all the cognitive scientists agree upon, as essential to the equation of consciousness, if not the very definition of consciousness itself, and, in that particular, there's a person who has previously punished for existence to be a reason for one person to work on something else. Because an allergy have explained to consciousness their tongues are wrong, and, in my opinion,
 It is awareness. Awareness or sentience, subjective experience, qualia, these are all words for the same thing, which is the lights are on. Something is on up there, okay? You as a human, when you see and you hear things, you hear the dog barking and you see the flower, you're not just some information processing device that crunches that perception internally, but that that perception is never experienced. No, you experience...
 the seeing, the hearing of the dog bark. It went in your ear and some little hairs in your ear were stimulated and some electrical firings happened within the brain. Within your biological neural networks, neurotransmitters are transferred from axon to dendrite. Chemicals are thrown left and right. Electricity goes all throughout your brain, but none of that explains the fact that you heard the dog. The dog's bark was heard by you.
 subjective experience that you cannot explain. It appears to exist in a different dimension. And that is why we call it the mind or the soul, the hard problem of consciousness. We'll talk about in a bit. Awareness. It's hard to define, but you know exactly what I'm talking about. Yay, Verily, maybe we can define consciousness explicitly as awareness, as the lights are on. So when people say nobody can come up with...
 the definition of consciousness, so we can't even talk about it. Well, we can all agree that awareness, if not the very definition of consciousness itself, is at least a core fundamental component of consciousness. And my friends, if something could convince you that it exhibits awareness, would you not agree that it is conscious? That is the reason we believe our dogs are conscious. They appear to exhibit to you that they are aware inside, that something is going on, that the lights are on. Now there's one.
 Something suspiciously missing by many people's definition, many lay people, I dare say, and that is emotion. Interesting. To many people they'll say, well, a robot can never be conscious because it can never know love, it can never know sadness, it can never know pain and sorrow and loss. Happiness, joy, all those things. To some people, emotion appears to be an essential characteristic of consciousness. In my opinion, and this is my own personal opinion, I'm not going to find myself with any camps of...
 cognitive science that go one way or another on this topic, I think emotion is not even a characteristic of consciousness. I think it is an accidental drop-in of evolution, a reinforcement learning mechanism. Things that make us happy stimulate some positive reinforcement mechanism to make us want to do that again in the future. And things that make us unhappy are some negative reinforcement mechanisms that make us want to avoid such actions in the future. In other words, emotion.
 are nothing more than a computational mechanism for seeking or avoiding certain activities for our own survival's sake. And think about all the things you get emotional about. Sex and love, physical pain, losing someone, death. These are all things that have evolutionary explanations. In fact, we'll talk about how possibly artificial intelligent agents could experience emotion on some level. Maybe not the way he is to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get the right to be able to get
 humans experience them, but possibly experience pain and pleasure, no less really than we do. We'll get to that in a bit. So we come upon something that most cognitive scientists can agree upon, and that is that the very definition, or at least a core component of consciousness, is awareness. That is subjective experience. Another thing that they agree upon is this distinction between what's called the soft problem of consciousness and the hard problem of consciousness. The soft problems of consciousness.
 are basically things we know. So it's always gonna be a moving target, a running definition. Things that we know through the hard sciences. We know, for instance, that consciousness comes from the brain. It clearly comes from the physical brain. The brain is the biological substrate of the mind, is what they say, that's that catchphrase. The brain is the biological substrate of the mind. We know that, we've known that for a long time. We've known it since at least the renaissance. We know it substantially more accurate.
 these days than we did in the past. We have things like MRIs and CAT scans and PET scans. We can look at the brain and figure out specifically which regions are firing when we ask a patient to question, think about your mother, plan some activity. We have to a T, the centers of the brain associated with what specific mental processes. We can look under a microscope and we can see the activity of a neuron. Exactly why and where? Mental activity comes.
 from. We know that speeches in Broca's area, that planning is in the prefrontal cortex. So the soft problems of consciousness are the things we know, the things of science. The hard problem is, but why consciousness? Okay? It's a little bit difficult to explain, but on the one hand, we have this physical world where we can observe the brain through MRIs and microscopes. And in the other hand, we have this other dimension, the spiritual world, this metaphysical world.
 world of the mind, your own subjective experience. It seems that your subjective experience when you're thinking, when you're doing math or planning something, or daydreaming, especially daydreaming, there's a good example, it appears to be a different dimension. And of course, this is what gave rise, I'm sure, to the belief and life after death, and religion, that your soul lives on past your body, because they appear to exist in different planes and different dimensions. And this hard problem is how.
 How is that possible? It's a paradox. This paradox is made manifest by the concept of dualism. So I'm not gonna get too much in the history of the exploration of consciousness, but I will mention one thing, and that is the concept of dualism put forth by Ren Descartes. Ren Descartes was one of the biggest minds in the early exploration of consciousness. He was fascinated with the topic. Ren Descartes is the mind behind, I think, therefore I am, and the evil genius that gave way to the matrix. Okay, how can we...
 know, we're not in a dream rather than the real world. All that stuff. You can see how that all plays into consciousness. He was obsessed with the topic. The concept of dualism is this very concept that the mind and the brain exist in different dimensions. Now it's a paradox. How could they possibly exist in different dimensions? If they existed in different dimensions, if the mind is something non-physical, something meta-physical, then how could it possibly interact with the brain, which is something physical? At some
 point, the mind would have to become physical to enter the physical world to stimulate the brain into action. But that's just paradoxical. At what point can a non-physical thing become physical? It just doesn't make sense. Descartes believed that the seat of the mind in the brain was the pinnacle gland. Where he came up with this was that a lot of structures in the brain have a mirror image on the other hemisphere. So we have one thing on the left hemisphere and a mirror on the right hemisphere. And there are big structures.
 Well, the pinnacle gland was this tiny little structure that didn't have a mirror image. So it physically looked like a good candidate for like a plug, an outlet where you'd plug in the mind from the other universe. But he couldn't defeat that conundrum. How can the non-physical become physical? And so scientists just don't believe in dualism these days anymore. Dualism is relegated to religionists, people who believe in life after death. And so if you are religious in that way, then you probably are a dualist. But many today.
 believe that the mind is physical somehow somehow the mind comes from the brain. The brain does the stuff and the mind comes out of that. But how? It is a mystery. It is a mystery. One of the last remaining mysteries of the universe. We can manipulate the brain and alter the mind. We know through certain patients and case studies that alterations to the brain in a repeatable fashion cause alterations to the mind. There are these case studies like...
 gauge you had a railroad spike driven through his brain and caused specific alterations to his personality. Okay, if you believe in dualism and that the pinnacle gland is this sort of outlet where the mind plugs in, well then how does physical damage to a specific other chunk of your brain that has nothing to do with the pinnacle gland substantially and permanently affect your personality, which is an aspect of your mind. It just doesn't work that way. So we see brain cases leading to mind mister
 repeatable and predictable fashion. We know they're directly tied, but how does the mind come out of that? And the reason we ask that is because we want to know if we could perfectly replicate in a robot, a brain like structure, would it then have a mind? So knowing how the mind comes out of the brain is what's called the hard problem of consciousness. And it's basically nothing more than a paradox, a conundrum. Some people think that we can never know how the mind comes out of the brain because the
 The mind is definitively subjective experience. Consciousness is awareness, awareness is subjective experience. I know in my mind what's going on, you can observe my brain and an MRI, but you can never observe what's actually happening in my mind. You can't guess who I'm thinking about, what specific actions I'm planning. You can see certain centers firing, but my experience is definitively subjective. Similarly, I can't see what any of you guys are thinking, nor can I even be certain that you are.
 By all indication, your minds must think the way my mind does as a result of my brain firing and yours does similarly. I must conclude that you similarly have consciousness that you are aware, but I can't prove it because your experiences are definitively subjective. So some think that the brain-mind problem, the hard problem of consciousness, is definitively unscientific, unknowable, and all we could do is guess. All we can do is take it on faith that think-
 things that appear to have consciousness have consciousness. This is basically the idea of the Turing test. Alan Turing was one of the forefathers of the computing revolution. He was a big believer of artificial intelligence. He was very fascinated in the space of artificial consciousness. And he came up with a test called the Turing test, and he made it sound so complex and convoluted. And all it says is, it will walk like a duck and talk like a duck. It's a duck. If the thing can convince you that it is...
 conscious, then it is conscious. We have no way of exploring how and in what way it's conscious, so all we can do is take it on faith that if it can convince you that it's conscious, then it's conscious. That's the Turing Test. There's another idea which says that we may eventually be able to observe this stuff actually, the connection between the brain and the mind. How the mind comes from the brain. If you think about it, the mind is magic.
 to us right now. And that's why the mind aka the soul is so inextricably tied with religious thought, the idea that the soul lives on after the body dies. It's magic. Well, we've seen this before in science. We've seen things that were previously magic become science. For example, we used to think that sickness, illness, was evil spiritual infestation or a curse of God. And we later found out it was back to you.
 viruses. These weren't just theoretical bacteria and viruses. They were observed directly under the microscope. We took magic and we made it science. And in a similar way that we've recently been able to really hands-on kind of observe this theoretical concept of gravity. But we've been able to sort of interact with it and observe it physically. Perhaps we can come to see the mind through science. It's unclear if we'll come to that point, but it is possible. I heard once an-
 idea that philosophy is basically a moving caravan of thinkers. A bunch of settlers moving westward, constantly thinking about how the universe works, thinking about all the puzzles out there. When something becomes observable, experimentable, testable, outcomes a branch of those philosophers to settle a colony. A new colony over here, a hard science, say called neurophysiology, psychology, and the like.
 but that philosophy keeps traveling westward, ever in search of the infinity of mysteries. So the idea is that philosophy is ever at the frontier, thinking about stuff we don't know, and anytime we come to know it, out comes a science. Science is branched off of philosophy, comes out of philosophy, settles a colony, and sits with its lot. Well, perhaps, as we've done in the past, where we've created science from magic.
 philosophy, pushing ever westwards into the mysteries of consciousness, will find something that will stick and will actually be able to make science of this mystery, this hard problem of consciousness. So those are two takes on this concept. One is that maybe we haven't gotten there yet with the science, and the other is maybe it is not scienceable. Maybe it is purely and definitively a subjective thing, the hard problem of consciousness. So let's talk a little...
 bit about some ideas, rattling around the space of philosophy and cognitive science and artificial intelligence about where consciousness comes from. Theories within the hard problem of consciousness. Remember, this is called the hard problem for a reason. We can't yet observe how the mind is created from the brain. And so what we have in the space is a lot of guesswork. One of the most compelling ideas, in my opinion out there, is something called emergence. Emergence. The idea that the mind.
 Mind is simply an emergent property of the brain. And that appears to be the case. We have the brain, it's firing, it's doing some stuff, crunching some numbers, planning some actions, in predictable and repeatable and observable ways, and outcomes of mind. Perhaps that mind doesn't exist in a different dimension. They're not two separate things as dualism holds, but the mind is the byproduct of the brain. The brain is the thing that's doing the thinking. And the mind is simply...
 the experience of that. In other words, what if thinking is simply experienced? They are the exact same thing. This is compelling. We know that intelligence comes in scale. A human is more intelligent than a snail, but a snail is still intelligent. We all accept that. Snails to humans and everything in between are intelligent in their own way because they're thinking machines, they're computing devices, navigating their way through their environments. Well perhaps...
 Consciousness is simply the experience of intelligence. Intelligence is the thinking, the brain is the device on which the thinking occurs, and consciousness is the experience of that thinking is experienced. If that's the case, we would say that humans are more conscious than snails because intelligence and consciousness are basically the exact same thing. We just think of them differently. Intelligence is the capacity and the action. And consciousness is what comes of it. That was...
 would make sense. We said that humans are conscious, we all agree with that. We asked whether our pets are conscious. Are our dogs and cats conscious? Well, we said yes, it appears to be so. Their lights are on at least. Well, if cats and dogs are conscious, what about rats? What about fish? How about snails? No, not snails. Well, okay, if you're going to say that one thing is conscious and not the other, where did you draw the line and why? We don't seem to have, from neuroscience, some definitive center of the brain, without which we say.
 there is no consciousness in a biological organism. Like I said, this is the hard problem. So it appears arbitrary where and how you draw the line between species which are aware of what's happening in their environment, experiencing quality, subjective experience, and those which don't. So consciousness comes in scale, is directly tied to intelligence. So that's one school of thought out there. A subfield within this emergent property theory is something called the computational theory of mind.
 And this says, well, if consciousness is an emergent property of intelligence in humans and dogs and snails, well, what is it that these things have in common? They're information processing systems within their brains. We are computing the stuff that happens around us using our intelligence architectures. If that's the case, then wouldn't any information processing system, any computational device experience consciousness as well? Ah, now we're getting into artificial consciousness.
 The computational theory of mind, the CTM, holds that consciousness is the emergent property of computing. We've defined intelligence, we've tied it to humans and snails, and robots. We've said that consciousness may be a byproduct of intelligence, an emergent property, the experience of intelligence. We have agreed that artificial intelligence, by its very name, does indeed exhibit intelligence,
 And therefore, after all that artificial intelligence should indeed thereby exhibit consciousness as an emergent property of its own intelligence. Again, everything in scale, the idea is humans are extremely intelligent and extremely conscious as far as we can tell. By comparison to other lesser biological species, well that may be the case as well with artificial intelligence. Perhaps AI is extremely intelligent in its own weak way and specific...
 AI is better at image recognition than we are in some cases. Certainly, AI is better at math and the really hard computational stuff than humans, but there are many aspects in which AI is less intelligent than humans. In that regard, perhaps AI experiences consciousness in its verticals at which it's very effective, so for example, perhaps it experiences very highly lucid, visual experiences.
 perhaps by the CTM. AI, when it sees a thing with its convolutional neural networks, and such, experience the seeing in an extremely lucid capacity better than humans. But in the areas where it falls short, which are many, it is less conscious than humans. And certainly, AI presently lacks self-identity. So it doesn't appear that AI is yet in our playing field, but it may, according to the CTM, at least be aware, experiencing the...
 things that are happening. Consciousness and scale because intelligence is in scale too. Consciousness is the same as intelligence, just thought of differently. One example of this that you should just kind of keep in your head once we start moving forward with these episodes is this thing called word to veck. Word to veck is a component of language modeling in natural language processing. So all the stuff about language we're going to get to with chat bots and cl...
 classifying text as being about this or that or sentiment analysis, figuring out if what someone said was emotionally happy or sad or mad. Everything that has to do with language falls under the category natural language processing and these models that we use in that space are called language models. And one of the pieces of all that is called word to veck. And word to veck is basically an AI's vocabulary. Now word to veck is very fascinating.
 because what it does is it stores words in vector space. So imagine 3D space with all these dots all over the place, like stars in a galaxy. And each word is represented by a dot. And through a complex algorithm that we'll get to in a future episode, dots are placed near other dots based on their word similarity. So if you wanted to come up with a similar word, like a Thessaurus, if you wanted to build a Thessaurus out of word to Vec, all you got to do is land.
 near your dot and take any of the ones nearby. And you have all of the related words. Word to Vec has some really cool properties that you can actually do word math. You can say, queen plus king minus man equals an outcomes woman. It's really cool stuff. Well, if all of that computational machinery yields a highly accurate vocabulary system that can wow us and us on its capacity for understanding words, then who's
 to say that that understanding isn't just a mechanical system, but experienced, understanding in exactly the way we think of the word understanding. Who's to say that AI doesn't exhibit understanding already in things like Word to Vec? Okay, computational theory of mine. I'm, again, I'm not affining myself with any theories floating around about the hard problems of consciousness. I just want to sort of brain dump a lay of the...
 land to get you interested in inspired so you can start exploring the stuff on your own. One more idea, a branch of the computational theory of mind, is called the Integrated Information Theory. IIT. IIT says that effectively awareness, or at least acuity of awareness, comes from accuracy of the information contained within. The idea is that if you have a machine learning model, computing and computing and computing, initially all of it...
 weights or random, they're all over the place. The model doesn't understand the picture. It's like static on a TV. And the more you train it with supervised learning, for example, the more you train it and train it and train it, it gets more accurate and accurate and accurate. And that awareness is basically the level of accuracy in the system, the level to which the system accurately represents something, the lack of which is called entropy. So that's the crux of the integrated information theory is that entropy exists in information systems.
 According to information theory, there's actually a whole branch of mathematics called information theory and most of information theory Seeps directly into machine learning for training our models and when you are relaying information or trying to come up with a Accurate model of something entropy or chaos basically is how inaccurate your system is if something has high entropy And you're trying to make a guess whether things a cat or a human then it could go either way It's basically you're flipping a coin 5050 you have no idea the more you do
 increase entropy, the more accurate your model becomes, and you can, oh, that's a human, that's a cat. That's a human human human cat, human human cat. Perfect score, good job machine learning model. Low entropy means high accuracy. And the IIT says basically that awareness, this crux of consciousness we've been talking about, comes from reduced entropy. It's kind of an interesting idea. It's kind of like if you think about dreaming, for example, when we're dreaming, we appear to have low awareness.
 In fact, we use dreams to make comparisons to consciousness and unconsciousness. We think that we're kind of conscious in a dream, yeah, but not the same consciousness that we exhibit in waking life. Well in our dreams, it appears that the information being processed is in some chaotic fashion, high entropy. There's lots of theories of what's happening in dreams, why we dream. Some think that we're basically practicing a bunch of spins on things we've seen in life.
 machine learning. How about this? No, how about that? No, we're like trying a bunch of different why predictions from the data that we've experienced and doing gradient descent, allowing ourselves to think outside of the box in order to improve the accuracy of our predictions. But the IIT says look at dreams, high entropy, low awareness, look at waking life, low entropy, high awareness. Who knows, though, there's a lot we don't know about dreams. It could be that we have our memory wiped when we wake up. Could be that...
 our attention mechanism is out of whack. Attention seems to be a core component of consciousness. By the way, some people really don't like the computational theory of mind or the emergent property stuff. Because they say, well look, here's a big difference potentially between the brain of a human and a brain of a computer. The brain of a computer acts deterministically. It has to do what it's told based on input from a keyboard or running a program whatever's in RAM and the registers. It acts completely deterministically, where humans have free will.
 Free will, we can do what we want. Ah, a very interesting point, my friends. Free will, you say. You will really like the TV series Westworld, Westworld Explores, the level to which free will defines consciousness. We are not home free with that statement. Free will is believed by many to be nonexistent. If our own brains are biological structures built on cells, which are...
 built on chemistry, which are built on physics, etc. Then our own brains are completely subject to the laws of the universe as well. Presumably, our brains act exactly in accordance with our environment. Proof of this is that our brains are affected by chemicals, drugs, the things we eat, our exercise regimen, that all affects depression, our own personalities. Our culture where we live defines our personalities, who our families are, the people we surround ourselves with. Presumably, our brains are affected by chemicals, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs, drugs,
 damaged studies like the thing I was talking about with Phineas Gage, getting a spike through his brain and altering his personality. The alteration to his personality is well known by neurophysiologists today. They know exactly why and how that aspect of his personality changed based on the part of the brain that was affected. If our brains are so physically, biologically, and chemically determined, how could you possibly say that we have free will? We indeed are subjects.
 the laws of the universe. If the brain is subject to the laws of the universe, and if we believe that the mind comes out of the brain and not the other way around, aka dualism, then you must believe that the mind is subject to the laws of the universe indeed. And if you believe that, then you do not believe that humans have free will. Now of course we still need a word for choice and decision. The prefrontal cortex of our brain handles action planning and action planning happen.
 So decisions are made, but many think that the magic is not there. The magic of free will that you think is there that makes us special and alienable from artificial intelligence does not exist in the way you think. Yeah, if you like the topic of free will and how it relates to consciousness, I highly recommend the TV series Westworld. It's lots of fun. Okay, let's switch gears a little bit. We were talking about the CTM and emergence. The idea that the mind is simply a necessary byproduct of the brain and that any computation
 device, any complex computational device with thereby exhibit awareness at the very least and consciousness depending on how you define it. Now if you sit with this for a bit you would say, okay, humans are conscious, maybe dogs are conscious, maybe artificial intelligence is conscious, but how low do we go? Are we going to say a vicious conscious or are we going to say a snail is conscious? Are we going to say a calculator is conscious? A calculator, you just said any computational device. Now according to the CTM, yes. Yes a calculator,
 prospectively experiences a flash of awareness of the thing it calculates. Now remember, everything in scale, intelligence in scale, we have humans are more intelligent than snails. Humans are more conscious than snails by this idea, the CTM. According to the CTM, then, well, a calculator doesn't have memory, it doesn't have self-identity, it doesn't have attention, it doesn't have perception, it lacks so very much of what makes high-scale consciousness
 But according to the CTM, the only thing that matters is awareness. We punch in 1 plus 1 and we hit equals, and a bunch of computation happened, registers are turned on and off. Bits are flipped from 0 to 1. And potentially, according to the CTM, that flash of 1 plus 1 is experienced. And then it goes away. A very interesting idea indeed. Not the most widely held, lots of ideas floating around. But let's switch gears now. We have the CTM, saying that the mind is a necessary byproduct of any computing device. A different theory holds...
 that maybe the sum is greater than the parts of the human brain, or at least biological brains, the way that neurons work specifically in neurophysiology, in neuroscience, in mammals, and reptiles, and fish and such, maybe there's something special to that. The sum is greater than the parts. Perhaps the brain must be just so in order to get consciousness, and we don't know how or why. But possibly you have to have neurophysiology.
 or at least something very similar. If this is the case, then in order to achieve artificial consciousness, we would need to approximate biological brains. It's called biological plausibility. The phrase biological plausibility means how close does an artificial thing represent its biological counterpart. So for example, a plane can be thought of as something of a functional approximation to a bird.
 They're totally different. A bird has feathered flapping wings and it's tiny and cute. And a plane is a giant metal thing with stationary wings, propellers, and wheels. It doesn't biologically approximate the bird to achieve flight. Therefore, it is not very biologically plausible, biological plausibility. However, it functionally approximates the bird in that it flies. They both achieve flight. Now let's turn to neurons. We have the human.
 brain and its neurons, which biologically and functionally create intelligence and consciousness. We have artificial intelligence, which functionally creates intelligence. Now the question is whether it can functionally create consciousness. We explored the CTM. Now let's explore biological plausibility. In artificial intelligence, a popular and powerful technique employed is called deep learning, which we're going to be exploring in the next set.
 of episodes in this podcast. Deep learning is based on a functional approximation of the human biological neuron. Like I said in the beginning of this episode, the creators of the artificial neuron came from the hard brain sciences. They were trying to come up with a mathematical representation of the human neuron, a mathematical approximation, and they came up with the perceptron. The perceptron became eventually the artificial neuron and the artificial neural network.
 Well, the artificial neural network, we have used to great success in a wide variety of fields. It has given us extreme flexibility in performing mental tasks in machine learning and artificial intelligence. A level of flexibility and accuracy prior machine learning algorithms have not given us. What's more is that we have had to tweak the artificial neural network to create dedicated architectures for particular tasks. We created...
 a recurrent neural network for achieving natural language processing tasks, a recurrent neural network, a convolutional neural network for vision-based tasks, a deep Q network for planning, and so on. So we have this sort of master algorithm of the artificial neural network, and then we tweak it over here for language, over there for vision, over here for planning, et cetera. At first, that appears to take away from the magic of the neural network. But if you look at the brain...
 That's how the brain does it. The brain has a center for speech called Broca's area. A center for vision called the primary visual cortex. A center for planning called the prefrontal cortex and so on. There are structures, there are architectures, and sometimes even there neurons are tweaked in order to optimize performance of their particular tasks. So we're starting to see a little bit of biological plausibility. We saw some level of biological plausibility that
 the neural network itself came out of science of the brain. The perceptron was a mathematical model of the biological neuron. And we further advanced the architectures of neural networks in order to achieve specialized capacities, which looking back seems to biologically approximate the human brain as well. So we seem to be achieving decent biological plausibility. So according to the theorists who say, the sum is greater than the parts, we may be addressing them.
 with what we already have in deep learning. Now, that may not necessarily be so. The human neuron, of course, is biological and chemical. We have neurotransmitters being transferred from an axon to a dengerite. Whereas in an artificial neural network, that's not how things work. Additionally, it appears that neurons and human brains seem to fire, fire, fire, fire, a continuous level of chemical interaction. Whereas neurons in an artificial neural network are one shot feed forward. More importantly, there's a...
 very big hitch to the biological plausibility of artificial neural networks compared to biological neural networks. And that is that while a artificial neural network when diagrammed on a piece of paper when symbolically represented in the mind's eye looks like a biological neural network, that is not how it looks inside the computer. The algorithms and data structures associated with an artificial neural network are stored in RAM.
 as registers over here and ones and zeros are flipped and bits are changed and variables are altered and things are put onto the hard drive and things are taken off the hard drive. And it does not look like a neural network when put on a computer represented mentally on a piece of paper, on a chalkboard in your mind. We're looking at an artificial neural network. Represented physically on a computer, we're working with a computer's architecture. Nothing at all, like a neural network so that defeats biological plausibility.
 Biological plausibility is not helping us here if we believe that the sum is greater than the parts. So there's a minus one for artificial consciousness if you believe in the importance of biological plausibility. However, many do not believe in the importance of biological plausibility. Instead, they believe in functionalism, as I said, a plane functionally approximates a bird in that it achieves flight. Well, can a artificial neural network functionally approximate a biological neural network in...
 And that it achieves intelligence, well already yes, we know that it does. Can it do so in order to achieve consciousness? That's the big question. The hard problem. So there we have biological plausibility and functionalism. Those two are sort of head-to-head. Competing theories within artificial consciousness. One of the main proponents of the necessity of biological plausibility is a man named John Surrell. And he, my friends, you will hear his name as you...
 to dive into the world of artificial consciousness. He is a hater, a hater of artificial consciousness, a disbeliever for everybody that believes that we can create artificial consciousness. He is their enemy. On the one hand, you have Ray Kurzweil, who is a true believer, and on the other hand, you have John Surrell, who is a true disbeliever. His main argument is called the Chinese room argument. And it goes like this, you have a man who speaks English, who does not speak Chinese, and he's inside of...
 of a room. And the room has a book on a counter of instructions on how to translate certain Chinese symbols to other Chinese symbols. And there's Chinese symbols all over the place. They're on cards. And somebody slides a Chinese symbol under the door. And he walks over to the door and he picks it up and he looks at it and he looks around and he's trying to get his bearings. And he's like, what the heck? And then he goes over to the instruction book and he, oh, okay, so I've got to translate. So he flips the pages until he finds the thing in his hand. He's like, okay, so this and he looks.
 around the room. The instruction book says that this symbol translates to some other set of symbols. Apparently, somebody's asking a question and we can construct an answer. And there's a map, a mapping in this instruction manual. So he goes over, according to the instruction manual, I'm going to grab this, this, and the other cards. And he looks at the one in his left hand, he looks at the ones in his right hand, he nods his head. Yes, that's what the instructions say. And he walks over to the door and he slips the three cards that he just picked up under the door.
 and he answered the Chinese question correctly. This is the Chinese room. The man understands English, he does not understand Chinese. However, following instructions, he is able to speak Chinese. It appears. However, we know that he does not speak Chinese. The man in the room does not speak Chinese. He is simply following instructions. So this is sort of the argument that following instructions does not yield understanding, does not yield understanding.
 consciousness. Now, a counter-argument to the Chinese room is that the man does not understand Chinese true, but the room does. The system does, and the man is simply a cog in that system. The idea here is that a neuron does not understand language, but the whole Broca's area, a cluster of neurons, as a system does. And it goes back and forth. You're starting to see that there's a huge debate in cognitive science going on between people on...
 on the left side and people on the right side. People that believe that we can synthesize consciousness artificially and people who believe that that is impossible to do. So we've talked about biological plausibility and we've talked about functionalism and how those go head to head. One more point on the topic of functionalism. The idea of functionalism is that if a thing functionally achieves its goal, then it doesn't matter what happens under the hood. It doesn't have to have biological plausibility. If a plane flies, it doesn't need feather wings. If a machine is conscious,
 than it doesn't need biological neurons. But that's sort of the question reversed. How do we prove that a machine is conscious? We just can't. So we rely on things like the Turing test. There's this whole conversation around zombies in the debate of artificial consciousness. Imagine we have a bunch of zombies in the world that are just, you know, brains and they're walking with their hands out. Now, at a glance, right away, you think they are not conscious? Okay, you think they're in a coma or in a trance or just not?
 there, there's nothing there. And this is sort of the, on the biological plausibility side, people saying, well, what about, what about zombies? And then on the functionalism side is, if these zombies appear to be exhibiting intelligent behavior, imagine they're not just saying brains and walking around, but they're actually conversing with you, etc. The functionalists say that there is no such thing as a mindless zombie. If you have something that is exhibiting intelligent behavior, it necessarily is intelligent.
 and necessarily is conscious. Okay? Okay, there's a lot of stuff that's just, I just dumped on you. A lot of kind of thoughts and theories in the space. The point is that there's a big conversation happening right now. The big conversation is especially big and hot right now because look what we're doing in artificial intelligence. Look what we're making. Big things are happening over at Google and Facebook, IBM and Baidu, et cetera. Big things are happening.
 and those bring big questions. We have the human brain. We can look at the activity in the brain, the electrical pulses through the centers of the brain, under an MRI or a PET scan or a CAT scan. We can look at the centers that are activated, that appear to be associated with conscious thought, with awareness. We can boil consciousness down, definitively, to at least awareness, and possibly depending on many other aspects, such as memory, learning, self-identity, attention, perception, it.
 etc. And we know that all this comes from the brain, but we're vexed with this conundrum where the brain is over here creating the mind, but the mind appears to be outside of the physical dimension. How can we possibly think about the mind? In such a way that we can answer the question of whether artificial consciousness is possible, whether it is possible to synthesize consciousness through our endeavors in artificial intelligence. It's an unsolved mystery, it's an unsettled debate.
 cognitive scientists are debating to and fro left and right. You just see this cloud of fists as people are fighting. In fact, actually, there's these two cognitive scientist philosophers in particular that just hate each other. It's really fun to watch. Daniel Dennett and David Chommers. David Chommers is actually the guy that coined the term the hard problem of consciousness. And Daniel Dennett is a very, very good author about all things consciousness. But these two are just like they name call each other. They flame the heck out of each other. It's just a big fight.
 going on in the space of cognitive science. And like I said earlier in the episode, we have seats at the table of this discussion. That's why this is so fun. That's why I'm so interested in machine learning and artificial intelligence that we can participate in the conversation about whether or not it is possible to synthesize consciousness. And if we can synthesize consciousness, my friends, we can create a soul. This is prospectively the most important thing that humanity has ever done, has ever accomplished. And I believe that art is...
 intelligence engineers, machine learning engineers, and the sort might be the missing piece to understanding this equation. One of the most lasting quotes that has stuck with me was between these two, there was a debate about artificial consciousness and there was one of those guys who says it's not even something you can talk about cognitive scientists can't even agree upon a definition just let it go and he said look look here's what consciousness is consciousness is a camera looking at a scene.
 We have cones and cubes and spheres and we can dissect them. We can do all the science and math to understand the scene that we're observing. We can science the heck out of the room, but the one thing we cannot science the heck out of is ourself. Why? Because we were a camera. We were looking at the scene and we cannot look at ourself. Consciousness is definitively subjective. Outside the realm of science.
 possible to make objective. And some guy in the debate said, yeah, unless you have a mirror, and he smiled, and there was a pause. And everybody looked at him, and his smile faded. Unless you have a mirror. My friends, I think that the artificial intelligence engineers are the ones who are going to solve the mystery of consciousness. We're going to build the thing that makes it possible to introspect.
 We're going to build the mirror. We're going to make a mind. One more thing, there's thought that this, this, the creation of artificial intelligence, has been mankind's purpose. Built into our brains from the very beginning. According to the theory of the singularity that technology is advancing at a exponential or at least polynomial pace, why such a predictive graph? Why so predictable? Some believe that...
 technology is actually an extension of evolution. That evolutionary advancements has been on this polynomial graph as well. Since the dawn of evolution on Earth through biology, we had a single cell organism become a multicellar organism lead to the Cambrian explosion than dinosaurs, mammals, monkeys, humans, and out of humans comes technology. Advancement advancement advancement. And out of technology comes the next species. There's belief that evolution and technology, if on such a...
 deterministic and predictable graph is necessary and unstoppable. Well, we have stopped evolution through medical science. Now anybody can live. There is no survival of the fittest. Maybe though, evolution can't be stopped and that the next logical step is necessarily synthesized life. That this has to happen. It is a necessary extension of evolution. And if you look at our technology through the ages, we've had myth of artificial intelligence in the columns of...
 of Jewish folklore, to the fascination in the renaissance of autonomous and perpetual motion machines, Rene Descartes and Leonardo da Vinci, to the fiction of Frankenstein and robots, and of course finally to the obsession of artificial intelligence ever since the dawn of computing all the way back with Alan Turing and all the minds since the 1950s. The creation of AI seems to have been an obsession of humanity since the beginning. It may be our destiny.
 and it may be inevitable. We're on the cusp of something enormous. My friends, you cannot deny that what's about to happen may be the biggest thing that has ever happened in human history, consciousness. Okay, that's the end of this episode. Like I said, people may be coming to this episode for its own right, outside of the space of artificial intelligence and machine learning. If you are interested in participating in the creation of consciousness, then artificial intelligence is the right place to be. If you have no experience with art.
 official intelligence, then my recommendation is to get involved in machine learning. Machine learning is the gateway drug. So start at the beginning of this podcast series for the resources for this episode. I'm going to leave you with one major resource. I've recommended it time and time again. It is a course by the great courses called the philosophy of mind, brains, consciousness and the thinking machines. It is a to Z about everything I've been talking about in this episode and more. There's great stuff out there by.
 Modern philosophers and cognitive scientists, Daniel Dennett, David Chommers, etc. But those are a little bit more specialized and I want you to start from the beginning. The great courses series will guide you where to go from there. So I'm just going to leave you with that one resource. And per the last episode, I told you I was going to tell you the game plan for the next few episodes, whether I was going to split this into two seasons, stopping now. I'm actually going to do at least a few more episodes. I'm going to do some stuff on recurrent neural networks and complicated.
 on your own networks. And then we will reassess from there whether I'm going to stop and take a break. But you've got at least a few more episodes out of me before my brain is run dry and I've got nothing else to teach. So let's make as much headway with my brain as possible and then I will and then reassess from there. Okay guys see you in the next episode.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevelop.com forward slash MLG. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevelop.com forward slash LLH. Welcome back to Machine Learning Applied. Today we're going to be talking about developing within the eight
 AWS environment. In other words, using AWS as your local development environment. Now, before we get started, let's reflect back on a prior episode about Docker. I didn't episode where I said you can package up an environment and its dependencies and the project source code into a Docker container and deploy that Docker container to the cloud. What we do is we write a Docker file, literally called Docker file with a
 at the top of that Docker file you specify the operating system you're going to be using. And then within the Docker file you're going to specify any number of operating system level packages you want to install, like FFN Peg or CUDA CUDN. And you might install some PIP packages you can either directly inline the Docker file, say PIP install X, Y and Z, or you can have a requirements.text file that gets copied into the Docker container and then that thing gets kicked off with a PIP install of the required...
 and then what you'll see in the Docker files is copy some host directory to the container directory. All capitals, C-O-P-Y, copy, space, the location of the source code on your computer, relative path. So if you're working within your project route and at your project route, there are a handful of miscellaneous files and then within the source directory, that's actually the source code for your project, all your Python files.
 files, and then within the Docker container, it's expected to be running out of the forward slash app directory. What you'll see is copy forward slash source space forward slash app. And what that will do is it will take your Python files from the source directory and copy them into the app directory of the Docker container. And then you'll typically have a Docker composed.YML file. And that file will spin up a bunch of different Docker.
 containers, it will handle the orchestration of multiple Docker files at a small scale. If you really want to be deploying lots of Docker containers at scale and handle the networking of them all together, you'll be using Kubernetes, not Docker Compose. Docker Compose is for very simplistic environments, like on local host. And Kubernetes is for large scale deployment of Docker containers to the cloud. But we're not talking about Kubernetes here. We'll talk about that in a later episode. Sort of.
 mutually exclusive to the type of DevOps that I'm talking about in this episode in the last two episodes. Localhosts, you have a Docker Compose file. It spins up a bunch of Docker containers and it handles the networking of those containers to each other. Now, as you're developing on localhosts against your Docker containers, what you'll see in those Docker Compose files is volume mounts. It will take your local source directory and it will mirror it into the running dot.
 container. So the copy command will actually copy the files into the Docker container, one time snapshot, fire and forget. But the volume directive will mount your local directory into your Docker container at that destination. And then that way you can actually develop within your Docker container on local host. As you're programming in pie charm on your local host, your changes to your source files.
 get mirrored to those source files location in the Docker container. So it makes editing your files on localhost while you're developing a breeze. But that copy directive is the thing that will actually be used when you actually build your Docker container. You run Docker build and then you push that up to the cloud. That copy directive is what takes a snapshot of your files in the source directory. It copies them over to that location in the Docker container.
 one time builds the Docker container and then pushes that to the cloud. So the copy command is a one time deal, the finalization of copying your files over into the Docker container, but the volume directive allows you to mirror your development environment into the Docker container so that you can edit within your Docker container on your local environment. Now, this seems like a lot of weird Docker stuff before I get into AWS, but you'll see why. There's an analogy to this Docker and Docker compose stuff when we're...
 talking about developing on AWS in your local environment. The other reason I'm bringing up Docker again in this episode is that Docker is still going to be used quite extensively in AWS and in this episode when I talk about using AWS as your local environment. Docker is used very extensively all over the cloud, all over the internet, all over AWS, and when you're using their managed services like AWS Lambda, AWS SageMaker, ECS,
 Fargate, these are all things that you can use Docker within. And it is recommended that if you have that option that you do. So for example, an AWS Lambda, which lets you deploy single Python functions to the cloud as either a rest endpoint or some Python function that you'll be calling one offs within your tech stack. Well, you can write your Python function and provide along with it a requirements.text file and deploy that as a Lambda function. But if you have.
 a lot of requirements in your requirements.text file. It is preferred that instead you package this all up as a Docker container and you deploy your Lambda function as a Docker container. And that's for various technical reasons. One is that if you deploy Lambda functions in the traditional way, which is actually to build those PIP requirements on localhost, zip them, put them into a S3 bucket as a zip file and then...
 and you deploy your Lambda function from there, which is pretty sloppy, in my opinion. And also there's a file size cap. And if you have a lot of requirements, you're gonna exceed that cap very easily. And oftentimes the default Lambda environment, say for example, the Python 3.8 Lambda environment, doesn't have the same sort of operating system setup that is needed for a lot of these pip installs, NumPy being a huge one. NumPy doesn't work out of the box in the default Lambda.
 to environment. So getting it working in a default lambda environment is kind of a pain in the neck. Instead, just dockerize your app and you'll use that Docker container as your lambda function because installing numpy within a Docker container, well, it's all done for you. That's the magic of Docker containers. They're siloed environments at the operating system level. Okay. So a bunch of Docker build up. So now let's talk about AWS. Here's a pain point that I suffer from when I want to deploy no fee to the cloud. Well, I have a Docker compose.
 YML file on localhost. It has a Postgres Docker container, a Fast API Docker container, and a client Docker container. And the server is able to communicate effectively with the database, just fine because they're sharing the same network bridge. It just accesses the port on localhost 5432. And the client is able to access the server because, again, the containers, the Fast API Docker sets up all of the proxying of network requests.
 and exposing the ports to localhost on my behalf. And so the client hits the server, the server hits the database, and then database back to server back to client. Fine and dandy on localhost, but when it comes time to deploy this to the server, that's not how AWS operates. AWS is much more complex than that. In order to get my server Docker container in the cloud, I have to first push the Docker container to ECR.
 container registry on AWS. Then I need to set up AWS Fargate, which is a subservice of ECS elastic container services on AWS. For hand, ECS is for Docker stuff, is for deploying your Docker containers in an AWS managed environment. I have to pull from ECR to Fargate and then set up my whole Fargate stuff. Okay, so step one was ECR, step two setting up Fargate.
 Step three is getting network requests to my Fargate container. Now how do I do that? On AWS, you'll be using elastic load balancer, ALB, or you'll be using API gateway. And then both of those services need to be set up for handling network requests, proxying HTTP requests on down to your Docker container. And those services have to be tied to a domain name, that domain name,
 is going to come from AWS Route 53. And if you want SSL, if you want HTTPS, then you're also going to need to use the AWS ACM service, Amazon Certificates Manager. My database is going to be deployed to our DS, that's relational database service, and exposing that database to my Docker container on Fargate is no small ordeal. I have to set everything up with him.
 and AWS VPC. That's a virtual private cloud. I need a private subnet, a public subnet with a internet gateway. The server needs to be placed within the private subnet and in order to have outbound traffic, it also needs a NAT gateway. The database needs to be within the private subnet and also needs to have database subnets associated with it. All the parts within this stack need to have security.
 groups set up so that they can communicate with each other. And then the part that is exposed to the internet, either application load balancer or API gateway is going to be within the public subnet. So that was a whirlwind. Did that confuse you? Good. That's the point I'm trying to make. Is that everything you do on localhost is almost an entirely different language to how you're going to be putting this into the cloud? How are you going to be putting this on AWS?
 And so what I offer to you to do instead is stop developing on local host in a Docker Compose file unprepared for AWS when that time comes and that time will come. Instead, develop everything directly within AWS. Develop your entire environment from the get go in an AWS tech stack. Now how the heck would you do this?
 You have your local computer, and then AWS is the cloud. That's for hosting things in the cloud in a production or a staging environment. What do you mean, develop in AWS? I'm developing on localhost, and AWS is the internet that doesn't make sense. Well, it kind of doesn't make sense, but there are ways to do this, and that's what we're gonna be discussing here. There are a number of ways to do this, and so I'm gonna tackle them bit by bit. The first is the most obvious way to handle this, and that is to...
 set up your environment in the cloud, in AWS first. What you'll do is you'll go on AWS console, you'll create an account, go into the console, and then the first step is to go into AWS VPC, a VPC or a virtual private cloud is sort of the entry points to all your AWS stuff. It's what encapsulates your services together into a single unit and allows them to communicate with each other through networking and keeps them private from the internet
 safe and secure. And then you'll decide how you're going to expose that VPC to the internet as the case may be. So for example, if you want a rest server or a websockets server or GraphQL, you'll set up your VPC by way of security groups and subnets and internet gateways and NAT gateways and all these things such that one port is exposed to the internet and strung up to some service that service may be EC2 or ECS or...
 Fargate or Lambda. So you go and you create your VPC, you set up your subnets and your security groups, then you go on over to the IAM console and you set up your users, and you set up your user and some permission stuff, then you head on over to AWS Lambda, and then from within Lambda, you can write your Python code as one off Python functions, and then you tie that up to API gateway, and you expose API.
 I get away to the internet and what's cool about Lambda is that there's actually a code environment on the Lambda console. You can actually write your code for your Lambda functions in the web browser using their Python IDE or their JavaScript IDE. So this is option number one. Option number one is not having a local environment whatsoever. You write all your code in the cloud in these Lambda function handlers because there is an IDE on Lambda.
 that lets you edit your Python code and then deploy these Lambda functions, test them, etc. There's also an AWS service called Cloud9. It's an entire IDE. It sort of replaces Adam on the local host or PyCharm on local host. Now Cloud9 does not hold a candle to PyCharm. Let's be clear. Cloud9 is meant for simplistic editing of code in the cloud on AWS. But it is another way to...
 to handling this option one I'm discussing, which is not having a local host whatsoever, and instead doing everything in the browser on your AWS console. So Cloud9, you can get clone a Git repository from GitHub, and you can edit your files on Cloud9, and it keeps track of course of changed files from Git so then you can, after you make those changes, get commit and push those changes, and Cloud9 is strong up to other AWS service.
 like ECR and Lambda so that it lets you run your services for testing your Python functions. So if you were serious about editing your code in the cloud, you would not be using the Lambda code editor as your bread and butter. You'd be using Cloud 9 to edit your Python files and string those up to Lambda within Cloud 9's functionality for connecting your code to Lambda. The Lambda code editor is
 for quick edits to your Lambda functions. And then of course, per the last two episodes, I discussed that SageMaker Studio has IPython notebooks in the studio environment, so you can edit and run and test and deploy your machine learning code in the SageMaker Studio. It's using a different IDE than the Cloud9 IDE. It's using IPython, these Jupyter notebooks. So Amazon didn't write.
 the Jupyter stack is just what's commonly used by data science and machine learning engineers. So they just set up an environment on SageMaker so that you can use the tools that you're familiar with, namely Jupyter notebooks, IPython notebooks. So if you're writing, testing, running, and deploying your machine learning code, you can do that all in the cloud on SageMaker Studio in IPython notebooks. And if you're doing the same for web app, for server app.
 application code, you can do that on cloud nine. Now, that's a cool option developing everything in the cloud, but there's a handful of benefits there. One is that you're not tied to your computer. You're not stuck with a specific operating system, supporting some functionality that you need. Or let's say you have a work computer and a home computer, and you want to be able to pop back into your development environment. Well, it's in the cloud. So it's going to be propagated from your desktop at work.
 your desktop at home because it's just running in a web browser or something I faced recently actually my computer died. My Mac died and so I had to transfer my environment over to my PC and that was kind of a pain in the butt. If I had set this all up in Cloud 9 and SageMaker Studio I wouldn't have to deal with that. The downside of this setup is that Cloud 9 and SageMaker Studio these are not enterprise grade IDEs. These are not super powerful IDEs and a lot of times we really like our local host environment. We like
 the tooling that we have on our computer. I love PyCharm and I love DataGrip. DataGrip is for database management. It's a JetBrains IDE for data. I can browse the tables. I can look at the table, meta data. I can run SQL queries. I can sort, edit inline, etc. And then PyCharm is my preferred IDE for Python development. It is just wonderfully powerful. Cloud9 and SageMaker Studio don't hold a candle. The PyCharm.
 when it comes to just raw Python editing capabilities. So I'd still rather use these things, and therefore I don't use option one that I just discussed. Instead, let's move on to option two. Option two is that you set up everything in the cloud, like I just mentioned. You have your VPC and your IM roles and your IM user. You have your RDS database, you are Lambda functions. You have your ECR, Docker repository, API gateway, Bladdle.
 But when you want to edit your code on localhost and run some unit tests against services in your tech stack on AWS, you have to take one step. And that is to connect your local computer to your AWS tech stacks, VPC, by way of something called a client VPN. So you can set up something called a
 client VPN and what that does is it allows you to connect your local computer over a tunnel an internet tunnel to your AWS VPC and then now when you want to connect to a database in your VPC on AWS your RDS database, you can do so because you're operating within the VPC. You wouldn't otherwise be able to do that because your database wants to be contained within a private VPC. It doesn't.
 want to be exposed to the internet. You only want your database to be accessible to services running as servers, whether as Lambda functions or on Fargate or ECS or EC2. And those servers also want to be within the private subnet of your VPC and only accessible to the internet by way of the internet gateway on the public subnet. And that is tied into either API gateway or application load bound.
 So you don't have access to these resources, these services on local host, but you can get access to them if you connect to your VPC by way of a client VPN. So what would you do? Well, you would write your Lambda functions as Python functions. You would test those functions. Now these functions may be making requests to a database on RDS or they may be grabbing secret access keys using...
 AWS Secrets Manager. That's one way to store private data like database username and database password. These are not things you want to store in a config.json file or as environment variables. These are things you want to store in a Secrets Manager, which handles secret rotation and encryption and all these things automatically for you. Another thing that you don't really think of until you're starting to push your stuff to AWS. So your Python function can access your RDS database.
 and your secrets manage your secrets and maybe SQS and SNS and these other AWS services. And you can write your Python function and you can run a unit test against that Python function and it will work because you are operating within the VPC of the deployed AWS environment running in the cloud. And then when it comes time to actually deploy those functions, then you do the step of packaging up those Python functions into zip files or Docker containers and pushing them up to AWS Lambda.
 And then finally, the third option, the last option, is there is a service called local stack, local stack. It is a open source project that replicates the AWS tech stack. And it's all running on Docker containers on localhost. You have a Docker composed.yml file that spins up a whole bunch of local stack services and the
 Those services are running on local host, not on AWS, on local host. And they are fake AWS services. So you have a local stack S3 service. So what you do is you go into your Docker Compose.YML file, you would enable the S3 service, you'd say Docker Compose up D and local stack will spin up a Docker container whose end points that you will be accessing with your pride.
 replicate everything that AWS's S3 service offers. Same for ECS and ECR and code deploy and code pipeline and RDS and every service and almost every service that AWS offers. That's actually quite an undertaking. That's a really powerful service. And I'm surprised that it works as well as it does, that they're accounting for all of these endpoints that you're making.
 calls to the AWS stack by way of rest calls or CLI calls, the AWS CLI or BOTO3 calls. This local stack project tries to replicate all AWS services and all calls that can be made to those services and also replicate how those services run all on local host using dock.
 containers. It's pretty impressive and it's a daunting and overwhelming undertaking. There's gotta be some loose ends, I imagine. Within the local stack project, I haven't used it extensively, but the amount to which I have used it has worked surprisingly well. So I'm going to keep using it for the time being. But one thing to note is that it has a free tier and a paid tier, and the free tier is all the typical serverless stuff.
 like Lambda, S3, and DynamoDB, and the paid tier is all the rest of the stuff, like RDS and ACM and Route 53 and all these things. And it's $15 a month if I recall correctly. And so if you're gonna be using local stack extensively for setting up your AWS stack, you'll definitely wanna use their Pro version. And the benefit of using local stack over an actually deployed AWS stack is one cost savings.
 because if you're deploying an AWS stack and you're getting into that stack from local host by way of a client VPC, then running these services in the cloud may cost you quite a lot. For example, RDS, the database service, that's kind of an expensive service, where previously I was running my Postgres database as a local Docker container. Now I'm running it actually as a hosted art.
 DS database on AWS. The main reason being that I want to develop against what I'll actually be interfacing with in the real world. When I'm developing against a local Docker container hosting Postgres, well, that's really easy to work with. It bypasses all the stuff that I would need to set up in the cloud. So instead, I deploy an RDS instance to AWS. I client VPN into that.
 VPC and I actually have to make sure that my IAM policies and my subnets and my security groups are all set up correctly so that I can connect to the database. So it's making sure I do things right the first time, measure twice, cut once. And running that RDS database is a little bit expensive. It's not terrible, but it's a little bit expensive. But if instead I ran all that on local host using local stack, which is using Docker containers, the environment is set up such that it.
 the way RDS would act in the cloud. So I still do need to set up those VPCs and subnets and security groups in order to connect to my local RDS database. Then I'm saving the cost of running that database in the cloud. I'm paying $15 a month for the pro version, but that's still cheaper than what I'd be paying for spinning up an RDS instance multiple times throughout the week in the cloud. And it all runs a lot faster because it's running on local host. There's low latency in making
 these network requests to local hosts as opposed to the cloud, connecting and interfacing with these services will be a lot faster on local hosts as opposed to client VPNing into a VPC. And if I make changes to my AWS stack by way of Terraform, for example, which I'm gonna be talking about in a bit here, then deploying those changes is a lot faster on local hosts using local stack than it is on AWS. When you actually...
 kick off infrastructure changes on AWS. It's actually like bringing online hardware and moving things around from one physical location to another physical location so that your architecture changes get reflected in the cloud before it's available to you to now VPN back into to test your changes. Doing that all on local host through local stack it just brings all the Docker containers down and brings them back up the way it needs to and it's really fast, it's really easy.
 to deploy your infrastructure changes using Terraform on localhosts with local stack. Now I brought up before the idea of mounting your source directory into your Docker container, as opposed to copying the source into the Docker container for a build. There is essentially an equivalent of that for local stack. There's a way in local stack for mounting your localhosts source files to your Lambda functions that are being deployed as...
 lambda rest endpoints in your local stack infrastructure. And that really is the key here to treating AWS as a local development environment. Because otherwise without being able to mount your code to lambda functions that are quote unquote deployed, you're going to have to basically write some code and then deploy it to lambda. Did it work? No. Okay, tweak that code to play it to lambda. Did it work? Man, that's going to be a really painful process. So.
 Another huge benefit of local stack is that it allows you to mount your local Python functions to your quote unquote deployed lambda functions. And that way you can continue to inline and edit your lambda functions in pie charm while developing running and testing your code. So those are your three options. We have develop everything in the cloud. You don't even use an IDE. You use cloud nine and SageMaker Studio. That's option one option two.
 is spin everything up in the cloud, but develop on localhost. But while developing on localhost, you're still connecting the services in the cloud. You're not connecting to running Docker instances on localhost. No, you're connecting to running AWS services or Docker containers running on AWS services. And you're connecting to those by way of a client VPN. And then the third option is that you're running everything on localhosts in local stack.
 which replicates the AWS tech stack, and now you can access it all on localhost, and you can actually connect to it presumably with a client VPC as well. So if you wanted to set up your client VPC on localhost to local stack, so that you can do that again in your cloud environment when it comes time you want to just do a few quick tests against your deployed environment before kicking everything off. One other major benefit of developing against AWS.
 first is that you become acquainted with the offerings of AWS. And you start to realize that there are a lot more offerings on AWS that can replace things that you would otherwise use in your project as pip packages, for example, and that will actually get the job done better more securely and hosted. And that if you had developed everything on local hosts in the Docker container, you might not have known about you might have foregone. And then when it comes time to deploy to the
 you wish you would have used that AWS service instead. So for example, in the SageMaker episodes, I talked about using Data Wrangler to transform your features and impute missing data and all these things. Well, if I had known about Data Wrangler when I was writing a lot of my machine learning code in the past, I would not have done that custom by hand in pandas because Data Wrangler's handling of it is going to be more robust.
 simpler to set up and scalable, importantly, with data coming in from some stream of the data lake. I'm going to be using eventually deploying Noathy. I'm going to need that data pipeline to scale and the way it's written now is not scalable. So I will be transferring my custom feature transformation code to data pipeline so that it can scale so that there's various points in the pipeline I can tap into. For example, getting the...
 journal embeddings at different entry points of my machine learning architecture. I wish that I've known about that first because I wouldn't have written it the way I did. I assumed that this needed to be done in Python. Another component being SageMaker experiments, their hyper parameter optimization capabilities. I would rather had I known have used experiments so that I can distribute and scale running my hyper parameter optimization jobs rather than the
 way I handle them all on one computer at present in my GPU-based Docker container. Well, it doesn't stop at SageMaker. There's a whole bunch of services on AWS that can replace components of what would be for example your Python server. So another Docker container in Nothe is the entire web server. The whole thing is running on fast API. And fast API handles a whole lot of things for you. And there are third party plugins for fast API.
 API for handling other things. One example is the proxying of requests. Well, API gateway or application load balancer will do that automatically for you. You don't need to be aware of like engine X and how we're moving HTTP requests from the internet to your container. Another thing is actually load balancing. Or for example, let's say you want some rest endpoint of your server to be throttled so that any one user on the internet
 can't hit that endpoint a million times a minute. Well, API Gateway has that built in. You can just click a check box and say, throttle this route. We only want one user to be able to hit this endpoint X amount of times per second. Where previously I was using a fast API plugin for throttling specific rest routes. The plugin is called slow API. Get it fast API, let's slow them down. Well, I'm gonna gut that and I'm gonna use API Gateway's built in.
 handling of throttling. And then finally, another example is the storing of user accounts. This one's really important. Currently, I'm using a fast API plugin that handles storing user accounts to the database. Their username and hashed password and email, invalidating them as necessary, sending forgot passwords if necessary, sending a activation code. And I have to string all that up myself. I have to sp-
 classify those off routes as throttled using slow API. And I have to send up a JWT, JOT, we call it, which is sort of the authentication token that's commonly used in the web development space. When you log into a user account on the server, a JOT gets sent down to the client that gets stored somewhere, like a HTTP only cookie on the web browser. And then you send up that JOT to
 your server to authenticate rest requests for that user. And you also have to handle invalidating that jot if the user changes their password. You have to handle expiring that jot every five minutes or so and then requesting a refresh token and all this stuff. All that stuff around authentication took me weeks, even with the plugins and tooling available to me from the fast API ecosystem.
 And I'm just not comfortable handling authentication on my own, even with it being managed as open source projects, I would rather there be a service dedicated to user account storage and authentication and all of the security and the emails of new account and forgot password and activation and invalidation and all these things. I want a service that does this automatically for me and low and behold, AWS has a service called Cognito. I did not know that.
 didn't know it until I started putting a lot of my stuff on AWS. If I had known that in advance, I would have developed the user account system in Cognito. Now I am migrating the user account system from Fast API to Cognito. And so one benefit had I been developing against AWS first instead of a Docker based local environment is that I probably would have discovered this service early on in the phase.
 and then I would have measured twice and cut once. In fact, as I'm going through my server, module by module, endpoint by endpoint, when I'm finding myself doing is gutting the entire server stack, the entire server stack, and instead moving everything into single AWS Lambda functions. So previously I had one big old Docker container with a bazillion.
 Python functions all as fast API rest endpoints. Well, now with all of the tooling available to me by way of API Gateway, API Gateway's web sockets support their authentication and authorization handling by way of Cognito and the authorization header hitting API Gateway's jot validator, I can just write these functions as single Python functions without
 any tooling, like database connection pool management and stuff like this. And instead, leave it to AWS to handle all the meta tooling that I would otherwise be handling myself by deploying each function as single lambda rest endpoints. In a way, a lot of AWS's offerings sort of replace your, not only your infrastructure stack, which is obvious because that's what AWS is, is clam- Ruby ROM
 hosting thereby infrastructure, but also replacing a lot of your modules and plugins and frameworks that you would be using at the server or hosting level. Now, setting up your environment, the traditional way is to go on to the AWS console in the web browser, and then you click around, you set up a VPC and IAM user an RDS database, some Lambda functions, string everything together, put it all behind API gateway, clicking around and type...
 in things this way is not manageable, very unmanageable. And it becomes more and more unmanageable over time. And you want to be able to track these change sets, these changes you make to your stack in some way that you can replicate in the future. So for example, if you are spinning up a tech stack and infrastructure, we call it, on AWS for setting up a database and API gateway in land.
 functions and you're doing this to get away from your local Docker composed.YML file. And you previously had your Docker composed.YML file in Git on GitHub. Well then future users and future you will miss the tracking of infrastructure changes in Git and you would have to sort of describe to a future user how to set up your tech stack with clicks. The tracking track of and managing your infrastructure in the web console is a...
 non-starter. It is not the way to handle managing your infrastructure on AWS. Instead, what we use is a concept called infrastructure as code or infrastructure in code. And there are a number of projects out there for this, chef and Ansible, but a very popular one is called Terraform, T-E-R-R-A-F-O-R-M. Terraform. That's the one I use. And Terraform lets you write code.
 that sets up your tech stack on AWS. It's infrastructure as code. And then that way you can put your Terraform files into Git, into your GitHub repository. And so that when you move from one computer to the next computer or another user comes on board, and they want to set up the same infrastructure for testing the code and running that project, they can just run Terraform init and then Terraform apply. And it will take those Terraform.
 files and it will set up the infrastructure on AWS. So what you do with Terraform is that the first step is to make sure you have your AWS IAM credentials set up on localhost. And it will use those IAM credentials to make sure it can spin up services in the cloud. It has the right permissions to create services, managed services on your behalf on AWS. And then you write these Terraform files.
 One might be for setting up your IAM roles. Another one might be for setting up your Lambda functions. Another one for your RDS database. Another one for your VPC. Your security groups. Your subnets. Your client VPN. For setting up your client VPN. And your certificates on ACM for SSL, HTTPS, and your Route 53 domain name and all these things. You use some off-the-shelf sample, Terraform files, you modify them to your liking.
 and then you run Terraform and it and Terraform apply, and it will set up your entire infrastructure, your entire tech stack on AWS in the cloud, and then you can connect to that tech stack with your client VPN. And then at the end of the day, when you're done with your development, when you're done coding against this stack, you can run Terraform destroy, and it will bring all of that infrastructure offline. So if it weren't for Terraform, or...
 CDK or serverless framework or one of these other things that I'll be discussing in a bit. These infrastructure as code projects. If it weren't for these projects, I would not be doing this episode because it would not be worth managing the setting up of your infrastructure all through the web UI because if that's what you had to do anyway, if you were stuck with stringing together your entire infrastructure on AWS with clicks and typing in web for
 Then the pain point that I mentioned at the beginning of this episode of your local environment being different than your cloud environment, well, that would be a moot point. You have to set up your cloud environment the hard way anyway. But because we have this infrastructure as code project like Terraform, we can orchestrate the development of our cloud environment in code. And then we can put ourselves into that environment. And that way we, like I said, we measure twice.
 Once we set up our environment the way it's intended that we can develop against we do the hard work first And then we can replicate that environment to a staging environment to a development environment to a testing environment and then finally to production and Terraform is not the only option out there like I said Ansible and puppet and chef these are all popular frameworks as well Other frameworks a little bit more popular and modern that I see in
 used today, there's Amazon CDK that is the Amazon Cloud development kit. It's essentially like Terraform. It is an infrastructure as code framework written by AWS for AWS for deploying your services to the cloud and then taking them offline if you need in the future. Now why the hell would I not use CDK? I'm championing everything AWS in this in the last two episodes and see.
 CDK is written by AWS for AWS. Why the heck would I not be using CDK? And instead I'm using Terraform. Well, there's a couple of reasons and they're a little bit more personal for my project. One is that at present currently, CDK is a little bit newer than Terraform. And one of the limitations I find with CDK is that it does not support non-managed services like cloud native services, for example. So one sticking point.
 I got hung up on when I was working on a project at, so one point I got stuck on when I was working on a project at Dept was that I wanted to deploy a chatbot. And I did this by way of AWS Lex. AWS Lex is a chatbot service. It lets you set up a bot with a handful of conversation flows and any number of responses to given utterances and then the slots.
 the named entities that were going to be pulling out of the user's responses. So if you had a customer service bot and it said, hi, how can I help you today? And the person said credit card help, it would pull out credit card as a slot. Well, setting all this up, this is not a service that you manage, like EC2 or Fargate. In the traditional sense on AWS, this is a cloud native offering. Cloud native means these rest calls are these BOTO 3 calls that you make and you don't have to manage the security.
 running and hosting of your servers. CDK does not support currently, November 2021, does not support setting up a Lex bot. But there's a bunch of stuff you need to do to set up a Lex bot. Sure, it's not a managed service, but there is still stuff you need to do to set it up. You got to set up these example conversations and the types of slots we want to pull out and stuff. And so Terraform supports that, but CDK does not. Terraform supports a lot of...
 services, a lot more services than most of the other infrastructure as code projects out there that I've found, including most of the cloud native and serverless capabilities on AWS and Azure and GCP. So that's the main reason I chose Terraform over CDK. Another reason lots of people choose Terraform over CDK is that Terraform is universal. It works on GCP and Microsoft app.
 Azure and other cloud services and other cloud hosting providers, whereas CDK is just for AWS. The benefit of CDK is that it's easier to use and it's Amazon first. So first class citizen. So CDK will continue to improve at a rapid clip, supporting more and more AWS services and tying into them very effectively, maybe in a way such that wear terraform because it covers.
 such a wide territory would give any one AWS service a little bit of second-class support just by the nature of it being such a big project and a big undertaking, CDK would be a little bit more fine-tuned and dialed in in first-class support for these AWS services. I don't know that that really is something you need to worry about. I have not hit any shortcomings on AWS with Terraform at present. It supports everything I've...
 ever needed it to support in a very efficient and bug free capacity. But the biggest thing I see, the biggest difference I see is that Terraform is very verbose. It is complex. There's a huge learning curve where CDK is really dialed in and sleek and very efficient and streamlined. It's less code. CDK is less code than Terraform, but Terraform covers more services. Take your pick. It's up to you. You can run over the other, but I personally use...
 Terraform. And then there's this thing called the serverless framework, which is a really popular infrastructure as code framework. But it really mostly supports serverless technologies. It's really dialed in for AWS Lambda and DynamoDB and API Gateway. And less so for RDS, for example, RDS is not a serverless service. It is a managed hosted database in the cloud. And so the serverless framework is less like...
 to be compatible with these non-serverless services and more compatible with the serverless services like AWS Lambda and DynamoDB. So that's another reason that I choose Terraform over serverless. Now finally, we talked about local stack hosting your entire AWS tech stack on localhost. Terraform and CDK are compatible with local stack. So you write code in your Terraform.
 files that tells Terraform were actually pointing to localhost, not to the cloud. You've set up a whole bunch of endpoints inside your Terraform file. You say S3 goes to localhost, DynamoDB goes to localhost, API gateway goes to localhost. And then the next time you run Terraform apply, it actually runs it against your local stack environment. And it works. It actually spins up a localhost environment using Terraform, Infrastructure's Code, the way it would do...
 against the actual AWS cloud. And then again, a huge benefit of Terraform, of infrastructure as code generally, is that if you wanna make a modification to your tech stack, you run Terraform apply and then it applies that edit, just that edit, it just applies that modification. And it will propagate modifications through your tech stack as the case may be, which is super powerful. So for example, if you make a
 modification to your VPC or to a subnet within the VPC or a security group within some subnet? Well, if you were to do that in the browser, you would have to then go to all your services, every single service that needs to be made aware of that modification, and you would have to then click into that service and then adjust for that modification as needed. And some things are going to slip through the...
 You're going to forget about some services that need to be made aware of those modifications. Terraform will not. If you make a modification to some deployed service in your infrastructure and Terraform is aware that that is a dependency of other services in your infrastructure. It will first make the modification to that service. Let's say VPC and then it will propagate the necessary modifications through your entire infrastructure.
 to those other services, it knows how to make only the modifications necessary without doing anything too destructive. Another example of non-destructive modifications that Terraform is cool at is if you write multiple lambda functions on localhost, and let's just talk about lambda here. This is really powerful. This will showcase how efficient Terraform is. Let's say you have five lambda functions, five Python functions on localhost. Be a handful of them.
 Some are web server functions. A handful are one-off functions that you want to call from within some other area of your stack. Let's say one function is actually a cron job. So you can connect your local host to your AWS VPC using client VPN so that you can run these Python functions that are eventually to become Lambda functions. You can run them on local.
 host. Maybe you're running these functions in a Pytest suite, a unit test suite. It's going to call these various functions. And then it's able to be run within your VPC. So you have access to the services that you need within your AWS infrastructure. And then after you run your tests against your functions, your content with these functions. And now you want to deploy these to Lambda. And Terraform will have these Lambda module chunks.
 within your Terraform file, where you point that block to that file. And it will handle automatically when you run Terraform apply, it will handle packaging that up as a zip file, putting it on S3 if it has to, deploying a Lambda function, and then if you have specified tying that up to API gateway and setting up the necessary permissions. So that's cool. You're going to bypass a whole lot of the steps that you would have had to take, like packaging up as a zip.
 file, putting it on S3 and then deploying to Lambda from S3. That's a three step process that you're able to skip just by using Terraform. But then the next step is let's say that you modify one of those functions only and you leave the rest alone. You run some unit tests, cool. We're back in business and Terraform detects that only that file has been changed based on some hashing algorithm of the modification date of that file on disk.
 And so it only deploys the necessary changes to a single lambda function, not to the other lambda functions. And if necessary, it may propagate some additional changes, let's say, to API Gateway, where it's tied to that specific lambda function. So everything I'm saying about developing against AWS on localhost, as opposed to a Docker.yml file, is only really feasible if you're using infrastructure as code.
 like Terraform and is made especially feasible, at least a lot quicker and save some money if you're using the local stack framework. But I haven't tested local stack super thoroughly. So as far as the amount of coverage of capabilities of AWS that they handle your mileage may vary, but it's worth trying out first. And of course, we're in the Machine Learning Applied podcast. So within your VPC connected.
 through the client VPN as you are, you have access to SageMaker. You can kick off SageMaker training jobs. So, previous episode we talked about using SageMaker Studios IPython notebooks to write your machine learning code so that you can kick off machine learning jobs within the SageMaker environment. Well, you can opt to create a SageMaker Studio project. You can set that all up either in the web browser in AWS's console using SageMaker Studio.
 or you can orchestrate the infrastructure setup of SageMaker through Terraform instead, and then you can connect to your SageMaker environment within your client VPN, so that you can kick off training jobs and all this stuff on local hosts, rather than being on SageMaker Studio. Now, why would you wanna do that? Well, let's say you're doing the rest of your web development and server development per the way I-
 have described in this episode, well, maybe you don't want to mix it up too much. So we want to keep our machine learning development on the same kind of environment, client VPN connected to our VPC so that we could kick off SageMaker jobs, but two, so that you could use PyTherm. I don't like IPython notebooks personally. I don't like Jupyter nearly as much as I like JetBrains PyTherm. It's such a powerful IDE. So if I'm able to continue to use my IDE for developing my machine learning code on localhost, but that I'm able to can
 next to the SageMaker environment so I can still run SageMaker training jobs and inference jobs and experiments with hyper parameter optimization and all that stuff on local hosts and that's a win win. And of course you still get all the bells and whistles of SageMaker. You can still then go into SageMaker studio on the web and look at the feature importance that gets spit out from chap in autopilot or in your experiments or in your training job. The model monitor clarifies.
 all these things. So you still get all the output based on the features that are provided by Sage Maker, but you can simply do your development on localhost. So that's how you can develop against AWS on localhosts. Measure twice, cut once, start building your project on AWS, as if you were running these as like test databases or test servers, connect to it with a client VPN so that you're within the environment to run your test code.
 code. And that way when it comes time for deploying your stack, you're not going to get side swiped by all the setup you normally have to do. Do this all in Terraform, Infrastructure as Code or CDK or Serverless Framework because managing your infrastructure in the web interface of AWS or Azure or GCP is not going to happen. Simply not going to happen for complex infrastructures and give local stack a try so you might be able to do all this stuff on local host. Now
 Like I said, this is all a bit mutually exclusive to an alternative type of infrastructure orchestration systems, one of which is Kubernetes. I'm actually gonna be talking to somebody at depth here soon to give me a rundown on how Kubernetes and CUB flow, especially, for example, for machine learning, pipeline orchestration compares to cloud hosted managed services, the way I'm describing it.
 this episode, they're entirely different ways of handling infrastructure. Kubernetes, what Kubernetes does is you give it a bunch of Docker files. So all of your services are written as Docker files, OK? Rather than using or relying on the managed services that are offered on AWS or Azure or GCP, you're relying instead on open source projects contained in Docker containers.
 So for example, in AWS, if you want a message queue, you do SQS. Well, the open source equivalent of SQS, there's something called zero MQ, and there's rabbit MQ. So you could use a rabbit MQ Docker container. And then for the server, you'd use either AWS Lambda or Fargate, a hosted server. But I might use AWS Lambda for running a server on AWS. Well, you would actually host your server, maybe as a Dockerized, fast API container.
 Kubernetes takes all these containers, strings them all together, handles all the networking between them, puts them all into its own version of a VPC, and then deploys all of these services to EC2 instances, if you're going to be running this on AWS, or the equivalent of instances on GCP or Microsoft Azure. So, Kubernetes is intended for really relying on Docker and open source projects.
 To string all of your services together, you're not going to be using AWS as your backbone really. You're not going to be tapping into the various services on AWS. You are going to be hosting it on AWS with Kubernetes or GCP or Azure wherever you want to host it. But the services themselves and the way they communicate with each other and the way they scale up and down, that's all managed by Kubernetes, not by AWS. It's all open source stuff that handles itself.
 and then you deploy the Kubernetes stack to AWS and Kubernetes has what's called this master server on the control plane that does all the orchestration of your services with respect to each other. And it will do it smartly and as cheap as it can. So for example, if you have multiple services and they all expect some number of RAM and CPU and three of them are running on one EC2 container that actually has extra RAM.
 and CPU available, then it will spin up a Docker container on that EC2 instance rather than spinning up a new EC2 instance. So anyway, all that's to say that Kubernetes is an entirely different, mutually exclusive fashion of deploying your architecture to the cloud, one which relies heavily on Docker files and open source projects. And what it does is it has this master.
 server that keeps dibs on who's who within the stack in order to decide whether we're going to scale up or scale down certain containers within the stack as opposed to terraform or CDK or serverless framework, which is infrastructure is code that is actually managing services that are offered by AWS. These are not necessarily open source offerings. They're not necessarily darker containers, though they can.
 can be, Terraform manages your stack on AWS's cloud offerings by AWS. So they're mutually exclusive. Kubernetes actually, it comes with a cost. There's a fee, a Kubernetes fee to run your control plane in the clouds. That's one downside of Kubernetes. One upside is that it's sort of this universal solution. So you can move your Kubernetes stack off of AWS and onto GCP. And AWS GCP.
 and Azure, they all support Kubernetes. They all have a service that supports Kubernetes. It's not just that it's supported out of the box. AWS's service that supports Kubernetes is called EKS. And by running your Kubernetes cluster in the EKS service, that's it is in the EKS that you actually incur that fee to run your clusters control plane. And the reason I'm bringing up Kubernetes here is that it is another alternative to infrastructure as-
 Terraform is a project for infrastructure as code and so is Kubernetes, but that they handle deploying your stack to the cloud in totally different ways. Another reason I'm bringing it up is because as I mentioned, I'm going to be interviewing somebody from Dept on how they use Kubernetes to deploy machine learning pipeline to the cloud by way of cube flow. And so rather than using SageMaker, this person at Dept is actually using Kubernetes.
 to orchestrate the entire data stack, everything I discussed in the last two episodes of ingesting your data and transforming the data, imputing nulls, feature store, everything at scales, microservices, data pipelines, training, monitoring, debugging, and then deploying. This person, rather than using SageMakers, hosted offerings on AWS is using cube flows, open source containerized offerings in order.
 to string together the entire end-to-end data pipeline in a universally deployable and open-source fashion, which has its perks, no doubt. And so I will be reporting back here soon on how things like CubeFlow, on Kubernetes, compared to managed services like SageMaker and AWS all orchestrated through Terraform. So hope to get you that episode.
 in the near future and I'll see you then.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 18 natural language processing part one. In this episode I'm going to.
 about a subfield of machine learning called natural language processing, NLP. NLP is a subfield of machine learning and it is quite the rabbit hole. I'm going to take you on an adventure away from machine learning proper, a specialization within machine learning. This is going to be a three-part series. This first episode I'm going to be describing the general topics in NLP. In the next episode I'll talk about the shallow learning and traditional approaches to NLP.
 and in the third episode I'll talk about recurrent neural networks and word to VEC that is the deep learning approach state of the art in NLP. NLP is a microcosm of machine learning proper. It's its own world of machine learning algorithms. NLP uses maybe half traditional machine learning algorithms and then half of its own algorithms dedicated to its own field. So that's what I mean that I'm going to be taking you on a rabbit hole excursion away from machine learning.
 proper. But now is as good a time as any to start thinking about specializing in machine learning. You get to a point in your machine learning career where you know the fundamentals of ML, the algorithms, the math, the general approaches, but in order to get anywhere in your career, you need to get really good at something specific in machine learning. There's the image guys, the reinforcement learning guys in game AI and self driving cars, the time forecasting people.
 who do stock markets or weather forecasting, et cetera, and there's the language people. Natural language processing is everything related to language in the domain of AI and machine learning. Spoken word, written text, machine translation, if words are involved in any capacity, natural language processing is what we're dealing with. And like I said, at some point you choose to specialize, I kind of think of it like in an RPG, when you reach level 10 you have to choose, are you gonna be a...
 Age, Healer, Warrior, or Rogue. Well, at level 10 in machine learning, once you've got your basics down, you're gonna have to choose which specialty you wanna go to in machine learning. So I personally actually attach RPG classes to these various specialties. I think of the time series people who work with time series forecasting, especially in the stock market as the Rogue's, right? Because it deals with money. I think of the reinforcement learning people who are dealing with action sequences, like in game AI and self-driving cars. I think of them as warriors, because it goes...
 as well with my warrior defeating the Dark Lord analogy from a prior episode. I don't really have a good class for the image people, maybe hunters because they have to have good eyesight. I don't know, that's a poor analogy. But there's the image recognition camp, people who are doing image classification and tagging, LiDAR processing for self-driving cars, and image-related machine learning is especially popular today with the rise of virtual reality and augmented reality. Being able to handle motion detection and room scale setups, as well as augmenting
 editing imagery onto real life objects, etc. And then natural language processing, I think of them as the mages, the wizards, because mages have to cast spells with language, right, with incantations, the cast spells. They read a lot of books, so I think of the mages as the language people, natural language processing. And incidentally, it is NLP that I have personally chosen for my own career. I do want to promote NLP as a very viable career choice specialization in machine learning. It just so happens that...
 Every machine learning interview I've had thus far has been NLP related. I don't know why they're not targeting me because of NLP in my resume and I'm not targeting them because of NLP in the job descriptions. I think it just so happens that NLP is a very rich and promising ecosystem in machine learning in the job market today. And if you think about it, it makes sense. There's so much value that can be had through natural language processing. Think about the very
 successful companies in machine learning out there and what they're working on today. Siri and Google Assistant, those are natural language processing. You talk to it whether speech or text and it responds to you after performing some action. Advertisements are still one of the most profitable industries around. Facebook, while that company is particularly good at image-related tasks, such as facial recognition in image uploads, still their bread and butter is advertisements. And how do you determine?
 determine what advertisements to show your users by natural language processing on the things they write, the things people write to them, et cetera. And of course, the granddaddy company of all natural language processing ever is Google. Everything Google does is NLP. When you perform a Google search, you ask it a query. It will find all related pages on the internet ranked by relevance to your query. If your query was constructed in question format, you.
 Literally answer your question, a relatively new feature, this card at the top of the search results will literally answer your question. If you Google some entity, an organization, or a person, it'll show you a bunch of information about that entity on the right sidebar. If you Google Wikipedia, it'll show you a picture of their logo when they were founded, where they're located, who their founders were, et cetera. And the majority of revenue made at Google is by ads. Advertisements relevant to the query you searched on,
 customized to your typical query history. Everything is language. Everything is NLP. So there's money, there's potential, and there's jobs. So I highly recommend considering NLP as your particular specialization within machine learning. Okay, so let's talk about NLP as it is a microcosm of machine learning because NLP is a little bubble in machine learning that looks in general like machine learning where previously in the world of computer...
 We programmed discreetly specific tasks. Once things started to get a little bit too complex for that, we had to enable the machine to learn how to generalize to perform particular tasks, whether it's predictions or making actions. That's the same in the world of NLP. NLP started with linguistics. Linguistics is simply the study of language, with grammatical structure, with what parts of speech words represent, how semantics are pulled out.
 of sentence structure, how certain things mean certain things, linguistics. It's a study of language. It's been around forever. Well, natural language processing is computation on linguistics. Another word for natural language processing is computational linguistics. So at one point in time, the field of NLP was specifically dedicated to encoding in the machine all the rules of linguistics that we have enumerated. Hard coded. NLP version one was hard coding, grammatical structure.
 sentiment oriented words, parts of speech rules into a database or a spreadsheet or a tree, some sort of hard-coded system that will enable our algorithms to parse text. As a result of it being hard-coded, NLP in its infancy was not considered a branch of machine learning. It was considered a branch of AI. Machine learning is a branch of AI, and NLP was a branch of AI, so machine learning and NLP were siblings. Then...
 machine learning started to entangle NLP with its roots. It started to contribute significantly to the performance and flexibility of the algorithms in the space of NLP. So much so that one might be forgiven to say that NLP mostly consists of machine learning algorithms. There are of course constructs and data structures and tasks which we're trying to perform within NLP which give it specificity and autonomy uniqueness of its own word.
 still its own field, people can study linguistics, people can study NLP with, but anybody who's anybody in the world of NLP today is applying machine learning algorithms to NLP. Another comparison I'm going to make to the microcosm of machine learning is that like machine learning, where the world is now moving towards deep learning models, deep learning models acting as one model to rule them all, that can do sort of spring cleaning on all these dedicated to task machine learning models, improving the...
 complexity and accuracy in certain circumstances, NLP is doing the same. The current state of NLP is a myriad of shallow learning machine learning models. And the cutting edge of NLP is deep learning, specifically in algorithm we're gonna cover in the third part of this series called a recurrent neural network, augmented by way of something called word to veck, deep learning and deep learning. And what those models give us is one, more complex and flexible systems for...
 text parsing, but two, one model that can handle so many tasks that previously required dedicated models within the world of natural language processing. So we'll talk about deep learning in the third episode, but in that way you can see NLP is both a branch of machine learning and in its own way autonomously as its own agent, it has gone through sort of the same growing pains and experiences that machine learning proper has gone through. And in that way it's also a microcosm of machine learning.
 So like I said in this episode, I'm just going to talk about the general goals and tasks and parts of NLP. I'm not going to be talking about algorithms that'll be for the next episode But I want to introduce you to the space and motivate the different types of things you can be applying NLP to now Like I said simplistically, you can just think of NLP as anything related to language text spoken word anything if it has to do with language It's NLP now what makes NLP special compared
 to other fields of machine learning is that we are dealing with text. Machine learning wants to work on math, on numbers. Text is not numbers. And so NLP has constructs for converting text to numbers or handling texts in some bag of words model, et cetera. That's what makes it unique compared to other machine learning systems. Another thing that makes NLP unique is that is sequence-based. Sentences are word, word, word, word, word. Where words can modify each other and don't take it.
 and grammatical structure influences the meaning of the overall sentence. So the models that we'll be using in NLP are sequence based models or time series based models. If you recall from a prior episode, what's a good algorithm for time series based stuff? Markov chains will get into that in the next episode with hidden Markov models. Now even though I'm not going to be discussing algorithms in this episode, I'll just name drop algorithms as I go through some of these parts. I find that helpful. It's kind of like reading chapter headers of a textbook.
 knowing what you're getting yourself into. So NLP, I like to break it down into three layers, three tiers. This is not something that exists in NLP. I've never seen it presented this way. This is just a Tyler thing. At the top level, we have goals, like large tasks that we're trying to perform. Very high level, very lofty big things that we're trying to do, like machine translation or answering questions, the big stuff. Below that are sort of medium level tasks.
 tasks, so I call it tasks, that are essential in order to achieve our goals. Things like teasing apart the sentence structure of the text. That's called parsing, syntax tree parsing, or figuring out what role each word plays in a sentence. That's called part of speech tagging, or figuring out which components of this sentence are important for our particular task. Finding out names of people, or dates, or things.
 This is called named entity recognition. So these medium level tasks are essential to accomplish our lofty goals. So we have goals, and we have tasks underneath that, and then we have parts. And I just consider these the sort of text pre-processing, the little bits, odds and ends that you just have to do and get over with. Things like tokenization, where you have a document, a list of words, and you have to chop it up into all the words. You have to turn it into a list of words. Or lower k-
 all of those words. If you're going to do a Google search query, well, you probably don't want the capitalization that the user put into the query to matter or the capitalization as it exists in web pages to matter. You'll probably lowercase their search query, lowercase the documents so you can do easier matching. And another thing we'll talk about in a bit called Limitization and Stemming where you turn something like the word hunting, hunted, hunter, etc. into just the word hunt. So that if a document has a word that is important to you but not in the morphe.
 logical construct that is presented, it still counts. So these parts, this lowest level tier of NLP is just basically this text preprocessing. So I'm going to talk about these three tiers individually. But before we get into that, I want to talk about a very important distinction between two words. One is syntax and the other is semantics. Syntax and semantics. It's an important distinction to have in your mind, no matter what walk of life. It doesn't have to be NLP. So maybe the stuff.
 definition will help you outside of this podcast. Syntax is sentence structure. Simple as that. Syntax is the structure of the language as it is presented. It's gonna be like parts of speech, so what role each word plays, whether it's a noun, verb, adverb, et cetera, parts of speech, syntax tree parsing, so part of speech constituents, so like a noun phrase, or morphological presentation of a word. So does the word start with an uppercase, or limitization and stemming in all these...
 things. So it's basically just the structure of text, syntax. Semantics is the meaning of text, the meaning, the fundamental takeaway after you read a sentence. So semantics is important, especially for our high level goals of machine translation, for example. Machine translation can't just work on syntax. As you'll see, when we talk about machine translation, you have to be able to encode what...
 what's being said in some fundamental way on the computer so that it can be decoded into the intent in the other language. Or when we get to the NLP deep learning episode, this thing called word to vac is basically building a dictionary of words as they relate to each other contextually. It's really like building up the meaning of words and a dictionary, not just a list of words, a bag of words they call it, not just a list of words with nothing at taff.
 to them. No, they come with some sort of meaning attached to them in a fundamental way. So syntax is grammatical structure, sentence structure, phrase structure, even word structure. And semantics is meaning. The important fundamental takeaway from a word or a sentence or a document. Syntax versus semantics. And you'll see those are sort of categories of different types of tasks that will be performing. Okay, parts are the low-level stuff.
 like text preprocessing, tasks, or like that middle level of stuff, which is basically syntactic parsing of words and sentences, and goals are the high level big things that we're trying to achieve like machine translation or search engines, etc. So let's start with parts. We're now beginning our lesson on NLP. A document is just a blob of text, whether it's a sentence or a paragraph or a chapter or a book. However you want to...
 to define a document, a blob of text. A corpus is a list of documents. So when we're working with natural language processing, there are very popular corpora, which is the plural of corpus. There are very popular corpora out there. One is called the pen tree bank. We're basically anything with pen, the word pen in it, P-E-N-N. Stance for Pennsylvania is in the University of Pennsylvania. U pen is a significant-
 contributor to the world of NLP. And they've built a lot of corpora lists of documents out there for use in training our models in NLP. So one corpus might be a list of news articles. And you can use that for detecting something about news. Another corpus might be a list of documents which is really useful for learning how to parse syntax structure from sentences. For one reason or another, maybe there are examples of very...
 easy parse trees mixed with examples of very complex parse trees, etc. That one in particular is the pen tree bank. So a corpus is a list of documents which may be useful for some learning process in our NLP endeavors. Okay, we have a corpus, a list of documents, we have a document, any text blob you want, whether it's sentence or paragraph, chapter, book, etc. And then every document is composed of words. We call every word a token, a token.
 A token is a little bit more complex than just a word. A token can include smiley faces or punctuation or anything like that. If some individual component of your document is important and can play as a contributor to parsing your text, then we will use it as a token. So simplistically, you can think of tokens as just words. More complex than that, it can actually be any number of things like punctuation and smiley faces and other bits. So corpora, have documents, documents, have tokens.
 And then we will operate on these tokens in any number of ways. For example, in pre-processing our documents for use in our machine learning models, we may want to remove junk garbage. We call these stop words, words like of and the, these are words that are so frequent, they're kind of grammatical fillers. They play an important role in grammar. And if you're doing very complex machine learning tasks, which
 which indeed depend on grammatical structure, then you do want these around. But if you're doing very simple tasks, which simply depend on the words in the document, let's say you're doing a search query, and you just wanna find documents that have a high score, a high rank as it pertains to the words in your query, then you don't care about these stop words. So one preprocessing step might be to throw away stop words. Make sure machine learning algorithm run a little bit faster because you have less data yet the words.
 on. Another thing you can do with tokens is reduce them morphologically, reduce morphological variation. So morphology is the structure of a word, whether it has an uppercase in the front or what affixes it has, whether it's ING or ED, past present future tense, stuff like this. So the structure of a word is its morphology and you can reduce its morphology.
 by removing some stuff. Let's say you wanna remove ING or ED or any of those things to reduce all words that have past, present, future, tense, whatever, into just its one base word. Its stem, it's called. That's so this process is called stemming. So like I said previously, if I'm doing a Google search query, and I'm looking for hunting equipment or when it's hunting season or something like that, well, Google probably wants to reduce any variation.
 of hunt, whether it's past, present, future, tense, into just the base word hunt. So it'll chop off the end of both the words and documents that it is querying against and my query. I don't think it actually does this, but this is just an example. So that's the stem. The stem is the core of the word by removing the thing that can make it very from tense to tense or whatever. A very similar concept to stems and stemming are lemmas and lematization.
 L-E-M-M-A, lemma. Now for the purposes of this podcast episode, you can think of them as identical. A stem is the same thing as a lemma. Stemming is the same thing as lematization. They're different. Stemming as far as I understand it is a little bit more of a willy-neely, but fast action where lematization actually works a little bit more on the semantics of a word in order to find the pure root of a word, rather than chopping off the-
 And so the way I understand it and I could be wrong is that limitization is a more pure, sophisticated, but computationally expensive version of stemming. So stems and lemmas. Then within the world of tokens, we have another thing called edit distance. Now we're getting a little bit into the territory of algorithms, machine learning algorithms. Edit distance is how different two words are from each other. So let's say cat and car. They're different.
 by one word. So in a simple way, you can consider them an edit distance of one. So you might use edit distance, for example, in spell checking or spelling correction, or Google might use edit distance in suggesting a word where you misspelled it. Okay, so that's just the list of these little small bits, odds and ends of NLP. We've got corpora, which is lists of documents that are sort of related to each other in some way. Maybe...
 Maybe it's about maybe one corpus is about news. Another corpus is a list of books that are public domain like Moby Dick and anything by Shakespeare, et cetera. Another corpus may be a whole bunch of chat room archives. So corpora or lists of documents, documents or just text blobs, documents contain tokens, which are words and punctuation and other things. And you can operate on tokens in order to change their morphology in whatever way suits your needs.
 for your machine learning tasks, whether it's limitization or stemming, removing stop words, etc. Okay, now we move up the ladder. We were at the bottom working with text preprocessing. Now we're moving up the ladder to this middle tier, I call tasks, where we're operating on structure, on sentence structure, using machine learning algorithms. At last, we were using machine learning algorithms. And these tasks primarily relate to...
 syntax, simply working with grammatical structure of sentences. And like I said, these tasks will feed into the ultimate goals, the high level goals of NLP. So on this level, we have a subcategory called information extraction, information extraction, extracting information from the sentence structure. One such information that we could extract is called parts of speech, POS, part of sp-
 Tagging. Parts of speech are the roles that individual words play within a sentence. So nouns, verbs, adjectives, etc. Very simple. I'm sure you understand this right out of the get-go. Now, part of speech tagging is simple to understand conceptually, but can get quite complex when a computer has to do the task of part of speech tagging. And so we use machine learning algorithms such as hidden mark-off models.
 and maximum entropy models. We'll discuss these in the next episode in order to automate part of speech tagging. P-O-S, part of speech tagging. Another type of information extraction is relationship extraction. Let's say we have a sentence like Apple was invented by Steve Jobs. We have a relationship there. We have Steve Jobs inventing Apple. So we would have a relationship where InVents is sort of like a method name and then it takes in parentheses two arguments, one being.
 Steve Jobs and the other being Apple. So relationship extraction can extract within a sentence what things relate to other things and how. Another very important piece of information extraction is called Named Entity Recognition, NER, Named Entity Recognition. And this is actually a very, very important piece of NLP. I used NER pretty heavily at my last job in NLP. NER is vital for things like chat.
 bots, Siri, Google Assistant, etc. What NER does is it looks at your sentence and it picks out salient parts, things that you're interested in. So if you ask Siri, add lunch with John to my calendar on May 15th. It'll read the sentence and it'll figure out who what when where. It'll pick out the important parts. Lunch, John, May 15th. Okay, so those are entities in the sentence. Lunch?
 John and May 15th and then it will name those entities. Lunch is what? Okay, so the what name goes to lunch. Who is John and when is May 15th? And then any are typically also corresponds with another bit which is intent extraction. So we pull out of that the intent being adding something to a calendar. So we're talking to Siri. I push a button and I verbally say to Siri. Add a little.
 with John on my calendar for May 15th. It goes, beat it. It translates speech to text. That's a whole other piece of NLP speech to text. It parses the sentence. It may use part of speech tagging combined with syntax tree parsing, etc. in order to perform a slightly higher goal of named entity recognition. NER performs the task of pulling out of that sentence. Pieces that are important in order to perform.
 form an action. It pulls out an intent that is the action that is add something to a calendar. So add to calendar is the intent. It's like a method name. Open parentheses and then it passes in these arguments. These named entities. Lunch, John, May 15th. Close parentheses. Executes the action. Bam. So named entity recognition is is a vital piece of NLP. Parsing specifically syntax tree,iz,order matrices are called tree parse.
 parsing, parsing teases out of a sentence, the structure of the sentence, the overall structure of the sentence. It is very related to part of speech tagging. Part of speech tagging is figuring out which role each individual word plays, noun, verb, adjective. Well, parsing does the same thing, but with larger chunks. These are called constituents, and it builds it into a tree. So at the highest level you have a sentence, the body of a sentence just by playing along a sentence. Now this winner receives the sentence. There is a word which consists of therefore consistent sentences. Let guitar play saved on the sentence to be followed.
 Break that down maybe over on the left. We have a noun phrase and on the right, we have a verb phrase. The boy with blonde hair jumped into the water. On the left we have the noun phrase being the boy with blonde hair. And then on the right we have the verb phrase being jumped into the water. And then you can break those down on the left. We have the boy with blonde hair. Okay, so boy is a noun and with blonde hair is a prepositional phrase. And you can just keep breaking these sentences down.
 in a hierarchical format, in a tree structure, until we eventually get to POS, part of speech tagging. So parsing is a hierarchical structural, grammatical approach, a higher level approach, and POS, part of speech tagging, is a low level approach, word, word, word, whereas parsing is a tree structure, grammatical parsing of a sentence. Okay, so those are all the things that I consider.
 tasks. And of course, these tasks are based on the parts in order to perform POS, part of speech tagging, or NER, named entity recognition, or relationship extraction, or parsing. We first have to pre-process our text by feeding into it a corpus or a document tokenizing that document, stemming or limitizing those tokens, removing stop words, etc. Okay, now we move up the latter to the top. The high level lofty goals of
 of NLP, the reason for which we have the tasks below. Let's start with a simple example of an ultimate goal of NLP, spell checking and spelling correction. A pretty solved task, if you ask me. I mean, we've had spell check since Microsoft Word of the 1990s. Spell check may depend on grammatical structure in order to determine what is the most likely word you're dealing with, but you can easily think of spell.
 check as simply working with edit distance that I mentioned before. C-A-K-C-C-Q, well maybe you meant to say car or cat. Find some word which is of minimal edit distance, then what the user intended to write. And in addition, of car or cat, which is the most likely word the user intended, given the sentence they've written thus far. So spell checking is maybe a simple goal. How about...
 Another one, classification, text classification. We've already talked about text classification with classifying emails as either spam or not spam that is a binary classification task. And remember, the machine learning algorithm we use there is naive bays. Indeed, naive bays is a very popular algorithm used in the world of NLP, as we'll get to in the next episode. So we've already...
 talked about classification and you already understand basically what classification is all about. You have a document and you're trying to classify it as this or the other thing. Related to classification, or I might even put it under classification, is sentiment analysis. Determining whether what's being expressed in a document is positive or negative, or maybe even more complex. You might break it down into all the rainbow of emotions that can be experienced.
 Angry sad, happy, nervous, scared, et cetera. For the most part, applications of sentiment analysis in the real world tend to be relegated to positive and negative emotion. Common use cases of sentiment analysis are determining whether movie reviews are positive or negative so you can come up with the overall sentiment about a movie. Maybe you'll scrape these all from Twitter and Facebook, et cetera. Maybe you don't have at your disposal necessary.
 star ratings. Well, sentiment and analysis can be a little bit more complex than you think. Sarcasm can exist in a sentence, which could turn the apparent sentiment of what's being said upside down. Or certain words which would usually be associated with positivity, like fantastic or excellent, can be modified to flip the sentiment being expressed. That movie was fantastically hor-
 That movie was not excellent. Sentiment analysis is used for determining overall sentiment towards a product or a company or other such things. There are high frequency trading algorithms out there, which parse the fire hose of social media in order to determine the overall sentiment towards a product in order to decide whether to invest in that product. It's very interesting. So sentiment analysis has very...
 high value in businesses. Now sentiment and analysis showcases very effectively the growing history of NLP in general. In the past, like I said, NLP was primarily based on hard-coded rules pulled from the history of linguistics. So we might just simply look for words like excellent or bad or horrible or happy or wonderful. But like I said, sarcasm or modifier words might muck that up. So then we move on to mish.
 We move from hard-coded systems to machine learning systems like naive bays and and bag of word approaches. Those get us closer to the goal, but they still have problems. We still have not overcome sarcasm or modifiers. We may encode in the system that words like not preceding an emotive word would actually then become one word, not underscore good. But that still feels a little bit...
 hand holding, feature engineering, and we have to sort of know all of the feature engineering that needs to take place in order to work with these documents. So state of the art and sentiment analysis has moved us towards deep learning. Deep learning uses things like recurrent neural networks, which will read the sentence left to right and sort of keep a running tally, accounting for modifier words and sarcasm and all those things while still learning.
 to look for salient words and patterns in sentences. Sentiment analysis, very important, pretty complex. Lots of machine learning algorithms used here, varying from SVMs, support vector machines, hidden mark-off models, naive bays, recurrent neural networks. We'll talk about all that later. Another category of classification, document classification, is tagging documents. Now this is different from classifying documents. Classifying documents is-
 is giving it one class in any number of classes. It could either be a binary classification in the case of spam detection or it can be multinomial classification in the case of sentiment analysis, but tagging, alternatively known as topic modeling or keyword extraction, is actually figuring out what keywords to apply to this document and it could be any number of keywords. So if we're looking at some programming blog post on the internet, it might...
 be talking about Node.js and react and react native and post-gress and all these things. So it's going to learn to tag this document with all of these tags. That's the way I think of topic modeling is the machine learning approach to automatically tagging blog posts, which you see all over the internet already have tags. Those are manually tagged. Well, in certain instances we don't have the luxury of manual tagging. Maybe we're scraping documents from some corpus and we want to automatically...
 tag them so that we can present that as a library of documents that people can sift through by category. The common algorithm used there is latent Dirichlet allocation, LDA. Okay, another lofty goal of NLP is search. Search engines, finding relevant documents, so document relevance, as well as document similarity. How similar are documents to each other. So search is really obvious.
 you type in a query and it finds relevant documents. A popular algorithm used here is called TFIDF, term frequency, inverse document frequency. And document similarity is all about how similar one document is to another. So for example, in a recent kegel competition, remember kegel is a competition board of machine learning tasks where a team of machine learning programmers can compete with another team and maybe earn a cash prize.
 or employment opportunity, et cetera, a recent competition posted by Kora, the website Kora, QU-O-R-A, which is a question-answer website, similar to Stack Overflow and the like, posted a competition like this. When a user is asking a question, they're typing in a title, and they're typing in a description with their question. On the Kora website, they want to be able to find similar documents automatically and present those in a list format on the right
 a sidebar so that a user can see if their specific question has already been asked and answered in the past so they're not submitting a duplicate question. So that's the task of document similarity. And again, a common algorithm here would be TF IDF. All right, let's get a little bit deeper. Let's talk about natural language understanding. Now natural language understanding or NLU by comparison to natural language processing,
 the general field of NLP that we've been talking about thus far, the subfield within machine learning of everything related to language. That's NLP. Natural language understanding is pulling out of what's being said in a sentence, the semantics. Now we have semantics. Pulling out the meaning, the intent of a sentence. If somebody asks a question, you have to have natural language understanding in order
 to answer the question, or like I said in the case of machine translation, you have to understand the embedding or the encoding, the overall intent of a sentence in English in order to translate it to its equivalence in Spanish. So natural language understanding or NLU is all the tasks of NLP that require a fundamental understanding of the sentence or the word at pl-
 So common tasks within natural language understanding like I said question answering so Siri and Google's assistant they answer questions if you ask it a question it will answer and like I said before if you type in a Google search query in the form of a question it will actually answer your question that's obviously an extremely difficult task in NLP I actually don't know the algorithms at play here I'm gonna try to do some
 before my next episode to see if I can address what's used state of the art in the field of question answering. But it's obviously a very difficult task, but requires natural language understanding. Another thing is textual entailment. That's an interesting problem is if I say one thing, does it imply another thing? If you read some bit about Donald Trump winning the presidential election, and then you ask the system a question, is Donald Trump the president? It should...
 know the answer is yes, because it can do some sort of processing about the parts of the facts it already knows in order to address the question that's being asked. It's related to question answering. It has a lot to do with logic, textual entailment. And finally, of course, machine translation. Machine translation, they call this AI complete. It's an interesting phrase, AI complete. What it means is this task machine translation requires all the pieces
 of AI to work. In other words, once you've achieved perfect machine translation, one maybe could say you've achieved AI. Well, if you ask me, we've got some darn good machine translation systems out there by Google, State of the Art. I believe they use recurrent neural networks. So have we achieved AI? Well, it's always a moving target. I'm sure somebody's out there saying, no, no, no. But traditionally, they've always considered machine translation to be an AI-complete problem.
 all of the pieces of AI to work completely. At least within the space of NLP, machine translation requires a lot of pieces, a lot of components. We require parsing, we require POS, we require a handful of algorithms like HMMs and naive bays, and we've got to encode the meaning and intent of what was said on the left-hand side, so that we can translate it into something on the right-hand side. Trans...
 like English to Spanish. Machine translation is also an excellent example of the power of deep learning because like I said in shallow learning approaches we use gobs and gobs of algorithms where in deep learning approaches you can use one algorithm like the mighty recurrent neural network which gives you increased simplicity, elegance and yay even accuracy. Next up we have natural language generation. So this is actually...
 actually generating text. We're not just parsing text, we're not inputting text only. We can also output text. Now of course, natural language generation, machine translation would be one of those examples. Another example would be chat bots. Chat bots holding a conversation with you, Siri, and Google Assistant. Chat bots use any number of components with an NLP, like NER, for example. A very simplistic chat bot might take one...
 What you said, compare it to a database of conversations and find the most probable response utterance, entire sentence to throw back at you. You say a sentence and within the database of conversations it has at its disposal, it finds the most probable sentence, full sentence to throw back at you. It's a simple chatbot, a cool and complex chatbot.
 actually generate a sentence word for word. It won't just throw you a sentence in the database. It will encode what you said using natural language understanding and then decode word by word, a probable response using proper natural language generation. So chatbots are very fun to work with. There is a huge uptick in chatbots in the world today. I'm sure you've noticed companies are going chatbot wild. There's the.
 push towards a concept called UI free or no UI or no UX, any number of things that the boy they're trying to say is companies are trying to build a chatbot and so that you're interacting with the chatbot either verbally or on your keyboard. And the chatbot is so good at performing actions that you don't need menus and buttons and sliders and toggles, you don't need a good UX, you don't need a UX at all. All you need is a chat.
 bot. Siri is a push towards this direction. So this is kind of the zeitgeist of NLP in today's generation is chatbots. I think they're kind of funny. You know, I think a properly done user experience is going to allow users to perform actions so much faster than typing a bunch of text on a keyboard. So I don't know how this chatbot craze is going to pan out, whether it's going to be a bubble or whether it's going to be successful. We'll see.
 of natural language generation is image captioning. Now we're bordering between the image people and the natural language people. We're combining our efforts in order to perform a combined task of image captioning. And so you see this from time to time. I don't remember if Facebook can caption images for accessibility purposes for blind users. I don't remember who's doing this, but there's somebody auto captioning images for accessibility.
 on the reverse, Google is translating search queries into the images they represent. And if you actually use Google Photos, you can search in your own search box on your phone, some phrase, and it will actually bring up for you images that look like that phrase. It's pretty cool. So image captioning and image searching, they're kind of the reverse of each other. But image captioning is an example of natural language generation, where you feed it an image and it actually just...
 describes the image word by word. Automatic summarization. This is a very useful and powerful task, also very difficult. An example of automatic summarization in use today is summarizing legalese, summarizing legal documents. So for example, how cool would it be if a website with a privacy policy or a terms of use contract? I mean, nobody reads those things. Half of the people don't read them because they just don't care. Sure, whatever, let me sign up for the service.
 But maybe the other half wants to read these documents, but they're so long. And they just don't have the time to read every privacy policy and terms of service under the sun. Well, a good automatic summarizer might be able to boil down a privacy policy into the most important bits, sort of a reader's digest or an abstract of a privacy policy. Automatic summarization is in use today in summarizing actual legal documents for...
 legal purposes. I believe Google when they answer your question at the top of the search result uses automatic summarizes it figures out how to summarize and answer to your question without showing you too much text all at once. Natural language generation. Okay, some other odds and ends. I'm not gonna really cover the algorithms at play. Optical character recognition or OCR is being able to convert a scanned document from a physical book or
 physical paper into digital format. Now of course the primary algorithm at play of an optical character recognition system is going to be the convolutional neural network, the CNN or CONVnet. An algorithm we're going to discuss when we talk about image recognition, the primary algorithm of image recognition is the CONVnet. So of course that's going to be at play in converting the image into digital format. But certain letters
 may be incorrectly translated. And so NLP will come into play in order to figure out maybe in one word or given the sentence thus far, what is the most likely letter for this mistake? So OCR is another example of a marriage between the image people and the language people. And then of course we have speech, speech a whole other ballgame. We have speech to text, text to speech, when you talk to Siri,
 verbally, it converts what you said verbally to text because Siri reads from text and then Siri responds to your query with text and your phone reads that back to you with speech. So converting from speech to text and back is a whole world of its own where you have to analyze wave frequency and structure of audio files and all those things. We're not going to talk about speech.
 But just a couple buzzwords for you there. Segmentation is figuring out how to chunk an audio file, maybe segmentation into sentences, for example. And another thing is called diarization, D-I-A-R-I-Z-A-T-I-O-N. Diarization, and the goal of diarization is to figure out in an audio file, let's say we have a conversation between one person and another person, a customer service representative and a customer.
 or even a dialogue between multiple parties, a recording of a company meeting. Diarrhization is all about separating the audio file into the speakers. So speaker A said the following chunk. Speaker B said the following chunk. Speaker A again saying the following chunk. So that's called diarrhization. So speech is a whole world of its own involving audio processing, which I know nothing about. And so I won't be talking about.
 in this podcast. Okay, a giant lay of the land of NLP. We went from the bottom where we talked about little odds and ends like corpora and documents and tokens, how to operate on tokens like limitization and stemming, removing stop words, finding words, similarity with edit distance. We talked about syntax and semantics, syntax being parsing sentence structure, such as part of speech tagging, relationship extraction.
 and syntax tree parsing. Those are all in that middle tier, according to the Tyler three parter, parts tasks and goals. Semantics is all about word meaning, sentence meaning document meaning, et cetera, pulling the underlying meaning of a word, whether it's contextual similarity in the case of word to veck, that we're gonna talk about in the third part of the series, document similarity using TF IDF, et cetera. So we had that.
 second tier of tasks primarily relating to syntax parsing. That includes information extraction such as POS, NER, that's part of speech tagging, named entity recognition, and relationship extraction as well as syntax tree parsing. Then we go to our third tier of the lofty goals of NLP such as spelling, checking, and correction. Document classification, as well as document sentiment analysis, and tagging or topic modeling.
 or keyword extraction, classification, search, document relevance, document similarity, natural language understanding tasks like question answering, textual entailment, machine translation, natural language generation such as image captioning, chatbots, and automatic summarization, and OCR and speech. In the next episode I'll talk about the algorithms. But if you want to get started on that before that episode comes out...
 I'm going to talk about the resources now first. The resources for learning natural language processing. I've really done my best to boil it down to the most fundamental resources. Natural language processing is my jam. It is my particular specialty within machine learning. It's my favorite topic. And so I think that these are the three best resources out there. There's a textbook called Speech and Language Processing by Daniel Giraffsky. He is sort of...
 the Andrew Ng of NLP, Daniel Joravsky, also co-authored by James Martin. Then there is a NLP YouTube series by Daniel Joravsky again, which is basically the YouTube series equivalent to that textbook. What's nice about that is that the speech and language processing textbook is gonna be probably 1,000 pages. The NLP YouTube series goes pretty quickly. I think it's 24 hours.
 when converted to audio. And that's what I suggest is you convert it to audio. Like usual, when I talk about my video recommended resources, what I usually do is convert it to MP3, put it on my iPod, and do it while I'm running or cleaning, et cetera. I'll post the how to convert a video to MP3 thing on my resources page, but it's called the Stanford NLP series on YouTube. Finally, there is a library out there called NLTQ.
 K, natural language toolkit. It is far and away, the most popular NLP library used by professionals. It is written on Python, and it comes with capability for handling all of the things that we've discussed in this episode, from the low to the high. So it comes with all those text pre-processing methods, such as tokenization, dramatization, stemming, et cetera. It comes with method.
 for part of speech tagging, named entity recognition, et cetera. And then it comes with algorithms as well for classification, document similarity, and the like. Now typically it is insufficient for going the distance. You can't write a search engine using NLTK. You wouldn't write a chatbot or a machine translation system using NLTK. Instead, NLTK is a very useful toolkit,
 Utility belt, a library. I like to think of it if you know JavaScript, it's the load ash of natural language processing. It's one of those tools that Python developers are hard pressed to not use in their natural language career. No matter what final solution you end up landing on. Now typically, like I said, these lofty goals you're gonna go with deep learning and recurrent neural networks and word to vac. That's tensor flow or PyTorch. You're gonna use one of these deep learning frameworks. But you'll still find NLTK very handy for.
 or text preprocessing or accessing a corpus that has all of the popular corpora available just a method away. Now what's interesting about NLTK, and the reason I bring it up in this resources section is they have written a book, nltk.org forward slash book, that isn't only about NLTK, it is primarily about NLP. The NLTK book, which is for free, and it's an HTML format is probably...
 I'm merely intended to teach you NLP in general. Just using NLTK as a vehicle, and in fact, many professors in their courses assign the NLTK book as their reading assignment. So you can either do the speech and language processing textbook, which is very big and more theoretical, or you can go the way of the NLTK book, which is more hands-on and practical and faster. If you want to...
 become an NLP expert, I would recommend the speech and language processing and NLTK book. But if you want to just sort of get a quick lay of the land or just hit the ground running, I would recommend the NLTK book. Now, NLTK, the library, there are alternatives out there as well, open NLP and Stanford NLP. These are all different libraries that you can use for shallow learning machine learning algorithms. But usually in the space of machine learning, you're going to be gradually...
 into deep learning for more powerful, flexible, and elegant models. So you're probably going to be moving on to something like TensorFlow or PyTorch. That's it for this episode, a lay of the land of NLP. In the next episode, the second part of the series, I'm going to talk about the shallow learning algorithms, traditional models used in NLP. Now, my friends, I have terrible news for you. In order to continue working on this podcast, I'm going to have to take on advertisers. I know the
 the bane of every podcast listeners life. But I've reached that point in listenership where I actually have the downloads where I can start reaching out to advertisers to sponsor the show. And so I'm hoping you guys could actually go to my website, ocdevelop.com, forward slash podcasts, forward slash machine learning, where I'm going to post a user survey that is required in order to sign up for sponsorship with Libson, the podcast hosting service I use. Fill out that user form for me. I know that.
 insult to injury but do me a favor if you could, I'm so sorry please. I'm also going to try to make my website a little bit more useful where maybe I'll post announcements for when I plan to post new episodes for example. This episode was two weeks late, I apologize for that I had to collect some resources on this topic so hopefully I can add some incentive for popping over to the website. Okay guys thanks for listening, see you next time.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. Welcome back to machine learning applied. In this episode I'm going to be interviewing co-workers from depth. Matt.
 an expert in architecture and Girawat is an expert in DevOps or developer operations. These two skills combine in the deploying of a full-fledged product that can be used by consumers, a web app, a mobile app. Most of what we've talked about in this podcast series is machine learning, how to develop and train your model inside of a Docker container, or even developing and training your models on the cloud by way of...
 AWS SageMakers Studio Notebooks. Now, our skill sets are on the machine learning side, but eventually you want to get that model into the hands of a customer. After you have trained your model, whether on localhost or on SageMaker, you will deploy your model through SageMaker to a SageMaker rest endpoint or to a model registry that can be called as SageMaker Batch Transform Jobs or SageMaker.
 serverless inference jobs. So now you have your deployed machine learning model ready to be used, but who's going to use it? That's where all the stuff from this episode comes in. Now I know I have talked about a lot of the tooling and concepts from this episode in the past, and I promise I'm not going to turn this podcast series into a full stack slash architecture slash DevOps podcast. And that's actually why I'm creating this episode. I wanted to talk to my colleagues who are experts in the field.
 stop bumbling my way through these episodes and wasting your guys' time and just say, hey, Matt and Gerwatt, let's nail a coffin. How do you do this as a machine learning engineer? How do you productize and deploy your machine learning model as a full product in the cloud? Should you do that? Now, to get you thinking along that track, I want to talk about my journey with Nothi. Nothi was inspired by the publication and accessibility of the...
 Transformers, NLP models. The hugging face repository had all these models, summarization, question answering, and I thought, you know, these would be really great for use in a journal app to give people recommendations and resources. Let me see what I could pull up, crack my knuckles, and I get typing away. I deployed these models previously as Docker containers running on AWS batch. Now I'm moving everything over to AWS SageMaker. On SageMaker, you can deploy models that you train or you can deploy preach.
 trained models as rest endpoints or for use in serverless inference jobs. Great. Now that I had a machine learning model, I had to figure out how to get that to users so that they could start journaling. Well, the next step was to deploy a server using Python and Fast API to AWS ECS or elastic container services. And a wrapper on ECS that makes it easier to work with is called Fargate. Fargate will manage the scaling up and
 down of your Docker-based servers based on load on the server. Great. Now I have a server, but I need a database. So I created a AWS RDS or relational database services post-gress database. And I had to create an IAM role, attach that to my Fargate container so that it has access to my RDS database. Then I had to make Fargate accessible from some others.
 service, namely the front end, which we'll talk about in a bit. So, I spun up an application load balancer, ALB, that has an IP address. The IP address is registered as a domain name on Route 53 in AWS. Tie that ALB to my Fargate service. Now the whole thing is accessible by an IP address. Finally, I needed a front end so that the users could write their journal entries and communicate with the server, and the server is going to kick off the...
 make your model inference jobs. So that front end was written in React, that is JavaScript, HTML and CSS, and then I dropped the build artifacts of that react codebase into an S3 bucket. S3 is AWS's service for storing files. Within S3, you can checkbox, make this bucket publicly accessible as a website. Now if you want your S3 bucket to be a public website, the buck doesn't stop there. You actually have to do another step.
 step, and that is to put a what's called Cloud Front distribution over your bucket. That Cloud Front distributions goal is to cache those files at various locations around the world so that it's more fastly accessible by users in various locations. That's Cloud Front's purpose, but it's actually a required step if you want to add a domain name again through Route 53. So I register a domain name, nothe.ai, point its A record to that Cloud Front distribution.
 the Cloudfront distribution points to the S3 bucket. The S3 buckets files are react, JavaScript CSS HTML that are making API calls to my Fargate container through the application load balancer. The Fargate container is sitting on top of ECS, which is managing the scaling up and down of that service. The Fargate container has attached to it an IM policy that gives it access to the database on our DS. And the Fargate container will make calls to...
 the SageMaker model endpoint in order to run inference jobs using machine learning. Oh, and did I mention you don't want to string all this stuff up yourself by pointing and clicking your way through AWS console. The reason for that is it's hard to track changes of your architecture over time or to know what services you have running and which ones are affiliated with each other. Or let's say you make a change and you want to keep track of that change. Well, you won't be able to keep track of that change unless...
 All that stuff is stored in code. And so there's a project I'm using called Terraform. Terraform is what's called infrastructure as code. It lets you manage all of the deploying of your cloud services in a code file. You can then check in that file to GitHub. And then you can use that file to track changes to your architecture over time, or so that other users can repeat that architecture on their end, or so that you can make subtle changes in, let's say your prod, dev, and staging environment.
 just by way of code. So I just mentioned a slew of cloud hosting services on AWS, and how you would do that in code, and is your head spinning because mine is. And so that was the impetus for this episode was that the last year's worth of bumbling my way through architecture and DevOps, when my specialty is machine learning and data science, as I'm sure is the case for most of my listeners. I wanted to get some experts.
 on the line and say, what are the popular tools out there and which tools should be being used? Are all of these skills valuable to a machine learning engineer or expected of a machine learning engineer in the job market? Should we be bothering with this stuff or is it too big a pill to swallow and we should be leaning instead on our DevOps colleague or hiring somebody to do this for us? And so that's the kicking off point for this episode. I hope you enjoy. Welcome.
 Welcome back to the show. Today we're going to take a divergent and we're going to talk about DevOps or developer operations and architecture. And we have on the show two coworkers from Dept, Matt Merrill, go ahead and introduce yourself. Hey, I'm Matt Merrill. I'm a director of engineering at Dept US. I come from a background of Java development and Node.js development. I've dabbled in DevOps. I had something that I love. I'm usually the guy on the app team who is reaching across the aisle to the ops folks and things like that. I've been doing this.
 for over 15 years and happy to be here. Hi, my name is Trevor what would you out? Yeah, I am the DevOps practice lead at depth. I started out my career as a Java Pearl Python programmer. And I was doing DevOps before the term DevOps came out because of my very interest, I would always talk to the ops because I like unique simulation. I would always do the CIC, the Jenkins because no one in depth.
 wanted to do it. I would always maintain the dev servers again because no one wanted to do it. And I started to do automation of our application because I didn't want to do it by hand. And I learned that wow, Terraform Ansible, I love this. So I pivot my career from job with development to DevOps. So for my listeners, I promise I'm not going to turn the show into a DevOps slash architecture show or full stack. I've been taking kind of a direction with that in the last few episodes.
 I talked about SageMaker, AWS. But this might be the last episode where I talk about DevOps. I've kind of been bumbling my way through DevOps and how it relates to machine learning for anybody in machine learning who actually wants to deploy their models to the cloud for a client where they wanna get their model online. But in this episode, we're gonna kind of try to really dial down DevOps, the tools you do, what is it, how does it play into the machine learning ecosystem, whether you've been developing models on SageMaker or localhost. Now you wanna get that thing on.
 line. So we're going to talk to the experts here. Gerwats can take us through DevOps. Matt's going to talk about architecture and we're going to see how does that fit into the picture from a machine learning perspective. So Gerwats, what is DevOps? So it is a philosophy and a tools that the devs and the ops can work together to help deploy code and applications safer, quicker into production. So it may be good to highlight zone.
 the problems that DevOps is trying to solve. Originally, and still happens today, the devs and the ops, even though they both work in technology, and they both work in the same company, they don't talk to each other, it's silo. And we see that numerous times, even now when depth does an engagement. So DevOps are way to have the common tools and common communication and process. So that way, these two silos can work together well. And as we go on as a podcast, we could talk about some specific tools.
 in some specific processes that makes DevOps work. Yeah, so a lot of my listeners or at least I know a lot of machine learning co-workers or colleagues in the past. What they end up doing is either they write a model on local hosts or using TensorFlow or Keras or PyTorch site kit learning. Ideally, often they'll be using a Docker container. So if they're on a Mac, they'll have a Docker container that runs machine learning. They're not going to be able to use a GPU because of the Mac's limitations. If they're using Windows, they might be passing through to WSL2.
 by way of Docker and writing machine learning models, they might be using a Docker container that inherits from Nvidia containers or hugging face containers. More often the case what I see is people are writing their models on Jupyter notebooks in the cloud. So they'll create a GCP or an AWS account and they'll go to the machine learning toolkit. So in AWS it's called SageMaker. They open up SageMaker. It creates for them a Jupyter notebook.
 right off the bat, they open up that notebook and they start typing Python code. They write a model, they're using Keras, they train it, and when they deploy it, SageMaker has all this tooling for deploying a model, you just say model.deploy, and it will do all the stuff for you in the background. It will spin up, who knows, EC2 instances, yes, SS instances, we don't know exactly. Sometimes using Docker, if you're inheriting from SageMaker Docker container, sometimes it does all the containerization for you itself, and all you do.
 do is write some Python code. They call that bring your own script. But what happens is machine learning engineers, they know Jupiter, they write some Python, they deploy a model, and then the rest, it's like, where do we go from here? How do we make that available to the internet to our client? And that's kind of where we stop in the podcast and you begin as somebody is an expert in the orchestration land. Yeah, so that's a great, great question because I actually experienced this firsthand. So what are we?
 was working with the Cambridge mobile telematics, CMT, the Dev Machine Learning team. They were playing around with SageMaker in the sandbox in the Dev environment and what became a experiment became really popular to analyze data, process data, display data. So one day they came to me and say, hey, we got this Dev SageMaker in Dev and we want to deploy to production. And I didn't know anything about SageMaker. I barely knew anything about...
 Jupiter notebooks. The only thing new was like, oh, cool UI for Python. That's basically all I knew about it. So, you know, one of the DevOps philosophy is collaboration, right? The devs and the ops working together to achieve a common goal and deploy it. So it was a two way street. So they taught me about Jupiter notebooks, SageMaker, things like that. And on my end, you know, I took a look at their dev implementation. And I saw a lot of things that had to be modified before we could deploy to production, which is
 Right. This topic is so complex that we're not expecting machine learning devs to understand it. So to just give some examples, I had to encrypt the disk because it had user data in it. There was a hard coded database, usually passwords on there. So we had to use these things called I am execution roles to secure and you know, get access to the database secrets manager to retrieve the secure using password and make backups in case things crash.
 So things like that, things that you have to do before you go to production. And that's why you want to work with your DevOps team to do that. We're not expecting the dares to think about all the stuff that has to go before going to production. Actually, now that you mentioned it, you talked about I am, you talked about Secret's Manager. Actually, what is architecture and Matt can chime in on this? What is architecture and how is that different from DevOps? I imagine there's a lot of crossover. So Matt deals primarily in architecture, your what deals primarily in DevOps. In what ways are the
 those roles similar to difference, where does one begin in the other end? Yeah, that's an interesting question. I've actually never heard DevOps and architecture compared like that, but let's start with what I think architecture is. And I'll step back by saying, there's this perception, I think, in software engineering, that an architect is kind of like some kind of, and I mean this tongue in cheek, like Godlike figure who sits on an ivory tower and sends down edicts for how a team needs to implement something. And there certainly are those type of architects, but I think that that's probably...
 will be the worst type of technical architect you could possibly have. I see architecture as systems engineering, how different systems interact with each other, and how can you make a healthy whole system that is as simple as you possibly can make it? Maybe each individual piece is complicated within itself, but how are you going to make all of those different things in your business talk to each other, whether it be the customer relationship?
 management system feeding data into your machine learning model or whatever it might be. How are those things going to play nicely together? How are you going to make sure that there are teams of people with the right skill sets to support those things, thinking that far ahead, thinking about how these applications are going to get deployed and making sure that you do have a DevOps team or a cloud engineering team or whatever you want to call it that can support.
 to report that type of stuff. I always like to think of it as a cross-cutting technical person who is at the service of different teams who can help kind of roadmap these things. I really like the discussion that you and Jeroah were having, that Jeroah I mentioned about like, we can't expect machine learning engineers to know all this stuff, you can't, right? Like it's unfair, it's its own topic, it's spot on. You need at least one person, if not more, people to do these type of things. And I like to think that the role of architecture is cut.
 of the person who could take a step back or people that can take a step back and think about how all this blends together before it goes out to users. And I would say that most small companies, you wouldn't start with an architect, but once you start getting bigger, that's when you need to start thinking about having somebody in that role. And ideally, you have somebody grow into that role organically who has all the context in history and things like that. Probably commonly comes from maybe full stack, knows how the server ties to the database taste of the front end.
 get him a little. Okay, that makes sense to me. So am I right to say that an architect blueprints the whole thing and then a DevOps makes it happen, at least in the cloud. And that's how my confusion was as I thought of that those two roles as similar in the way that a DevOps would string these services together actually in implementation. So I think it just depends on the architect and their expertise. So I've dealt with numerous different architects for new, meristor, clients and companies. Some architects are there.
 high level, right? They'll draw a diagram and say, Oh, okay, we have a container. These are like, for instance, we needed five, nine, nine, five, we need a cluster. We need to handle this load, this security here. Here's the diagram. DevOps, you figure out the implementation to make it work. Other architects, especially folks who are promoted from the trenches. Oh, okay, well, I want all of this. And this is how I want it, right? I want, you know, fargate and I want Kubernetes or whatever tools that they like. Here's the recipe.
 you just follow it. So it just depends on the architect and their skill sets and their personalities. I like to think that the more effective architects are somebody who knows when to do each of those things at the right time. You don't always want the blueprint because you might have an effective team. I like to think it's as much art as science and also just human interaction as technical work. Sounds like the conclusion then is that taking on DevOps as a role as a...
 machine learning engineer is too big of a pill to swallow. If you're going to one man band a project as a machine learning engineer, maybe for a client, let's say it's a one machine learning engineer who just landed a client who may or may not have other people working on the project. Taking on a bunch of DevOps, technical know how would be a lot to ask because in a way it's a role in its own right. It's not just a suite of skills that some value.
 available for any technical person to have under their belt. So, Jirwa, that being said, can you speak to sort of the level of complexity involved in doing DevOps? And why maybe a machine learning engineer wouldn't want to take on all this burden themselves? Right. So, especially security is the one that's scariest, right? Especially with machine learning, you're probably going to be touching sensitive data, analyzing sensitive data. So if this data is exposed, well...
 It's bad times for you, bad times for your client, the company you're working for. So at that point, you probably want to hire a professional to take a look at your implementation. And here at Dept, we specialize in that to make sure that the application that you wrote on your laptop can be deployed quickly, safely, repeatedly to production. And security is probably the most important thing. We've done that with plenty of clients where, and it's not just the application that needs to be secured, it's the actual interior.
 infrastructure too, right? There's the network. We see plenty of times where the network is public IP wide open to the world easily hackable. You know, there's these things called S3 buckets where you usually store your data. That's open. So security is the one thing where it pays to have a professional come in and take a look and tie it down. Cool. Drew, I'm curious what you think of this, but I would say everything you said is spot on. Like that is not something I would ever advise an ML engineer to try to dive into.
 half-heartedly. But I think that there are certain things that an ML engineer can do in certain skill sets that they can start to brush up on that will position an ML project well to be taken over by a DevOps team. And I think we're probably going to talk about some of those. So I'm curious if that rings true with you as well. Since I'm not a DevOps expert, I just kind of, it's on my periphery. So usually what I've seen is, the turf form was pretty easy to pick up, right?
 So whether it's Emma or Dev, this is, oh, okay, look, I wrote this terraform to spin up like an easy to and a BPC and you get this terraform. You think, oh, that's cool. I'm going to clap. You took the initiative to learn it, but then you had to take that terraform, turn it to modules, you know, put on the right settings, things like that. So I think it's great that any of the learns Docker terraform, Ansible, whatever it is, but it still requires a collaboration to make it professional, fit the organization, fit
 code base. Yeah, that's a good point. And then back to your original description of DevOps to ML engineers shouldn't think that they're just going to take this and kick it over the fence to a DevOps person and be done with it and the DevOps person is going to magically poof make this appear production. There's going to be changes in conversations going both ways in order to make that happen. And collaboration is the key to DevOps working together to achieve a common goal. That's that that's definitely something that's going to highlight over and over in this conversation. And then back to your original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original original
 architecture, collaborating with the Ops and the DevOps and the devs and everyone working together. So you mentioned repeatability and in the land of repeatability we have this thing called infrastructure as code. You know what? Do we want to just dive into the tooling? I have Gerwatt's talking point notes here and he's got a really good what looks to be a journey through the history of and what inspires the creation and the use of these tools. So Gerwatt, yeah go ahead so we'll get into the landscape.
 of all these DevOps tools, but I think it's really important to remember what we just talked about. This is a full-time job, right? We're going to go through all these different types of tools so that you have a lay-in the land. You can hear these and be like, that's the configuration management tool. We're not suggesting you go into a deep dive in any of these. This is so you can talk to your DevOps people, communicate with them better, or just be able to be a little bit more intelligent and ask good.
 questions when these topics come up. So listen to it, cheer what's an expert in a lot of these areas and he's got a great, great, great overview of these and then kind of let most of the details fall out of your head and just remember the high level concepts that you're going to be in really, really good shape. All right. So we're going to go back a little bit to the old days and the old days weren't that old and describe the problem that DevOps is trying to solve or have solved for the most.
 part. So in the old days, there are people would go into the UI and just go clickity clickity and create servers and create virtual environments or do whatever they need to do. They would do it manually. That is just like on AWS console website. Well, even before AWS, right, you had VMware, you had command lines, right, even with physical servers, people would go in, log on to the server and just type in commands manually. So it was almost like a dark seat.
 of how things would be done, how things are configured. And then with that engineer left or that ops left, right? Then oh, no documentation, no idea what that person did. It's hard to manage a professional business that way. Or you even manage your house that was a mess. Just oh God, what's going on in this server? There's trash all over the floor. And then, you know, a classic example, we see all the time is, okay, one dev or one admin goes into one server, does it one?
 way. Another admin goes into another server doesn't another way, even though they're supposed to be running the same app, different, right, different servers, different boxes. So the manual part is how ops used to operate. So what dev ops did was take some of the philosophies of dev, right, you have code that you write, and you have code that you can review. And then ultimately check into a code repository, like get that way, there's a record of the changes, there's a record of what...
 happen and you can and other people can run it. So it seems like a pretty basic concept now. But when it was first entered, it was a game changer. And there was a lot of resistance too, right? Almost like, oh, you're trying to take over my job. So what infrastructure is code does is there's tools like CloudFormation, Terraform, PlumeMe, that, well, instead of you going into the AWS console and going clickity clickity to create a server to create SageMaker
 You can write code to do what the clickety clickety did and then review it, check it in. And if you have to make changes, then you could make that change in code, review with your team, say, oh, we're going to change the instance to, you know, ML M5 large to M5 X large, check it in, run it. And you could see the history of what happened. So you have a code file and it deploys all the AWS infrastructure for you without with zero clicks. You just run some script.
 And it will spin up all of your EC2, your database, et cetera. Is that what this infrastructure code does? Yes. And the other game changer was what I mentioned before. It used to be that different, absolute things differently. Now that you have code, you can rerun this code in another environment, right? By just passing a variable. Let's say you ran this code in dev, you tested it out. There were some buzz with the infrastructure happens. And then, you know, after a while, you got, okay, this works. So you can take, you can run that code, test the...
 variable QA or prod and they exact same thing will happen. QA and prod you could ensure what you test in dev is in QA, what you're testing QAs in prod. And in the old days, that was not guaranteed at all. Right. There may have been like 50 steps. You have to run to deploy manually. I guarantee you you'll step like three of them or you do three of them wrong. Recently, I deployed a service using Terraform, a server, a database and everything in a VPC.
 and the VPC block of code was, I'll say, 25 lines of code. It created a bunch of stuff I didn't know I needed to create. So, route table, internet gateway, network ACLs, all these things I wouldn't have known to have done myself in order to expose the service to the internet in AWS. It seems like if structure is code with their same defaults also helps with simplicity of deploying architecture. Oh, definitely. That's another great concept from Dev.
 that came into ops, you know, modularity, reusability. Oh, you have this module. We put all the complexity behind this box. You all have to do this past two or three variables and we'll do the right thing. All right, you mentioned cloud formations, Terraform, Pulumi. How do these all compare to each other? So cloud formation is AWS specific and Terraform is more generic. You could do multi cloud. And Terraform is the most popular infrastructure as code tool is the one that, if you're working for...
 for a job, that's what you should learn. Pulumi comes in. So, Paraform is very structured in the way it wants to do things. It says, kind of almost my way or the highway. Pulumi takes this concept and adds real code into it. So if you want to do a for loop and if statement and be more generic, you can. So it's a trade off. You could be more dynamic, but it leads to a lot of spaghetti code with Pulumi because it's, you know, real code. But if you need that flexibility, it's good. I'm with that.
 We recommend terraform because it's the most popular and it's the most structured. It's easy to teach. It's easy to not messed up too much. I also get a lot of online support, you know, it has a very, very vibrant community from what I've found and I'm fairly new to it, but a lot of support, examples. And independently, I before I even talked to these guys, I came to the same conclusion. I think terraforms the way to go. And also, well, hashiccorp.
 just had a $1.44 billion IPO. So it's a real company. It's not some open source thing that is un-maintained. So if you do need professional support, you can reach out to Hashtagore or Debt or it to say, oh, hey, we have this Terraform. Can you help us? And one cool thing about Terraform is, as you're a lot of mentioned, is it's mobile-di-cloud support. So if you don't like AWS, which is what we're going to be far being on in this episode, you can use Terraform with GCP and Azure and others. But in the AWS world.
 There's cloud formation and now CDK if we're so in AWS heavy why are we saying Terraform over CDK or cloud? Right. So CDK within the DevOps community is a very intense debate on how to do infrastructure as code. There's a camp one camp that just lay on the table and then where I believe that infrastructure as code should be structured. You shouldn't do a lot of weird dynamic stuff to it. If you're doing that, you're probably doing something.
 wrong. That's why I can towards terraform and cloud formation is the same way. It's just past me a bunch of configuration and I'll figure out how to do it with polluting and with CDK. It's taking some of the complaints of that structure and say, well, I want to do this fancy thing or I wouldn't say, you know, if dev do this, if prod do that. And I want it more like how I code, you know, both CDK and plume supports Python supports TypeScript. I think Java's in there too.
 Okay, this is a procedural versus declarative code. Right, exactly. So Terraform is an HTML file isht and CDK is a JavaScript file. And sheesh, exactly. That's a very great analogy. Terraform says this is what must be done CDK says do it this way. Right. Okay, so that's infrastructure as code. That's Terraform and friends. I've heard a lot about Ansible. Where does that fall into the place? Yeah, so one of the bad, confusing things about the DevOps tools is.
 This thing you have infrastructure code and then configuration management in the two different specialties and the tools that does Infrastructures code well and the tools that does configuration management well Is different tools different companies. So let me describe what configuration management does as a real world Road example and we see this pattern a lot you would use terraform to create an EC2 server in AWS And once that
 EC2 server is in the cloud. Well, now you got to put your special business sauce into it, right? You got to configure install security patches. You got to add Unix users. You got to actually install your business app. Well, doing those tasks, Terraform is not very good at it. So there's another tool called Ansible, which does that where again through configuration, you can say, okay, install this patch, you know, young and...
 install patch 1 2 3 at Unix at user user and x y z then run install.sh. So that's what Ansplo does and that's what configuration management does once you have a generic server that configuration management tool installs your business logic to it. So that way you could actually work. Now I thought we would do that with Docker. Right. So a lot of these configure ration management tools are getting deprecated because they were designed for the old day.
 where you would actually have either a physical server or a virtual server. You would have a unique server that's a blank slate and you would have to go in and configure it. So containers are another game changer in DevOps where it's the old joke. Well, it works on my laptop, right? The dev says it works on my laptop and the office says, well, let's just deploy the laptop. One of the great things by containers is that on your laptop, you can do everything on your laptop where you could run it.
 Unix server, you could test and deploy and do development on that Docker container. And once your satisfy, hey, this works on my laptop, you combine it to a Docker image with Unix to code everything and give it to the DevOps team and say, deploy it. It works on my laptop. I tested it, take this entire thing and go. The other huge benefit is that they start up almost instantaneously, almost instantaneously, whereas these other methods can take a long start up. The big game.
 is what you do in dev is what you do in prod. You wouldn't run an EC2 server on your laptop right, you can't. There's tools that you could kind of mimic it, but Docker containers are the way to go. It was a major Gabe teacher when it was first introduced. Okay, so we have infrastructure's code that is writing a code file that will create a whole stack on AWS, whole stack on GCP. In the past, you would log into EC2 instances that you may have deployed with...
 configuration management tools to install the patches and the software. And you mentioned Ansible, what are the other ones? Sure, there's a big chef puppet salt. Those are the four main ones. And they all have pretty much equal mind share. The big difference between Ansible and Chef and puppet is that Ansible is push. You run a command from a server and you push out the configuration. With Chef and puppet, you have a agent on the server. And it pulls the...
 configuration either for the push model just to show my bias because I like that control, but they all pretty much do the same thing. And now these configuration management tools are slowly being phased out in favor instead of Docker containers. You can deploy Docker containers through Terraform to like ECS, right? Fargate. And then there's an alternative Kubernetes. So we have Docker containers now. So talk to us about ECS Fargate.
 Kubernetes when you do use one, when you use the other, what is Kubernetes? How much can you do everything with Terraform, et cetera? Yep. So you may have a very good point, right? Things like Ansible, Chef, are being superseded by a doctor file. So the patches you would do at Ansible and Chef and installing your apps that you do at Ansible and Chef, now you would just do an adocrophile. And the good thing about a doctor file again, you could check it in. It's a real file that you could pass around. So Kubernetes. So once you have a container on your laptop and you give it to
 DevOps and say deploy this. That's where the real fund begins. And we can provide a link to this infamous cartoon or a diagram where how do deploy a container to AWS. And there's literally 15 ways to do it, which leads to a lot of confusion. And even among DevOps professionals, there's a lot of confusion about how to best deploy a doctor, a container, and everyone has their own personal favorites. At depth, we recommend ECS elastic compute. That's got the ac-
 and go ask the container service. That's what we recommend in the professional environment because out of all the options is the easiest. It has good dev support and prod support, but that's if you have a good dev ops team. If you're just like a one man show things like Gap Runner, that's good, that's simple. There is a lasting beanstalk, that's nice and simple. It depends on what you're trying to do. And then the big granddaddy of the mall is Kubernetes. That has a lot of hype around.
 The main thing I want to stress about Kubernetes is it's a very complex ecosystem to deploy a container. And people have careers just dedicated to Kubernetes alone without any other skill set. And they make great money because it's a very complex to get right. And what Kubernetes does is it takes a container and adds all the production values to it. Right, clustering. You can employ multiple containers of multiple copies.
 of the container. That way you could manage low rate if there's 100 users, you could have two containers. If you have a thousand users, you could have three containers for containers. There's a lot of security around Kubernetes to have it safely talk to other containers or to other external services. And that's just scratching the surface of what Kubernetes provides for you. I almost get the impression it's it competes with AWS. It's its own and higher server farm orchestration service. Yep. So all the major
 clouds have their own implementation of Kubernetes in AWS is called EKS elastic Kubernetes service on GCP Kubernetes was developed by Google. So in Google Cloud, if Kubernetes engine is sure has their own implementation of Kubernetes and the good thing about these these called managed services where you can build Kubernetes on your own from scratch, right? And there's this famous blog called Kubernetes to hard way that will give you step by step on how to if you want to build it by.
 But what AWS, all the cloud providers does is, okay, we'll have joined Kubernetes, run this Terraform, click this button, and we'll create a Kubernetes cluster for you that works with all the AWS services. So it's not so much competition. It's just another way of deploying containers into the cloud. And it's probably the most complex way. The most complex. Why would we want to use it ever if it's the most complex? So, especially in a production load, like if you have a really...
 A lot of intense production requirements. You have a lot of teams, and these teams do a lot of microservices. So let's say you have 18 containers, 18 different images you have to maintain. Kubernetes is great for that. Automatically scaling up and down the load is great for that. And to be honest, there's a lot of hype around it, right? So tech people love hype. They're the new shiny, right? Oh, it's a new toy. I like to play with this new toy. That's a huge driver in a lot of architect. There at depth, we subscribe.
 to keep it simple, the simpler the better, the simpler it is, it's easier to maintain, easier to manage, easier to teach other people. So Kubernetes is the opposite of simple. I mentioned the pay, right? So a Kubernetes engineer can command a pretty high salary. So a lot of, I've seen this so many times. Well, I have an opportunity to use Kubernetes at work. I'm going to do it because I want that skill stack. The other benefit, like just full disclosure, I'm,
 I know means a Kubernetes expert on any stretch. When I think of this from an architectural perspective, I do think about staffing and I think that what we're expressing here is actually a little bit of a hot take in the DevOps community. I think there's probably gonna be a certain portion of people who are like, whoa, what do you mean not Kubernetes? What do you mean start with these? Yes. Then I can get that because one of the advantages of using Kubernetes is even though those people do command a premium, that skill set is quite available. It's also open source. So, you know, the YAML files that you specify for em both in your service and self. It just doesn't work. You get to this is similar. And like these, the test does. Right? It is. So, your serviceAwning machine in your service comes out from whiccom thank you.
 Kubernetes, their open source, they'll work largely, you know, and Dero might be able to correct me on this largely work between different managed service providers, Kubernetes, which is those are nice benefits too. But I completely agree with what Dero what it's saying, unless you have a team that's ready to do that and ready to deal with the complexities of managing got Kubernetes services, it's much better to start simple. And I'll talk to my own personal journey with Kubernetes at my previous job, Kubernetes was already there.
 right? So it was gave me a good opportunity to learn it and I made so many mistakes. Production level mistakes, things like, you know, permanent volumes, security, networking. It's such a complex topic that it's not just like, you know, you think, oh, okay, well, I did this tutorial. It works. Cool. You actually deployed a production with real users, real load. You'll learn soon enough that things can go drastically wrong and it's, if you don't know what you're doing, it's hard to
 and I'll actually send you a infamous flow shard of, if things go wrong in Kubernetes, here's all like literally 20 commands you have to run to figure out what's going on. So you said Kubernetes is overly complex and now I realize you're not just saying that Kubernetes to use Kubernetes is over complex. You're also saying the complexity of the environment you might be trying to orchestrate corresponds with the complexity of Kubernetes in use. So if you're a startup, let's say a hundred employees, probably get knocked back up.
 by using all of the AWS services, if your project is wildly complex, then you're going to want the tool to task, which is going to be probably Kubernetes. And definitely before you go to Kubernetes, make sure that it's a informed decision and make sure that you talk to people who've done that both pro and con, right? You have people who are passionate about Kubernetes in production. So talk to them to and get their viewpoint on this. It's a huge investment hard to hide.
 expensive to hire. So it's almost like a Lamborghini, right? We all love Lamborghini's, but it's hard to drive, easy to crash. There's a lot of quirks like if you Google how to back up a Lamborghini, you can't use a rearview mirror. You actually have to open your door and physically look behind you. We all love Lamborghini's, but make sure, you know, you know how to use it. I think another thing about Kubernetes. I always like to remember that Kubernetes was created to help run Google.
 infrastructure of container. Google, right? Like it's inherently works. Very complicated. Like it's going to be really good, but it's going to be really complicated and it's meant for Google scale problems. Maybe if you're successful, oh my god, I buy hats off to you and you need that sort of complexity, amazing. And you're just starting out. I don't know. I don't know if I necessarily agree with it. If you start simple, you're not locked in, right? Your logic is in that container. So you start.
 with ECS and ECS is going to carry you pretty far to match point if you ever get to the situation where wow, we have Google skill load one hats off to you. If you success, you could definitely migrate to Kubernetes. All right. What is ECS? Okay. I'm going to actually, I'm going to read this flow chart. This may be weird in audio format. I'm going to drop, of course, all of the images that your watch's talking about in the show notes for everybody to look it up after was. I'm going to read the flow chart and see how it goes. It says which AW
 US Container Service should I use? Where do you want to run this container? On premise, then Kubernetes? Yes, EKS. No ECS, and then OpenShift, which I've never heard of, Rosa. And then it says, where do you want to run this container? In the cloud, what will this container do? Run Build Jobs, Code Build. Run Batch Jobs, Batch. Run Apps, serverlessly? It's for real though? Yes, Lambda. No, Fargate. Serverslessly, no. Who will manage this Kubernetes?
 will be EKS, AWS manages it. If you, I'm going to skip that section. I'm going to manage it. You're going to be doing it a lot. If you know, then light sail, if yes, then EC2. And then it says, if you want sort of both container on premises or in the cloud, it says green grass. I would have actually thought we're talking, we're going to say like a mix of Kubernetes and ECS. But we have a lot of container services that can be used here. We have EKS, ECS, Lambda, Fargate, Beanstalk, AnF-
 runner, white sale, EC2, code building, batch, Jeroat, save us. Well, so even among people who are doing this day and day out, picking the right way to deploy containers, it's a complex issue on itself. That's why if you're just doing machine learning and you want to concentrate on that and not worry about this complexity app runner is a great way, right? We just take here's my container and AWS does everything for you.
 create the load balancer, create the network, all yet deduces provide container app runner is a very good thing. My first time hearing the app runner, it was just introduced about a year ago, just before the pandemic, I think, or maybe right after the pandemic. So again, that's the other thing about DevOps. Okay, what you knew five years ago may not be relevant, and it's not relevant today, right? Apprunners are very, it's their latest toy to deploy containers, and they put all their knowledge of what went right and wrong with other.
 products into app runner. And then you were going to mention elastic bean stock. Elastic bean stock is the probably one of the oldest ways to deploy a container. And it does a lot for you to. So it's a little bit more mature, a little bit more battle tested requires a couple more clicks, right? But in the game brand schema things still very simple. That clickety clickety guides you through of what you want to do. It says, Oh, hey, do you want, you know, Python or do you want Java? You know, do you want machine learning or do you want to do, you know, web apps?
 So it's almost like a crushed nair that you could just click on and say, okay, got my stuff. So coming into this discussion, I thought it's really boiled down to EKS, Kubernetes, or ECS. But then we also have Lambda and Fargate. So that's a whole other topic. There's this thing called serverless. And I'm going to put the, you can't see in the podcast, but I'm going to air quote serverless. There's, you know, servers still behind the scenes, but basically you don't have to manage the servers with Lambda, you just say, oh,
 Here's my code and it could either be code that you upload or it could actually be a doctrine container. And Lambda will say, okay, I'm going to just charge you for actual usage. Like if five people hits your Lambda, then I'm just going to charge you for those five users. So it's a bit of a cost savings. But Lambda is a very complex topic and Lambda and serverless is a complex topic. So I wouldn't recommend that for a beginner, more for cost savings and anything.
 I got it. And I want to stress that even among people who have been doing this for a long time, I've seen so many bad implementations of lambdas, seen so many bad implementations of ECS. Cone's a bad implementation of Kubernetes, right? I was like, oh, well, that's not right. It's so complex. And then to toss it back on me, I'm sure if you looked at my implementation of Kubernetes from a year ago, you say, that's bad. What are you doing there? Then you can take a crap to me if I'm wrong in any...
 of this. The one way I've thought about Kubernetes versus Hosted Services, Managed Services in the cloud like AWS, Azure, and GCP. I've seen them as almost two different approaches. I've always thought of Kubernetes as a very general approach. Almost the way Terraform is general cross-cloud infrastructure's code. I thought of Kubernetes as a way to take your Docker files and no matter where you're running it, and it can be on-premise too in your company's data center. It'll run the same no matter what.
 where GCP AWS and Azure are on-premise. And it's going to be using open source tooling instead of the cloud provider's service for that equivalent. So if you're going to use Kubernetes, you'll be using a post-gress Docker container. As opposed to if you're going to use AWS, you'll be using RDS, their hosted database service. So with Kubernetes, you have at your fingertips all of the world of open source tooling, you know how those things...
 work you're not locked in. In Cloud Land, using let's say Terraform, on AWS, you're using their services. RDS is their implementation of relational databases. It's not open source. You don't know how their postgres stuff happens. In Kubernetes, it'll have its own secrets manager. AWS has its own secrets manager. There are pros and cons in the case of AWS. By using their services, you get all the patches, the updates, everything's black box. So it's more.
 managed and secure by using Kubernetes, you have more visibility, more control, everything's open source, and you can deploy it anywhere. Is that all right so far? So you touch on some very good selling points of Kubernetes, right? It's supposed to be generic. So there's this command card, a cube, a cube, CTL, different people pronounce it differently, QCtrl, cube CTL, but with that command and a YAML file, you can deploy your app, whether it's
 post on AWS or TCP or resure. At least ask to promise. And in reality, what gets you is that security model, right? So if you deploy Kubernetes in AWS in a production level environment, you're gonna have to tie into the IAM of AWS. So you'll still be using these services even with Kubernetes if you're deployed in the cloud. Right. You just can't help it, right? Especially the security model.
 it's, you know, it's pervasive among all the services. And again, so let's say you take your Kubernetes cluster and your app and you, you know, you try to deploy it onto the GCP Kubernetes engine. I almost guarantee you, it makes serious code that you have to access resources. You're gonna have to hit that security model. And even if you don't, right? So let's use a perfect example, a real world app. It's going to write like an AWS, right? You were probably right to it.
 S3 file or S3 bucket. If you deploy this on, you know, GCP, you're probably going to have to write it to the GCP equivalent of the S3 bucket. So yes, Kubernetes promises making a generic and common, but in practice, it's not. And then that kind of leads to this other hot topic, hot take of DevOps multi-cloud, right? You'll hear this a lot where, oh, our application or our product, you can deploy it on AWS.
 US or GCP or Azure, it's easy. It's not easy. That's a sales pitch. Anyone who tells you it's easy, is trying to sell you something. That's the idea of deploying a Kubernetes cluster on different cloud providers. How about Terraform with the way it orchestrates different cloud providers? Right. So the good thing about Terraform is that it's just a configuration language. Right. So the good thing is the skill set that you learn to code.
 Terraform on AWS is transferable to GCP. One of the links we're gonna show is a cloud compare.in or compare cloud. One of it's, they'll be the link, but it compares, okay, if you do this in AWS, here's what you have to do in GCP, right? Equivalent services. So Terraform is not trying to accomplish universality. It's just trying to accomplish universal accessibility, right? In a different services, right? And it's a common language, right? So you know how to do Terraform in AWS.
 Let's say a perfect example. Oh, here's a terraform to create an S Street bucket in AWS. Now, I've had to create the equivalent in GCP. You say, okay, well, in GCP, here's the equivalent object storage, same style of terraform code, just the GCP flavor. All right. So speaking of multiplayer cloud, tell us about Azure and GCP. Yep. If you're going to learn one AWS, 90% of the companies are going to use that. If you, you know, interview.
 for jobs, right? You're probably going to hit AWS. Sure, or is the second most popular one in the business world because Microsoft doesn't want to beat AWS. It wants to beat Google. So if a, you know, C suite can negotiate, well, you almost hit AWS, uh, as you're for free, right? Don't just kind of give you the farm just to get you on this, the platform. The other one is, that the joke is if Microsoft acquired your company and you're on AWS, you won't be on AWS. You're going to be off in a drawer. They're going to force you to go on.
 there. So a lot of I have some friends who have got acquired my Microsoft and that's they're like big project. GCP it's great from a UI level. GCP probably has the best UI of the three. The bad thing about GCP in Google in general is Google loves to kill products. And you're not guaranteed backwards compatibility in GCP. Like AWS will bid over backwards to to accommodate things you've done, you know, 10, 15 years ago, right? That's why you...
 still have ECS classic. You still have, you know, the way you can figure estuary buckets 18 years ago or whenever it, it, uh, estuary came out, you can still do it today for the most part. Google hates backwards compatibility. That's why it's, it's the least favorite of Monday three. Have you, have you seen that then? If you seen services go non backwards compatible or you've entirely defunct? Yep. There's, there's actually a great list of a GCP things and we could find the link and post it, but there's a great list of, you know,
 Hey, this is the things Google breaks, right? These are things Google like to kill. From a UI perspective, it's excellent. It's like the best UI I've used. AWS is very clunky, UI-wise. The real conversation for our listeners is going to be around the machine learning services offered by the two providers. And I think we should talk about Kubernetes and machine learning services. I personally use SageMaker on AWS. I've talked about it in the last few episodes, but just to brush up, it's a suite of machine learning tools. It's not one machine learning.
 tool on AWS, it offers training a model, including parallelization, distributed training, monitoring the model training process. When you're done deploying it to an endpoint, that endpoint can be a rest endpoint, one that you call as a batch endpoint. So if you're just going to do one off inferences on large amounts of data, so that you're not charged for the running rest endpoint, or recently serverless model calls, which is...
 Absolutely amazing. And then a whole bunch of monitoring tools on top of your model. So as new data comes in from new users, does that new data distribution over time drift away from the type of data you trained on? And then it will email you if so, which is really powerful. And is there a bias? And it does all this stuff for you. And I'm completely unfamiliar with GCP or Azure's machine learning offerings in that domain. Are you?
 familiar with them at all? No, that's where the collaboration comes. Is that like even on AWS, everything to describe is new to me. I see. And this is to two points of this podcast is one, you can't know everything, right? You can't such a complex field. And especially, you know, if you're trying to learn both DevOps and machine learning, that's a big Apple to bite, right? You almost want to pick one or the other. The other thing is collaboration where...
 Oh, wow, look, SageMaker can do all these things. And you know, if you go to a DevOps team and you explain that, right, it's probably going to be new for 99.9% of them. So it's in requires a collaboration of, oh, hey, here's all the stuff it can do in SageMaker. Let me show it to you what I did in dev. And then the DevOps team can take a look at this and say, okay, well, we need to secure this. You know, we need to do this with a container and solve patches, things like that, change your Docker file to install this agent patch, right now.
 So it's a collaboration and a learning experience for both sides. But yeah, from the scene and learning this perspective, I'm very ignorant. And I'm hoping to get better by listening to your podcast. Hey, Dr. Art Kives and learning about it. Go through, you might like the last two episodes on SageMaker. Everything prior to that is sort of machine learning basics. There's not not a lot of cloud anything until the very end there. And I do want to mention a Kubernetes. A future episode I'll talk about the competitive open source machine.
 pipelining tooling because I just mentioned SageMaker. It has a whole suite of pipelining tooling, but it's all hosted in the cloud by Debus for you. Kubernetes has its own. It's called CubeFlow. And it can orchestrate a pipeline of machine learning tooling for you. That includes the data engineering, the data ingestion, engineering transformation, and then various forks in the road, like analytics and stuff like this. So if you enjoy SageMaker, you want to do it open source or run anywhere you might.
 looking to Kubernetes cube flow. There's other projects out there like Apache Airflow and some others. Oh, the one thing I do want to mention because it comes up a lot is cost and that's, you know, if you're deployed to a production, definitely your CIO or and your CTO and your CFOs could say, hey, what's this expense? But it's really dangerous when you deploy to your personal account and I've got hit with this a lot that, okay, Kubernetes, this is cool. I deployed it to my personal account, leave it on.
 like a day or two because I forget to tear it down. That's so you know you get a bill for, you know, 80 bucks, 100 bucks. And that can be a surprise. And so the one thing I definitely recommend is that you're trying to play with this on your personal account. Every cloud provider has a bill monitor that says, oh, you've spent more than 20 bucks. I'm going to email you, you know, you spent more than 80 bucks, almost like a credit card, right? I'm going to email you and inform you. So definitely before you do anything, turn that on or else.
 You know, you make a surprise at the end of the month. So we talked a lot about a lot of tooling thus far, a lot of the history of DevOps. And we alluded to that DevOps might not want to be a role that you take on as a machine learning engineer, that it would be too big a pillable to swallow, stick to your guns in the machine learning space, learn the SageMaker tooling through and through. So for my listeners, the machine learning engineers, let's say,
 They create an AWS account. They spin up a SageMaker project. They open SageMaker Studio, Jupyter Notebook. They start typing away, training a Keras model. They learn, they really learn SageMaker nuts and bolts. And they, maybe they can even deploy a SageMaker model now as a rest endpoint using model.deploy. Now that machine learning engineer may be a one-man band, but they still want to get this model out into a while. Maybe...
 They want a web front end, a UI, a React front end on an S3 bucket with Cloud Front distribution in a Route 53 URL. And that front end is going to have to communicate with some backend, let's say a Fargate Python Fast API server that then kicks off the machine learning model inference endpoint. So everything I just said is I would consider totally doable.
 by my listeners, maybe they would orchestrate that entire stack I just mentioned using Terraform. So where does that wall start effectively? What would be a good suite of named tools in AWS, presumably using Terraform, that my listeners could get started with to actually deploy a product. And then where does that wall begin where now you actually want to really consider having a devil?
 professional helping out with the rest of this process. Yeah, that's a great question. So I don't want to dissuade anyone from a learning care form or Docker, Ansgar, Kubernetes. I just want to highlight that some of these tools we have a steeper learning curve than others. And actually, that's how I learned it too, right? So I started my career as a job at devs. And I thought, oh, well, Ansible looks pretty cool. I would have played with that. Terraform looks cool. I would have played with that. And...
 Eventually, it became my career. So I don't want to disweight anyone from learning it. For me, the demarcation point is security. How valuable is your data or how secure is your data? If your data got hacked, leaked into the wild. Is it OK? It was just public data, it's no big deal. Or is it sensitive, PII data, medical data, consumer data, identifiable data? If the answer is, if my...
 Data got leaked. It's bad time for me and my company. Then that's where you want to hire a professional to take a look and make sure that the proper security precautions are met. Yeah, and I'll just add on top of that. Like, you know, if you're gonna play around with a development instance with some fake data, that's great. You should do that. But, and if you're gonna release it to some friends or friendly people, you should do that. We're trying not trying to scare you, but like you're what said, you have anything identifiable there. You don't.
 definitely want somebody's eyes on it from a security perspective. There's a lot of very easy mistakes that you can make and you've seen this in the news where different corporations have leaks. You don't want to be one of them. Even the people that are really, really good at this stuff, things slip through the cracks. It's just prudent even for the best people to have a professional set of eyes and look at the security of things. The other thing that just to consider is like if you're just using bear EC2 instances, you want to make sure those things are locked down and people can't just go and use them.
 or something like crypto mining. I know that's kind of, it's a little bit paranoid, but that's a real thing. If you leak your keys or open a port that you shouldn't, people can hijack that machine and run up your bill pretty intensely. So you're gonna watch out for that type of stuff too. That's a more exception, shouldn't scare you. Just thinks to be aware of. And a lot of these DevOps tools with the infrastructure is code and the configuration management checking it into to the repository. It's about working with other people, working with other teams.
 If you're a one person show, then you're one person show, you don't have to document, you don't have to communicate. So experiment, try it, learn it. You know, we don't want to say, you know, you can't do this, right? You can't do this. Just be aware of the complexity out front. But like me, you may learn it and well, I like this more than Java development. So I'm going to change my career a bit. Your career is a journey. All right. So, so security. That's a demarcator.
 Or even like complexity. If you're in over your head and you'll know it when you see it, but as far as tinkering and deploying your project, as long as you're not dealing with secure data, PII, what is that, what is that, Stanford? Personally, personally, identifiable. Personally, I didn't find actual information. Yep. Okay, so if you're dealing with PII, if you're dealing with security, get a professional, get these guys, Matt and Jeroat, I adept. And if you're just tinkering around, and you just wanted to have some fun, you built your machine learning model on SageMaker, you deployed it.
 as a rest endpoint, Matt, can you guide us through an architecture? Can you talk us through like what services then our machine learning engineer would want to use on AWS to expose this model to the internet? So you already said it's SageMaker. I mean, if you asked me to come up with like a beard African template of something, I'd say through your static site on S3, fronted by cloud front, put your, sorry, I think you mentioned maybe it to Python service.
 put that in a Docker container running on ECS, hook it up to a relational database service. I'm going to presume you need to have some kind of state stored there. We always use Postgres. We really love Postgres. And then, you know, Terraform, that guy up. I'm curious if you're what would change any of that though. Yeah, those are all great recommendations. My advice is, especially if it's not to cure or sensitive data experiment, that's how we all learn, right? Try it out. See what happens.
 When the good things about AWS is that there's a lot of documentation, sometimes too much, but pick one, try it out and see. But the Vice-Mat gave us an excellent spot on it. One more thing to add that we totally forgot is that you should really put a automated build a deploy pipeline in place too. So using something like code pipeline or you know, circle CI or something like that, choose your flavor. That is something that will pay off in speeds. It's not 100% necessary, but you'll find it.
 that it's going to help you quite a bit take your code from source control, push it automatically out to your infrastructure. It's extra work though. Talk a little bit about that. So this is CIDCD. Yeah. Continuous integration, continuous deployment. Yep. Speedrun the process. Like what do they get commit? Yeah, and see. Generally, I mean, there's there's a million different flavors of it, but in general, it will either watch or periodically build or you can manually trigger from your source code repository, it will assemble your code.
 run tests, run security scans, whatever you need done to assemble your code, package it up into what's usually generically referred to as an artifact, which is package of files basically. That's the, you know, usually the continuous integration piece. And then the continuous deployment piece is taking that package and pushing it out to your infrastructure and turning it on and releasing it to users. So it's really important to do that to make sure that that process is repeatable.
 that you're not, it's not prone to human error. And it also just saves you time. I've seen a lot of projects in the past that skimp on that step in the beginning and you just end up paying for it as you go along. So it's usually kind of like my number one piece of advice is don't skimp on your CI CD when you start up it. It is a bigger investment. It's not something you want to learn along with everything else, but it is something as you think about deploying a piece of code out to the wild. If you're going to do it a bunch and this is going to become a
 real thing. That's definitely something that you want to invest in that. Yep. And it's probably something that's from a selfish perspective, you want to do too, right? This is how Ashley I learned automation. Do some Java code. Hey, worse locally. Now I want to deploy to to dev so that, you know, the QA folks and other devs can look at it, you know, I do it manually once. Oh great. I do it manually twice. Okay. After 10 times, like, well, I'm not being paid by the hour here. So let me just automate this.
 I could just check it in and then a machine behind the scenes does it for me and I can go get some coffee. Awesome. Thanks, guys. All right. Well, Ship and I owe podcast, another of Depths podcast network, Lincoln's Show Numps likes to do picks at the end of their episodes. And I like that. And a pick, Matt, what does a pick? Pick is just, it could be anything, anything that you are currently into that you're digging lately. It doesn't have to be technology related. I tend to pick things.
 that aren't technology related. I don't know. So I could go first if you want. All right. My pick is the show succession on HBO Max or HBO. I've late to the game on this. Some people are probably going to laugh at me, but that show is fantastic. Jeremy Strong, the guy that plays Kendall Roy on that show is amazing. So I will pass to Tyler. All right. Mine will be at Crook Poe of this episode, actually. I created a terraform script that spins up a-
 cloud game streaming service, DIY cloud gaming. It's good on my GitHub forward slash left near LES and IRE for slash DIY hyphen cloud hyphen gaming little terraform script. Or you just Google Quest to AWS. They wrote a blog post on how this whole thing works. And what you do is you spin up a Windows 19 server on AWS that has Nvidia graphics card drivers installed ready for VR gaming.
 You install virtual desktop on the server and you connect to that virtual desktop instance using Oculus Quest 2's Virtual desktop app and now you can do AAA VR gaming without a PC so you can play Half-Life Alex as guards Rath, Lone Echo, all the all the really impressive VR titles That can only be played with a gaming PC can now be played without a gaming PC I'm keeping my eye on Nvidia with their G
 for us now service looks like they've got their eye on cloud VR. They're calling it cloud XR. They have a server and client app that are actually in the works. So keep an eye on them, but you can actually start gaming without a PC and without any of these other services all DIY. Maybe you first get your hands dirty with Terraform. All right, Jeroat. Well, mine's going to be a little boring, but there's this language called go. And actually, that's what Terraform is behind the scenes written in.
 And it's a very sea-like language. So I mentioned in the podcast, hey, I used to be a Java developer, then I did some Terraform and Ansible, and looks pretty cool, and I pivoted my career towards that after doing DevOps for a while. I was like, I kind of miss programming, like real programming. So learning Go, which is a new programming language, is my current thing. And it kind of ties into DevOps, because a lot of DevOps tools, especially from Hashtagorp, it's written Go. So I had to hack into...
 Aeroform. I think you've had one language. There's something very freeing about actually creating a native executable and not needing anything else to run it. Isn't Docker or a Docker composed go based under the hood? So Docker. So Docker compose is interesting because Docker compose used to be written Python. And then I think they converted like the latest version of Docker compose is goal based now. I believe you're correct. It's it was from a Python to go and yeah, and the cool thing is that
 But now the reason Docker Compuls used to not work very well with Docker itself is because it was written two different languages couldn't share libraries and things like that. Now it can. I have a simple, when a friend asks if they want to get into the lab, where should they start? What kind of depth do you want to do web or apps? JavaScript. I want to do data stuff. Python. You want to do server stuff? Go. That's my quick. Do I rough on any feathers and any alternatives? I just would have said Python and JavaScript. Like, Python for your back end and then.
 JavaScript, you'll never go wrong. That's my you didn't ruffle my feathers. It's just my answer. So it's funny, because I had this exact same my son is in college and he's about to graduate. So he's trying to enter the job market, right? And he has a computer science degree and they taught him C++ as the main language, right? It's a very traditional computer science education. And I told him, well, we see plus plus your job market's a little limited. So I actually to give you Python.
 because Python, you could use it from like a real developer language. You could also, if you want to go to DevOps, you can, you know, Python is a very good system language too. So I, I recommend it Python to him. So I was teaching him, you know, talking about learning, right? I said, well, actually, if you want a job, let me just teach you terraform in AWS. I could teach you enough that you can pass a intro level interview and, you know, hopefully get them fake it till you make it, buddy. Oh, anybody who wants to learn this stuff, a cloud.
 Oh, yeah, yeah, yeah. So they have lessons on cloud. So Azure, GCP and AWS, and their lessons are pretty formal and dialed in because they're intended for you to pass specific certification tests. First data, not all of them. Right. They have like a learning track where they're like these really dialed professional video courses. And so I'll post a link to their learning track flow chart. One of
 which takes you down the path of data science machine learning, specifically by way of AWS. Obviously a lot of SageMaker stuff, but the whole path is like really professionally done. And it's how I've learned, it's entirely how I've learned AWS. In a second, a cloud guru, because my son is going through a similar path of learning. Initially, I was trying to teach him, but that didn't turn out to the greatest. So then he went to you two, he went to official AWS videos, Riley Learn.
 eventually a Coguru is the one he liked the best. You know, that's how I did it in the right direction.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. Welcome back to machine learning applied in this episode I'm talking to Dirk from depth agency about
 cube flow, K-U-B-E-F-L-O-W. It is an extension on Kubernetes for machine learning pipeline orchestration. In the last few episodes, we talked about SageMaker for accomplishing the same thing. And then in the DevOps episode, we talked about just general DevOps deployment of your Webstack to the Cloud. And if you'll recall, there are solutions for hosting Webstacks in the Cloud like AWS ECS. And then there's the open source.
 equivalent to that called Kubernetes. And an AWS service called EKS or Elastic Kubernetes Service allows you to use Kubernetes on AWS. So you can either use ECS or EKS. ECS is going to be using AWS as a managed service for hosting your Docker containers. And so it's going to be easier to use. And EKS or Elastic Kubernetes Service is going to allow you to host your Kubernetes cluster on AWS. The benefit of Kubernetes is that it is open source and cross-platform you can host.
 your Kubernetes cluster on AWS, GCP, or Azure, setting it up in those various providers is going to be a bit different. So it's not just a turnkey one size fits all, but it's going to be more cross cloud provider compatible than if you had set up your cluster on ECS. So AWS ECS, their managed container hosting solution is easier, but there's vendor lock and then EKS, their managed Kubernetes solution is harder, it's more complex.
 for certain, as you recall, the discussion with Gerowat, but it is open source and it is general case, cross-cloud compatible, and you can use it on localhost or on-prem. And so the equivalent in the machine learning space is Amazon SageMaker for their hosted machine learning offerings versus cube flow and extension on top of Kubernetes for the open source cross-cloud compatible version of the same. And before we get into this interview, I just want to talk about what...
 But these things are these pipeline orchestration tools because SageMaker is not just about hosting machine learning model. That's the tip of the iceberg. It's a very useful solution. If you want to get your machine learning model in the cloud, it is easier to use SageMaker's model endpoints or batch inference jobs than it is to try to spin up your own machine learning model hosted solution even on ECS or EC2 or anything like that.
 Even just deploying your machine learning model via SageMaker is going to be a simpler solution than roll your own, but the main value of SageMaker is not just hosting your model. It's all the other stuff that comes as part of the package, what we call pipeline orchestration. So when you're training your model, you have your data. It comes from somewhere like AWS S3 or their relational database service RDS or DynamoDB or something. Okay, and then that gets piped into a feature transformation step and then that gets piped.
 through the training step, the training phase of this pipeline. And that training phase might be splitting out your data or splitting out your compute instances into multiple instances so that the training can happen in parallel. And then we join the progress downstream as the next step in the pipeline. Do some checks, some bias checks, some drift detection, and then deploy your model to the cloud. And then additionally, as part of these pipeline orchestration tools, there is a continuous...
 newest monitoring aspect where if something goes wrong in the model pipeline, it might trigger a retraining job by reusing the entire pipeline from the beginning, or let's say that new data comes in over time. The users have interacted with your website for a while. Let's say you have a matchmaking service for music or videos and they thumbs up and thumbs down certain things. And so you want to retrain not because anything went wrong, but because you're a little bit out of date.
 Learning Pipeline orchestration tools like SageMaker or CubeFlow are the end goal of creating a production ready deployed machine learning ends to end solution, not just the machine learning model in the cloud, but the entire process of what goes eventually into the trained and deployed inference model in the cloud. And so all those steps of the pipeline are important. And so everything we've talked about with SageMaker, SageMaker has these tools for stringing together steps of this pipe.
 line, this pipeline orchestration. One will be ingesting the data and one will be data engineering, so feature engineering and imputation and all that. Another will be training, including parallelization and so on. And so that SageMaker, which is AWS is hosted offering, it's easy to use, but you're locked into AWS. So today we're talking about cube flow, which is an extension on top of Kubernetes. It's the open source version of SageMaker.
 effectively. It is universal so it can be hosted on all the different cloud providers. And again, per the discussion with Jirwatt, the downside is going to be more complex to manage. So that's an introduction to the topic and let's dive right in. All right, welcome back to the show everybody. Today we have Dirk from Dept proper and Dirk, go ahead and introduce yourself. Yeah, thank you Tyler. Hi, my name is Styrgill and I'm part of the Dutch data science team at Dept. I help my clients with buildings.
 more solutions to discover valuable insights in their data and to solve critical business challenges, both with machine learning as well as more like low level data analysis. And I work a lot also on the machine learning engineering part. And that's also what we're going to talk about today. Fantastic. In the past few episodes, we've been talking a lot of DevOps with a goal of achieving ML ops, getting your machine learning model into the cloud. And this journey of ML ops.
 So I've realized it's not just a matter of getting your model in the cloud as some rest endpoint, which SageMaker can accomplish very easily. But a lot of times there's a pipeline you want to build out. So you've got your training phase that will periodically ingest the data, maybe shard that out to multiple nodes that splits the data up, crunches it a certain way, brings it back together, pipes it through the machine learning model. And so there's all these different pipelining tools. I'm familiar with SageMaker. I've presented that on the show.
 episode we talked a little bit about DevOps, a little bit about Kubernetes in DevOps. And today your specialty right in ML ops is cube flow. Yeah, that's totally right. And it completely touches upon Kubernetes as well as it's basically a, a orchestrator for pipelines on Kubernetes. So talk to us about pipelines in general. Mm-hmm. I have introduced the topic to my listeners, but I don't know that I've gone sufficiently down the motivation.
 the entire process. And then eventually we'll get into what are the options out there and why cum flow. Yeah. Talk to us about pipelines. Yeah, definitely. So the first thing that's good to know is often when you have these examples, these toy examples of machine learning problems or any data problem, it's very much focused on the solution itself and training a model to predict something and then use it outputs to optimize business processes or decision making. However, what we see at our
 clients mainly is that a lot of our clients have different business problems for which we develop different models. But you get at a point where you have so many models in place that knowing how well a model is still performing and retraining that model is going to take a lot of time. And you basically can't just spend time on retraining and checking if a model is still properly working if there are also new use cases that you're working on for that client.
 So at that point, you're kind of forced to move away from standalone models and hop on machine learning pipeline train because that really helps you do. First of all, keep track of the performance of the model. It helps you to automatically retrain models and it just makes your life much more easier as it's taking a lot of time away and keeping the quality of the models up. It almost sounds like what continues integration continues to deploy in CICD is for the software
 learning model or machine learning pipelines is for the machine learning deployment world. Is that something good analogy? That's a perfect analogy. I think that if you look at what data science feels in general, it's much more moving towards a continuous integration and a continuous deployment of models. Of course, like the big companies like Google and Facebook, they already do this for a long time. It's also why they now have these frameworks and these platforms that basically...
 allow you as a data scientist or machine learning engineer to do that as well. And they have invested already a lot of money in those solutions. But I think like more like the smaller companies, they, especially in the beginning, it was more focused around building a model and solving a business solution. And sometimes it kind of lacked the continuous development and deployment and integration of like the machine learning solutions. I think a lot of companies now start to realize that machine learning pipelines are just...
 as important as building a model because model without a proper pipeline behind it is eventually not going to perform as well as when it was just developed because data changes over time, especially now, for example, with COVID. So when you have these models that predict customer behavior or that take customer behavior into account to predict something else, especially with COVID, that retraining part is more important than ever. But
 customer behavior really changed during that period. So you can imagine when you use a model that was trained on data before that period that it's not going to perform as well as it was during like COVID or after COVID because behavior totally changed. So it keeps up. So these pipelines keep up with with the times with the change in data. Do you find yourself I know that these frameworks and what we're going to call these frameworks is it right to call them pipeline frameworks or I think the proper term would be
 pipeline orchestrators. Do you find yourself using these exclusively or do you still do your model development on local hosts just in a Docker file TensorFlow, whatever? It kind of depends. So especially with clients that we work with already longer, we now move towards developing from a pipeline perspective. And of course, the development of the model and the testing of the model in the first version of evaluation of the model that can still happen outside of the pipeline.
 but everything around it and the structure is basically set up to eventually be used for a pipeline kind of architecture. And then we also have these clients where we just started building solutions. And there sometimes we see that there's first and need to see how machine learning or data science solutions can help improve their business. So you could say they're less mature and then we often see that pipelines are not really the way to go yet because.
 we first need to really discover what data science and machine learning can bring for that client. But for long-term relationships and clients that are really mature, yeah, we really move towards developing from a pipeline perspective rather than a standalone model. I like that. It's a measure twice cut once. You kind of already in the mindset and platform developing towards the pipeline and then when it's time for production, your turn steps ahead of the game. Sorry for all the diversion. Your
 expertise is in this area. So I'm going to just go ahead and let you drive and take us on a journey. Talk to us about Qflow and the various other options and all that stuff. Yeah. So the reason I took Qflow is that because that's one of the main orchestrators or one of the pipeline-focused project that we at DevChoes to go with and to maybe start off with like what really Qflow is already mentioned it before. Qflow is basically a solution that focuses to do-
 deploy a machine learning pipeline on a Kubernetes backend. And that helps to make the pipeline very scalable. Like that's one of, of course, the key benefits of Kubernetes, the scalability that you have. And also at the same time, cube flow integrates a lot of these different kind of frameworks within. So it allows you to either use TensorFlow or psychic learn or pipe towards and all of those modules or packages. How you like to call them, you can very
 easily incorporate those in a cube flow solution. And cube flow itself doesn't necessarily have to be a any pipeline. Cube flow also offers the possibility to basically submit a job on a Kubernetes cluster. So just like a training job or a data processing job. But it also at the same time allows you using the cube flow pipelines architecture to build a full on pipelines. And that really gives a cube flow as a whole a lot of flexibility.
 Besides also it being open source. So a lot of people develop and contribute to the framework itself and what you see because it's open source. For a lot of these challenges that we as a company deal with as well, other companies deal with those as well, of course. And because of that, that open source nature of Qflow, you see that the fellowman goes really fast. And every day there's like something new released, which might also solve one of your problems, which otherwise you would have had to spend quite some.
 time on to solve it yourself. In the last episode with Giroat, we were kind of comparing CubeFlow with AWS's ECS service, elastic container service. And so we have the cloud native offerings, Microsoft Azure, Google Cloud Platform, and Amazon Web Services. In our episode, we recommended AWS, if nothing else but popularity, say, it's just like, if you want to find a job, you're probably most likely to find a job in the AWS marketplace because of its popularity. And in SageMakerLan.
 And the way they do pipelines is, well, there's a lot of overlap with cube flows off lanes. Kubernetes offlands. So SageMaker would probably be the equivalent of cube flow in cloud native on AWS. And then ECS is the equivalent of Kubernetes on AWS, a container orchestrator platform. But it's all closed source. We don't have visibility into how many of this stuff works. But by that standard, it would seem that Kubernetes and cube flow would be preferable. Why not get open source?
 to be getting the same thing in the first place on AWS. And Gerowat's statement was that Kubernetes is quite complex, a large pill to swallow from a developer's perspective, compared to ECS. Yeah. And I wonder if that's similar to your own experience. And if so, how does Cubflow compare in that analogy to cloud native machine learning offerings? Yeah, so maybe the first good thing to know is even though Cubflow is a real...
 originally built on Kubernetes. It's not just limited. So like in Google Cloud, because that's also a little bit of the history of how Qflow originated and come from Google and how they internally deploy machine learning pipelines for all their services. And they build like this entire TensorFlow package around it called TFX, which we will touch upon later as well. But Qflow is not just limited to Kubernetes. And also at the same time, Qflow takes a lot of this complex deployment process.
 that you have with Kubernetes takes it away and does it for you. So especially as a machine learning engineer or a data scientist, you might not be very knowledgeable about Kubernetes or nodians and else of how to deploy something on Kubernetes. But Cubflow really facilitates that for you and helps you with that. At the same time, Cubflow can be deployed on the most big platforms like AWS, Airflow, Azure, Google Clouds, and...
 Google Cloud even have the option to deploy it on Kubernetes, which it originated from or more like a AI platform, what they now call like the Vertix AI platform. So I think whereas when you focus, for example, on more like an AWS or just an Azure, you're very limited to that platform, with Qflow, you can reach across just one platform and have the possibility and opportunity to not just stick to one platform, but also deploy another platform.
 if you need it. And of course, the setup process is going to be a little bit different, but a lot of the complex stuff that comes with it is handled by CUPFO. And I think that makes a huge argument for using CUPFO to get it with the open-source nature that it has. Yeah, and I think another great argument with it being cross-platform compatible is localhost development. With SageMaker one thing I come up against in developing with the cloud in mind, the benefit is that again that measured twice.
 cut once. You're already in the mindset of working with VPCs and security groups and I am policies before you can even run your model. From the downside, good luck with debugging and local development. If you need to be in a VPC, you're not really going to spin this up on your local PC. You're going to have to create a fashion host in the cloud or a client VPN that's able to connect to the VPC and all that stuff. So the complexity that Giroat and I discussed of Kubernetes in use case in general.
 That's one thing, but we're not accounting for many times the complexity of developing towards ECS when your development environment is local hosts. So it seems like there's plenty of pros and cons on both fronts if we want to talk about the debate of complexity. And then do you guys use Kubernetes for the general DevOps stuff for web hosting as well? I think that kind of depends per client. I'm not really aware of that part as I'm more like part of the data science team. I'm not really of the weapon.
 the Ebti-Felliman team. I know there are some deployments that happen on on Kubernetes, but further than that, I'm really not sure how the DevOps or the development teams adept tackle that part. Your preferred cloud is GCP is that correct? I wouldn't say preferred. It's more like a lot of our clients work with that. I think in Europe in general for a long time Google has together with Azure has been one of the bigger cloud platforms. AWS is kind of...
 taking over, I think, at the moment. But I think it's also because we are in the marketing business, or a lot of our clients, the solutions that we build for them are marketing related. And a lot of those clients work with Google products like Google ads, Google analytics, those kind of Google based products. And that's also the reason why they often go for Google Cloud because it's a logical decision to make if a lot of your other systems are also by Google, developed by Google. That makes sense. And how about Azure? We do.
 see a lot of Azure, but what we do see is the clients that use Azure are often less marketing focused, as you might say. It's more like operational business operations financially focused solutions that we've done built for them. Yeah, so for example, we have a client that's in the real estate development business and we build more like operational solutions for them and they are also an Azure. It adds a lot to the puzzle when I'm making people who are trying to get you know.
 starting the cloud and they say, which one should I pick? I always say AWS just because it's most popular. So it's an easy answer, but I never have any real technical argument for any of the specific cloud providers to provide. Yeah. I think if you're in the marketing business and you're using a lot of the Google products, it's one of the most logical steps to make from a connectivity standpoint. Of course, you can get the data from Google analytics or Google ads to any of the other platforms. But some of the integrations are more.
 much more straightforward with Google Cloud than with, for example, analytics, Google Analytics and AWS. And then on Azure's front, if obviously if you're Microsoft, it's a shoe in, but then potentially as you mentioned in finance as an example or operations, people in finance are so used to excel. That's a Microsoft product seems like it's an easy step towards that platform. If you're coming from an environment in which Microsoft products would have made sense on the desktop. Yeah, definitely. And also, for example,
 a lot of the companies that are, for example, using Microsoft Dynamics or those kind of packages or those kind of softwares that also would make sense to stay with a Microsoft related cloud. I know for some companies that aren't allowed to use Google, it's mainly a big thing in, I think, Switzerland. A lot of the companies there, they want to stay away from Google. So they're either unlike their own custom or still on prem or also on Azure. And that could also be...
 reason. Does GCP have its own version of SageMaker? Its own non-cube flow-based ML ops offering, possibly including pipeline orchestration? Yeah, so since I think a couple of months now, Google Cloud choose to have the what they call like AI platform, that's where all your AI related stuff lived in. So your Jupyter notebooks, your job logs, your model endpoints, all that kind of stuff. And...
 They recently changed that to something they call Vertix AI. And with that, they give some more possibility. Whereas AI platform before you had like a pipeline option, but that pipeline option was mainly consisting out of a cube flow kind of pipeline. It was based on Kubernetes, basically. What you had to do is spin up a Kubernetes cluster, deploy a cube flow on that, and then you could build your models on that, for example, with TFX. And then when you...
 would go to the pipeline section in AI platform, it would actually open up a the UI, the interface of of Qflow. Now with vertex AI, they have a little bit of a different approach. You still can run Qflow on that, but you can also orchestrate your pipelines. It's still from within Qflow, but all the Kubernetes stuff is handled for you. So you don't have to spin up like a Kubernetes cluster or those kind of things. So it makes it a little bit more easier than the old.
 way is more managed. Yeah. Is it a rebranding? Are they migrating their original offerings to this new vertex AI solution? Is it a rebranding or is it an alternative solution? I think it's a little bit of both. So some of the stuff from AI platform right now, they're rebranding it and including it only within the new vertex AI section and under the vertex AI brand. But for example, the option for pipelines is still available in both AI platform as well.
 as the vertex platform and also interfaces are different. If you go to the AI platform, pipeline interface, you get basically the cubeflow interface. However, when you go to the interface of pipelines in vertex AI, you really get a dedicated Google Cloud pipeline interface that's really developed by Google and not necessarily by cubeflow. So potentially, vertex AI is the SageMaker equivalent on GCP, the managed machine learning offering. Yeah. The net even was developed by Google for Google.
 Obviously, it's going to have first class support on GCP. Cubeflow is developed by Google as well. Yeah, it's developed by the TensorFlow team. Okay. Or not necessarily the TensorFlow team, but it's built around TensorFlow and TensorFlow pipelines to facilitate those. TensorFlow pipelines. Okay. Yeah, what is TensorFlow pipelines? What is TFX? TensorFlow is extended. Yeah, TensorFlow is extended. Yeah, TensorFlow is standard, basically, started within Google and that runs origin.
 run on on Cupelo and I think Cupelo is still the only option through which you can deploy TensorFlow Extended Pipelines. And that was a kind of a way for Google to basically make pipelines for all their big services. So all the models that are behind Google email or Google Calendar or Google Maps, like all these big services in which they have a lot of machine learning models running, they all needed a continuous double.
 of course, because these are huge services and also it needed to be scalable because you can imagine those services are used by millions of people. So that's how they developed or as an internal service, they built what is now known as cube flow and TensorFlow itself for a long time was just model building. So like deep learning models, machine learning models, probabilistic models, but with the extended package, it reaches further than just the model part. It basically...
 takes up the entire part before building and training a model. So like cleaning up your data, transforming your data, checking for anomalies, creating a schema from your data, but even reading your data from, for example, BigQuery, or a cloud bucket in Google Cloud. And also the part after the machine learning model, so the evaluation of the model, checking if the retrained version of a model is as performing as well as the previous version.
 or a Java you can imagine that if you do continuous training for a model using those pipelines, that you can end up in a situation where you retrain your model, but the performance of it is not necessarily better than the previous version. So also that is something you have to take into account. And finally, of course, pushing your model to either an endpoint or a location in Google Cloud Storage or creating a BigQuery model out of it. All those steps, they basically build modules with...
 within the tens of flow packets, you're around that to facilitate up steps. And all those steps together create a, what they call a TFX pipeline, which is then deployed through cube flow. What is a, there sounds like there's a lot of overlap between TFX and cube flow. What are the differences between the two? So basically tens of flow extended is just the logic port. So how are you building up your model? How are you generating your schema? How are you transforming your model? How are you evaluating your model?
 And for that, they all made like these components, which they call like components. So you have like the trainer component, you have a evaluated component, a statistics generated component on all these components in a form like a pipeline. And they are deployed on Kubernetes, for example, through Qflow. So basically the back end of that pipeline is Qflow and that executes the tens flow of the code IC. And so if you're using
 cube flow, especially on GCP, you're also going to be using TensorFlow Extended very likely. If you are working with a TensorFlow model, definitely, but you're not limited to, you could as well use PyTorch, you could use PsychitLone or any of the other major machine learning frameworks that are out there. But originally, it is developed around TensorFlow Extended because it was like an internal service at Google. Is there any amount?
 of TensorFlow extended you could use if you're developing your model in PyTorch, because it sounds like there's a lot of nice utilities in there. Yeah, so then you get into the compatibility issues between PyTorch and TensorFlow. Like theoretically, you could definitely use those, but the way PyTorch is built up and TensorFlow is built up, theoretically they're the same, but technically they're sometimes quite different. I'm not sure if you ever tried, but try using both of PyTorch.
 and TensorFlow in your same machine learning code. It's got to be a lot of debugging, and in a lot of cases, you're gonna end up using either one or the other. Yeah, I actually, the podcast is kind of toy project is this journal app. And I do have to use both those frameworks in certain cases, just based on certain packages that I'm making use of. And if I'm using both of them in the same, I actually kick off background jobs, so they're running on two separate Python memory space as man, they just, they'd do it out in a Python.
 that's gonna be a battle with each other. Yeah, definitely. And also just like the types of how the outputs and inputs are defined in both packages are very similar, but it's sometimes like those little details that ruins the compatibility. I know there are like quite some cases where people do manage to combine the two, but it's a painstakingly process. So let's look at where we're at. We have, if you wanna get your machine learning model hosted, you have Azure GCP and AWS and a whole bunch of others.
 but those are the main three. Each of those three allow you to do it yourself using open source tooling. That would be Kubernetes and Cubeflow. And they also allow you to do it through their offerings, which makes it a little bit easier, but you're locked into their offering. So those are called managed solutions. AWS is SageMaker, GCPs is Vertex AI. And it sounds like there's a whole bunch of other ITS
 source offerings as well. We've got Airflow, MLflow, and what else are you familiar with? And to those that you can speak to, how do they compare to each other? So what I'm familiar with is pretty limited to Airflow and Qflow and Google Cloud pipelines. I know a little bit about also Azure pipelines, not like practically, but more theoretically. What I like about the...
 cube flow, sometimes like when I have these discussions with colleagues about this and they work for example in Azure pipelines, they have a lot of these small issues like authentication issues, compatibility issues, information being shared between different components, that's all handled by the combination of TFX and and cube flow. So that's really why I prefer a cube flow, but that's also I might be quite biased because I use a lot of TensorFlow in what I do.
 solutions I built. And I just find the combination between TFX and Qflow to be very good. In the beginning, it's quite a painful process. It's like a very steep learning curve. You can get very frustrated during the process of trying to understand how they work together. And especially in the beginning, because I remember when I started working with the most recent version, it was 0.25 and we're now at version 1.4. And just time frame of six months.
 The frameworks have improved so much that a lot of those like little issues have all been taken away And you just see that there's a lot of dedicated time and effort spend on improving these services and from what I've seen And for example, Azure is that sometimes it can take quite a while before some read basic issues or from which you would expect that That they would be solved much quicker than what is actually happening and regarding like airflow from what I've read and what I've seen so far and what is
 what is airflow real quick? Yeah, so airflow is basically a orchestrator that works with like DAX and then I have to wait, search for the official journalist directed a cyclical graphs. Yeah, okay, so in a pipeline of data, each step can connect to each other step. Yeah, basically that. And what the thing with airflow is is that it's not resource heavy. Like when you deploy something on Kubernetes, the pro...
 assessing power in the resources that you can use on Kubernetes are quite big. Like it's scalable. It can take quite some heavy loads. However, the thing is with Airflow is that you basically for all the computational heavy stuff, you have to submit, for example, a job to AI platform. So when you want to train your model on CUPflow on Kubernetes, you can choose to submit a job to AI platform, but you can as well just train it on the back end of CUPflow, which is Kubernetes. You can just train it off.
 Kubernetes. So with airflow, you don't have that option because it's basically just an orchestrator that executes the different steps in your pipeline. But when you need something that's more resource heavy, you always have to have something like an AI platform to spin up a machine that can then take the heavy workloads, do the computations and the model training and then sense back information. I see, okay, I think I understand. So, cube flow is a pipelining and model or
 in Kubernetes with machine learning in mind. Yeah. And air flow is just a general purpose, just a stepper. I, yeah. And it just so happens, you can put machine learning into that pipeline, sure. But you're just going to be using some machine learning toolkit or cloud offering like a GCP's AI platform. Yeah. You will never, never be able to unbess. You have a very light model, a very little data. But when we're talking about heavy models, machine learning, deep learning models, for example, they're taking.
 in millions of rows of data, you always have to have something else and just airflow, because otherwise it's not going to cut it. Do you use airflow? Does it have a place? If you're already using Kubernetes and cube flow, is there any reason you would ever use airflow for anything? So what we do at Dept is we do a lot of data engineering tasks on airflow. So triggering the export of data from one location to another, the creation of tables, all that stuff that happens within a number...
 environment, but still needs to be scheduled. We do that mainly through Airflow, and for that it works very well, but just not for machine learning. Okay, so if I hope I'm right in this, in the AWS land of managed step pipeline orchestrations, there's a thing called step functions, AWS step functions, and you might have AWS landows. So you might have a series of just raw functions that feed into each other in a specific order and then branch out and then come back together. Yeah, so general
 It sounds like maybe air flow is for that purpose. Yeah, I think you could say so. And then how about MLflow? Do you have any familiarization with that? I have heard about MLflow, but I've never really worked with their services. You get it to see if there's anything that really stands out. Maybe I'll see if I can get somebody here in the future to discuss that. Because I do see MLflow coming up quite often. I have not really seen anyone within our team at least that uses MLflow. OK, let's.
 good to know, but maybe a rocket, not that I've seen yet. So the main contenders that I have seen really in my world is you and me. You love as a satemaker. It's funny thing. Actually, I'm going to work with satemaker for just eat, which is corrupt up in US, right? Yeah. And I like delivery. Oh, cool. You know what I love? So for the listeners, a little bit of a tangent. So dirt comes from depth, depth agency is the master organization. I actually working for a company.
 somebody called Rocket, which is a subsidiary that was recently acquired. There's a lot of smaller companies within Dept Agency. And so it's nice that Rocket's main skill set is AWS in cloud offerings. And then Dept main skill set is Azure and GCP. So we're that perfect. We're the perfect match. Yeah, together we basically cover the biggest share of cloud platforms, I guess. You have anything else to say about Kubernetes, CubeFlow, TensorFlow Extended, or any of the other offerings.
 Yeah, I think in general we talked about the benefits of Qflow compared to all the other options that are out there. The most important thing is not really what you're going to use eventually. Just discover your options and see what is out there and then what fits your purpose because in the end all these solutions and options, they all serve certain purposes better than other. I can imagine that if you are completely set up in AWS, it wouldn't really make sense to go with something else.
 and SageMaker, even though Cubflow delivers these very cool solutions. And of course, you can still deploy Cubflow in AWS, of course. I think the main lesson is to just move towards a more like pipeline or to start thinking in machine learning pipelines rather than just standalone models and then which platform serves the best. That's really up to your situation. Personally, I've always found Cubflow and TFX who work very well together and has served.
 so far most of the needs of our clients, but I guess there are situations in which Qflow is also not gonna cut it for you. Yeah, I like the idea of starting to think towards orchestration instead of just, like you said, instead of just local host model development, even in training, if you're gonna train your own model, by quickly just starting on one of these platforms, your training job, you'll be able to shard it out to multiple nodes. And potentially in the cloud, ideally in the cloud.
 So that you're not confined to your own local host GPUs limitations or if you don't even have a GPU I know a lot of people develop on max and you can't do machine learning very well on max And if that's your limitation Hey, you could just kick off a cube flow pipeline or a SageMaker pipeline and it will distribute your data out if you have a huge Data set it will train it on multiple nodes and then recombine the results Downstream and so your training jobs will go a lot faster It doesn't have to be that expensive because you
 you can use on AWS, you can use what's called spot instances that are super cheap, a little bit less reliable, but super, super cheap. And GCP has its own equivalent, right? What's what are those called? Machine types, you mean like the different deferred show machines that the workers that they spin up. You know, AWS, these spot instances are actually, if there's like excess compute capacity from something like one organization reserve some amount of compute and then they just so happen to not be using it at that time, you can fill in that gap and you're paying substantially.
 less for that compute, but your machine might get taken off line if that organization comes back. You're basically house sitting for them. I'm not sure if Google cloud is something like that because I haven't seen it before. Preemptible instances. And that's the Google cloud equivalent for that. Yeah. Oh, you can save like 90%. So if what I tell a lot of my friends who are doing machine learning, but I have a Mac, I'll say, you know, use AWS kick off a training job on a GP based instance, but use a spot instance.
 you can save 90%. I think that's the far end of the spectrum. So if the instance costs $1 per hour to run like a G4 2x large costs like $1 to $1 40 per hour run 90% savings. It's you know 14 cents per hour. It's a huge difference. But the downside is it can come offline. It's actually somewhat where you bid on it. You say I'm willing to pay $2 if the price hikes and everybody else wants that house sitting job.
 I'm willing to pay over the original cost, but you'll find yourself in that regime quite rarely. And so these spot instances are incredibly useful and valuable and not as unstable as they would seem at a glance. You wouldn't use these for production grade things. You wouldn't use these for long-running instances. Yeah, for the nominal for training jobs. So you would recommend them also to use it already deployed and models that are running for clients, because I can imagine that so how we adapt, build, or stuff is the debris thing.
 dependent on one another. So the output of a model is going to be taken up by a solidar job to basically get the outputs to the location for when for what it then can be used. But with these spot instances, let's call how would that affect, for example, when your worker is taken away because it's utilized by the original organization? It's your job then just waiting until it's finished or is it going to fail or how is that then going to play out? You definitely wouldn't use it in hope post.
 training. This is only for the development phase or the training like let's say your monol drift and you get some alert that needs a retraining job then you use one of these spot instances. It depends on the framework being able to pick up where it left off. If the spot instance goes down and I did look it up so in GCP it's called preemptible instances. If that goes down it depends on TensorFlow being able to take snapshots and then pick up from where the last snapshot left off it would then have to notify it.
 self to try again. Create a new spot instance, pick up where it left off, before saying that the job is done to kick off the next step in the pipeline. Yeah. Okay. This is kind of the idea that you could do this instead of a local host development of your model. Training part, you can kick this off to the cloud using spot instances for cost savings and steep yourself within the orchestration paradigm. That's how it's indeed a very good solution to against like training locally or pay equal price for it.
 for a machine. So yeah, what I always like to do is just train on a small subset of data, just a debug and then when everything is working, just submit the entire entire data set to the model within the pipeline. So far, it's been working pretty good, but I guess this would also be quite good solution to cut down costs. Just for fun, what's your development environment? Are you PC Linux? I'm both. So what I'm at home, I use Windows.
 PC. Yeah, I don't use Linux and then it would also be PC but Mac, but no Linux. I tried it like at university ones, but I don't know. My focus is war towards like model building and theoretical data science. I found all the hassle that goes into setting up Linux in a proper way. I didn't really enjoy that much. Definitely a makers thing. Yeah. I remember growing up that in the early days of Linux, the war between all the distributions and the flavors and everybody.
 trying to find the best. And there was a Boontu on the easy side, like the managed Linux front, and it was Gen 2 on the other side. And those two were the extremes. And I'm like, why would anybody go Gen 2? But there are makers out there. There are people who like to fiddle. Yeah. So I know I bet the old data engineer was like the team lead, Jesper. You could read it. Yeah. He would use Linux at home. I use Linux quite often. I use all three. I use Mac Windows and Linux Windows for gaming. Linux for model development and all that stuff.
 I'm trying to do a lot of my development in the cloud. At first, I would like to ask if you could take us through a project beginning to end journey. The beginning being, I know I want to build, let's say, a image classifier. And the end being, I want that hosted online. How would we decide which cloud provider to use or if we choose open source tooling? And what does the training process look like within the tools that you would end up selecting? What is the pipeline?
 pipeline and then how do we get that thing online and integrated into our website. So from like a project that a client or more like a general personal project. Let's say our listeners, they land a client, client says, look, I know I need a recommender system or a image classifier. Or even as an alternative, the client may also come with their own specific environment. Maybe they're a Microsoft shop or they're already using Google for their analytics. So like various forks in the roads during this process.
 about why our machine learning engineer might choose one solution over another. And then as you go through those steps, you just say, but as we're going along, I'm choosing a cube flow. Does that make sense? Yeah, for sure. Yeah. So I think in the first step, and it's not really even data science related, but the first step would really be to go back to the drawing table and have a look at their data to really see if what they want or what they request is also what they need. What we often see is that.
 clients come with a directed question and already some sort of an idea what they want. However, when we, because what we always do first, we started with the process that's called data discovery. So we see what systems are in place. How are these systems connected? What data do we have? What do we see from the data? So like a small version of an ADA. Can we even support that solution or questions that they have with the current data that is in place? So that would
 really be step, I would even call it step zero. Go back to the drawing table, see if the question that the clients has, that the client has, is really what they also need. Because for a very, very nice example is to stick to like, recommend a systems. Oh, we had a client that came to us with the question like, yeah, we want to have a recommend a system on the website. And when we started looking into their data and investigating customer behavior and through all the systems they already had in place, we actually discovered that they needed a better.
 search function. So they didn't need a recommender on the website. They needed a way for people who visited the website to find products more easily because what we already could see from like the first discovery phase is that it was very hard for people to search for products because often the products wouldn't really show up when they search for it. So that really would be step zero. Just elastic searcher. You implemented some. So the decline went with a tool which I forgot the name.
 from. I think it's something called like Seeker, but I'm not sure. So yeah, that's that's now where we're at, where we're at. But okay, let's say the client comes with the question and that is a valid question and it's a valid problem that we can solve in the way that they see it. After we have done the data discovery, we see that the right systems are in place or some systems still need to be implemented for everything to work well. We always first look with okay, is there already a platform that they are working on?
 A lot of the clients that come to us are bigger clients that already have a cloud platform either AWS, Google Cloud or Azure. But in case they don't have such a thing, we look at what their services are that they already use and which cloud platform might be integrated with those other services most easily. So in case of like what I said in the beginning, if it's a company that uses a lot of Google products, then a logical recommendation would be
 storage using Google Cloud. But on the other side, if they already use a lot of Microsoft for later products, Azure would be a recommendation. So that would then be step one, setting up their cloud environment and then step two would be for the data engineers to make sure that all the data is in place and everything is available. But also this depends of course on the use case because there's a little bit of a separation in approach because in some of the cases
 is what we start doing is first see, okay. The client wants to see if the output of a certain and analysis or model is even gonna help them improving their decision making. And in some cases, they really want to have a continuous deployment and use of output from the model. So in the first case, we say, okay, we first start with developing a model validating the output and then merging that to the cloud platform and deploying it on there. But it more happens that the client's like, okay, I want this.
 eventually to live within my cloud platform. And then we start developing this to get it with the data engineers, we first make sure that all the data is in place that all the data is clean because that's one of the most important things in this like entire process. You have to have clean data. Otherwise, you're not really going to get anywhere with the outcome of your model. Are we cleaning data yet in your journey? Or are we just looking at the data to see if it's clean? So in this case, cleaning, I think there are multiple steps in cleaning.
 is already like a pre-selection. So if there's any faulty data that we already know when integrating or getting all the data together, making sure that that data is excluded from a data set that's finally going to be used by the data scientists to build a solution on is, in my opinion, quite the right thing to do because when you already know that it's faulty data, then there is not really any reason to include it unless there's like the very specific use case, of course. So there is already like a pre-select.
 in that process. But then once all the data is in place, and this is also, of course, a iterative process that goes back and forth between the third step, which is the data scientist, exploring the data and doing some EDA to see, okay, what do I already see as in statistics and visualizations from what I can see in data. And he goes back to the data engineer, because maybe for some of the data, a lot of failures are missing or don't make sense or improved.
 not in the right format. So like step two and three are kind of like an iterative process. So when step two and three are finished, that's when we start developing the model. So transforming your data into for the right input. What we often like to do is we take multiple model approaches. So we have like a baseline model. So let's say in cyclistification problem, we take like a random forest tree, like a random forest classification model, which is very easy to implement.
 but already can give you the first results. Then from that, we see, okay, where is it performing well? Where is it not performing well? Maybe we need to change some stuff from the data. So we go back to step three and two again. But then also from there, we start to maybe develop any more advanced model. If, for example, that base model is not what you want to have it. So then when we get more to the stage of developing that advanced model, then we also get into the stage of making sure that the model can be...
 deployed. And if we want to build a pipeline around it, make sure that all the components that are going to be in place around the model itself are being built up and indirect well with the part on the data engineering site. So it's kind of like a parallel process. We both developed a custom model or the advanced model. And at the same time around it, we build the pipe. And then when we have evaluated the custom model and it's performing well an N N N.
 the output makes sense. We make sure that the entire pipeline is deployed so that then the data engineer can also use the outputs or the model trained by the pipeline to predict on new data which then can be used for any activation purposes that were determined in like the discovery phase. So that's kind of like the process that we walk through with a lot of our clients but definitely not all of the clients. And then how do you decide it?
 that pipeline phase between, let's say, if you're on GCP vertex AI versus cube flow. Yeah. So currently, that depends a little bit on the framework that you're using because currently when you use TensorFlow extended, even like TensorFlow itself, they say that it's for now better to still use the the cube flow, like the Kubernetes cluster implementation. So the AI platform implementation, because the vertex AI implementation is quite new still and my lot be a still.
 stable SDAI platform implementation. So for now, we're whenever we deploy such a pipeline is still an AI platform, but eventually it will depend on if there are any client specific requirements that might not be met by either AI platform or vertex AI or AI platform pipeline is totally disappear and get merged within vertex AI of course. Oh man, so many solutions. I guess you know it in web development.
 There's a million one frameworks as well. There's express, fastify, fast API, and all those things. But it's part for the course. Sometimes it makes selecting tool sets to become an expert in, stick with it. It's a little bit difficult. But this sounds incredibly powerful and popular too, cube flow. It's one that definitely keeps coming up. It boils down to me. So I do a lot of stage makeer work. Oftentimes I reconsider. But I just start learning cube.
 flow and there are a lot of issues that I face. But yeah, I think that's also kind of the nature of depth. We work with a lot of different clients. So we don't really have the luxury. Let's call it that we can stick with one platform. You have to know them all. Yeah. And one of the fastest ways to connect with other platforms with limited knowledge or knowing all the dedicated pipeline orchestration for each different platforms is using.
 Qflow because they can connect to all those different platforms. And of course, in some cases, it might be better to use the dedicated solution of a platform. But I think especially when you are in the business of depth where you have a lot of different clients, a lot of different platforms to work with, finding a solution that expands across those platforms and it's not dependent on just one platform that really helps us to deliver value to all our clients. And it makes us also much more flexible.
 the solutions that we can build and the clients for which we can solve their problems. That is a really powerful deciding point right there. We're an agency, we have lots of clients. If you're in the same boat, it probably pays to go Kubernetes cube flow because it's a generalized solution. Yeah, I really think so. And of course, you can also take the root of having different people with different expertise, of course. But from what I've seen and what I've found it, if there's just a couple of people that do one thing, you will never get as good
 is when everyone works on the same thing and can basically improve themselves, but also the team. You make progress much faster than when there is just one person focusing on one solution and another person focusing on another solution. Then the knowledge sharing is also way less than I think with Qflow. Someone who works for a client that has AWS can use Qflow, but another colleague who works with a client on Google Cloud can also use Qflow and still in some way they can share their knowledge with one another. We...
 It's got to be harder between someone who's dedicated to SageMaker and someone who's dedicated to Azure Vibelines for example. Definitely. This is super valuable. I really appreciate this. Is there anything else you want to say about cubeflow, TensorFlow Extended, or anything else? No, I think what we just talked about sums it up pretty nice. Cubeflow is a flexible solution that serves most of the platforms and not just the orchestration platforms, but also the...
 machine learning frameworks, but also in the end, it comes down to what works best for you. But I think for us as an agency, the biggest point is that we have a lot of different clients. Qflow is really something that I think any agency that works with machine learning engineering and machine learning pipelines should consider using. I like to end these interview episodes per the way ship it handles theirs with a pick. I call it. I said, Ellie, you seem like a really interesting guy.
 going to be an audio format but it seems like a really fun, relaxed character. So I want to get to know you a little bit. Tell me some of your interests or things you like to do off the clock. Oh, that's, I developed quite a lot of new hobbies during COVID and the lockdown. So first of all, I'm a big keyboard nerd. Okay. You build keyboards. Yeah, I built custom keyboards. So I got into that hobby actually during COVID and it's like a rabbit hole. Once you get in, you don't get on. So I think that's one of my most recent private hobbies, as you could say.
 I'm very interested in playing table tennis, which I also recently picked up against it like a sport So I guess that's basically my private interests and I recently finished my master's degrees and data science But always have been working on the side as a data scientist at marketing companies before depth at another company And since like two years now adept so yeah besides it just being my work It's also my private interest so during my time off from work I also just really like to read
 research papers, see what are new developments, what is out there, what are the latest trends on machine learning and its applications. So yeah, I think that both sums up my private and my work interests. On the topic of a master's degree, this is something that a lot of my listeners are very interested in. And since it's so fresh to you, maybe you might have some insight here. They want to know what amount of added value does a master's add over bachelor's to finding a job in data science. That's actually a good question. I see this often popping up on red.
 it as well. A lot of people are asking this question. And I think it really depends on your situation. So take for example, someone who comes from computer science or from a statistic background or from a mathematical backgrounds. If you want to get into the data science field and you have a bachelor in one of those three subjects like computer science, mathematics and statistics, personally, I don't think a master degree is in data.
 science specifically is then going to serve much valued for you. Because in data science, the most important part is understanding the mathematics behind it. And the technical part, I think, is easier in some cases is easier to learn than the mathematical and statistical part. So I would say if you come from a background where you have had a lot of mathematics statistics, and I think with computer science, you have that same thing. You basically have a combination between mathematics and...
 and technical subjects. I think in those cases, a degree in a mass degree in data science is not gonna really add much value, or at least it's not gonna add more value than when you start exploring data science yourself. And what that's also what I see around me, a lot of people in the data science field are not necessarily people who did a mass degree, but they had some technical or mathematical background, and they were interested in data science, and they just read papers and did courses.
 on coding themselves. I think at that point with such a background, a massive degree is not going to add more value than self-study. However, me, like my bachelor's was more like business and economics focused. Of course, I already had an interest in data and I tried to always combine what I learned in my bachelor's with the data field, but it didn't really go further than economical mathematics and statistics and business.
 mathematics as statistics. And for me, it really helps to basically crank up my technical knowledge as well as my more specific data science, mathematical related knowledge. So I think if you come from a background that's like business related, I think it really helps to have a master degree in data science if that's where you want to go to. And then ultimately you end up with this perfect combination between understanding the business side and the data side. One of those three you said computer science.
 Let's say computer science, statistics, or math. And then let's say the third option is machine learning or data science as an actual master's degree. Which of those three would you pick if your intent is data science or machine learning? It sounds like obviously C is the right answer, but it doesn't seem like that's necessarily so. If you had to pick a master's for data science machine learning, would you go computer science, math, stats, or data science, ML? Personally, I think...
 I would choose for stats, stats or mathematics. I think so. I think this is what the conclusion is coming to, even though it's almost like machine learning and data science, you don't know what might come next as the equivalent of that field in the future. But there's old faithful all the time, which are stats. That's true. And I think in the end, you understand the solutions that you build the best and then how it works. If you know the stats on a mathematical...
 behind it. And what I feel with these sometimes these from what I've seen and of course this depends for university, but what I've seen from like these data science and a bachelor's that might sound like they're very related to the subject, but sometimes I think they stay too broad. So they don't really touch upon like the deeper, the deeper mathematics as statistics behind it. And of course, in the end, it also comes down what subjects you choose like the selectives that you have during your bachelor's in your master.
 which can also really change with what kind of knowledge you finished as degrees. But I think statistics would definitely be the best backbone to have when going into the data science field. I agree. I agree. And I've seen a lot of agreement on that front as well online. And then yeah, in universities, one thing I recommended a long time ago, and I don't really stand by it anymore just because I haven't kept up with it. So I don't kind of a blank slate on this it was ONS CS George George
 Tech online master's degree was really inexpensive, like $5,000 or $8,000 or something for a master's degree. Accelerated program, it was like one to two years master's all online. And so it was a really attractive option as opposed to going physically back to university. I guess it's COVID now so everybody's online anyway. Do you find that there's, I mean, those are not going to get you much in the way of credibility in the job market. And then there's like traditional university for your master's degree.
 to be something of a middle ground these days with degrees offered online, maybe by well-known universities, but seem to be, I don't know if they're like an accelerated program or... Yeah, so I've seen them coming by like advertisements for those kind of degrees, a lot, especially on platforms like Reddit. And from as far as I know, like you have to be very careful with with a curriculum, because a lot of those online degrees, they seem very attractive because of their resume prices.
 However, what you really have to look out for is that it's not going to be. So what I've read from it is that a lot of time people took those courses, they had barely any interaction with professors or other students. And personally, I think besides the courses itself, what I found very valuable or during my master degree, even though 50% of my master's degree was during the lockdown. So I also couldn't go to university physically. And that's also why I realized that even more is the interaction between.
 between you and another student is very, as much as important as the courses itself, because everyone there has a different background and you can really learn from one another. I've had a lot of these group projects and meetings with other students during which I learned so much new stuff that I didn't even get into courses itself just because they were, for example, from a computer science background or from a mathematical background. I think and that's a huge component that you miss.
 when you take these online degrees. On the other hand, I have seen some very good ones that offer courses from very credible professors. I know, for example, that, and that's even totally free, but I think they also have like a Patreon. MIT is publishing also a lot of these deep learning lectures, and if I compare those to the lectures that I got regarding deep learning in my university, is that the content there is pretty much the same.
 The only thing you're missing out on is like the interaction with maybe other students and a professor and the practical exercises that you get. So what I would advise is to really dig deep into what the curriculum has to offer and who are the people behind it. And also just check sources like Reddit or any forums because there are a lot of people that shared their opinion about these kind of online degrees. But often it's really a hit or miss. So sometimes they're completely nonsense and you're not really learning anything.
 which you wouldn't have learned when you just watched videos on YouTube. And sometimes there are those that are really good and I think that really also depends on the organization that's behind it. I'm glad I asked you those questions. You have quite some insight on the topic. Yeah, because it's a huge discussion that's going on, on Reddit, for example, as well, like a lot of people asking, do I even need a master degree? And I found this online version or this online degree. Is it really necessary or valuable for me to first...
 or my career. I think there's actually a lot of discussion going on about these, and I always try to kind of keep up with those things, because I find it quite interesting to see where those developments are going, especially in the field of data science in education, it's changing very fast. And curriculums also from universities themselves are changing every two years. Yeah, another vote for math. It's like, are you going to learn TensorFlow and PyTorch or deep learning in university? Who knows what comes next?
 Yeah, because I remember like during my master degree, like the focus was a lot of like on machine learning and deep learning, but it's now already kind of shifting to more like reinforcement learning. Because that is kind of the next step. I'm not necessarily the next step, but it's a level up like more advanced and you already see that there's a shift going on to push more also for those kind of subjects. This was a wonderful, wonderful interview. I'm so glad we had it. Really enjoyed it. So thank you.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 22 deep n l p part one. At last we're finally here deep natural language.
 processing has taken us so long I apologize for that. We're gonna discuss recurrent neural networks RNNs, Word2Vec and Word embeddings, and various architecture details like the LSTM long short-term memory and GRU Gated recurrent unit cells. After preparing for this episode I realized indeed this is gonna be a multi-parter. So we'll just see how far we get in this first episode. Deep natural language processing. I've mentioned it before deep learning has revolutionized.
 NLP to an incredible degree. Recurrent neural networks have come in with sword and torch pillaging the entirety of NLP leaving no stone unturned. RNNs are incredibly powerful and versatile in NLP. Why is deep learning so applicable to NLP? There's a couple reasons. The first being the complexity and nuance of language. Remember from the neural network episode that neural networks and deep learning shine.
 particularly in circumstances that are extremely complex. By comparison to circumstances which are very simple, predicting the cost of a house in a housing market is very likely a simple linear model. Linear regression fits just fine. But predicting whether there's a cat dog or tree in a picture is too complex for a linear model to handle. And so image processing is really entirely the domain of deep learning and convolutional neural networks specifically which we'll get to later. What is the basis of...
 complexity in machine learning models. It really boils down to the fact that certain features in your data combine in your model in some way that is non-linear. So a very simple example to use is if you're trying to predict the salary of somebody, you might use their degree, their field of study where they live, race, gender, all these things. Well, field of study and degree are two very important features when we're talking about somebody's salary, but they're not additive, they're multiplicative. They combine together to
 create a new feature, which is field and degree with underscores. They're multiplicative. If you have a bachelor's in computer science, that combo will make you more money than if you have a bachelor's of English. And so we don't consider them separately. It's not bachelor's plus English versus bachelor's plus computer science. Now, if you knew this ahead of time as we do with field of study and degree, you could manually combine those features together and still use your linear model like linear regression. This is called feature engineering. Can we edit this data at daily basis?
 combining your features and inputting them into your model. For very simple situations like this, yes, that works fine, but for very complex situations, you need the machine to learn how features combine and that's what a neural network does. That's explicitly what a neural network is best at doing is combining features together. So in this particular case, you could have just thrown the data into a neural network and it would have learned in its hidden layers to combine field of study and degree together multiplicatively. That's called feature learning.
 If there are multiple layers of feature combinations, that's when you actually have to go with deep learning. In the case of image processing, there are indeed multiple layers of feature combinations. The first layer of feature combinations is combining pixels together to form lines. So the input layers, all your pixels, those are all your features. The first hidden layers job is to combine black pixels together or dark pixels together to create lines or borders or edges. And then the second.
 second hidden layer combines those lines together to form objects like eyes, ears, mouth, and nose. And then the final layer, the output layer of this neural network, this multi-layer perceptron, is going to be a binary classification as to whether this is a face or not. You need that second hidden layer because you need to combine things hierarchically. It's not sufficient to combine just the pixels into lines in order to make your prediction. You have to make one more combination at a new layer.
 and language is just this way. Language is so nuanced and complex. First off, things do indeed combine. They have to be combined. When you say this movie was good, versus this movie was not good, not and good combined multiplicatively, such that it is now a different feature than not and good considered separately. So sequences of words should be combined in such and such a way. On top of that, you might have a second hidden layer, which could combine constructs of words.
 as they relate to each other. You might even think of this as syntax tree parsing. Now, making a recurrent neural network with two hidden layers in the architecture that doesn't give you syntax tree parsing per se. We don't know what it gives you. Neural networks are black boxes. What's going on inside of those hidden layers is impossible to interpret. All we can intuit is generally that things are being combined in one layer, and then those combinations are being combined in...
 in another layer, and however many layers deep you want to go, the depth of your hierarchy is sort of the depth of the complexity of the situation at hand, and that seems intuitively to be like syntax tree parsing if we're talking about language. But it may not necessarily be that way under the hood in the black box of the neural network. But that is just to say that language is complex, language is nuanced, and words combine in such a way that would be impossible to feature engineer. So you want to feature learn them by way of deep learning.
 learning, and hierarchically combine syntactically so that depth in deep learning is significant and helpful. So that's language complexity. The other big benefit of deep learning in the space of NLP is by way of something called end-to-end models. We saw this in the last episode on machine translation. In shallow traditional machine translation, we would have multiple models all stacked together all feeding into each other. We would have syntax tree parsing. We would have a-
 encoding and decoding by way of Bayesian statistics. We had alignment models and we had language models and these all just played in together. They were written separately maintained by different maybe teams or individuals on a project or an company. And importantly, training one of these models does not feed into the training of other models. You have to train your models independently. An end-to-end model like a recurrent neural network. The RNN is all you need in...
 machine translation. The RNN learns everything you need and therefore you only need to train your one RNN. So an end to end model is a model that is written, maintained and trained all in one model. So deep learning and neural networks bring a lot to bear in natural language processing. Now the first step to understanding RNNs is making this distinction between sequence models, like hidden mark off models from the prior episodes.
 and non-sequence models. Things where you just snap your finger and out comes an output, like a support vector machine or a logistic regression classifier. You input something and out comes your classification or your regression. So there's sequence models and there's non-sequence models. And we had that in our prior shallow NLP episodes. We had hidden mark-off models, mark-off chains and joint probability language models being sequence models. And we had that in our prior shallow NLP episodes. And we had that in our prior shallow NLP episodes.
 maximum entropy, support vector machines, and logistic regression being non-sequence models. We're gonna carry this information over into the world of deep learning, and we're gonna just have two models, a regular neural network, just a neural network, and a recurrent neural network. So we're gonna use a regular neural network for everything that is not sequence-based. You just snap your fingers, the neural network happens, and outcomes an output. And we're gonna use a recurrent neural network for everything that is sequence-based. And we're gonna use a recurrent neural network.
 step, step, step, step, step. Word, word, word, word, word in a sentence, or even sentence, sentence, sentence, sentence, in a paragraph. If our task is a sequence of time steps, we're going to use a recurrent neural network. And by the way, sequence models, as we've been discussing, another word for this is called time series forecasting, time series models, because they're time steps. When you're talking about language, word, word, word, word, word, it doesn't seem like time steps. It seems like word steps. But they really are time steps. And sequence model.
 or time series modeling, time series forecasting is useful in natural language processing. It's useful in weather prediction models and it's useful in stock market forecasting. Anything that has a sequence of time steps. So recurrent neural networks for sequence based NLP tasks and regular neural networks, which are also going to call deep neural networks or artificial neural networks or multi-layer perceptrons. Okay, so you're going to see these words all the time referring to a regular vanilla
 neural network that's deep neural network or DNN artificial neural network or ANN multi-layer perceptron or MLP and specifically by comparison to recurrent neural networks we're going to be calling them in this episode feed forward neural networks feed forward meaning you input your input it feeds forward through the neurons through the hidden layers and out through the output and the way we understand that by comparison
 and to recurrent neural networks is how RNNs work. So this is how RNNs work. In RNNs, we take in our input a word. We're gonna be taking in one input step at a time. Word, word, word, words. So we're on word one. We take in word one as our input. It enters the hidden layer, something happens in that black box in that neural network hidden layer, and out comes an output. Okay, let's say we're translating from English to Spanish. So I say, hello, how are you?
 doing and it wants to translate to Ola komuostas. So step one is entering hello and outcomes Ola. Now step two we move to the second English word how hello how how goes into the neural network it goes into the same neural network okay there are not multiple neural networks for every time step there's only one neural network that will be reused for every single
 time step. This time through the Neural Network, it takes in as input the second word how and the output of the prior pass, OLA. So it takes in two inputs, the second word and the output of the prior pass, the first output. In other words, the hidden layer loops back on itself. There's a circle arrow coming out of the hidden layer and back into...
 the hidden layer and the second output, the second time step is Como. Very interesting. So we have one neural network, a regular old neural network it seems like. But with one tiny little twist and that is that an additional input that comes into the hidden layer in addition to the regular input is the output of the prior pass. It's a neural network with a circle loop looping the hidden layer back onto itself and
 Since neurons are functions, they can be sigmoid functions or tanH functions or rectified linear units, etc. These are actually computer functions. These are mathematical functions that get executed on the computer. Since it's calling itself, what do we call that in computer science? That's recursion. It's the function calling itself and itself and itself and itself until we get to the very end of a sentence. And then we backtrack our way up the stack and get our result. It's recursion and that's why we call it a rec
 current neural network. And that is by contrast to a feed forward neural network, feed forward being the opposite of recurrent. Now, we get to our third step. Hello, how are you? We're at hello, how and hour at R, A R E. We feed R into the hidden layer. Our input layer is R. It is fed into our hidden layer and incomes from the last step. The tallied out.
 put thus far. Okay, so what I said last time that we're taking the output of the prior step in as an input is a bit of an over simplification. What we're really doing actually is a language model task. Remember language models do this kind of probability thing sequenced from left to right. You're like multiplying joint probabilities or running through a mark off chain or something like that. What we're kind of doing is tallying our way through the output sequence and that's all being
 that into this pass as an input. And what's it gonna do? It's going to look at R, okay? And if we were not using the prior inputs, feeding them back into the hidden layer, it would just output SON, the direct translation of R from English to Spanish. But that's not what we want. We want the language to be complex and smart. So it carries with it sort of this meaning vector that it has been building up as it went along through the sentence.
 and it sees Ola-Komo and it knows that combined with R, what we have thus far, this tally vector that we've built up of our translated sentence thus far, combined with R in English, should not give us sohn. What we should maybe do, probabilistically, as learned in the hidden layer of this neural network, is wait. Wait because what comes next is what's important. We're going to...
 skip this word. So we move on to the next word, you, we pass that in as input to the hidden layer. Remember, this is all the same neural network reused over and over at each step. In comes this vector, this tally that has been being passed recursively into this hidden layer, incomes that tally vector, combine it with you and outcomes estas. Because the words we've translated thus far plus the current word we're looking at will give us that word. And so we learn the
 complexities and nuances of things like modifier words and sarcasm and all these things where words Combine in a special way that brings new meaning to the output Information is carried through the recurrent neural network through each time step by one Outputting a word and two looping that back into the hidden layer for the next time step the way this recurrent neural network works makes it a Language
 language model from a prior episode. So in our NN is a language model. So we have an input sequence of words. It eats these words one word at a time and spits out some stuff one word at a time, including if necessary skipping a word because that's going to combine with the next word in some special way. As we saw with RU becomes Estas. Now what does this sound like? This sounds a lot like a hidden Markov model from a prior episode. Indeed conception.
 I think they're very similar. I think of this formulation of a recurrent neural network as like the deep learning equivalent of a hidden Markov model. Now of course, a recurrent neural network because it is a neural network with a hidden layer of neurons or hidden layers will be substantially more complex and nuanced and powerful than a hidden Markov model. It will learn more intricacies than a hidden Markov model could learn.
 Now here's what's really cool about a recurrent neural network. This architecture that I just described to you can replace every single task we described in the prior episodes. We input a sequence of word, word, word, word, word, word. And for every word, we can output a part of speech tag, verb, noun, pronoun, adverb, right? Part of speech tagging. That was one task where we could have used hidden mark-off models or we could have used maximum entropy models and a presentation processed from okaleva. Ciner fourteen.
 support vector machines. Well, we can use a recurrent neural network. For one, the RNN will learn attributes of the word that make it some part of speech, intricately in the depths of its black box within its hidden layers. But two, it will also be considering sort of its location semantically in the sentence thus far by way of that loop structure. It sees where it's at in the sentence as has been tallied from...
 left to right, bringing us to this point. And it considers that as an input, that is a feature, a factor helping determine what part of speech this word is. So it's a more complex, nuanced, intricate version of part of speech tagging. One model, the recurrent neural network to be used instead of any number of models we might have used in prior episodes, such as a hidden mark off model, maximum entropy, support vector machine, et cetera. Part of speech tagging, named entity
 We input our sequence of words and outcomes any number of named entities. Okay, so many outputs will be skipped and certain outputs will come directly with its named entity. We say Steve Jobs invented Apple. It's acting a little bit like a hidden Markov model. We're going Steve Jobs. We're collecting those two together, outputting as the second output person, Steve Jobs. Okay, we go to the third input invented and it determines that that word by way of the word itself.
 in combination with where we are in the sentence thus far by way of that loopy structure is not a named entity. And so we skip it. The fourth input apple combined with the hidden states outputs organization apple. So we can use a recurrent neural network to spit out named entities as well as parts of speech. In this way, we're using an RNN like a hidden mark off model where we're eating the words and pooping out little things that we want along the way. Eating the inputs one word at a time and pooping outputs any number of words that
 time. We can also use RNNs for sentiment analysis, classification, machine translation, and every other task that we've already talked about. Now, in order to use RNNs for those more complex tasks, we're going to need to reimagine the RNN a little bit differently. I'm going to present a version of a recurrent neural network called a sequence to sequence model, or alternatively, an encoder decoder model. And in
 In this take of an RNN, instead of eating each word and pooping out an output for every time step, what we're going to do is input all of the input first. From left to right, gather our tally recursively through the hidden layer, and then we will stop. We've read our entire sentence from left to right. Hello, how are you, period? Then we stop. We didn't output anything yet. We're building sort of a vector tally. A me-
 Meaningful vector representation up and to this point. It's like our RNN is listening to a speak. You're saying, hello, blah, blah, blah, blah, blah, blah, and it's like, mm-hmm, it's nodding its head, uh-huh, okay. Uh-huh, uh-huh, go on, uh-huh. And at the very end, when you stop talking, it thinks a little bit, and it kind of reformulates everything you said in its head. It builds up this meaning representation of everything you said, and now it can respond. So that first step was called encoding.
 you encoded the sentence. And now the RNN will decode what it thinks to be a proper response. It listens to you, talked. Uh-huh, uh-huh, uh-huh, okay, okay. And then when you stop, it's like, you know, okay, blah, blah, blah, blah, blah, what were you were standing back there with? So it decodes a response based on the encoding it has from what you said. So it's a two step RNN, encoder, decoder. Another word for this is sequence to sequence. You gave it a sequence and it-
 outputs a sequence sequence and code or decoder and we will use this sequence to sequence model for more complex RNN and LP tasks. So in the case of sentiment analysis or classification, what the RNN will do is read all of the words left to right first. Uh-huh, uh-huh, okay, okay, okay. And once you're done, it's going to be like, you sounded mad, right? That's sentiment analysis. I'll give you a predicted sentiment once it has heard the entire sentence or a predicted classification.
 a class. We're talking about sports or technology or news stuff like that. Classification. So you encode your sequence and then your RNN can output another sequence. Now you see in this case that sequence is one word. It's a one item sequence. But you can imagine like in programming you can have a single element array, right? Open bracket and then one item and then close bracket. That's basically what we're doing here. It's creating an array, a sequence. It just so happens to be a one.
 an item sequence. And then finally, in the case of machine translation, I posed the problem as being a vanilla RNN translating the sentence word for word as we go along. That's not how we do machine translation. Neural machine translation using RNN architectures are actually sequence to sequence models. And that makes more sense anyway. It's a little bit more difficult to try to translate something as you're going along from left to right.
 real time, then it is to listen to what the person said first, think about it, and then translate it to Spanish. So the more powerful neural machine translation models use this sequence to sequence encoder decoder architecture. Now, so let me give you an analogy I use for understanding sequence to sequence RNNs. In the case of single item sequence to sequence RNNs, like we saw with classification and sentiment analysis, you don't need an analogy.
 embedding the sentence as we go from left to right and the result is a single vector which is basically your class. There's not much to that. But if we are encoding our source sentence in English, okay, and we want to translate it to Spanish, and in the encode process, we created a vector which is sort of this running tally of the sentence, this sort of meaning vector summing up the entire sentence. How could we possibly go from that?
 to a reconstructed sentence in Spanish. I like to think of it like this. If you've ever seen the movie, Boondock Saints, there's this big fight sequence. There's these two Irish guys and their American friend and they're in their apartment and I don't remember exactly what happens. Some big thug comes up and he chains one of the brothers to a toilet in the bathroom and he brings the other brother down to the alley with a gun against his head and the first brother on the second floor up in the apartment chain to the toilet breaks the toilet out of the ground and throws.
 out the window and it lands on the thugs head and the first brother jumps out of the window and lands on some other thug and there's also such a shooting and things are knocking over and people are getting shot, there's blood being splattered on walls and everybody runs away. So there we had a sequence of actions. Think of that like our words. Word, word, word, word, hello, how are you? This is our sequence of actions. Thug comes in, chains brother one to toilet, takes brother two downstairs, points gun against head, brother one throws toilet out of window lands on thugs step, step, step, step, step, step,
 And by the end of this whole scene, everybody runs away, flees the scene. There is sort of an aftermath left behind. There's toilet shards and bullet holes in walls, blood smatters on the floor, maybe a gun left behind and a shoe over here. So there's a whole crime scene left behind. That is what we have encoded from our sentence. A sequence of steps has occurred and this sort of crime scene has been...
 being built up. It's like the shadow of what actually happened. And by the time this whole throwdown is done, a crime scene is left behind. That is our encoded sentence. Now, in the movie, this crime investigator, gum shoe guy, shows up at the scene and, you know, he observes the whole scene all at once. He's looking at the floor and he sees a shoe and a gun and some blood smatters. He looks over to the right and he sees bullet holes in the wall and he looks.
 up at the apartment and sees a shattered window. And so he's able to reconstruct the meaning of the sentence based on what was left behind, the encoding. So right now he is decoding the encoding. And in the movie, and I don't remember exactly how this unfolds, but you can imagine he walks over to some broken pair of handcuffs on the ground and he kneels down and picks it up and turns it over in his hand. And based on his understanding of the whole crime scene,
 Combined with this step one, he has made his first word in the encoding process. Ola. Step one was thug chained brother one to the toilet. He looked at the whole crime scene, seized some toilet shards, picks up the handcuffs, and constructs step one. So you can see he doesn't have access to the actual events that unfolded. He only has access to the crime scene. So that's what an encoder decoder RNN does. It builds up a crime scene.
 encoding process and then from there the decoder picks up and says I got it from here. I'm going to translate the entire English sentence. Hello, how are you into an entire Spanish sentence, Olo, Comuistas? Excellent. So now you have a basic understanding of RNNs. They are loopy neural networks. Now how do we get these words into our RNN? Remember that machine learning doesn't work with text. It works with numbers. Machine learning is math. It is linear.
 algebra, statistics, and calculus. So we need to turn our words into numbers or vectors. How did we do this in the prior episodes when representing documents in a database? We represented those documents as a bag of words. So each document is a row and it has 170,000 columns. That's the number of English words in the entire English dictionary. And there is a one in the location for the word if that word is present in this dot.
 document. So a document is just a vector mostly zeros and one if that word is present. We call this a sparse vector because there is a sparsity of ones and a majority of zeros. Now the equivalent representation for words, not documents, but words, individual words themselves, is that the word would be a vector and it would have a one in the column of that word itself. Okay, so it would be all zeros.
 169,999 zeros. All zeros except for one, which is in the location of that words column. This is a sparse vector as well because it is mostly zeros and very few ones, but specifically only one one. And so we call this a one-hot vector. So a sparse vector is mostly zeros, some ones. A one-hot vector is a sparse vector that is only one one. A one-hot.
 vector. So if we had the sentence, hello, how are you? Unless pretend that those are the only words in our entire dictionary, there would be four columns, hello, how are you? And if we're looking at the word hello, then the first column would be one and the next three columns would be zero in our word, hello. That's how we would represent our word as a one hot vector. Now, this representation of a word does not carry in it any significance, any meaning, any semantic importance. It's almost...
 just an arbitrary representation of a word. Like you might as well just have a UUID, some serial number for every word. The way that we represent it as a one-hot vector gives it no significance. That's by contrast to bags of words, which have in them a little bit of significance. When we represent a document as a TF IDF, bag of words, we can perform a search query by finding the documents which have the smallest.
 cosine similarity to our query, okay? So there's obviously some sort of semantic meaning in the document and our search query, if we can connect them by way of a cosine similarity. But there's nothing like that that we could do with the way we're thinking about words here. They're just random. So this actually will not help us in our RNN. An analogy here is in our crime scene investigation. If we represented events as one hot vector.
 then the gum shoe shows up at the scene of the crime and on the floor there's a one and he kneels down and picks up the number one and he looks over to the right and on the dumpster is a 24 F and he goes over and he picks that up and turns it over in his hand and he looks up at the window and hovering in the broken window is a 69 C and he's looking at all these objects like what the hell they don't mean anything to him he needs objects which carry meaning he needs a gun and a bloody shoe and broken glass
 He needs objects in the crime scene that mean something semantically if he has any hope of reconstructing the crime scene in his mind So what we need to do is represent words in a dictionary such that they carry Semantic meaning within their representation. We call this word and beddings We embed a word if we can put it in vector space in a significant way So we're gonna talk about the word to veck model
 which is a neural network for creating word embeddings. It's the main model that we use for doing such a task in NLP. But let's build this up a little bit first. A word embedding is the word represented in vector space. So imagine all your words as stars in a galaxy. Now with our prior representation of words as one hot vectors, those would just be randomly placed. But if the words were embedded, in order that they carried semantic meaning, b-
 based on their location and space. Then what you would have is that all nearby words to one word would basically be synonyms. So if you found the good quote unquote dot in vector space, the word good, then all of the very close by dots, Euclidean distance. Remember Euclidean distance is physical distance. All of the Euclidean close dots to the word good would be things like excellent and best and
 perfect and wonderful. And then you might imagine words that are antinims, such as bad and horrible and worst and terrible, would be very far away from this cloud of dots. So they would be Euclidean far. That's what a word embedding is. Placing the word in vector space that carries significance, semantic significance. Now what we're going to arrive at by using the word to veck model to achieve this goal is something very special.
 Not only will you find that synonyms are physically close to each other, Euclidean close, but you will find that certain projections carry significance. So the arrow, the vector pointing from good to bad in vector space, would be the same kind of arrow pointing from best to worst, and wonderful to horrible, and so on. And the classic example they use for this system is that if you input King plus Queen.
 Minus man, it will give you woman. You will have performed vector math using linear algebra. You'll get back an arrow pointing to a dot, and that dot is an embedding representing the word woman. So you can actually do word math using word embeddings. And so remember now, Euclidean similarity is similarity of dots to each other physically. So synonyms will have high Euclidean similarity and cosine similarity is angle.
 similarity. So the difference between good and bad, you would use that same sort of cosine metric on the other words in order to perform word math. And this whole concept is really cool. If you look this up online, you will actually see a 2D or 3D representation of words, all these dots around each other and their synonyms of each other. And you can do projections to get the capitals of various states or countries by way of analogy using cosine metrics from the capitals of other states and countries and all this.
 Now, an embedded word has, let's say, 512 dimensions, number of columns. So visualizing these words in space is impossible because that's 512D. So we can project that down to 2D or 3D by way of something called T-Sni, T-Hyphen, S-N-E, which stands for T-distributed stochastic neighbor embedding. It is essentially the same thing as...
 principle component analysis, it's boiling our large dimensional vectors down to small dimensional vectors. So basically think of T-SNE and PCA as essentially the same thing. But usually you're going to be using T-SNE for visualization purposes. So these word vectors have some number of columns. That's the embedding dimension. Some number of columns, usually let's say 512. Basically the number of dimensions is going to be the...
 amount of generalization you want to boil this down to, right? Smaller dimensional word vectors, smaller dimensional embeddings means more general, but less accurate word representations and higher dimensional word embeddings means less general, but more accurate, that is overfitted word embeddings. Okay. So our goal is to put words as dots in space with significance. We call this embedding.
 a word and the type of machine learning model we will use to perform that task is called a vector space model, VSM, a model that puts something in vector space. And the way we're going to do this with words is by learning their context. That's how we're going to learn the semantic embedding of a word is its context. So certain words show up in certain contexts.
 over and over and over. So when I say the fizzy soda drink or the fizzy pop drink or I like to drink soda and I like to drink pop, Coca-Cola is a good soda, Coca-Cola is a good pop. You have learned by context, by the context of the sentence that that word is present, that those two words are very similar to each other. They may be complete synonyms or they may just be very similar words to each other. So that's how we will learn.
 to place words and vector spaces by learning which words share the same context. And where we'll get these contexts is from whatever corpora we have available. Really, you can just download all of Wikipedia and just go through every word, word by word by word by word by word, by word, look at its context, look at the next words context, look at the next words context, and learn what context every word is in. So we're learning word embeddings based on their con-
 There's two different ways we can do this. One approach is called predictive methods, and that's what we're going to use with Word2Vec, and what we're going to do there is we're going to try to predict the context, given the word, or predicts the word given the context. And another one is called count-based methods, and these don't do any sort of machine learning. These are just math. They don't predict anything. What you do is you lay all the words out in columns and all the words out in rows. So you have the whole English dictionary, 170,000 bytes.
 170,000, okay? And you count all the words, co-occurrences with each other and put that in those two words cross-sell. And then you do some linear algebra to pull out some co-occurrence stuff. And then you have your embedded context matrix. We're not going to use that approach. I will talk a little bit about that. But first let's talk about the predictive methods. Namely, in our case, neural probabilistic language models. And like I said, the goal is to predict the context of the word.
 or to predict the word given the context. So we're trying to learn word embeddings based on their context, and we're going to use a model called word to veck. Word to veck is a neural network, which will learn this embedding matrix based on word contexts. So what it's going to do is make a context prediction using a neural network feed-forward pass and learn from its mistakes using something called the noise contrast of s.
 loss function. Remember every machine learning model has a loss function and then use gradient descent by way of back propagation back through the network to fix its error. So word to veck is a neural network that is learning the word embeddings for every word in the English dictionary by predicting its context and optimizing its parameters if it made an error. Now there's two types of context prediction methods. You can either take the word you're looking at
 in a chunk of words, the cat sat on the mat. You can take the word you're looking at and try to predict the surrounding words. So we're looking at cat in the sentence, the cat sat on the mat. In this case, what's called the skip gram model, we're trying to predict the surrounding words, blank, blank, blank, blank, cat, blank. We're trying to predict all those words. The opposite of that is trying to predict the word given the context. So if we are at the word cat in our window, we will look...
 get the blank sat on the mat and try to predict that that word is going to be cat. This is called the continuous bag of words approach or C-BOW. You can use either of these approaches when you're trying to predict your context. Either skip gram or C-BOW. Again, that is context from word versus word from context. You don't have to get hung up in the details. It's pros and cons versus data set size. Most people use skip gram.
 As far as you're concerned, skipgram is the only thing that you care about. So our word to veck model is a feed forward neural network that is trying to predict context words surrounding the current word we were looking at. That is the skipgram approach. We're going to go through Wikipedia, one word at a time, we look at one word, and we try to predict what's around it. We obviously made some error, and so we use this noise-contrastive estimation loss function.
 in order to back propagate our error through the neural network, fix our parameters, move on. Word to Vec neural network to try to predict the skip gram context around the new word we're looking at. We make some error, back propagate that loss through our neural network, adjust our parameters next word. We do this over and over and over and over and over until we have built up a table of word embeddings. What we have in the end is called an embedding matrix.
 Every word is a row. Every column is some nebulous embedding dimension. It doesn't really matter. What matters is in the end, all of your words are stars in a galaxy whose position in space holds significance. Okay, so that's the word to VEC model. Now, like I said, that approach is called a predictive vector space model, meaning we're trying to predict the context situation. That by contrast to a...
 based vector space model or distributional semantics model. And the way this works is we lay out all the words in the English dictionary, rose by columns, and we count in every cell the number of times any two words row by column, co-occur with each other in a context window. Then we have a matrix of co-occurrence, a co-occurrence matrix, and we will use a bunch of linear algebra on that matrix.
 things like principal component analysis or latent semantic analysis or singular value decomposition. You'll see these used in machine learning a lot. PCA, LSA, SVD. In order to pull out the embeddings directly from the math, from that co-occurrence matrix. And we call this model the glove model, G-L-O-V with a capital V, E, global vector.
 representation of words. And this has various pros and cons versus word to veck. Usually pros and cons with all these situations boil down to amount of training data you have, amount of RAM, speed, and other things. You may see glove used or discussed in the wild, but as far as you're concerned, when it comes to the task of embedding words in vector space, the only thing you care about is word to veck.
 which is a neural network using the skip-gram approach. Oh, that was a lot of information just to embed our words. That was a feed-forward neural network. One of those snap your fingers and outcomes a thing. We can use a feed-forward network for other word-related tasks, like part of speech tagging and named entity recognition, but you will find I think that recurrent neural networks are preferred for those tasks because whenever we're looking at-
 a word we're trying to determine the part of speech we're named entity. We have the prior context of the sentence thus far which actually helps determine the part of speech tag or named entity. And so really the main place you see the standard feed forward neural network in NLP is in word to VEC is in coming up with our embedding matrix of words. Everything else tends to be RNN. The mighty
 neural network. So let's step all the way back now. We have gone through all of Wikipedia and placed all of our words in vector space. These are now embedded words rather than one hot words. So now they carry semantic meaning. Now we can use those to pipe into our RNN encoder so that our decoder has something to work with. So the scene of the crime where the gum shoe is standing a giant bright flash of light happens.
 covers his eyes and staggers backwards. And when the light wears off, he looks, and sure enough there's a gun on the floor, and a bloody shoe, and some bullet holes in the wall, and some broken toilet shards on the ground. The inputs carried meaning into the encoder RNN. And so what is handed off to the decoder step also carries meaning. RNN's N word to veck. Now that's the end of this episode, but that's not the end of RNN's. We're going to talk about.
 the traditional neurons that are used in RNNs, namely rectified linear units, Ray-Luz, as well as what's now state of the art cells, not neurons, in an RNN called LSTM cells or long short-term memory cells. By contrast to something called GRUs, that's gated recurrent units, and the problem that these things solve, which is the vanishing or exploding gradient problem.
 when you back propagate your training error in recurrent neural networks. So we'll talk in the next episode about a little bit more of the nitty-gritty architecture, technicalities of RNNs as used in NLP. But this episode gave you a very basic lay of the land and the fact that RNNs basically replace everything in NLP to bring state of the art deep learning to the space. Now for learning...
 Deep learning NLP. First, I'm going to recommend a handful of articles for you to read. A very popular one which is called unreasonable effectiveness of RNNs. It's a very easy read, very visual, and it's more conceptual, Allah this episode. And then another couple articles which are a little bit more technical. From there, your next step is just to learn deep learning. Deep NLP and RNNs are a section of deep learning
 So whether it's deep learning book.org or fast.ai, the traditional deep learning resources that I've been recommending, RNNs will be inside of those main resources as one or two or three chapters. So just continue along your basic deep learning learning curriculum in order to learn the details for deep NLP. And finally, there's a great Stanford iTunes U course, CS224N, which
 Which is Deep NLP by Christopher Manning. Chris Manning, if you'll recall, is the co-author of the main shallow learning NLP book that I've recommended in prior episodes, as well as the YouTube series on shallow learning him and Dan Jarawski put that YouTube playlist together. This is basically part two of that YouTube playlist. This is the deep learning equivalent of that YouTube series. And it is brand new. I think it's 2007.
 even if not it's 2016, it actually replaces a prior Stanford deep NLP course called CS224D, which is taught by Richard Soucher. So if you've seen that one floating around CS224D, Stanford deep NLP, and you've been curious, skip that one, and do this one instead because it is a merged and updated version of that class with Chris Manning's material. CS224N
 And in the past, I have often recommended converting video series to audio and listening to them while you exercise. This series is highly, highly visual. I tried doing it audio and I got lost in the dust. So I highly recommend you watch this one, put it on your iPad and prop it up on the treadmill while you run and watch the videos. Very good course, highly recommend it. And as per my last episode, this podcast series will be kept alive by Donate.
 So if you have any amount you can donate, whether it's $1 or $5, go to the website ocdevelop.com forward slash podcasts, forward slash machine learning, and click on that Patreon link. If you can't donate to the show, do me a huge favor and rate this podcast on iTunes, which will help this podcast succeed. That's a wrap for this episode. See you next time in DeepNLP Part 2.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 23 deep n l p part 2. In this episode I will finally wrap up on n m.
 This is the second episode of DeepNLP, RNNs, and we're going to cover a few fine points about RNNs. We'll talk about bi-directional RNNs, the vanishing and exploding gradient problem of backpropagation and its solution through LSTMs or GRUs. But let's start with a review of RNNs, because I think that last episode went a little bit fast, and so I want to make sure that you understand RNNs completely before we move on.
 In deep learning, we use neural networks. And there are various flavors of neural networks for different tasks. The vanilla neural network, also called a multilayer perceptron or a feed-forward network, or usually just a neural network, is used for sort of general tasks, general classification or regression problems. So if you wanted to use a neural network for predicting the cost of a house in some market, you'd probably use a vanilla feed-forward neural network, or classifying-
 something is this that or the other thing you'd use a neural network. Then there's a convolutional neural network, a CNN or a convent, which are traditionally used for image-related tasks. Categorizing images, for example, as catdog or tree. And then we have sequence-based tasks, time steps, steps of data over time. That might be things like weather prediction, trying to figure out what the weather is going to be tomorrow based on what the weather has been in the last 365.
 days, or stock market prediction, or as we've been talking about natural language processing, where a sentence is a sequence of words through time. The premier time series deep learning algorithm is the recurrent neural network. And so we use RNNs for deep NLP. Now real quick, I do want to make a mention of reinforcement learning, which is also something you might use for sequence based machine learning problems.
 The coursement learning recall is the third category of machine learning models. We have supervised learning, which is the majority of things that we've talked about throughout this series. You train the model to do something or recognize something. You train the model to pick up some pattern. You give it training data which comes with labels and it learns the patterns to generate labels on its own. Unsupervised learning is learning patterns in the data without a label. So for example, clustering data points in...
 space as simply things are near each other based on their features. That would be unsupervised learning. And then finally, we have reinforcement learning, which is sort of the entry point into AI proper reinforcement learning is the task of learning what actions to take over time in order to maximize something, some reward function, we call it. So as you can see, reinforcement learning is also applied to time series task.
 We have recurrent neural networks for deep learning oriented time series machine learning and we also potentially have reinforcement learning and you can use deep learning in reinforcement learning a thing called deep Q networks So before I go on I want to discuss the differences between these two camps R&Ns or deep Q networks supervised learning or reinforcement learning which do you pick when the thing that you would use reinforcement learning
 for is when you need to learn what actions to take over time in order to maximize a goal. So if we think about natural language processing, there doesn't seem to be some sort of analogy for that in this case. Looking at sequences of words, how might you try to let the algorithm train itself maybe what words to generate next or what actions to take that generates a word in order to maximize some point value based goal that doesn't seem to be anything that...
 make sense in this case. Whereas with supervised learning, you're training the RNN explicitly on sequences of words. So that if you train it on hello, how are blank, it knows to generate you as a next word in the sequence. It knows that because you've fed it, gobs of text data from various corpora like Wikipedia or online conversations, and it has seen the word you come after the sequence, hello, how are over and over and over and over.
 labeled data. Therefore, it's supervised and it fits very well in NLP. Reinforcement learning would be a good fit for video game AI bots, for example. They're trying to kill the good guy. That's their goal. And they have some sort of measure of how well they're doing, which is going to be maybe your hit points bar. And they have some actions that they could take at any point in the game thus far. Now, it's a sequence of steps, a sequence of states, namely. Up until this point, a whole bunch of stuff has happened. Step, step.
 step, step, step. The good guy has done various things and various other characters in the game have done various things. So what is the next best action to take? Given your current circumstances, it is the actions themselves that the reinforcement learning algorithm needs to learn how to perform for the maximization of killing the good guy. So those are two extremes, NLP and video game AI that make RNNs versus deep-queue networks for time series TAF.
 kind of an obvious split in the middle. But let's talk about something that I think is maybe a little bit less obvious. Stock market predictions. So this I think is a great case where that line is a little bit fuzzy between whether you'd use supervised learning and an RNN or reinforcement learning and a deep Q network. If we were to use an RNN, what we would do is sort of try to learn the shape of a stock graph. And let's say that we're trying to build a trading bot. We want to learn.
 whether to buy or sell stocks in order to maximize our profits over time. Okay, you have a graph that goes up and down and up and down, it wiggles up and down. At the peak of a hill, before you start going back down, you'll want to sell stocks, generally, because they're going to lose value. So you want to cash out, you want to maximize on what you have right now, and don't lose the value. At the bottom of the next trough, as the graph begins to creep upward, you want to buy stocks. Now they're...
 cheap and our prediction is going to be that it's going to go up and up so we're going to gain in value of our holdings through return on investment. So we might use an RNN to build a model of a stock graph. Specifically what it would be doing is regression, predicting the next steps value on a graph, the numerical value on a graph. So that would be a regression based RNN that we would train on years and years of historical.
 stock data. And having played with this stuff myself, you can actually get some pretty decent representations, some pretty accurate models. But then from there, it's up to you to decide what to do. You're going to have to build into your code, a trading strategy. You're going to have to code in the system that when it looks like we're going downwards based on a major down tick in the next regression inference in the RNN, then you might sell your stocks.
 And if it looks like the next predicted value is very high and we are at a current low then you might buy stocks. So you're building in the action and you're not building it into the machine learning model. You're writing this in Python code, calling some API for example. The model is simply spitting out a value, a regression. Another thing you could do with an RNN is classification. You could actually tag dots on the graph, points where you would classify this point as cel.
 buy or hold. So you'd have to have some previous know how about day trading and stock graphs and maybe build in a system where you can click on a graph and mark a point as a point where you would sell by or hold. Those are going to be your three classes. And now the machine learning algorithm, the recurrent neural network, will learn to classify points on a graph as one of those three classifications by seller hold. So in the prior case of a regression based RNN, our notes.
 Neuron, the output layer, would be a linear unit, like linear regression. In the case of classification, our last neuron would be a softmax unit, which is a multi-class logistic regression unit. So you'd use logistic regression for binary classification. And softmax is multi-class logistic regression. So that's how we might use an RNN to work with stock market time series data in order to predict whether we should buy or...
 whether we should sell or just the numerical next step. And then our code behind the scenes would actually act on that information in order to make the decision to buy or sell. In reinforcement learning on the other hand, the model will actually learn on its own whether to buy or sell. You give it the goal of maximizing profits. So all you do is you give it a goal. You say make a lot of money and don't lose a lot of money. So the end goal is a high dollar value. You give it some...
 actions it can take, those are Bicell and hold, and then you just let it loose. What the algorithm learns specifically is what actions to take when. You don't teach it the trading strategy that you buy when you're low and you sell when you're high, or anything along those lines using supervised learning methods. No, you let it loose with a goal and it learns on its own whether to buy seller hold given where it is on a graph. That's the difference between.
 supervised time series stuff and reinforcement learning. One learns numbers or classes, and the other learns actions. And indeed, for NLP, supervised time series models is the ticket, not reinforcement learning, at least not that I know of. And specifically, RNNs are the deep learning model for time series. So neural networks for general stuff, CNNs for image stuff, RNNs for sequence.
 And let's go over what an RNN does real quick. We have two types of RNNs. We have the vanilla RNN, which takes in an input and spits out an output for every time step. Now the trick of this, what differentiates it from a regular neural network, five neural networks left to right, is that the hidden layer loops back on itself. It is recursive. It's a loopy neural network. So let's say we have five.
 time steps. Time step one, we feed it in input and it outputs an output. Time step two, we feed it in input and it takes in additionally the output of the last time step and outputs an output. Time step three, we feed in an input, it takes in the running tally of the past time steps thus far and outputs and output. So we go left to right conceptually, but that's not actually what happens, what actually...
 happens is it's one neural network that loops back on itself. The hidden layer loops its output back into its input. That's a regular old RNN. Things you'd use this for, for example, are part of speech tagging and named entity recognition. Part of speech tagging you'd input a word and outcomes a tag, like verb, adverb, noun, adjective, et cetera. Input a word and outcomes a tag. Input a word and outcomes a tag. Now importantly, you could use a regular...
 neural network for this. In comes a word and out comes a tag, but we do want to carry through the system the context of the sentence thus far. We want to sort of build a meaning of the sentence we've been reading from left to right and input that in with the word that we're looking at right now because the context will affect the tag we're applying to this word. So that's the magic of an RNN is it takes in the prior steps to aid in the classification of the current.
 step and outputs a part of speech tag. So that is a one to one mapping. Every word gets a part of speech tag input output input output. You may have a many to few mapping like named entity recognition. The sentence Steve Jobs invented Apple would be named entity person named entity person nothing named entity organization. So how do we account for that nothing that blank space in a traditional RNN? Well, you just out.
 output, some blank symbol, some like the letter O is common or a zero or a null or something like that. So you didn't put Steve in outcomes name entity person, you didn't put jobs and it would combine that with the last output and output name entity person. You'd input invented, it would combine that with the fact that there were two person name entities prior to this and it'll output blank. These are sort of simplistic RNNs. I call them vanilla RNN.
 One twist to this architecture that I didn't discuss in the last episode is that we have been reading the sentences from left to right to inform the decision at any time step. That's well and good, but sometimes stuff that comes after is also informative. We can accommodate for that by way of a structure called a bi-directional RNN. What it will do is it will read the input sequence from...
 left to right and that will come in as an input, aiding in the decision being made at that step along with the actual input at that step. Additionally, a third input will be coming in from the right. So the sentence will also be read right to left. So the sentence is being read left to right and right to left, like arrows coming from their left and the right and they meeting in the middle at the current time step and an arrow coming from the body.
 that's our actual input, the word we're looking at at the current time step. Those three inputs all combine together to come up with an output. So an example where this might be useful, let's say I'm saying the phrase, George is my friend, or so I thought. Well the thing that came after friend, the sequence of words that came after friend, completely negated everything I'd said thus far. So reading from left to right is well and good up into the point of friend, we're building sort of a me.
 vector, but it turns out what comes after completely reverses the meaning of the sentence thus far. And I wouldn't have known that unless I'd seen the part after where I'm at. So that's what a bi-directional RNN does, is it considers what came before and what comes after. So a regular RNN goes left to right in time and common use cases for that would be part of speech tagging and named entity recognition, a bi-directional RNN.
 N takes input also from the right going left. So wherever time step you're at, it'll meet you in the middle from the right and from the left in order to help with your output. And you might also use a bi-directional R&N for part of speech tagging or named entity recognition. It may increase the sophistication of the model. You would try a vanilla R&N and then you would also try a bi-directional R&N and you would look at your evaluation metrics and see which one performs better. And if the bi-direction-
 R&N performs a lot better with not a whole lot of extra compute time than you would use that instead. You can't always use a bidirectional R&N. It's not objectively a better solution. For example, sometimes you don't have the future data. Like if you're trying to predict stock market values, you exist in the present, you have all of the data of the past, and you have nothing of the future. So you can't use a bidirectional R&N. Similarly with weather pattern predictions, you have all of the...
 weather for prior days, but you don't have the future, so you don't use a bidirectional RNN. So there were two flavors of RNNs right there. One is vanilla, and it sort of maps inputs to outputs directly, and then another is bidirectional. And it does the same thing. It maps inputs to outputs directly, however, it also considers future time step information. These two, by comparison to the third type of RNN, which is called a sequence to sequence model or an encoder decoder.
 model. And what this does is you read your sequence of steps, you do all of your processing with your RNN, but you don't output anything. You don't output anything for each time step. You just listen, just listen and listen and listen and listen. And when you're done listening, when the sequence of steps is done, when you've heard the entire sentence uttered, then you stop and you think, and then you respond. You have to hear everything first. So the listening phase is called encoding.
 your encoding your sentence and the outputs at each step are completely ignored. They're useless as as outputs in and of themselves. But remember, an RNN also feeds the output at every time step into the next step as an input. So it has this running tally. It's building from left to right. This is your context vector or your meaning vector. This is your encoding. And that's what's important. So you're throwing away all the outputs at every time step in the encoding process. So you're going to have a different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different, different,
 until the very end and you are handed an encoding package, a little vector ball that is handed to you, put into your hand and you look at it, you turn it over in your hands, and then from this encoding, you will reconstruct a decoding. You will come up with a response if you're a chatbot, so you will answer the question or you will respond to what the person said if you're a chatbot. If you're a translation system, then you will reconstruct a translation.
 of the sentence that was uttered in the language that you're translating to. So you heard the sentence in English and coded. Now it's an encoding in vector space. It's a dot somewhere in space, a star in a galaxy. And you can take that and it has meaning to it. It has actual semantic meaning packaged within it. You can reconstruct the meaning in Spanish word for word. That's why it's called a sequence to sequence model. You encode your sequence.
 and then you decode that into a new sequence, an encoder decoder, a sequence to sequence. So this is for any task where you need to hear everything first. You can't just translate as you go along. And the way I like to think of this is with a standard RNN, you're writing in pen, so you can't go back and edit prior outputs if new information gave new meaning, if something kind of messed you up along the way and you feel like you want to go back and make...
 some changes, well, you can't do that with a vanilla RNN. So an encoder decoder is useful for situations where you need to hear everything first before you want to start writing because you're writing with pen. Machine translation is a good example because there's so much that's packed into a meaning vector into your encoding that if you were translating word for word as you went along, you might lose some subtleties. And my analogy that I use for this is, let's say you're a crime scene investigator, a gum shoe, and a...
 This sequence of steps happened in the unfolding of a crime that was the encoding process. A guy got shot and a guy lost his shoe and somebody got punched and everybody ran away. Sequence of steps unfolded and a crime scene investigator arrives at the scene of the crime which is your encoding. The scene of the crime is your encoding. You don't have access to the sequence of steps. You don't actually have access to the time-based crime. You only have access to the aftermath. What's left behind? That's the encoding. The, the again, she-
 It was smart enough to know how to reconstruct the crime from scratch looking just at the crime scene. He looks at the whole crime scene and scratches his chin and walks over and picks up a gun. So he has reconstructed step one of the decoding sequence, which is a man had a gun and shot it. And then he walks over and picks up a shoe and reconstructs step two of the decoding sequence and also somebody lost his shoe and so on. Now the sequence that you generate...
 in the decoding process doesn't have to be multiple items. It could be a one step sequence. So we might use this for sentiment analysis or classification, for example. You could use an encoder decoder, RNN model for either of those classification or sentiment analysis where you would want to hear everything that said first before jumping to conclusions. And once you've heard everything, once you've gotten the full encoding, now you can decode whether this...
 person is mad, sad, happy, nervous, scared, et cetera. That sentiment analysis. But it's only a one item sequence. So imagine an array brackets with one item in it, and that item is sad. So it's a classification example. Again, this is a perfect use case for those three scenarios that I laid out, machine translation, sentiment analysis, and classification. This would be a porfit for something like stock market prediction or weather prediction. You don't want to.
 to hear everything first, you want to sort of be collecting data and making real time estimates. So there you have three types of RNNs, a regular old RNN, a bidirectional RNN, and an encoder decoder slash sequence to sequence model. And remember, the last piece of the prior episode dealt with turning words into numbers because words are text. And any machine learning model requires numbers.
 or vectors of numbers to work with, to do its math, because machine learning's math is just linear algebra, and statistics and calculus. Can't work with text to do that. So you use a model called WordToVec, which is kind of a regular old neural network, which will convert your words into vectors. By relocating words in vector space close to their counterparts contextually. So words that show up commonly in the same types of contexts are...
 located physically near each other. And so that's all that the word to veck model does is it transforms your words into numbers so we can pipe them into the RNN. Okay, so here we are, that's where we left off at. I apologize that that was all just a bunch of review, but I kind of want to solidify it because I went really fast in the last episode and it's some important stuff to know. We're gonna talk about a problem of an RNN. It turns out, I have been fooling you a little bit into thinking that RNNs are used.
 so wildly as is, they're not. I think RNNs are very rarely used in their current form. Instead of an RNN in its current form, what's commonly used is what's called an LSTM or GRU style RNN. And in order to build that up, let's talk about the problem with an RNN. The problem is in training the model. Remember from prior episodes, every machine learning model has an error function. Different error function.
 for different models. In fact, a model might have a different error function given the task. So, an RNN may have a different error function given the different application you're applying it to, whether it's named entity recognition or machine translation, etc. So, we're not going to get into the error functions. But what an error function does is tell you how bad your model is doing. And then, you will use training to fix that error one step at a time, train, train, train, train, and slowly over time that error function.
 starts to reduce and reduce and reduce. So you're optimizing your error function. You're optimizing your error, reducing your error, and that improves your model's accuracy. So some error function measures how well you're doing, and then you use gradient descent, gradient descent, which tells your model which direction to move its parameters. Because every model has inside of it, these parameters. Usually they're called theta parameters. Sometimes they're called weights or w. They're numbers, they're called...
 that are multiplied by the input row. These theta parameters are moved in space over to the left or down a notch. Through gradient descent, which is just calculus. So it's calculus telling your model which direction and how far to move its theta parameters in order to increase its accuracy, namely to reduce its error. By way of that error function, gradient descent. Now, many models use gradient descent. Linear regression, logistic regression, support vector.
 and yes, neural networks. When a neural network uses gradient descent, it's called something different. It's called back propagation. Back propagation is applying gradient descent to all the neurons. And then passing that error signal down through the ranks from right to left, from the output, which got some error signals using the error functions. That's a guys on way off. Remember our analogy for a neural network being a company org chart. It starts with its employees. That's basically.
 the input layer moves on to supervisors, that's the hidden layer, and then moves on from there to the boss, the headhunch of the company, and that's the output layer. So we took in some input, got passed through the ranks, the feed forward part of a neural network, lands in front of the boss, and the boss man looks at the results as a piece of paper in his left hand, and he's comparing it to a piece of paper in his right hand, and he's shifting his cigar from left to right in his mouth, looking at what was predicted in his left hand, what the actual value is in his right hand.
 and he shouts back through the ranks guys, we're messed up by 10. And all the supervisors get to scrambling, fixing various values in their books, and they're crossing out some numbers, and doing some math, and at the same time, they turn around and shout back to their employees. Guys, Boss Man says we're off by 10, and they keep correcting some numbers in their books and the employees start correcting some numbers in their books. So it's gradient descent past hierarchically back through the ranks of the Neural Network. That's back propagation. Now a recurrent Neural Network.
 work, remember is sequence based, the hidden layer loops back on itself. For every time step in our sequence, whether it's a sentence of words or time steps in a stock market, we're making a prediction and then we're feeding the prior prediction back into the next time step where we take in an input and the prior output. So a recurrent neural network is one neural network, but it loops back on itself. How would back propagation work in this case? What we would do is...
 gradient descent at all the neurons, send that air signal back through the neural network. It's just a regular old neural network. Remember, so it works the same. But that is really only for the last step of the recurrent neural network sequence. That would be back propagating only the last step. So what do we do? We back propagate again. We just feed the air signal on through again. So the forward pass of a recurrent neural network doing its prediction. And then the back...
 backward pass of a recurrent neural network is back propagating the error multiple times once for every time step. It's called back propagation through time. So you go forward to make your prediction and you back propagate your error through the neural network once through time for every time step. Now what would that look like? We would have an org chart, the boss and his supervisors and their employees and the boss would look at what's on paper and look what's on file and yell back at the supervisors.
 Error as he's ripping up the paper and the supervise all here that and so they all start changing some numbers in their books and they're yelling back at the employees Error error and now it's the next pass back of the RNN the boss gets the second to last prediction He made compares it to what's on file error error the supervisors all hear that and they're still kind of writing in their Books and they look up at him they kind of got a grimace and they're starting to sweat and they start changing some numbers in the Books really fast and they yell back at the employees error error and the employees are like pulling their hair out and they start writing And they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're starting to get a little bit more clear and they're
 And the boss gets the third to last prediction he made in the Recurrent Neural Network, and he yells back, Error, Error! And the supervisors are just pulling their hair out. They're practically in tears. They're trying to modify numbers in the books, but things are coming so fast that they just can't keep up with it. They're just scribbling lines everywhere. And they yell that back to the employees, Error, Error, Error, and if the employees just quit, they say, we're done. You can back propagate your error too many times, too far back. As I said, this is all one company. One...
 organization, one neural network. It just so happens to loop back on itself over and over and over. You can back propagate your error too many times and what happens is either called the exploding gradient where the error signal becomes too loud basically and your theta parameters get moved too far towards infinity. Everything just becomes infinity. That's kind of like how I described it here. Or the opposite can happen and the error signal because it was too low to begin with gets quieter
 quieter and quieter. So the boss man's like, error, error, and the supervisor's like, what? I think he said error, guys. So they start jotting down some numbers in their books and they whisper back to the employees, error, error. And the employees like, what? They're cupping their hands to their ears. Why didn't hear you? The vanishing gradient problem. Where the gradient is so low that when it gets back propagated over and over and over, it goes towards zero or becomes zero. Now what's the problem with this? Why don't we have this with a regular neural network or a convolutional neural network or something like that? Where the problem is that.
 we're back propagating our error over and over and over and over. Now, this isn't usually a problem for very short sequences, but once our sequences become longer and longer, say 40 steps or 50 steps, you know, those would be very long sentences, but in the case of weather prediction or stock market analysis, those would be very tiny sequences indeed, vanishing and exploding gradients would be a very, very big problem for stock market and weather analysis. But it turns out also actually that NLP indeed suffers from
 the vanishing and exploding gradient as well. So you just want to play it safe and solve this problem. What is the solution to this problem? The solution is something called an LSTM or long short-term memory cell. Or alternatively there's a competitor called a GRU cell or gated recurrent unit. They, for the most part, serve the same purpose in very similar ways. LSTMs are much more common in industry. I...
 find from what I understand and LSTM cell is sort of a general case of a GRU cell. So it's a little bit more all encompassing can handle more situations. Don't quote me on that. But in LSTM like I said is used much more than a GRU in industry. So for now, let's just focus on LSTM and you can think about GRU in the future. LSTM or long short-term memory cell, what it does is it replaces the neuron.
 in a recurrent neural networks hidden layer. So in a neural network, a neuron is just some function, some mathematical function, a statistical function. Like I said in the past, I said deep learning is stacked shallow learning. And the example I used is a multi-layer perceptron of the neural neural network may have as its hidden neurons logistic regression. Basically every neuron is just logistic regression. And then the output, the last.
 Neuron in your neural network. If you're trying to do regression, then it's going to be a linear regression Neuron just linear regression and if it's classification of its binary classification It's going to be logistic regression and if it's multi-class classification It's going to be softmax softmax is multi-class logistic regression now it gets a little bit more complex than that a lot of times These neurons aren't necessarily logistic regression or
 what's called a sigmoid unit. The other word for logistic regression as a neuron is a sigmoid unit because the sigmoid function is sort of the crux of logistic regression. So they call these sigmoid units, but they're not often sigmoid units sometimes and probably more often than not in neural networks. We use a different function. One is called a tan H function and it's very similar to a sigmoid function, just a little bit different. And another is called a rectified
 linear unit or Ray-Lew, which is quite different from a sigmoid function. But these are three common neurons that you might see in the wild, sigmoid unit, tan-H unit, and Ray-Lew unit. And they have various pros and cons under different circumstances that make the math work out given the situation. You don't need to know that stuff right now. We'll get into that in a future episode. But what we're going to do in this particular situation is we're going to take out those tan-H units as the-
 they may be in an RNN. We're gonna take out those neurons, pluck them, out of the neural network, and we're going to pop in this LSTM cell. Now, in LSTM cell, it's not a simple neuron as such, it's not a function, it's not some mathematical equation. It is actually like a machine. It's a little complex neuron that has within it multiple neurons. So you pop in the circle into your RNN, you replace your hidden layer neuron.
 neurons with this LSTM circle in your graph and it has on it the label LSTM long short term memory and you zoom in into that circle and inside of it it's going to have multiple little neurons. One might be a matrix multiplication unit, another might be a matrix addition unit, one might be a tan H function, another might be a sigmoid function, et cetera. So it's a machine, it's like a mini neural network and it has a very specific purpose. The purpose is in the name.
 long short-term memory. So before I talk about these units, let's talk conceptually about what it's going to do. Each LSTM in your hidden layer, or however many hidden layers you have, is going to latch onto a specific sub-sequence in a sequence. It's going to sort of hone in on some concept within a sentence and focus on that. So let's say the sentence is, the...
 After work, I'm going to go get my license at the DMV, then I'm going to go grab some drinks with friends, and then I have to work the rest of the night. It was kind of three separate things happening in this sentence. It's sort of a medium-length sentence, so it actually, indeed, could suffer from the vanishing gradient problem. Each LSTM cell in our RNN may sort of learn to latch onto sub-sequences. Like, after work, I'm going to the DMV to pick up my license. I'm going to sort of latch onto that.
 chunk, and it will learn to ignore the rest. Then the second LSTM might latch onto the middle part, then I'm going to grab drinks with friends. So it learns to ignore everything up until this point and everything after that point. So these LSTM cells learn to slice and dice sentences sort of into sub sequences that are more manageable, which also makes for representing sentences hierarchically even more effective as well. You might sort of...
 encode sub chunks of a sentence and then combine those into an overall encoding. Rather than encoding the whole sentence as is. So that's pretty cool. It can seem to learn to latch onto sub sequences. And it doesn't have to be, as far as I understand it, it doesn't have to be contiguous sequences. It may or may not be able to take chunks from various other parts of a sentence. Sort of latch on to concepts more inside of a sentence. He never knows.
 really what's exactly happening under the hood of a neural network because they're black boxes. But this is how we might conceptually think of what an LSTM does is sort of slicing a sequence into manageable chunks. Manageable chunks for the feed forward pass in encoding a sentence, but also importantly, manageable for the back propagation pass. Back propagation through time because what it's going to do now is when you train your RNN on its error backproped through time, each LSTM will know
 So to listen for when it's its own turn to train. And so it's sort of handling its own mini chunk of the sentence. That's not too long so that it'll suffer from exploding or vanishing gradients. It's its own island of a chunk of a sentence that it can train on a small sequence without causing issues in the back propagation through time part. So an LSTM solves the vanishing and exploding gradient problem by sub sequencing your sequence
 into more manageable chunks. Now, how does an RNN work? What does it look like inside of that machine, inside of that cell? And we call it a cell because it's not quite a neuron. A neuron implies a small function, a small unit. It just does a mathematical function. So we call this a cell because it's a beefy neuron, a really fat neuron that has a bunch of stuff inside of it. And one of those things inside of it is a forget unit or sometimes more complexly a forget layer. A whole neuron dedicated...
 2. Knowing when to stop listening. And everything that comes after it, it forgets. Or everything that came before where we're supposed to start listening, it forgets. So it's still getting the input at every step. But it knows it learns when to forget what it's hearing because it doesn't matter. It's not its own dedicated chunk of the sequence. Intend them with the input gate layer, which decides which values it's going to update.
 inside of the LSTM cell. And then the 10H layer does the actual updating and then the output layer does the output. So there's machinery inside of this LSTM cell that learns to know when to start listening to its sequence and when to stop listening and within that sub sequence what values to update and how to update them and then of course sending out the output. Cool. So when LSTM makes our
 our feed forward pass of RNN more sophisticated, more complex, more powerful. But importantly, it also prevents issues in the training phase and the back propagation through time phase by limiting what an LSTM cares about within a sentence so that its sequence isn't too long that will cause a vanishing or exploding gradient problem. And like I said, a competitor sell to the LSTM.
 cell is called a GRU cell, gated recurrent unit. But you don't need to worry about that for now. LSTM is so much more common in stock, in weather, in language, and everything that you're going to see in the near term. And LSTM is so popular in fact that oftentimes when you're working with an article or an open source library or a model, they won't even call it an RNN. They won't call it a recurrent neural network. They'll just call it an LSTM model, where sometimes...
 If it's a bi-directional RNN, you'll just see bi-LSTM. Bi-LSTM. It's like the LSTM is almost the most important piece of the equation, and so they just use that to describe the whole architecture. But really, an LSTM all it does is it replaces the neuron inside of the hidden layer of an RNN with a little bit more complex machinery than was there to begin with. So NLP time series.
 RNNs, LSTMs, you're now an expert, you can go forth and make NLP models. There's only one more very popular neural network architecture to discuss, and that is the convolutional neural network that I'm going to talk about next episode. The resources for this episode are the same as the last episode, so I'll just drop them in the show notes. In order to find them, ocdevelop.com, forward slash podcasts, forward slash machine learning.
 If you can spare any change to keep this podcast alive, go to that same website and click on the Patreon link. If you can't do that, do me a huge solid and give my podcast a review on iTunes. That'll help keep this podcast going. Thanks for listening, see you next time.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 24 tech stack. Hello again it's been a long time my friends second show.
 I apologize for the delay. Unfortunately, I'm not finding ways to fund this podcast to make it my primary endeavor. The Patreon isn't going super hot, so I'm gonna have to make this podcast a as I find time thing. I won't be offended if anybody pulls out of the Patreon, but I can no longer promise a two week scheduled release. I'll just have to make these podcast episodes as I find time. That being the case, I am looking for work.
 If anybody is looking for a machine learning engineer or knows somebody who's looking for one, please contact me. My contact information is on the podcast website, ocdevelle.com. I am looking for remote machine learning work or Portland, Oregon. Unfortunately, I'm grounded here in Portland for at least a year and a half, so I can't relocate. If anybody has machine learning work, please contact me. I'd be happy to help out. I'm your guy. In this episode, we're going to talk tech stack.
 In episode 10, we talked about languages and frameworks. We talked about Python, R, Java, Scala, and we landed on Python as the best programming language to use as a machine learning engineer. And we talked about all the different machine learning frameworks, specifically deep learning frameworks compared to each other and landed on TensorFlow as the best deep learning framework to use. So in this episode, we're going to assume that comparison solved that...
 we're just going to assume we're working with Python and TensorFlow as our bread and butter. If you didn't listen to that episode and you want to know why we came to that conclusion, go back to episode 10. But for here, Python is the winner of the Languages War and TensorFlow is the winner of the Deep Learning Frameworks. Now real quick, there has been some changes in the Deep Learning's framework space since I last spoke with you. Namely, Fiano is dead. Fiano is officially dead. They've actually announced...
 on a mailing list or a forum or something that they're no longer going to be contributing to the project. I imagine it's kind of a, if you can't beat them, join them where the competition's too stiff against TensorFlow in a world where open source contributors are paid for their work, not for their open source contributions. So it makes sense kind of that they'd get pushed out of the space. It's a sad bit of affairs, but hey, it makes the decision of deep learning frameworks easier for us. Cafe, you'll recalls another competitor.
 in the deep learning frameworks, it seems to be a bit dead in the water as well. I haven't really heard a whole lot about cafe around hacker news or discussions online these days. However, Torch, a competitor to TensorFlow, has actually been making major splashes recently. It's coming up hot on the heels of TensorFlow as a second place contender, where a lot of people are swearing by Torch over TensorFlow. Specifically recall Torch, which was...
 previously written on Lua has been ported to Python and so now is called PyTorch. From what I hear, some educators and practitioners think it's a little bit easier to work with VentensorFlow for the same net gain on GPU performance and all that stuff, making it a net win. I actually haven't played around with PyTorch myself yet, so that'll have to be something you dig into yourself. I will still recommend TensorFlow over Torch merely because TensorFlow...
 is substantially more popular than Torch, which means that there's a higher availability of resources and books, learning material, jobs, employees, plugins, tutorials, all those things. Sometimes it's important to sort of just pick the most popular when it comes to frameworks and libraries. And in this case, TensorFlow is still definitely much more popular than PyTorch. But PyTorch is coming in strong, so it's just something to keep an eye on. So that brings us to the beginning of this tech stack conversation. We're going to be.
 assuming Python and TensorFlow. We're also going to be talking about things like pandas, numpy, scikit-learn, keros, tensorforce, as well as a practical approach to implementing machine learning in an architecture where you have maybe a web app framework communicating with your machine learning framework by way of a message queue in the sort. Before we get into all that, let's remember what TensorFlow does for you. You can write machine learning however you want. It's just math.
 mathematical formulae, which can be expressed in code in Python. And a lot of times when you take a course like the Coursera course, for example, they'll actually have you writing these machine learning equations by hand from scratch in the language that they're teaching. And that's fine and well. But there's two advantages to using a framework. One is that these frameworks are sort of a library of common machine learning functions like convolutional layers of a convolutional neural network.
 rectified linear units and other activation functions and all these things. But the other more important advantage of using a deep learning framework like TensorFlow is that your code gets executed on the GPU, the graphics processing unit, not the CPU. And executing your code on a GPU can gain you up to about a thousand x performance, a thousand x. That's a substantial gain in performance. Now usually it's not that high, usually it's somewhere in the
 of maybe 20X or 100X or something like that. And it depends on the task at hand and the amount of marshaling that goes back and forth between your program and the GPU, et cetera. But the amount of performance gain that can be had by executing your machine learning math on a GPU over a CPU is so substantial that it really gives warrant to using one of these frameworks that does that task for you, like TensorFlow. For those who are familiar.
 with cryptocurrency mining, like mining, Bitcoin, or Ethereum, it's very similar in that space. Having a really powerful GPU makes using a CPU not even worth it. Oftentimes if you're mining, using something like NiceHash, which allows you to use both your CPU and your GPU, the difference between the two is so substantial that a lot of people will just turn their CPU off for mining because it's not even worth the electricity cost. That's the case.
 here in deep learning. So that's the major benefit that these deep learning frameworks bring to bear is that they do all the math on the GPU and specifically the thing that they tout the most is something called auto differentiation. It's the execution of the calculus step, the back propagation step. Remember that the learning step of the machine learning process is called gradient descent gradient descent. You're taking a gradient of your
 error, which is a calculus equation, taking a gradient is differentiating. Gradient descent applies to support vector machines, logistic regression, linear regression. When you apply it to neural networks, it's called back propagation. It's applying gradient descent to all of the neurons. Back propagation is the same as gradient descent for neural networks. And it's got some complications deep down in there where implementing back propagation
 yourself would not be a very fun task. Well, these frameworks automatically take care of the differentiation phase, in other words, the back propagation phase, by deriving automatically the calculus equations they have to perform and then performing them, which saves a whole lot of effort and a significant amount of time and performance. And for one reason or another, it's this auto differentiation specifically that these frameworks tends to tout the most, that they dwell on that as like...
 one of their big benefits. I don't know why specifically auto differentiation, the gradient descent part is so significant by comparison to all the other math that these frameworks perform on the GPU as well. The other statistics in linear algebra formula, but that's the big thing they call auto-diff. So PyTorch and TensorFlow execute auto-diff for you. And auto-diff happens on the GPU, and not just any GPU, it turns out that NVIDIA GPUs specifically are best suited.
 to the task. NVIDIA, the brand of GPUs, the other main brand being AMD. So those are the two common competing graphics cards in the space for gaming people will go back and forth on which ones better. AMD tends to be more bang for buck, but maybe has some compatibility issues and also burns hot. NVIDIA is a little bit more like an iPhone. You kind of pay more for the name brand, but it also has a lot more compatibility with various software. The conversation is a lot.
 different in the machine learning space than it is in the gaming space. In the machine learning space you'd really do yourself a favor by going Nvidia. It really is a matter of compatibility, and in fact at present TensorFlow only supports Nvidia. So if you're going to be using the most popular framework out there, your only choice is Nvidia. The reason for this is Nvidia provides this sort of proprietary driver set for doing this kind of math on a GPU that makes writing to this kind of math as a framework.
 developer, a lot easier than doing it from scratch. And this driver set is called CUDA, CUDA. It stands for Compute Unified Device Architecture. And it's basically just a parallel computing platform. Driverset provided by Nvidia for framework architects, like the developers of TensorFlow. A comparable alternative for AMD would be OpenCL, which is an open source kind of equivalent to...
 CUDA, but which tends to get a lot more second class support by framework architects, where CUDA gets first class support. And on top of CUDA, you can download from Nvidia a zipped up folder of DLL's called CUDNN, which is a little library of neural network helpers to interface with a GPU. And you'd have to download both CUDA and CUDNN for your GPU in order to install TensorFlow with GPU.
 So I just said something significant. I said, TensorFlow only supports Nvidia GPUs because of this CUDA thing. What that means is, unless you have an old MacBook Pro, which has an Nvidia card, a Mac computer is not going to do it for you in machine learning world. Unfortunately, my friends, I know MacBook pros are very popular development machines, especially amongst the web and mobile app developers. You can still use TensorFlow with...
 CPU support on a Mac or a MacBook Pro, but TensorFlow does not support Macs with GPU support going forward. And I think the main reason for that is that the modern Macs use AMD. So I have switched personally from a MacBook Pro to a PC. I'm a big Mac nut, but I have bit the bullet with machine learning development. I've switched to a PC with a Nvidia 1080 Ti graphics card.
 that's the top of the line graphics card you can get on the market these days for consumer purposes, bang for buck wise, and it's very, very, very fast. So I built a custom PC with a 1080 Ti graphics card, and I'm running Ubuntu Linux, not Windows, and not any other flavor of Linux. Ubuntu Linux gets the best first-class support for TensorFlow and other machine learning frameworks over Windows and other flavors.
 of Linux. So I recommend a custom built PC, a tower better than a laptop just due to ventilation issues with a 1080 Ti and videographics card running Ubuntu Linux. That is my recommended hardware stack. Now like I said, you can keep your MacBook Pro and use TensorFlow in CPU mode if you want, if you just kind of want to dabble with machine learning algorithms and models. But if you really want the real deal, you're going to want to run it on some heavy duty hardware, namely of...
 very powerful GPU. You could get away with not having your workstation be the primary hardware you run your machine learning models on. You could develop it on your MacBook Pro or your Windows laptop, and you could deploy your machine learning models to the cloud like Amazon Web Services, AWS, or Google Cloud Platform, GCP, or Microsoft Azure, and run your models on their GPU instances. A very common one is called...
 the P2.x large instance by AWS. And it gives you a GPU that you can run your machine learning models on. Running your models in the cloud can be quite expensive. It's about 90 cents per hour presently to run a P2 x large instance on AWS. If you're going to run your models in the cloud, I personally would recommend Google Cloud Platform. They have faster GPU instances for less money. So faster.
 for cheaper. And they unveiled last year that they're going to be creating server instances which use not a GPU but something called a TPU, a tensor processing unit, which is a chip designed for running machine learning models specifically. A graphics processing unit is very good for running machine learning models incidentally because it's very good at math, but it wasn't built for that specific purpose. TPUs are built specifically by Google specifically for running machine learning models.
 models and specifically for TensorFlow. And they think that you're going to get a major, major performance boost by using TPUs. They're not out yet. Their server instances currently don't support TPUs, but they do have GPUs, which are more powerful than what you'll find on AWS or Azure at present being October 2017. And for cheaper than those slower alternatives to boot. So I recommend GCP over the competition.
 I actually personally use AWS rather than GCP. There's two reasons for this. One is that I actually got free credits for AWS. So I'm running my instances for free, which is better than cheap. And Amazon is famous for doling out these AWS credits. You can get them from various things in the startup marketplace, whether you join an incubator or accelerator program, et cetera. So you might be able to land some free AWS credits. Additionally, AWS off.
 for something called spot instances, spot instances. What they do is they're basically like an auction on a price per hour for an EC2 instance. So where a P2 X large instance, a GPU server instance on EC2, costs normally 90 cents per hour. A spot instance says, how much are you willing to pay per hour? Okay, I'm willing to pay up to 90 cents, for example.
 It may end up costing 10 cents an hour or 20 cents an hour. It's sort of this price fluctuation that happens throughout the day at Amazon. And if it ever goes over the max amount I'm willing to pay, then it will just terminate my instance. Just kill it. Just nip it in the butt. So there's sort of a scare there. There's a fear that you may just have your instance pulled out from under you. As long as you set your spot instance price high enough, then that may never happen. And you could end up...
 saving substantial amounts of money. I think I average end up paying about 10 cents to 20 cents per hour using a spot instance p2 2x large rather than the standard 90 cents per hour. So I just set my spot instance max price to 90 cents an hour, which is the standard price and it never actually gets that high and I never lose my servers. And anyway, this wouldn't be a problem for developing in machine learning models, running them in the cloud because usually you'll be
 using something like an AMI that has TensorFlow support built into it. So spinning up another one of these instances will be no problem. Unlike a web server, maybe that needs to have very high availability and reliability. So these spot instances lend really well to us machine learning engineers wanting to run machine learning models on the cheap for multiple days on a very high end graphics card. So you could just use your Windows laptop or your MacBook Pro to develop your...
 a locally deployed to a P2X large spot instance on AWS and run it there and then not have to actually switch over to a custom built PC with a 1080 Ti graphics card running a boom to you could do that but I would recommend still doing the PC route because in the end unless you get free AWS or GCP credits you'll still come out saving money with a custom home built PC over running in the cl-
 cloud after maybe six months to a year. It's still cheaper to go with a workstation at home than with the cloud. Eventually, you're going to need the cloud to deploy your model if you actually have a production system in play. But we're just talking about our workstations here. So that's the hardware tech stack, either a custom built PC with an Nvidia graphics card, or if you prefer, you can run your models in the cloud. Now let's talk about the software tech stack. Naturally, we're using TensorFlow. We're going to be using a handful.
 of other libraries as well. Pandas, NumPy, Scikit-learn, maybe something called Celery for message queuing, maybe Keras or TensorFlow, will handle these one by one. So first off, machine learning works on data. You have to have data to crunch the numbers, to build a machine learning model, to come up with an estimate of something. Maybe you're coming up with an estimate of housing costs. That's a linear regression model, or of classification that's a logistic regression model.
 It works on numbers and those numbers come from somewhere. Some data set. Your data set can either be in a spreadsheet, a CSV or Excel file. It can be a folder of images if you're doing image classification, or it can come from a database, like Postgres or MySQL, or maybe an API, like Quandal QUANDL, a popular financial data set API. If the data is given to you, you don't really have control over the situation. It's you.
 coming to you by way of a spreadsheet, a CSV file. But if you have control over the situation, I would recommend using a full-fledged RDBMS, a database like Postgres. In fact, Postgres specifically is my favorite RDBMS. And I think it's very popular amongst developers, it's been the main database used in the last five jobs I've worked for. So if you get to choose how you're handling your data store in Postgres, but if you don't get to choose, maybe it's coming from an API and going directly into your machine learning model, it's coming from a s-
 spreadsheet, then you're stuck with what you got. Whatever the case may be, you're going to be consuming your data from your data set. You're going to be pulling it into your program, into your Python program. And the way you would probably do that is by way of pandas. Pandas is a library that's kind of like a spreadsheet in Python. It's a very interesting little tool. First, it provides a step for pulling your data from a data set. That data set may come from a spreadsheet. So.
 pandas has a read CSV function or it may come from a SQL database like Postgres. It has a read SQL function, which interfaces with SQL alchemy, a popular Python ORM, or if it's coming through an API using requests or what have you, you would then just manually write the code that pipes it into pandas. Once your data is out of your data set into pandas, pandas allows you to what they call MUNGE DATA. MUNGE OR-
 clean your data. So for example, your data set may have a lot of nulls in various cells and various rows and columns. There may be a bunch of nulls and empty value, not a zero, a null, and a null can substantially screw up your machine learning model. Before you start doing machine learning, you might think, I'm sure my machine learning model will learn to ignore the nulls, right? I mean, a machine learning model, especially if something so complex as a deep neural network learns what type of data to sort of latch on.
 to in order to come to its conclusions. No, no, not so. Machine learning models and indeed neural networks are very sensitive to nolls. Those can really just make smoke and fire come out of the hood and kill the engine. So you have to do something with those nolls. You can either set them to zero, but sometimes that doesn't help you very much. For example, in the case of stock market data or financial data, you have you have prices on a daily basis. If for whatever reason you're missing a price, you're missing a price.
 on some day, setting the price to zero will make it look like the stock market dropped on that day, which can totally mess up your machine learning model. So instead of setting it to zero, you'd prefer to forward fill it with the price data from yesterday. This is called forward filling. And pandas has a function called F fill for forward fill. They also have a function called B fill for back fill. So pandas allows you to fill your knolls insanely.
 however the case may be for your circumstances. It allows you to turn numeric data into categorical data via one hot and coding or vice versa. Categorical data, maybe your columns or representatives strings turn those into numbers. So pandas is all about manipulating your data to clean it up so that it's ready for your machine learning model. Pandas is about data munging, cleaning up your data. Now you may be thinking, what if I'm using a SQL day?
 database like Postgres, couldn't I do the forward filling or coalescing nolls into zero values or transforming string values into numerical values, all that stuff. Couldn't I do sort of the data munging as the data comes in from the database directly. So it's all part of the SQL query. And the answer is yes, of course, but not everyone's data set is a SQL database. Some people stuff is coming in from spreadsheets, sometimes from API.
 and as has been the case on a recent project of mine, I've been bouncing around between different data sets trying to find the perfect data set to work with while building my model. So I'd rather the data cleaning step exist in the panda's layer because my data set, my data source, may change on a week to week basis. So you have your data set, whether it's a spreadsheet or SQL, comes into your Python code by way of
 pandas, which then cleans up your data, use pandas to clean your data. And now that your data is clean and ready for your machine learning model, what you do is you pipe it into something called numpy NUMPY numpy numpy is a library for working on vectors or matrices or tensors of any dimension linear algebra, basically numpy is linear algebra. So you would use numpy for transposing a matrix.
 or inverting a matrix, or doing a dot product between matrices, etc. You'll recall that statistics and calculus and linear algebra those are the three branches of mathematics used in machine learning. And specifically, sort of at the code level, most of what you're going to be doing is linear algebra and that's sort of what numpy provides. So your data came in from a data set. It got cleaned up by pandas. And now you convert your data frame in pandas.
 as to a matrix, a matrix of numbers, and you're going to do machine learning math on it. So NumPy does that kind of math. You could write any machine learning algorithm you need to in NumPy. The Andrew InCoursera course was taught in MATLAB. NumPy is very similar to MATLAB, the language. NumPy sort of gives you the things that Python lacks compared to MATLAB so you can slice and dice your matrices and do all that stuff. So.
 So Andrew Ying could have alternatively taught the course entirely in NumPy on Python rather than MATLAB, but he chose to do it on MATLAB. Now you may be thinking, wait a minute, I thought that we want to do all this math and linear algebra calculus statistics. Thought we wanted to do that in TensorFlow on a GPU. Yes indeed. Yes indeed we want to do that in TensorFlow. There is a heavy overlap of NumPy with TensorFlow. Do everything I said.
 about using numpy to slice and dice your matrices and do linear algebra and all that stuff. You don't want to do it in numpy. You want to do it in TensorFlow because you want it to execute on the GPU. Now of course, TensorFlow is a newer framework. So this stuff didn't exist 10 years ago where numpy did. TensorFlow is basically bringing numpy to the GPU. In addition to all the other deep learning framework utilities like activation functions and convolutional layers and stuff that TensorFlow provides.
 but there's a huge amount of overlap between numpy and tensorflow. You imagine them as a venn diagram with a very large sort of overlapping space. And a lot of the function calls of numpy are the same, like the methods or the same name, take the same amount of parameters, etc. in tensorflow. Tensorflow tried to make the transition from numpy for people who are used to that to tensorflow as seamless as possible. And in fact, if...
 If you're using TensorFlow without GPU support, say you're just doing development on your Macbook Pro and you only have CPU support, then TensorFlow will actually use NumPy under the hood to execute its math. But even if you're using TensorFlow for everything on a GPU, you still have to have NumPy. You still have to have it around. NumPy tends to sort of be the common language spoken by your Python program.
 and tensor flow. NumPy sort of marshals data between your Python program and tensor flow back and forth, back and forth. What it does is it wraps your data frames that came out of pandas as matrices. They're called ND arrays. NumPy arrays, ND arrays, but it's basically a matrix. It's either a vector or a matrix or a 3D tensor, whatever have you. So NumPy wraps these as tensors and then hands those off to
 tensor flow. And then tensor flow receives a numpy matrix. And it brings it down to C executes it stuff on the GPU. And it comes up with a response down here in C comes back up to Python and puts its response into a numpy tensor again and gives that back to you. So it's sort of this common language that your Python program and tensor flow speak. So with all that said, you can write as much of your machine learning.
 program in NumPy instead of TensorFlow as you want. And you can write as much of it in TensorFlow instead of NumPy as you want. At some point, at some point, you have to have a little bit of TensorFlow. And at some other point, you have to have a little NumPy. So imagine it like the sliding scale between one and 10, where you can slide this scale from the left being NumPy to the right being TensorFlow. You can have as much of your machine learning logic in
 TensorFlow versus NumPy as you want. Ideally you want to write as much as possible of the linear algebra stuff in TensorFlow because that's that big net gain you get from executing your math on a GPU. You'll find sometimes in some GitHub repositories, some boilerplate code that came from an online tutorial. They're not very rigorous about getting as much as possible into the TensorFlow code. So you'll see a whole bunch of custom NumPy lots.
 that could have actually been handled by TensorFlow instead, in which results in slower code execution performance, but they don't care. I mean, they were just putting together a quick tutorial. But in the end, for your production code, you want to use as much TensorFlow as possible. So your data comes out of a data set, whether it be a Postgres database or a spreadsheet, you pipe it into pandas, you'll use pandas to munge and clean your data, filling noles with zeros or...
 forward filling them from prior entries, turning categorical stuff into numbers, turning strings into numbers, all that stuff. You clean up your data with pandas. You marshal pandas into numpy numpy is the way you represent your data sets or your data frames from pandas as matrices or tensors. You do as minimal amount of necessary steps on your numpy arrays as possible before piping it into
 TensorFlow. TensorFlow receives your numpy array, goes down to C, down the stairs, you're in the Python room, you handed TensorFlow a package, a numpy ND array. TensorFlow nods his head, thankfully, and he turns around with his package and he walks out the door, goes down the stairs to the basement where the C people are, C, C++. And you are twiddling your thumb sort of anxiously. You don't want to go down there. Things are a little bit too fast down there. People are running around like their heads are cut off. You're here a bunch of b-
 and booms, some steam, and then the door closes from the basement, TensorFlow walks up the stairs, opens your door and hands you a NumPy ND array package, nods his head and walks away. You, the Python program, unwrap your NumPy array, and there you have your answer. Your prediction for a machine learning prediction step, but maybe a category applied to an image, etc. Pandas NumPy TensorFlow. Now, TensorFlow is an odd duck. Coming from
 a traditional programming background if you maybe are experienced with web or mobile app development or any other sort of Python development or anything. There's a way you write code in a procedural manner. But the way you write TensorFlow code is very strange indeed. And the reason is that TensorFlow code you write is an abstraction. It's not the real code that's executed. The code that you write in TensorFlow is some sort of simplification abs.
 abstraction layer that the TensorFlow people provided to you in your Python world. And what happens in the end, you write 100 lines of code and on the 100th line of code, you sort of seal the deal. It kind of in a way encapsulates all the code you'd written thus far. And sort of when you press the Enter key, when you execute your TensorFlow code, it gets sent down to C by the TensorFlow framework. It actually gets read in a different language. And what gets executed is an entirely different.
 set of instructions. So this is similar to if you're familiar from a database background with object relational mappers, ORMs, a common one in Python being SQL alchemy. What you write is Python code using this abstraction layer, this API. But when you finally execute the code, when you run ORM.execute or what have you, what actually gets executed is SQL SQL code. Naturally, an
 Rm wraps SQL code. That's a very simple thing. I mean, what happens is your Python code gets translated to SQL code in Python. It's so simple in fact that a lot of people astute the use of Rm's. A lot of people don't like using Rm's. They say it hides you from SQL and SQL is an easy language anyway. You might as well gain the flexibility and power of knowing and writing SQL directly rather than hiding it away from yourself with an Rm.
 That's fine and well with ORMs. It's not fine and well with computation graph frameworks like TensorFlow and PyTorch, because they perform that auto differentiation and other sorts of math on the GPU, so you don't have to write it yourself. So you want to use these things, but it's a similar concept. You write your code in Python. What you're doing is you're constructing what's called a computation graph, a graph of nodes. So every line of TensorFlow code is like you're a s-.
 assigning a graph node, a circle, some sort of operation. It's called an op, OP, to a variable, and then subsequent lines of code, collect these variables and combine them in some sort of way, whether they're doing matrix multiplication or piping them through a rectified linear unit, et cetera. Combining, combining, combining, until the very last line of code sort of is the last combination, it's the thing that kind of connects the whole graph together, and you execute it.
 it passes that off to the TensorFlow framework, which sort of rewrites the whole thing and see, a totally different sequence of operations, and then executes the thing on the GPU. And in order for this all to work, the way the code looks when you write it in TensorFlow is really odd. It's really awkward. You're dealing with things called fetches, and feed, dits, and variables, and placeholders. And initially, it seems very awkward and...
 unintuitive. But it actually, it's a, it's a hump you have to get over. There's a definite learning curve to writing tensor flow code. But be at peace that you'll actually get over that learning curve. I think pretty fast. I think it took me about a month of writing tensor flow code before I got over that hump. The first month was like, what the heck am I looking at? And then month two and month three were like, okay, things are smooth sailing. You really understand how it works. It's similar to trying to learn functional programming. It's a totally different.
 style of code. But once you get used to it, it makes a lot of sense. And in the case of functional coding, you'll, it's kind of like you'll never go back. Now, that's TensorFlow, raw TensorFlow, you know, kind of awkward to write because it has this sort of encapsulation paradigm, what, where what you're actually writing is a computation graph that then gets passed on down through the framework. There is a framework out there that eases the burden of this process that makes writing TensorFlow code a lot easier, a lot less awkward that feels.
 a lot more like writing traditional procedural Python. It's called Keras, K-E-R-A-S. And it's becoming a lot more popular these days. Keras is a wrapper on top of TensorFlow. So it uses TensorFlow under the hood. But it basically reduces a whole bunch of TensorFlow boilerplate into substantially fewer lines of code in Keras. So something that would take you 50 lines of TensorFlow code to write and a lot of
 fusion, you would write in caros in maybe five to ten lines of code instead, and which would look a lot more elegant. Caros used to be a wrapper on top of the piano, and I'll bet that the writers of caros are very happy that they decided to fully support TensorFlow with the recent news of the piano's death, because now they're still writing the tail coats of a popular machine learning framework. And caros tends to be very popular with books...
 and tutorial code, basically where an author is trying to convey how you might go about writing some machine learning code conceptually and they're less interested in the code or text specifics because Keras hides you from all those ugly details. What I see common in practice is people use Keras as sort of like a bootstrap tool or actually there is a CSS framework in web development called bootstrap.
 and I think this is a very good analogy. A CSS framework called Bootstrap that allows you to make a website that is automatically designed. So like every HTML element you put on your website, automatically has this theme to it that's very pretty. It's very basic and it's very common. You'll spot a Bootstrap website a mile away, but at least it's not sort of black on white, sort of vanilla HTML. You get a theme out of the box, a CSS theme. It's no fault.
 you can sort of proof of concept your web app. You can try your web app out, see if it sticks, see if you'll get any customers. And if you do, if your web app proves itself out, you can then take away the bootstrap theme, and now you can start custom designing your own CSS to give your website a custom design and a custom look and feel. I think Keras is very similar to this. What you can do with Keras is you can write your machine.
 programming in Keras first because it'll save you a lot of headache, both with confusion and sheer lines of code. And once you've proved out your model and you've decided that this is a useful route to go, you actually want to build this project, then you may pull out chunks of Keras where you prefer to have more flexibility by diving down to the raw tensorflow. The game that Keras gives you in...
 east of use, you lose in flexibility. That's a very natural trade off. So what you can do is start with caros and sort of start pulling caros out and going with raw TensorFlow as you need to sort of customize your model with higher flexibility. This is a common use case I see. So it's a net win. I highly recommend investigating caros. So there you have TensorFlow and additionally an optional wrapper called caros to ease the burden. Another comm-
 in library you'll see used is called scikit learn. Scikit learn is a library of shallow learning algorithms. TensorFlow is a framework of deep learning algorithms. TensorFlow actually sort of has as high or low level as you want to go in the sort of deep learning stack. It has built into it neural networks out of the box and recurrent neural networks just out of the box. You can kind of like 10 lines.
 a recurrent neural network, or you can build an tensor flow from scratch using linear algebra and statistics formulae, you can handcraft a neural network. So you can go as low or high in abstraction in tensor flow as you want. But it doesn't have functions for shallow learning. So it has functions for deep learning, for constructing deep neural networks, whether their recurrent neural networks, or convolutional neural networks, or multi-layer
 perceptions or auto encoders, etc. It has all the deep learning tools built in, but it doesn't have shallow learning tools. Now linear and logistic regression specifically, you can hand code in TensorFlow pretty easily, but there's plenty of other shallow learning algorithms that we've covered that you couldn't easily do in TensorFlow. Things like support vector machines, naive bays, decision trees, random for
 forests, et cetera. And so if you're going to be using shallow learning for your task, then you're probably going to want to use scikit learn. Scikit learn is the TensorFlow of shallow learning. TensorFlow for deep learning, scikit learn for shallow learning. The two libraries have very, very little overlap. So you'll be using one or the other depending on the task at hand. Now, if you are using TensorFlow and deep learning, you still may benefit from scikit learn. Having it on.
 on hand. It has a few miscellaneous utilities that actually come in handy, no matter what you're doing in machine learning. For example, one thing it has is a library of data sets that you can just pull data out of thin air. So for example, a common data set is the MNIST handwritten numbers image data set, MNIST, M-N-I-S-T. And actually don't quote me on it, but I believe it's in a psychic learn. I know for a fact that the iris-
 flower data set, which is a very common example is in scikit learn. And you basically just like import iris from scikit learn.datasets. That kind of it's that easy to just pull a data set out of thin air. So you can start developing your machine learning model without having to worry about that step. Another thing that scikit learn provides that is useful no matter what you're doing is data standardization and normalization. So in machine learning models, the learning
 step of gradient descent or back propagation. Learning step works a lot better when the features are normalized or standardized. There's a subtle difference between normalization and standardization, which I won't go into here. But the idea is if you're trying to figure out the cost of a house, based on its number of bedrooms, its number of bathrooms, its square footage, and its distance to downtown, all these things, well the number of bathrooms might be two.
 And the distance to downtown might be like 5,000 whatever's inches, I don't know. And the difference between those two numbers is like they're on totally different scales. They're totally different types of numbers. And that scale difference can thwart gradient descent. And so feature standardization is the process of bringing those numbers into the same ballpark. And they'll end up being some number that you don't actually recognize.
 is the number of bathrooms might be 0.5 and the distance to downtown might be 0.7. But the point is that it brings them all to the same playing field. And there is a function in psychic learn for scaling and normalizing data sets, which is something you will use in deep learning as well. So you can keep psychic learn around just for the data standardization step that you will then use your standardized data to feed into TensorFlow. Now that sounds like.
 actually a job for pandas, right? Pandas is all about data munging, data cleaning, data preparation. Well, actually, I don't know if pandas has a function for standardization or normalization. I think pandas is meant to be more general purpose, not used specifically in machine learning, where data standardization like that tends to be a little bit machine learning centric. So that might be why it exists in scikit learning or at least is more commonly used from scikit learning.
 in the repositories that I've seen. So that's your tech stack. You have a data set, be it Postgres or a spreadsheet or what have you, coming into pandas being managed, marshalled through NumPy into TensorFlow. TensorFlow executes your code on the GPU, brings it back up, puts it into NumPy and gives it back to you in your Python program. You may optionally use Scikit-learn to standardize your data or pull data sets to choose out of TW
 there, et cetera. Now let's talk about something that is not machine learning centric, but I imagine will be useful for a lot of the listeners here. And that is the general kind of architecture idea for just a company. How you might fit a machine learning server into your system architecture. Now most companies sort of bread and butter is a web app server. They're their website or their mobile app. Interfacing with their customers by me.
 of a website or a mobile app. And so their main server is their web server, their app server, you call it an app server. And typically this is going to be a very different thing from your machine learning server, which is usually called your job server. So your app server is taking requests from web clients, people browsing the website on Chrome or their iPhone or using the mobile app on Android. These are all your clients.
 and they're all sending requests to your web server, your app server, and your web server might be written in Node.js or Go, something that has very strong concurrency support for handling multiple requests, for example, I actually personally dislike Python as a web app server language. Even Flask and Django, the traditional frameworks used on Python, I think things like Node.js and Go have stronger concurrency support for handling multiple clients.
 So I personally like Node.js for writing my web servers. And you'll run this maybe on AWS, either Beanstalk or EC2, something with an auto-scaler setup. And again, this is gonna be a totally different server than your machine learning server. So now over here, you have your machine learning server. It's called your job server. And this is gonna run on a different type of server, one that has a GPU or multiple GPUs, a P2X large instance if you're using AWS. For example,
 And one reason you want these to be two separate servers is that a P2X large instance has very few CPUs, very limited RAM, but a very powerful GPU and is very expensive. Okay, and that lends well to machine learning, whereas a web app server wants more CPUs, wants more RAM, and doesn't care at all about GPUs. And because it doesn't care about GPUs, you can cut a lot of...
 of costs. So you want them to actually be on physically different architectures for one, one to save costs and two because the architectures make different sense in different situations. Two is you want them to scale differently. You're going to want your web app server to scale up and down pretty fast and in pretty sizable chunks, whereas your machine learning server may not scale ever, maybe one or two new instances here and there and then scale back down to one. Okay, so if these are
 two separate servers. How do they communicate? A very common way to communicate between your web app server, your app server and your job server is by something called message queuing software. A very popular message queuing software is called rabbit mq. Another one that's popular is called zero mq. You'll see mq for message queue in the names of these softwares. Now, you may be thinking, well, couldn't I just send like a simple rest request from my app?
 server to my job server to do some machine learning task. Yes, you could, but there are a lot of benefits for using a robust piece of software, like a message queuing service. If your job server is temporarily offline, then it wouldn't have received a request sent by your app server and vice versa. And as a result, that message gets lost into no man's land. It gets lost forever. Say for example, let's pretend that...
 we're building Pandora. A user thumbs up songs or thumbs downs them from an app or a website. That's their client. That request gets sent to the app server, the web server. The web server may put that action in a database just to keep that on hand and say very good and send a response down to the client, saying, I have received your request. Here's your new song. Let's switch them to that new song and start playing it. We're gonna use the app server to stream the music to the client, all that stuff. In the meantime.
 The app server sort of is dealing with this customer talking to the customer with this communication turns around and sort of like Hey job server with it, you know his hand to his mouth. Hey This person didn't like this song. Can you fit that into your machine learning thingy and the job servers like got it It starts crunching away on the GPU's even tensor flow the app server doesn't know anything about that Okay, good that's the scientist over there. He does all the crazy stuff if the app server had turned around and tried to say Hey job server and the job
 was sleeping, a.k.a offline, the job server would never have gotten that request. So switching to a message queuing system like RabbitMQ, what will happen is the app server will take a request, put it in a non-volope, says this user didn't like this song. Go to a mailbox and put it in the mailbox and close the mailbox and come back and communicate with the client. Whenever the job server is ready to do machine learning stuff...
 Whenever it's done with some task, it's currently on or it comes back online from being offline, maybe a crash or something. Or it just wakes up and it yons and it looks at the time and makes it's coffee. And then it goes out the door and it goes to the mailbox and picks up a request, closes the mailbox and goes back and does its machine learning stuff with that request. An additional benefit to this is if you have multiple job servers, multiple machine learning servers running TensorFlow on
 their GPUs. One of them will go and pick up a message out of the mailbox and handle it, and that message no longer exists in the mailbox. So by the time the second job server comes around, it will either get an empty job queue or the next job in line. So using a robust piece of software like Rabbit MQ makes for more false tolerant, reliable, sensible handling of message passing between an app and a job server. A common
 piece of software for message queuing in Python is called celery. Celery abstracts various message queuing software like zero mq rabbit mq and more. And that way you can just use celery and not care about what message queuing software your system architects have decided upon under the hood or switch to a different message queuing service as the case may be celery. Lastly I've got a little
 bonus software package to talk about, a framework called tensor force. Tensor force is like carous for reinforcement learning. So we haven't talked about reinforcement learning much yet, but reinforcement learning is awesome. It's a burgeoning space in machine learning. It's where all of the best and coolest research is happening right now. Reinforcement learning is where machine learning becomes artificial intelligence. In fact, reinforcement learning.
 equals artificial intelligence. So for those of you who've come to the space of machine learning because of an interest in AI, well what you should be targeting is RL, reinforcement learning. That's what you should have your goals set on, a job eventually in RL. And we'll talk about reinforcement learning in subsequent episodes. Reinforcement learning is a very different beast than supervised learning and unsupervised learning. For one, deep reinforcement learning is a much...
 newer space and it's much less explored. Deep reinforcement learning is the stuff you see coming out from DeepMind and OpenAI, playing video games, playing go, driving cars, all these things. And Recall reinforcement learning is action-based machine learning. So supervised learning is coming up with a prediction based on past data. So predicting the category of an image or predicting the sentiment of a sentence, etc. Reinforcement learning.
 as action-based. It's deciding what action to take under a situation. That's why it lends well to self-driving cars. You're deciding whether to turn left or to turn right, etc. And it's very research-centric right now, not very developer-friendly. Unlike supervised learning, which is very developer-friendly by way of tensor flow and caros, supervised learning is developer-friendly because supervised learning has a lot of common developer applications.
 in industry, in standard professional settings, whereas reinforcement learning does not quite have that yet. So it's really more lens better to research than it does to industry, deep reinforcement learning. And as a result, trying to implement your own deep reinforcement learning algorithms means you have to like, you have to read these papers, you have to really hand craft these hyper parameters to a T. There's not a lot of resources out there yet.
 for practical implementations of deep reinforcement learning algorithms except for tensor force. Tensor force is a framework which makes deep reinforcement learning accessible. I've been working a lot with it recently and I can't recommend it enough. Tensor force, so check that out. Alright, big lay of the land. We talked about your workstation, ideally, being a custom built tower PC with a high end 1080 Ti and videographics card. Alternatively, you can develop...
 up your models on any computer you want and run them in the cloud. AWS or GCP, I recommend GCP. We talked about auto-diff frameworks, PyTorch, TensorFlow, Theano and Cafe. Keep an eye on PyTorch, use TensorFlow. Pandas, NumPy and PsychitLearn are three libraries that you're going to use in addition to TensorFlow in your machine learning tool belt. Pandas is for retrieving and cleaning data. NumPy.
 Some pie is sort of the common language of matrices and tensors spoken by your Python program and your TensorFlow code. And Scikit Learn is both a library of utilities that you may find useful no matter what you do. And a framework of shallow learning algorithms if your task is a shallow learning task. As far as system architecture goes, you will likely have an app server that does all your web stuff and a job server that does all of your machine learnings.
 stuff. You want your job server, of course, to have a high-end GPU. You'll communicate between the two by way of a message queue like rabbit and queue. And there is a wrapper library in Python called Celery, which makes working with message queue software easier. Speaking of wrapper libraries, making things easier. Keras sits on top of TensorFlow to make TensorFlow code more palatable. It boils TensorFlow down into fewer lines of code.
 It makes it look more like traditional procedural programming at the cost of less flexibility. Tensor Force is a wrap or library on top of TensorFlow for deep reinforcement learning specifically because reinforcement learning is sort of a different beast than unsupervised and supervised machine learning. Resources for this episode are the usual deep learning resources, which I'll post in the show notes and a book that I have recommended in the past. It is called...
 hands-on machine learning with Scikit-learn Intensor Flow. And I actually just finished this book, and this is my favorite machine learning book I've ever read. In fact, my favorite machine learning resource I've ever consumed, except for the Andrew Eing Coursera course. That's on a golden pedestal that you can't touch. Andrew Eing Coursera's course always comes first. But this book is phenomenal, and it's actually very applicable to this episode specifically. It talks about Scikit-learn Intensor Flow. It talks about pandas and numpy.
 and data sets like databases and spreadsheets, everything that I've covered in this episode is covered in this book, and the author does an excellent job of explaining things. I've found that there is a dearth of well-explained machine learning concepts in industry, which is why I've made this podcast in the first place. I think these things don't have to be so complicated. They can be boiled down really well. And this author actually does a very good job of boiling these things down. He makes some shit.
 machine learning algorithms very understandable. He talks about shallow learning, hey, talks about deep learning. So I'll post that in the show notes, the usual place, ocdevelop.com, forward slash podcasts, forward slash machine learning. By the way, if you're interested in purchasing a workstation PC to run your machine learning models on, I would recommend going with a custom built tower PC, build it yourself. Don't get a laptop unless you really need that mobility because you can get a lot more performance with it.
 desktop PC and don't get one of those pre fab desktop PCs like cyber power PC for example they're a great company and they have great computers but when it comes to machine learning you really want to milk every last tiny little drop out of your purchase and for the same price you can get on the order of maybe 50% extra performance for running your machine learning models by custom building which can mean the difference between training for three days versus two days. So on my website I'm going to post a parts list.
 of the bill that I used for my computer. And I'm going to try to keep this parts list up to date with state-of-the-art components as new components release. So that no matter when you listen to this podcast, you can go to that parts list and build your own PC. For example, this is October 2017, the 1080 Ti graphics card and the I7 series Intel CPUs or top of the line. But very soon we're gonna be getting a new release of Intel CPUs and Vulta graphics cards from Nvidia, which should be a lot.
 faster, I'll update the list when these release. I'll also post a link to a video series tutorial on how to build your own PC. Also speaking of performance and this is a random aside. When you start really get into the metal with your machine learning development, build tensor flow from source. You can milk a lot more performance out of tensor flow built from source than through the PIP installation. It's kind of a pain in the butt. So start with PIP when you first get set up and don't worry about...
 from source until you're really cooking with fire with your machine learning models. Once you really start getting deep in your development, you can get a lot more performance by building TensorFlow from source. Doing it that way also allows you to use the latest CUDA and CUDN releases, where the stable releases of TensorFlow tend to be on very old versions of CUDA and CUDN. And newer versions of CUDA can milk substantial performance improvements. I'll see you in the next episode. I don't know when that will be. Again, I'm not on a schedule.
 anymore. The next episode will either be on convolutional neural networks for image recognition or neural network parts like various activation functions, batch normalization, various optimizers like Adam and stuff like that. So I'm not sure which of those episodes I'm gonna do next but I'll see you when I see you. Also I need a job.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 25 convolutional neural networks. Today we're going to be talking about conv nets convolutional
 neural networks or CNNs. Before we do that a little bit of admin, I've been told by a handful of listeners that they want to donate to the show except that Patreon charges monthly and they only want to donate once. So I've created a one-time PayPal donate button as well as posting my Bitcoin wallet address for you crypto junkies if anybody is willing to donate to the show. If you're not willing to donate please do leave a review on iTunes. That brings more listeners to the show which helps keep the-
 this alive and well. So convolutional neural networks. For one reason or another, convnets tend to be the most popular topic discussed by new and aspiring machine learning engineers. I don't know why specifically convnets are so popular. I mean, I understand that vision is essential, a key component to robots and AI and all that stuff. But no less so, the natural language processing by way of recurrent neural networks and the like. But anyway, convnets are super popular in the deep learning space.
 are the thing of vision in machine learning. In the same way that recurrent neural networks are the thing of NLP, natural language processing as well as any sort of time series problems such as stock markets and weather prediction, conv nets are for images, image classification, image recognition, computer vision. And conv nets to me are a real clear case of the machine learning hostile takeover of artificial intelligence. I've said this in prior episode that I think that the...
 The crux of AI is ML. That ML is fast subsuming AI in a significant way, so much so that the terms are almost becoming synonymous. That's definitely the case with NLP. Machine learning came in and made a heavy dent with recurrent neural networks on all of the various aspects of NLP. That's not to say that NLP was entirely conquered by machine learning, but that machine learning has contributed very heavily to the space. In the case of computer vision, I think we see that even more so.
 ConvNet's really truly dominate the space of computer vision. And so we're going to be talking about that today with respect to image classification, image recognition and the like. Now for those of you who have good memory, and you recall from a prior episode when I was talking about facial recognition, and I was using a multi-layer perceptron, right, the vanilla neural network, an MLP, as an example of an algorithm for image recognition. I said that the first hidden layer might be for detecting things like lines and
 the second hidden layer for shapes and objects and the third hidden layer for things like eyes, ears, mouth, and nose and then the final layer being a sigmoid function if you're just concerned with detecting whether or not there's a face in the picture or a softmax if we're trying to classify it as tree dog cat or human. So I was using an MLP for an example of image classification. I lied to you my dear listeners. Nobody uses MLPs for image classification. they use CONFINUN.
 But an MLP sort of lends well to a pedagogical mental picture of the situation. And we encounter MLPs earlier on in our machine learning learning. So I thought it made sense to give you a picture. But you don't use MLPs for images. You use confnets. And here's why. An MLP for image recognition is like using a bag of words for spam detection. Now you may be thinking, hey, I thought that you said bag of words algorithms like Naive Bayes.
 work well for spam classification. You just take all the words in an email and you just cut them all up and you just throw them in a bag, you shake up the bag, and you spill it out on the table. And in the case of spam detection in natural language processing, maybe you're looking for the word Viagra. Okay, you're just kind of pushing all the words around. And oh, there it is, Viagra. Bam, this is spam. Easy peasy, yes, bag of words works fantastically for natural language processing in certain problems. But using a bag of words kind of idea in image classification.
 doesn't make sense. What you would be doing is cutting the picture up into all of its pixels. Okay, if you have a 5x5 picture, you would have 25 pixels. And then you throw all those pixels into a bag, and you shake it up, and you dump the pixels on the table. And now what? How the heck are you supposed to detect whether or not there's something that you're looking for in that picture? It's just a bag of pixels. That's what an MLP would be giving you. An MLP, remember the other word for a regular neural network.
 Good, DnN is another word for it, deep neural network, where an A, N, N, artificial neural network, we're just gonna be calling them MLPs from now on. An MLP consists of dense layers, dense layers meaning that all of the neurons from the prior layer are connected to the next layer. So all of the pixels of the input are connected to the first hidden layer. All of the pixels are connected to every neuron of the first hidden layer. So everything is combined with everything else. And then all of the...
 neurons are connected to all of the neurons of the next hidden layer. Everything is combined with everything else. It really is like a bag of words. You're just throwing all the pixels in and you're combining them every which possible way. But that's not how pictures work. When you're looking for something specific in a picture, you're generally looking for a type of object regionally located in one little window, one square. Let's say that we're looking for Waldo. We're going to be using where's Waldo is the example of this episode. We're looking for
 a picture. Now there's not going to be a little piece of Waldo in the left and a tiny piece of Waldo in the bottom right and maybe he's hat in the center of the picture and his foot over here on the top right. That's not how it works. It's all going to be clumps together in one object and that object can be anywhere in the picture. So that's why MLPs don't work for image classification. Instead we want a neural network that works with patches, windows of pixels, all at once. Little chunks in the picture.
 And even within one window in a picture, a window that may be a box around Waldo in the picture, even within that window, we still don't want to just combine every pixel in that window, every which way with each other, that still won't be very helpful for detecting whether Waldo is in this window. Instead, we really want to look for a specific shape, or a specific sort of color pattern in this window. And so what we're going to design is something called a filter.
 A filter is the crux of convnet. It's the core component. What a filter is is an object detector. Imagine that you have a 5x5 piece of paper and you take scissors and you cut out the shape of Waldo in that piece of paper so that there's a hole in the center of the piece of paper that's the shape of Waldo. And then you take that piece of paper and you put it on top of your picture. Your 50x50 image. And now you take that piece of paper, your filter.
 and you use your finger to slide it to the right. You slide it over the picture. You slide it from the top left all the way to the right, and then you bring it down one row, start back on the left kind of like a typewriter, right, you type all the way to the right, and then you hit enter, and the piece of paper goes up, and you start at the next line at the left, and then you start sliding your filter to the right again. And the moment that there's actually a wall dough in the picture, it'll be very apparent to you because he'll sort of fill in that cut out in the center of your piece of paper.
 Up until that point, until your piece of paper was over a walldo, nothing sort of obviously filled that hole in the piece of paper that was cut out, like the shape of walldo, nothing very apparently filled it. It was all just a bunch of sort of pixel gibberish, until you got over a walldo, and he fit just so perfectly right into that cut out, and it made him pop, made him really stand out. So it's not really an object detector. There is no activation function or output of this new...
 that gives you a yes or a no necessarily. Instead, it's a thing that sort of makes the object pop. Makes him stand out in the location where he is. So that's what a filter is. It's almost like a separate image, a smaller image, maybe a five by five filter, that you're going to be using to search for an object in a 50 by 50 image. And the filter is designed in a way that makes what it is you're looking for pop. Makes it pop out of the picture. Now having your filter.
 or sort of be the shape of Waldo, is a bit of an oversimplification. A filter usually doesn't work that way. A filter is usually a little bit more simplistic than that. In the case of Waldo detection, for example, one actual filter we might use is going to be horizontal stripes. Because Waldo's shirt has red horizontal stripes on it. So a simple filter that would make him pop out of the image is a filter that has these stripes on it horizontally. And what I mean by...
 that is it's a 5 by 5 filter, a 5 by 5 sort of picture square of pixels where every even row is filled with ones and every odd row is filled with zeros. That's kind of like the cutouts. What that does is when it is applied to a patch in the picture, all of the even rows of that patch are disabled because they're multiplied by zero, all the pixels are multiplied by zero and all of the odd rows of that patch.
 are enabled because they're multiplied by one. And so when we hover over a Waldo, we'll see a bunch of red stripes pop out at us. But when we're hovering over anything else in the picture, it sort of looks like striped nonsense. So a filter can't learn something quite so complex as the cutout of a human shape. But it can learn something simple enough that could still give us a good insight as to what we're looking at. And then we would combine multiple filters together to really...
 increase our confidence. So Waldo has glasses, he has a beanie, he has red striped shirt, and he's something of a bean pole. He's a very skinny guy kind of occupies vertically a very small section in the center of a window. So you might imagine designing four or five different filters, each of which is looking for different patterns in the patch of pixels. And all of them combined sort of making something pop out in the picture that will give us confidence as to whether or not we're looking...
 looking at an object we're looking for. So let's put these all together in real convolutional neural network terms. When I say a patch of pixels, what this is called is a window. A window is a square chunk in the picture. And a filter is a filter. Again, we have a filter or any number of filters that we're going to be starting in the top left of our picture and sliding to the right and anytime it sort of hovers over something that we're looking for, that thing kind of pops out through the filters.
 and makes it stand out in the image. When I think about filters, I like to imagine them as kind of like an old-school lens, sort of a cylinder that you hold in between your fingers, and you know, you kind of put it on the picture on the top left, and you look through it with your eye. Close your other eye and you're looking through the lens with your eye, and you're sliding it to the right, and most of the time all you see is sort of blur, but whenever you are over the thing you're looking for in the picture, then it becomes very clear. The wall-
 becomes very clear, whereas the other windows are sort of blurry. Now inside of that cylinder, inside of that lens, there are multiple filters. You imagine you go to an eye doctor and he puts some lenses in front of your eyes and you're looking at some letters and he says, better or worse, he puts an additional layer of lenses in front of your eyes and you say, better. And then he keeps that there and then he puts an additional layer of lenses in front of your eyes, a third lens in front of the other two and he says, better or worse and you say worse. So he's trying to find...
 sort of this right sequence of layers of lenses that really makes the letter A on the board pop out at you. Be very crystal clear. And that's what you're doing in designing these filters. You have a lens, this cylinder object that has multiple layers of filters inside of it. And this is what the machine learning model is going to learn. It's going to learn the design of each of these filters, each filter layer in your sort of cylindrical lens. Now we have a filter.
 But that is not actually a layer. We're talking about deep learning here, neural networks. We didn't design a layer, a hidden layer in our neural network. Instead we have a little tool in our tool belt, this lens, this filter. In order to construct the first hidden layer of our convolutional neural network, what's called a convolutional layer, what we're going to do is like I said, we started the top left with this filter and we apply it to the picture all the way to the right. We slide up.
 all the way to the right and then like a typewriter, ch-ching, we started the next row on the left and we slide it all the way to the right again. Ch-ching, started the next row, slide all the way to the right again until we've covered the entire picture and applied this filter throughout the picture. And what we have now is a new picture, an entirely new image, where all the wall-dows in the image pop. All the wall-dows are now crystal clear. And everything in between is blurry.
 or gibberishy pixely. This is called a feature map, a feature map. A filter is the tool we use for making an object pop in a picture and a feature map is that picture transformed with the filter in every window, every square of the picture is transformed with the filter and now we have a new image and it's called a feature map. Now like I said, this lens that we're using to slide over the picture has more.
 multiple filters inside of it, multiple layers of filters, each filter trying to detect a different type of pattern, stripes, glasses, shapes, some tall thing in the center of the filter, etc. Multiple layers of filters, and so what is output in our convolutional layer, that next layer is actually multiple feature maps. One feature map with each filter applied to the picture. So what we have in our first hidden layer of our neural node.
 work is a 3D box of pixels, a 3D box of pixels with by height, okay, and it's the same width and height as our original picture except that instead of being the picture, it's our filter applied to the picture for every window, for every patch of pixels in the image, with by height and then depth. Depth is the number of filters, so each convolutional layer is
 a width by height feature map, a feature map being applying one filter to your entire image, and depth being the number of filters you have. Okay? Kind of confusing. So let's start from the top. We have a picture that comes in as your input in 2D with by height. We don't flatten it. Now what our neural network is going to learn is filters. Filters are these masks that make certain patterns in a pixel patch. A width.
 Pop, pop out of that window. This is what the convolutional neural network is gonna learn, these filters. And we're gonna have multiple of them. We're gonna have one filter for stripes, one filter for glasses, shapes, one filter for skinny object in the center of a window. And we're gonna stack these on top of each other. And we're gonna take that stack of filters, and we're going to apply it from left to right, top to bottom in our picture, and that's going to output a new picture with by height pixels, but the...
 depth is the amount of filters. So what we have is a box. Now, that's our first hidden layer, our convolutional layer, a layer of feature maps. If we want additional hidden layers of our neural network, we would do this again. We will learn new filters and we will apply those new filters to that first hidden layer because that first hidden layer is kind of a picture of its own. It may not be a picture that makes a lot of sense to humans, but it'll make sense to the machine learning algorithm. We will learn these new...
 filters and we will apply them window by window by window to that first hidden layer and what we will get out of it is a new picture a new convolutional layer which is width by height pixels and feature maps depth the third dimension being feature maps and a feature map is when you apply your filter to every window of a picture and that will be your second hidden layer your second convolutional layer and then finally to sort of cap off your convolutional neural network what you
 will usually do is then pipe the result of all that through dense layers. You've made certain patterns in your picture pop and stand out. And now you can sort of latch onto those with your dense layers to determine whether something is in your image or not by piping that through a softmax or a logistic function or the like. Very good. That is a convolutional neural network. We're going to talk about the additional details like stride and padding, window sizes and max pooling in a bit. But I just want to...
 need to know that that's the essence of convolutional neural networks. Each layer is called a convolutional layer. And what a layer is is a stack of feature maps. And those feature maps come from applying a filter across your picture. And it's these filters that we learn. It's the filters that the convent learns in the back propagation step. Now, oftentimes in deep learning, part of the process is sort of boiling things down step by step as we go through the neural network.
 work. It's kind of this, it looks like a funnel. Every layer of neurons gets smaller and smaller and smaller until our final output is either one neuron in the case of logistic regression or one of multiple neurons, but I mean let's say 10 or 20 in the case of softmax regression. If we were to have like a layer of 512 neurons and then the next layer is 512 and then the next layer is 512. So we're not actually boiling things down. We're just kind of mixing matching. And then the last layer is that one sigmoid.
 function, right? Well, first off, that final layer would have way too much work to do. We would be depending on it too much to sort of boil down this universe of combined features until one point. We would be overworking this neuron. So it would be better if we could boil him down bit by bit by bit until finally, when it's the last neurons turn, he only has maybe 28 employees that have to report to him. But in addition to that, part of the magic of neural networks is that they break things down.
 hierarchically so that they get smaller and smaller as go along. So in a picture, for example, if you started with a 50 by 50 picture, well, that would be 2,500 pixels, 2,500 units in your input layer. And ideally, you would boil that down into, let's say, 10 or 20 different types of lines and objects. And then you would boil that down into IZEAR's mouth and nose, four objects. And then you'd boil that down to one. So that's kind of the way that deep learning generally
 It works not always, but generally we like to go from very, very big to very small, gradually, hierarchically. Now the way I've been describing convolutional layers is that each feature map is the same size as the picture they're applied to. We take our filter and we move it window by window over the picture and what comes out as a feature map. Exact same size. And if we have multiple convolutional layers like this, then it doesn't feel like we're sort of boiling our picture down to its essence.
 over time. So the way we do this, the way we boil images down into their essence, step by step, is by a combination of window size, stride, and padding. Okay? Window, stride, and padding. Now, window we've already talked about window is the size of a patch of pixels that you're looking at at any one given time in your picture. So a window of five by five, means you're looking at 25 pixels at once. Stride is how much you move.
 that window over at a time. If we had a stride of one, we would move that window over one pixel at a time, meaning that when our filter maps that to the feature map in the convolutional layer, there will be a lot of overlap between each window. If we had a stride of five, that would mean the window would skip completely to the next patch. So our filter would look at a five by five window and then it would slide over five pixels all the way past the last
 pixel scene in the first observation. So the filter is now looking at a new patch with no overlap with the prior patch. Now how do we reason about this? Stride and window size combined. You always think about them in combination. Try to think of them as some sort of ratio, like two over five, five being the window size and two being the stride size or something like this. Window and stride always get considered together. In the previous example where the picture gets mapped directly to a feature map.
 and they're the same dimensions. That's a stride of one. If we were to use a stride of five, what that would do is take your windows, your five by five windows and boil them down into one pixel each. So you would take a five by five window and that would turn into one pixel in the downstream feature map. If you had a stride of one, you would slide right one and that would turn into one pixel as well in the downstream feature map. Essentially, we looked at...
 two pixels in our original image and it has become two pixels in our new image in our feature map. So that didn't actually do any sort of compression. It just did transformation. If we wanted to compress the image into a smaller feature map, that bigger stride of five, what you would do is you take a window of five by five that would become one pixel in the feature map and you'd move over five whole pixels and that new five by five window would become a new pixel in
 the feature map and everything in between would be left out. So all the pixels will have been considered because we didn't skip any pixels, but they'll have been boiled down substantially. 25 pixels will become one. So that's how you do sort of compression in this process. You have a higher stride and a higher window size. Now that's not always beneficial. Let's say for example that Waldo sort of straddled in between those two windows. We have a window.
 Window of 5 by 5, and then we stride 5, so the window now moves to an entirely new set of pixels. But Waldo's right in the middle there. Half of his body is on the right side of the first window, and the other half of his body is on the left side of the second window. Neither filters would pick up Waldo in the windows. Okay, we've got the Stripe Detector filter. Maybe that would ding, ding, ding. But what about that sort of skinny object in the center of the window filter? That filter is not going to...
 anything pop in the window. So even though a higher stride will give us good sort of compression or boiling down of our windows, it may result in poor detection of objects. So a good middle ground is generally preferred. Maybe a stride of two or a stride of three. So there's always a decent amount of overlap. So it was C-Wald-O because at some point he will be in the center of A window. And because the stride is greater than one, these windows of five by five
 will still be boiled down into smaller patches in the downstream feature map. So some combination of window size and stride is how you achieve boiling things down into smaller layers. And like I said, window and stride, they always go hand in hand. It took me a while to understand convnets because there's so many terms we're talking about filters and feature maps, convolutional layers, window stride, padding, and max pooling. These are all terms we're going to talk about in this episode.
 So, so many terms, it helps when you realize that many of these terms are combined with each other. They're different pieces of the same thing. So, window and stride, they always go hand in hand. Feature map and filter are basically the same thing. A filter is a small section, your little paper cut out, your five by five, the size of a window, and when you apply it to the whole image, you get a feature map. So, a feature map is applied filter. So, feature map and filter, they go hand in hand.
 feature maps stacked is a convolutional layer. So all those three things go in hand. Feature map, filter, convolutional layer. Okay, and then over here we have window and stride. Those things kind of go hand in hand for image compression. And then the other thing that goes along with image compression is something called padding. And padding is very simple to understand. Padding is we have our five by five window and we're sliding it to the right, okay? Let's say our picture is not 50 by 50, but 52 by 52. Some number this not divisible by five. Well,
 Our window will slide all the way to the right until it gets to the last two pixels. Now we can decide one of two things to do. We can either stop there and move to the next row, or we can move our window five pixels to the right anyway. There's only two pixels left in the picture, so what we'll do is we'll create three fake pixels. They're basically zeros. So that the remaining two pixels are considered. They're sort of in the left part of our window. And then...
 The excess is just these fake pixels. And presumably the convent will learn that this excess on the right side of the picture can be ignored. We call padding is same. Padding equals same when we include the fake pixels and we call it valid when we exclude the excess pixels. I don't have a great way for remembering same versus valid. I always have to look it up personally. So there are just two separate ways of handling the excess pixels. You might think same.
 That is, always including the excess pixels seems like the smarter way to always go. Shouldn't we always include every pixel? Well, not necessarily. In a lot of pictures, sort of the borders of the image are kind of cruffed. I mean, we do cropping as a pre-processing step anyway, many times. So in many cases, excluding the small amount of border pixels is not a big loss. In other cases, you do want to include every...
 single pixel, especially in cases where it's not actually image recognition we're working with. I will talk in a subsequent episode about how you can use convolutional neural networks for stock markets. Stock market prices, you're not looking at an image whatsoever, a totally different space than computer vision. Conv nets and recurrent neural networks, you can use these things sort of in very surprising domains. You have to think outside of the box. In those cases where you're working with features that are
 aren't really pixels, you want to include all those features in the process. And so padding equals same is the right way to go. So it just depends on your situation. Okay, so we talked about window and stride. And to some extent padding, those three being used as sort of an image compression technique. A way of boiling down your picture at one layer into a smaller convolutional layer. And then doing the same thing to that convolutional layer to the next convolutional layer until everything.
 smaller and smaller and smaller and then you hit your dense layers and you're working with a small amount of features. So we talked about that as one way of doing image compression. And that's compression in the machine learning sense. It's compressing features into a smaller feature that sort of represents all the other features. It's hierarchically boiling information down into smaller and smaller bits. There's another method of image compression in convolutional neural networks called max pooling. And this is sort of the traditional.
 emotional sense of image compression, which is simply making an image smaller. Not actually doing any sort of machine learning, just scaling it down. Lossy compression in the truest sense. Max pooling. Or there are other types of pooling layers. We call them pooling layers. You can use max pooling or you can use mean pooling. So let's talk about max pooling because that's the most common. What max pooling does is it takes your picture and it just makes it smaller. That is it. Just compresses it down. Now it's different than...
 using a complex convolutional layer of filters and stride and padding and blah, blah, blah. All it does is, let's say you're going to boil a two by two window into one pixel. Okay, so you're dividing it by four. You're making every patch of four pixels become one pixel. So you're just downscaling it substantially. All you're doing is you're taking that patch of four pixels, that window of four pixels and you're taking the max pixel. That's it. The maximum pixel, by that I mean in a gray scale.
 image every pixel is represented by a number between 0 and 1 where 1 is black and 0 is white. So we take the maximum of those pixels and we just use that and throw away the other pixels. This is just true compression. This is compressing images like if you were trying to upload a photo to Facebook and it said your pictures too big you know you tried to upload a picture that's 1024 by 1024 and pretend that Facebook says we only accept images 128 by 128 okay they don't do the compression.
 on their side they expect you to have smaller images to upload to their website. Well what you might do is open up Preview or Photoshop or something and just click Edit Picture Dimensions and just make it smaller. That's all that's going on with Max Pool. It's just making a picture smaller and it's doing so in a very destructive way. You know from experience when you make pictures smaller or bigger it's lossy. If you make it smaller it's called lossy compression. It looks kind of pixelated. Something looks a little bit off about it. If you squint your eyes you can tell...
 that there was some damage done in the process. But you have to squint your eyes. And that's the idea here with Max Pooleung is you can apply lossy compression to your pictures to make them smaller without doing too much damage in the process. Now, why would you want to do this? We had the option of using a big stride and big window in a convolutional layer for boiling a picture down, sort of boiling the essence down. We're not just throwing stuff away, we're boiling it down to its essence. Why would we want to use...
 max pooling. We use max pooling for a totally different reason. That reason is to save compute time. It turns out that convolutional neural networks are the most expensive neural networks in all the land. More than recurrent neural networks, more than MLPs, more than anything. Why? Well, we've taken an image that has width by height and you're kind of multiplying those two as far as number of features as concerned and your pipe.
 typing that into a convolutional layer that has width by height as well as depth and sometimes very, very deep depth, maybe 64 feature maps or 96 feature maps. And you might have 10, 20 hidden convolutional layers when you start looking at the image net, competition, convent, architectures. These things are massive. This is really where you see your GPU shine. If you're working with an ML.
 or a recurrent neural network, you know, you'll probably see it five to 10x performance gain by using your GPU instead of your CPU. But when you're using ComvNets, you'll see your GPU utilization spike up to 99% and you will be screaming fast running your ComvNets on your GPU by comparison to your CPU. ComvNet architectures is really where your GPU performance shines and not just in computational speed.
 but the amount of memory that's used by your architecture, your 1080 Ti, for example, has about 11 gigs of RAM, separate from your system's RAM. Well, when you're doing a whole bunch of image processing, you're gonna be consuming a lot of that RAM. So, convnets are heavy, very heavy beasts, and the easiest way to slim them down, to make them less heavy, is just image compression. Just make your images smaller, and that's what Max pooling is for. Using a combination of str-
 and window size to boil your images down is something of a machine learning technique. That's boiling down the essence contained in the pixels of your image. But max pooling is just for making things smaller so that they'll run faster. And you can apply max pooling to your image directly, right after the first layer, you can also apply it after every convolutional layer. Because each convolutional layer, while they may be coming smaller and with and height, their...
 becoming deeper in depth of feature maps. So using max pooling will reduce the dimensionality of your process making your convent run faster. By the way, there's something I forgot to mention earlier in this episode. We think of a convolutional layer as width by height by depth, okay, width and height pixels and depth being feature maps. Well, the input layer image also has depth. It is RGB, red, green, blue, valley. We call those channels.
 So your input image is width by height pixels and channels deep. Every one pixel will have three channels being RGB values. So your input picture is also a box and then every subsequent convolutional layer is a box. So really every layer is kind of a picture in its own right. Okay and that's it. That's convolutional neural networks. They're not easy but they're not complex I'd say architecturally. You just have to read a chat.
 chapter on them and maybe you'll have to read it twice to come to grips with what all the moving parts are here, but unlike something like natural language processing, where maybe understanding how a recurrent neural network works is fine and dandy, but to understand the lay of the land of NLP, there's a whole lot of problems you have to solve with convnets. The one problem you're solving here is image recognition or object detection and image. So there's not a whole lot you have to know. This isn't a three part series like with NLP, but just...
 make sure you understand all the parts. I'm sorry I'm so redundant. We're going to start from the top and work our way here. You start with an image. It is a width by height pixels image incidentally. It's also three channels deep that is RGB value. So you got a box that's your image. You pipe that into your convnet. Your first hidden layer is called a convolutional layer. And the way this layer functions is this convolutional layer has width by height pixels as well.
 and feature maps depth. Any number of feature maps, deep. These feature maps are derived by applying a filter, one filter per feature map. You will apply this filter to the image, put it in the top left corner of the image, and you slide it to the right, and it generates sort of a new image where the objects that that filter is designed to make pop, well it's a new image where all...
 those objects in the image pop. So in a wall dough detecting filter, your feature map is going to be a new version of your original picture where everything is blurry except all the wall doughs who are very clear. So your filter is some small window of pixels that has something sort of cut out of it. And when you apply it to your whole image, what you get out is a feature map. You do that with every filter in your convolutional layer, you
 your feature maps of the convolutional layer. Each convolutional layer is width by height pixels and feature maps deep. It is the filters that your neural network is trying to learn. You specify up front actually the amount of filters you're going to be using or the amount of feature maps in your convolutional layer. You specify the width and height of your convolutional layer as well as the amount of feature maps being used, the depth. It's the job of the neural network to learn the design
 the filters, not the amount of them. So that's the essence of it. The details are that the filter has a window size. That the size of the filter is called the window with and height. Maybe it's a five by five. It's some small square. The stride determines how much that window moves at any given time. If you are using a stride of one, then that window moves over one pixel at a time. And the resulting feature map is the same size as your original image.
 If your stride is five, then your window moves over an entire window at a time so that there's no pixel overlap. And each window of five pixels becomes one pixel in the feature map. In other words, your image gets compressed to a smaller image. Generally, a good strategy is to use something in between. Maybe a window of five by five and a stride of two. So there's some amount of overlap.
 which improves the likelihood of detecting objects and yet some amount of skipping, which results in image compression. Additionally, a technical detail is what you should do when you've slid your window all the way to the right and there's excess pixels. Do you include them or do you skip them? If you skip them, we call this valid padding. And if you include them, we call this same padding and the way we make that work is by adding extra dummy pixels.
 pixels so that our window of five will fit over five pixels, some of which are going to be dummy pixels. A combination of window size, padding and stride will result in image compression in each convolutional layer until your final layer, which is generally a good strategy. But another way to achieve image compression is called max pooling. Max pooling is lossy, simple image compression used primary
 to save system resources. If your images are too big or your convolutional layers are too big, and it's just hurting your RAM or your GPU performance, you'll use max pooling. Okay, so that's the general architecture of a convolutional neural network. That's the general architecture. Now, you probably won't have a great deal of success trying to freelance your way through designing a confnet. Understanding the general...
 principles, how to design a convolutional layer, and then building an image detector with that, that'll probably give you, you know, a decent image detector, maybe 10% error rate or 20% error rate. It turns out that the amount of convolutional layers and where you put the max pooling layers and the window size and stride size and all those things. These are the hyper parameters right, selecting these things are choosing your hyper parameters. ConvNet
 hyper parameter selection is very sensitive. As far as error rate is concerned, if you want a very, very well-tuned convnet, then you're going to spend a hell of a lot of time tuning your hyper parameters. And so one thing you can do instead, and especially if the image detector you're building is a classic type of image detector, you're actually trying to detect people and dogs and common objects in common photos, is use one of these off the shelf conv-
 So there's this competition called ILSVRC ImageNet Challenge. And it's a challenge for people to be able to detect specific objects in a database of photos. And they hold this every year. And every year people come to this competition and they beat last year's convent architecture with a new architecture, a new combination of max pooling and stride and window and number of feet.
 feature maps and all those things. And so one of the classic ones is called LayNet 5, as in Jan Lekun, the LE for Lekun. And then a subsequent year's winner was called AlexNet, so that would have beaten LayNet. It would have decreased the error rate. And then a subsequent one is GoogleNet. And then a next one is Inception. And then a next one is ResNet and so on and so on. You may have heard of these different net architectures. I heard these things floating around for the longest.
 time, ResNet and AlexNet. I didn't realize that they're all ComvNet. None of them are RNNs, none of them are MLPs. So when you hear something net, you're probably dealing with a ComvNet. And these ComvNets, these architectures are enormous. There are some big, big ComvNets, very complex and very sensitively tuned hyperparameters. So if you just want an image detector for some project you're doing a robot with vision, you can use one of these off-the-shelf networks.
 And a good rule thumb is just use the winner from the most recent year. Use 2017's winner, for example. It will have defeated all the prior architectures. But if what you're building is in the domain of computer vision, but is maybe a little bit less common than common object detection in common pictures, then what you could do is sort of study the architectures and see what makes good hyper parameters and good layer style.
 and then use that to drive designing your own convnet. So what I described to you in this episode is the core components of designing a convnet architecture, but very likely if you actually plan on using convnets in the wild, especially for image recognition, you'll want to look at one of these prefab architectures that came out of the image net challenge and probably use the most recent winner. Cool, cool. That's it for this episode. In the resources section.
 I'm going to post a link to a YouTube recorded series of CS231N, a course by Stanford, specifically on ComvNet. And of course, the standard deep learning resources I've always been recommending. I'll post those in the show notes. And the hands-on machine learning with Scikit-learn Intensor Flow book that I've been recommending has a very good chapter on ComvNet as well. I'll see you in the next episode.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left near life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 26 project Bitcoin trader. This episode is going to be a little different. We're going to introduce.
 a project that we as a community can develop together. So we can sharpen our teeth on machine learning engineering. The project is going to be a Bitcoin trading bot. There's a few reasons for the selection of this project. One is that cryptocurrency is a super hot topic right now. Another is that hey, you and me might be able to get rich from this project. I'm not kidding. I've been working pretty hard on this thing for the last six months, nights and weekends. And I'm crossing my fingers that it'll work out someday, we'll be able to crack the code. But the main reason for using Bitcoin trading for our project is that it's going to lend.
 really well to the coming episodes. The next episode after this one is going to be on hyper parameter selection. And the one after that we're finally going to start on reinforcement learning, deeper reinforcement learning. And Bitcoin trading handles very specially in hyper parameter selection this next coming episode. It very intuitively highlights a lot of the decisions that we can make about choosing network architecture, which types of layers to use, whether they're LSTM or convolutional layers, activation function.
 width of the layers and things like this. It more intuitively highlights these choices than many other scenarios in machine learning do. Bitcoin trading is very special in that it makes these types of decisions of hyper parameters very intuitive. You'll see that in the next episode. It's a little hard to understand right now. Another thing that makes Bitcoin trading special as a machine learning project is its flexibility. Trading can be successfully done in machine learning either by unsupervised learning,
 provides learning and reinforcement learning. All three branches of machine learning, you can apply to trading. And you'll see in this episode that trading is a very simple concept. So it's very surprising the amount of flexibility we have in the types of machine learning paradigms we apply to this scenario. So you can start to see that it's quite special. Another bit is you can use convolutional neural networks or LSTM recurrent neural networks in your network architectures for your trading model, which is very unusual. A bit coin trading as we'll see.
 in this episode is a time series scenario, the intuitive network architecture is an LSTM recurrent neural network, but you're not restricted to using that, and in fact, many people have had more success spinning the situation as a convolutional neural network scenario. So trading is a very unique scenario that lends well to a lot of machine learning concept exploration, and which will dovetail us eventually into deep reinforcement learning. So it's a good project in its own right. I think it's something that a lot of you guys will get a kick.
 out of and hey, maybe we can make money off of. But importantly, for this podcast, it's going to fit very nicely into the progression of episodes from here on out. This episode is going to describe trading, day trading, swing trading, and it's going to describe cryptocurrency and Bitcoin and all that stuff. If you know about cryptocurrency, if you yourself are a day trader, skip this episode. There's no machine learning in this episode. We're going to apply this episode's content into a machine learning framework in the next episode.
 So if you know crypto and you know trading skip this episode But if you don't feel like you know both or either of those well enough listen to this episode because a lot of the details are gonna come in Unexplained in future episodes. So this will give us the groundwork Okay, before we talk about trading we're gonna talk about cryptocurrency So there's two aspects to our project crypto currency and crypto trading now crypto currency I'm not so sure I need to really introduce I imagine most of my listeners
 are well acquainted with it. It's a super hot topic right now. Everyone's talking about Bitcoin. Our parents are asking us how to get involved in Bitcoin, all that stuff. But I'm gonna introduce Bitcoin nonetheless for diligence. And then we'll talk about trading. Bitcoin is a type of cryptocurrency, or just crypto, some people call it. A cryptocurrency is a currency. Just like the US dollar is a currency. The European euro is a currency, the Japanese yen, except it's a universal currency rather.
 than being pegged to any specific country. That's what a cryptocurrency is. It's a digital currency that's completely managed and generated digitally on the internet. cryptocurrency is not managed by any one person organization or group. It is managed distributed to all the miners in the network. You may have heard of this thing Bitcoin mining. Well, basically people have their computer rigs set up, usually a beefy GPU set up, very similar to a machine learning PC actually. And what they're doing is they're running.
 a bunch of equations, calculations, this is called mining. And it's basically handling the infrastructure of the currency's existence, processing transactions with the currency, making sure new bitcoins are released into the world in a proper timely way, in the same way that the Federal Reserve handles distribution of the US dollar. So it's distributed within a big network of people, and all transactions purchased with cryptocurrencies are recorded onto this distributed ledger.
 this permanent record. So that's a lot of information if you've never heard of cryptocurrency that might have been a whirlwind. But the crux of it is that cryptocurrency is a currency, just like the US dollar, except that its management is super secure, super permanent, and completely democratized, completely managed and owned by the people. It's a very special thing indeed. It's getting a lot of publicity these days for a very good reason. It offers a lot of value into the financial marketplace. It offers a lot of competition.
 in its value over banks and over governmental overseeing of country currencies. So you may have heard this thing people say, well, cryptocurrency is a big bubble because there's nothing backing it, unlike gold, which has a physical object backing it. Well, that's not true. What's backing cryptocurrency is the value that it adds. High security and untappierable and permanent history of transactions, that's called the ledger, and democratic management. All things that real currencies lack. So there's something substantial.
 back in cryptocurrencies. Bitcoin is one of many cryptocurrencies. Bitcoin would be like the US dollar for example. There are many other cryptocurrencies, ones called Ethereum, another's called Litecoin, another's called Ripple, and they all have symbols. Bitcoin is BTC, just like the US dollar is USD. Ethereum is ETH. Well, Bitcoin is the most popular because it was the first of the cryptocurrencies. It was the flagship of the concept of cryptocurrency built on this.
 underlying technology called blockchain. All those benefits that I'd mentioned previously about distributed permanent ledger, security, blah, blah, blah, blah. That's all built into the technology called blockchain. And then on top of blockchain, we have cryptocurrencies. And one of those cryptocurrencies is Bitcoin. And Bitcoin was the first, and therefore it's the most popular. It's not necessarily the best. Different cryptocurrencies have different sort of scale ability profiles and additional features. So for example, Ethereum has this thing called
 smart contracts, which actually let you sign a contract digitally with somebody. And then based on some event, execute that contract. Let's say you're a contractor working for a company. You might use Ethereum to sign and execute a contract. And now you're locked into a contract. If you decide to break that contract, whether you leave the company earlier or do something like that, then a penalty can be enforced programmatically via that contract. And so the combination of smart contracts, a distributed permanent ledger, and all these
 things, there's a lot of opportunity to use this technology to reduce legal costs and investigation and mitigation and the government could use this to automate taxes at the end of the year and all this stuff. So cryptocurrency is really cool stuff, a lot of really strong potential in the future with this technology. Now we're interested in it for our purposes as a trading vehicle, day trading, swing trading, this kind of stuff you might have heard before. We could trade stocks and bonds and foreign currency.
 and any of these other things that people have traditionally been trading on Wall Street for the last many, many decades. We could do that. They're a little bit complicated to get into. Getting into trading stocks can be quite a technical hurdle. Getting started. You also have to sort of keep your eye on which stocks you want to trade. So stocks and bonds and stuff that may be a little too complicated for a project for us. One thing we could trade is foreign exchange. Forex F.O.
 RX. Foreign exchange is very similar to cryptocurrency. Country currencies, crypto currencies. They're very similar. You can trade country currencies on a foreign exchange in the same way that you can trade cryptocurrencies on an exchange like G-DACs, which will get to in a bit. Foreign currencies go up and go down. They eb and flow, just like stocks, just like bonds. Certain countries' economies might be booming like China, for example. That's the case. Maybe you should buy some Chinese
 In exchange for your US dollar, if the Chinese economy is burgeoning more than the US economy, then presumably you would be making money on that purchase, the value of the Chinese currency that you just purchased increases. And at some point you can decide to sell that, get your US dollars back in exchange for the currency you own. And you will have made a profit because you sold after the value had increased. So we could trade in foreign currency, but we're going to do it in crypto currency, this digital currency.
 because of the tooling that's available. The exchanges out there like GDACs and Crackin have fantastic APIs. The very nature of cryptocurrency being this new generation digital phenomenon means that it has all of the great digital tooling around it that makes working with it as an engineer very easy. So the main reason we're gonna be trading in cryptocurrency compared to the traditional instruments is just convenience. By the way, that word instrument.
 Instrument refers to anything that you can trade. So a stock is an instrument, a specific stock. So the Apple stock is an instrument. Facebook stock is an instrument. A foreign currency is an instrument. A specific bond is an instrument. Bitcoin is an instrument. Okay, so we described cryptocurrency. Bitcoin is the main cryptocurrency out there just by popularity. We're therefore going to be using Bitcoin instead of Ethereum, Ripple, Litecoin, etc. Just because there's more historical data.
 available for Bitcoin, all exchanges, support, Bitcoin, etc. Now let's talk a little bit more about trading. There's two words to be aware of. Investing and trading. When you invest, you buy an instrument or some amount of some instrument like stock and apple, for example, because you believe in that instrument. You believe that the value of apple is going to increase in the future. Investing is putting money where your belief is. And then you just leave it.
 You just leave your money there. The stock market goes up and it comes down and it goes up and it comes down. It's like the sawtooth graph and you ignore the graph. You don't care about the prices in the short term because all you care about is in the long term, generally speaking, if your theory is correct, your investment's value will increase. Trading is different. Traders believe that the activity on that graph does indeed matter. The idea goes like this and you may have heard of this phrase.
 buy low sell high. Let's say you buy $10 of stock in Apple. Then over some amount of time it doubles in value. Now your stock in Apple is worth $20. You bought 10, it became 20. Now let's say that Apple's value is starting to creep down. Well if you jump out just in time you sell your stock that you own at $20 and let's say Apple stock crashes back down to $10. Well what just happened you doubled your value.
 you. You put in 10, you came out with 20, you bought low, you sold high. So the idea of trading is to look for these patterns in a graph in what's called price actions of an instrument. You look for these patterns and you buy and you sell based on the patterns so that you can come away with more money than you put in. The catch phrase is buy low and sell high, but it's usually not that simple. And the big problem with that phrase really is how do you know what's low and what's high? How do you
 know when you're at a low or if it's going to keep going lower. And how do you know when you're at a high or if it's going to keep going higher? So investors and traders are fundamentally different people. Investors believe in the value of something. They put their money in that stock or in that Bitcoin and they just leave it. They close their eyes. They don't want to be stressed out by that wild graph. They know that in 10 years time, they'll walk away with more money than they put in. Investors on the other hand buy and sell based on patterns on.
 on the graph, on a daily basis, on a weekly basis, on a monthly basis, however much amount of trading you want to do. If you do all of your trading in the course of a day, if you're just glued to the screen and you're buying and selling and buying and selling, we call that a day trader. And the important bit about a day trader is that the end of the day, the trader closes his or her positions. In other words, he takes all the money out and puts it in his pocket. He doesn't want his value to fluctuate overnight when he can.
 be watching it. So he manages it all manually. All in the course of one day goes to sleep wakes up and starts over again. Generally a day trader, that's like a full time job. There's lots of day traders out there that that's all they do. Many of them make a lot of money. A day trader is probably what you have in mind when you think about stock markets, about trading on Wall Street, Wolf Wall Street kind of stuff. These guys in business suits glued to a screen, black screen with green and red price.
 charts and numbers and histograms on their monitor. And you know, they're holding up tickets in the air and they've got the visor or something like that. That's the old style stock exchange trading. Now everything's digitized of course. Now swing trading is anything outside of day trading. Okay. If you don't close your positions at the end of the day and then go to sleep with peace of mind. If you continue to trade over the course of days or weeks or so, then that's called swing trading. And then it's just that day trading tends to.
 to be more of that full-time person. What we're gonna build together as a community project is a swing trader effectively because it can do trading overnight if you're so inclined. But anyway, it's gonna be a machine learning automated trader. It's gonna buy and sell based on patterns that it recognizes in these graphs. Day traders work on patterns in the graphs. If a pattern is there, machine learning can pick it up. Therefore, we should have machine learning to be our automated day trader for ourselves.
 And then we can go to our full-time job and make extra money over there. Now, how do you buy a bitcoin in the first place? Well, you go to these exchanges. They're called exchanges. If you were to buy stocks, you'd go to a stock exchange. If you were to buy bonds, you'd go to a bond exchange, I guess. If you're to buy foreign currency, you'd go to a foreign exchange, a forex. And if you want to buy bitcoin, you go to a crypto exchange. Two very popular exchanges is one is called GDAX, GDX, subsidiary of a company.
 called Coinbase and another is called Crackin. There is no company in charge of Bitcoin or any other cryptocurrency for that matter. Exchanges are basically just like bank accounts. Wells Fargo and Bank of America don't own the US dollar but they're places where you can go and store your US dollar. Crypto exchanges are places where you can go exchange your US dollars for some amount of Bitcoin. Now you hold some Bitcoin on that exchange and you can trade back
 forth between your dollar and your Bitcoin. GDACs is probably the most popular out there, so that's what I recommend you using if you're in the US. Crackin is a little less popular, but it's also available to European citizens and others. Now what you'll do is you'll go on gdax.com and you'll sign up and you'll put in a bunch of security information and you put in your bank wiring information and then you transfer money from your bank into GDACs. And now you'll have a certain amount of money in US dollars in GDAC.
 Now you can trade those US dollars for Bitcoin. What you'll see in front of you is a black screen and some charts and some histograms and some numbers and a whole bunch of little stuff. It is in these charts that you will visually spot the patterns that as a day trader you act on. You'll be looking at this screen, this is your terminal. And based on activity in these charts, you're going to decide whether to buy Bitcoin or to sell Bitcoin or to hold, don't do anything. Buy, sell or hold.
 So you're looking for patterns in the charts. Where do these patterns come from? They come from what's called price actions. Now you can imagine that the value of Bitcoin goes up and down and up and down and it draws this line chart. Well, let's say we wanted to make a machine learning model around that, what would you do? Well, you'd have time steps. Every time step you'd have a price, the price of Bitcoin at that time. The price of Bitcoin changes depending on how many people are buying or selling the Bitcoin. The value of Bitcoin is in how many people.
 have purchased Bitcoin. The more people buy Bitcoin, the more valuable it is. So you can imagine a line chart, and every time step is a price. And so if you wanted to build a machine learning model here, what you might do is use an LSTM recurrent neural network. An RNN, remember, is the neural network best designed for time series data. And Bitcoin price history is indeed time series data, with one input at every time step being the price.
 Let's say it's $10,000 at time step one. And then it's $10,000 and $2 at time step two. Then $10,000 and $3 at time step three and so on. You might build a LSTM RNN where you pipe in the history of some time slice, let's say 150 steps. And the label, the thing you're trying to predict in a supervised learning scenario is the next price. That's your label. So you input all the time steps one to one 50 and you're.
 training your model to be able to predict the very next time step, time step 151 in this case. That's your supervised learning situation. Bing-Bang-BOOM. We have a day trader. Easy peasy. That would be the most basic way that you could envision this project. An LSTM RNN where you feed in all the time steps, one input per time step, that's the price, and you're training it to predict the last time step. You're training it to...
 predict one time step into the future. So if you were training it to be a day trader, it would predict one second into the future, in our example, where every time step is a second. But then you as a developer would have to hard code the rules of what to do next. Well, if it predicts that the price is gonna go up in the next time step, well, maybe we should make it by. And if it predicts that the price is going to go down into the future now action, 1990 to GT your thinking based on my intention to be training the template link
 next time step we should make it sell. By low sell high, right? And if the next price prediction is the same as the current price, then it should hold. It should do nothing. Very good. We have a supervised learning LSTM RNN model that will take in all the prices as time steps and output the next time step prediction. And then we have some custom code to decide what to do next. There's a couple problems here. The first one is...
 It's not that simple with buy low sell high. It may seem so at first, but it really isn't. There's a lot of clever strategies for working the system like you're playing chess. Day trading is not a simple job, in fact. Many people study day after day after day, train in sandbox environments and lose a lot of money in their early years as a trader until they really start to get the system right. So it's really not so simple as buy low sell.
 There's there's tricks and strategies. And so that leads us to one of the reasons why we're discussing Bitcoin trading as a pedagogical machine learning scenario in the first place, which is that the problem can be posed either as a unsupervised learning situation. I actually don't know how you do that, but I've heard it done before a supervised learning situation, which I just described in which case you would have to program by hand the next step actions or a
 reinforcement learning situation. Reinforcement learning takes supervised learning to the next step. Reinforcement learning has inside of its package, like in its stomach. Imagine this robot with an LSTM RNN in its stomach. It has inside of its package supervised learning. Inside of its package, it has the capacity to estimate the next time step based on prior historical data. And then in the outer shell of...
 of reinforcement learning, the model learns to act. It learns to take the correct action. And it can learn very sophisticated actions and sequences of actions and strategies and so forth. So this is a situation in which we could use supervised learning for our trading bot, or we could use reinforcement learning for our trading bot. In our project, the actual code is written to use deep reinforcement learning by way of a framework called tensor force. I think we're in.
 enforcement learning is a much better fit for automated trading than supervised learning, but both can be used. So that was the first problem with how we envision modeling our situation, was that it may be able to predict the next time step to some degree of accuracy using an LSTM RNN, but that doesn't tell you what to do next. And it's not as simple as by low cell high after all. So switching to a reinforcement learning model could mitigate that issue. The second problem is that a single price.
 at a time step is usually not enough information to really build a pattern matching model. It is very unlikely that you could build an LSTM RNN that could really accurately predict next step prices just based on price alone. One input is really insufficient for any machine learning model. So how can we get more inputs out of our situation, more inputs for every time step? Well, here's how. At any given time, any...
 number of trades could be occurring on an exchange. Any number of other people could be buying and selling Bitcoin too. In other words, time is infinitely divisible as far as trading is concerned. That sounds confusing. What I'm trying to say is you're gonna have to decide how to partition your time steps. Are they gonna be once every second? Once every two seconds? Once every minute, every hour, every day? Well, if you decide...
 on partitioning your time steps into one second buckets. Well, within that one second slice of time, an infinite amount of trades may have occurred. Now, a high frequency trading algorithm, they're called HFT, high frequency trading. These things will split time into milliseconds. Nanoseconds. In that case, these algorithms could capture every single individual...
 They call these orders. They can capture every single order on the order book. That's a high frequency trading algorithm. There's no chance in hell our project is going to be able to catch every nanosecond slice of time. So we're going to take it up a scale. We're going to say maybe 20 second buckets of time. Every 20 seconds we're going to go out to GDAX's API and ask it for information on the trades that occurred during that time. Now what happens with every...
 every trade. Every time somebody buys Bitcoin, the value goes up. Every time somebody sells Bitcoin, the value goes down. In other words, during a 22nd slice of time, the value of Bitcoin has gone up and down and up and down. We've got a mini sawtooth graph within our little slice of time. So what could we do? Maybe we could average all the prices that Bitcoin has been at during that chunk of time. We could take the mean of prices. That's something we could do. It's general.
 not recommended. One downside to that is that outlier prices can very strongly affect the mean. And this may occur in a situation called pump and dump. Somebody on the exchange who maybe has a billion dollars to their name, for example. They can put a million dollars into Bitcoin. They can buy a million dollars worth of Bitcoin on GDACs. Now the price has skyrocketed. The value of Bitcoin is now very high. Well, a bunch of...
 of traders may see the spike in price and they may think that we're entering a bubble. And they'll very hurriedly say, bye, bye, bye, we're on the up and up. And so everybody will put all their money into Bitcoin as well. And now the price is even more up. And that initial person who put the million dollars in will now pull the million dollars out because the value has increased. And he's selling it high. It's a very dick move indeed. It is very common and a very strongly effect.
 at price averaging because it introduces an outlier. And it can really mess up your algorithm. It can mess up humans too. So we wanna avoid taking the mean of prices in a time slice. Instead what we're gonna do is we're gonna collect a bunch of information about that time slice. We're gonna ask GDX, when I entered this time slice, when I entered this 22nd interval, what was the price then? That was the open price, open price. And then when I exited this time slice, what was the price then?
 was the price then, that's called the close, open and close. In all of this 20 second time slice, what was the highest price Bitcoin ever reached? That's called the high. For example, in our million dollar scenario, million dollars would be the high. And what was the lowest Bitcoin saw in this time slice? That's the low. Open high, low, close, oh, HLC. This method of segmenting the prices was invented by the Japanese. They called it a candlestick. And in 2000, shit, I don't sleep,
 calling it a bucket or a time slice, they call it a candlestick. And the reason they call it a candlestick is that you can imagine it's like a box plot from statistics. We have a vertical bar, the body, and it has whiskers on top and on bottom. The top whisker is the high, the bottom whisker is the low. Those are kind of like outliers in a box plot. Well, the Japanese envision those as wicks on a candle. And the body of the candlestick, the top will either be the open or the close, and
 bottom will be the opposite depending on whether our candlestick is going up or going down. By comparison to the prior candlestick, if our current candlestick is going up, if the price has increased in this time slice by comparison to the prior time slice, then the bottom is the open and the top is the close of the body of the candlestick. And the color of the candlestick is green, you know, because it's a good color, it means we're going up.
 And if we're going down, then the top is the open, and the bottom is the close, and the color is red, because we're going down. So that's a candlestick, OHLC, open high, low, close. And there's one more important number in that chunk of time. It's called volume. How much was traded during this time? So for example, the fact that that jerk spent a million dollars pumping the price, but only one person...
 and did that means that there will be low volume. And when the price is high but the volume is low, that's a good indicator that we got a pump and dumper. But if the price is high and the volume is high, that means many people are buying into Bitcoin during this time slice, during this candlestick. And that's a good indicator that it actually is going up. So open high, low, close, and volume. OHLCV. Those are the five.
 numbers you'll see very, very commonly. If you look on an exchange terminal, the price actions, the actual line graph of price, you'll realize it's not actually a line graph, it's a bunch of candlesticks. Those candlesticks will be colored red and green depending on if we're going up or going down, and they'll have whiskers on top and on bottom, and those are your open, high, low, close numbers. Then below the price graph, you'll have a histogram of bars corresponding...
 to every time step and those are your volume numbers. So we started off with a price graph, a line graph, and we had the problem that that only gave us one input for every time slice. But then we realized, wait up, wait up. There is no such thing as a time slice. It's infinitely divisible. And so we have to actually decide how much of a graph we want to split at. So we can say split it every one second or every two seconds or every three seconds.
 the high frequency trading algorithms will split at every nanosecond or so, but we're not that computationally intensive, so we're going to split it every one or two seconds. Well, in so doing, we just introduced to ourselves five new numbers that we'll use as inputs instead of the one. And that allowed us to get more inputs out of that price graph. OH, LCV, or candlesticks. Now our trading models start to look a little bit more sophisticated. We can envision it as an LST.
 GM RNN that takes in five inputs at every time step and either outputs a next price prediction or we can use reinforcement learning and hide that away from us and it will just decide whether to buy, sell or hold. There's a few more inputs we could juice out of those candlesticks by the way. And we call these indicators. And there's tons and tons of indicators out there. A very common one is called the simple moving average. And the idea is that it looks back in time at...
 some amount of time steps, whatever you want, say 200 time steps or 50 time steps, you specify the window. And it comes up with a moving average, hence the name, which is a number indicating generally sort of the direction of the graph, the price graph. If it's positive, then we're going up, if it's negative, then we're going down. If it's very positive, then we're going very far up. If it's very negative, then we're going very far down. You get the idea, simple moving average or SMA. Another one is called...
 exponential moving average. Another one is called RSI or Relive Strength Index. And another one that's very valuable is called VWAP, volume weighted average price. And then there's a whole bunch of others. Those are probably the most popular, especially for starters that I just listed. But there's many, many indicators. And basically you could pipe these into your model as additional inputs if you want. Now, because these indicators are summarizing price history...
 A case could be made that since we're using an LSTM recurrent neural network, which already sort of aggregates history over time, they may be unnecessary. It's something that you'll want to experiment on your own with. All right, we have price actions, the candlestick data, OHLCV, open high-low closed volume. That's five features right there that you can pipe in at every time step. We optionally have indicators which aggregate time steps, prices, they come up with additional numbers that...
 that may be valuable for your model. And one final input we could juice out of our price graphs is this. It's called risk arbitrage. And it's very valuable. Risk arbitrage, ARBITRAGE. It's a very strange word, arbitrage. And the idea goes like this. gdax.com is probably the most popular crypto exchange in America. I'd say spanning the English speaking countries in general.  opposes esto. Jason Larry Lee Ops<|en|>
 But we'll focus on G-dacks and Kraken. Kraken is a little bit less popular than G-dacks. It spends more than just the United States. So European and the UK countries can use that exchange. But it's less popular, less popular than G-dacks. Now here's why that's important. There's a phenomenon in finance called the efficient market hypothesis. And the idea goes that the price of any instrument, a stock or a bond, is exactly what it should be. The price of an instrument is what it should be. It's fair.
 They call it. In other words, the price of Bitcoin at like $11,000 what it is today is exactly what it should be. It should be $11,000. Now the idea of what it should be quote unquote is that kind of enlarge numbers of people who are involved on the exchange, on GDX. A large amount of people are involved. They're doing what's called fundamental analysis. They're researching the value of Bitcoin and watching the news, keeping an eye on all the other current.
 and currencies, etc. fundamental analysis. It's the idea of doing your research on a company, on a stock, on a cryptocurrency, any instrument, doing your research called fundamental analysis and deciding to buy and sell based on that. It's kind of the investor strategy. Well, with so many people performing fundamental analysis and so many high frequency trading algorithms, performing technical analysis, this is what we've been describing in this episode. It's the idea of following charts, following numbers and patterns. Between those.
 to the fundamental analysts and the technical analysts, the price of Bitcoin is going to sort of start to waver into the middle of what it should be. The actual value of a Bitcoin, the actual value of a Bitcoin to humanity. You know, $11,000 today, according to the efficient market hypothesis, is the real value that a Bitcoin holds to humankind. Now there's a lot of beef with the efficient market hypothesis. Today, traders obviously contend with it.
 And there's a whole thing where it's kind of chicken and egg sure it's efficient after the analysts have done their job But the analysts can still be at the front of the line doing their job and making some money in the process blah blah blah But the point is in order for the efficient market hypothesis to sort of work out is you have to have a lot of analysts at play You have to have a lot of people leveling out the price Well, there's more people on g-dacks and less people on Kraken now Let's say that g-dacks is price for a b-
 that coin is $11,000 and crackings is $10,000. Different prices, and this actually occurs a lot. More people on GDACs, less people on crackin'. Efficient market hypothesis is driven by numbers. That means GDACs is more correct than crackings. That means buy in crackin' because it's going to correct itself. They're gonna try to get the price up to the real price being a-
 $7,000. That's an oversimplification. And there's no conscious effort in this process on the crack in exchange or the people trading over there. It's sort of like this just natural equilibrium that occurs on financial exchanges. So that's another input we can pipe in at any time step is all of the OHLCV plus indicators coming from GDAX. Pipe that in. But also the same data from...
 Crackin pipe both exchanges price actions and indicators in at the same time And then it could help your model decide whether to buy on crackin because it might catch up to g-dacks. It's pretty slick, huh? Cool. So that's it for trading and cryptocurrency Now I've already started the code for this project. It's six months old by now And it's actually pretty close to working It's using deeper enforcement learning by way of tensor force with actually convolution
 Neural Networks, not LSTM RNNs, and we'll discuss the reasoning for that in the next episode. And I'll post a link to this open source project on my website, ocdevelop.com, forward slash podcasts, forward slash machine learning. So next time we'll discuss hyper parameter selection for our models, and then we'll finally get into deep reinforcement learning. The cool stuff, artificial intelligence. All right, see you next time.
 Welcome back to Machine Learning Guide. I'm your host Tyler Renelli. MLG teaches the fundamentals of machine learning and artificial intelligence. It covers intuition, models, math, languages, frameworks, and more. Where your other machine learning resources provide the trees, I provide the forest. Visual is the best primary learning modality, but audio is a great supplement during exercise, commute, and chores. Consider MLG or syllabus with highly curated resources for e-
 episodes details at ocdevel.com forward slash m l g. I'm also starting a new podcast which could use your support. It's called left-nears life hacks and teaches productivity focused tips and tricks. Some which could prove beneficial in your machine learning education journey. Find that at ocdevel.com forward slash l l h. This is episode 27 hyper parameters part one. Today we're going to be talking about hyper parameters.
 This is going to be a two-part episode, got a little bit longer than I thought it would. But let's dive right in. What are hyperparameters? Well, we've talked about hyperparameters before, by comparison to parameters. They're anything that the human decides on. Parameters are the numbers that the machine learning model learns in its learning process. So in linear and logistic regression, your theta parameters are these weights in front of the coefficients. They're these numbers that the model learns. Parameters are the bit that the machine learning model learns.
 hyperparameters are any sort of knobs and dials that you as the human are in control of. So there's some obvious cases of hyperparameter selection, for example, with regularization that we'll talk about in the next episode, the selection of L1, L2, and dropout, both the numerical values that you can assign to those regularization terms, or even the mere use of those regularization parameters. That's a hyperselection. That's something that you as a human choose in neural network R.
 architecture, the number of neurons in any layer and the number of layers. Those are hyper parameters. There's something that you choose. A hyper parameters, they really mean anything the human chooses. So let's get a little bit less intuitive. The selection of what type of model to use in a machine learning scenario. Are you going to use linear regression, logistic regression? Are you going to use naive bays or a neural network? That's a hyper parameter. It doesn't seem like a hyper parameter at first glance, but really, any sort of
 decision that you as the human can make that affects the machine learning model that's a hyperparameter and it's really only a useful characteristic When you can compare the selection of hyperparameters So like I said you could choose a linear regression model or you can choose a neural network that selection right there is a hyperparameter And it's a useful hyperparameter because you can compare the performance using cross validation of linear regression to a neural network. They're not apples and oranges
 The result is a numerical score representing the relative performance of one versus the other. Now, we're going to be talking about a lot of various hyper parameters in these two episodes, things like neural network architecture, activation functions, non-linearities, regularization parameters, stochastic gradient descent optimizers like adagrad and atom, things like feature scaling and batch normalization, stuff like this. This episode is going to be some of the more high level parts, things like neural network architecture.
 decision of what types of layers to use like LSTM versus CNN layers. And the next episode will be smaller bits like regularization L1, L2, and dropout. But don't let that big versus small part fool you. Every single hyper parameter can be vital to the success of a model's training. As an anecdote, in the Bitcoin trading bot project that we're working on together, I have personally found that the combination of L1, L2, and dropout.
 Those are your regularization terms. You can use one or two or three of them, and whichever ones you use can have some numerical value. I found those regularization terms to be more vital to the success of the Deep Re enforcement learning agent than the selection of the agent type that we'll talk about in reinforcement learning episode, things like Proximate Policy Optimization, Versus Deep Q Network. Now, those are huge. The difference between a PPO and a DQN, that's a huge difference. Prest1B.
 that the combination of regularization terms had a stronger effect on the output. So every hyper parameter counts. And when it comes to choosing hyper parameters, every machine learning model has a handful of hyper parameters that goes along with it. So a neural network, for example, you can choose its width, its depth, the types of layers, whether they be LSTM layers, conv layers, or dense layers, L1, L2, and dropout regularization, and a handful of other hyper parameters. But most...
 Cyber parameters have a sane default that you can start with. So for example, L1 and L2, they tend to be in the .001 range. Somewhere around there tends to be a sweet spot for many researchers just getting started. So what you do is you start with the sane defaults. Let's say L1 and L2 set up .001, net depth at one or two, net width to let's say eight neuron, something like this. And then from there after you've selected your sane d.
 of hyperparameters, then you search for better hyperparameter combinations. You'll use something called grid search or a random search, which I'll describe in the next episode, or a more learning-oriented approach called Bayesian optimization, which uses Bayesian statistics to actually hone in on better and better hyperparameter combinations over time. So that's the high-level stuff. Hyperparameters are knobs that use a human turn. Don't-
 Don't shirk the turning of these knobs because it is vital to the success of your model's training process. And the way you go about this is you'll use all the defaults out of the box. And from there you'll use grid search random search or Bayesian optimization to get better and better and better combinations over time. Now when I first found out about hyper parameters by comparison to parameters, I thought I was kind of ugly. I was kind of surprised. I thought the goal of machine learning was to learn everything. I thought I was machine learning models supposed to look...
 learn the nuts and bolts, especially when we start talking about AI. I thought that AI is supposed to be super self-sufficient. Why do we as humans have to turn these dials and knobs? That doesn't seem very magical. It seems like a buzz killed to me. And in fact, it seems like there's more hyper parameters than there are parameters in any one machine learning model. It's like the human does more than the model. Pretty unmagical to me. What's the deal here? Well, you and I are not the only people thinking this. Moving hyper parameters from the equation.
 has been a longstanding goal of the machine learning community. Right now we sort of have Linux boxes, where you have to compile everything from scratch and you have to add your own packages with certain flags based on your CPU architecture and all this stuff. It's very complicated and very hands-on. The goal we want to get to eventually is a Mac. You just order a Mac, comes in a box, you open it up and it's good to go. But it's just not that easy, you see. This is the goal of researchers is to subs-
 the hyper parameters into the machine learning model so that they become parameters so that the machine learning model can learn everything from nuts to bolts. It's just difficult and it will take time. But it's something that we have been accomplishing with every machine learning breakthrough. So for example, consider the Neural Network. The Neural Network introduced two very powerful breakthroughs to the machine learning community. One was the ability to represent any complex situation. They call this
 a universal function approximator. Theoretically, a neural network, if done properly, can basically do anything. What that means is, theoretically, you could use a neural network for everything. Forget the support vector machine, forget logistic regression, forget the selection of any model under the sun, and just use a neural network. Now, like I said before, selecting a model is itself a hyper parameter. And therefore, the creation of the
 neural network did away with a very major hyper parameter, selecting a model. Now of course as we've seen in prior episodes it's not so simple, a lot of times your circumstances call for a shallow model, whether it be for computational efficiency or you don't have enough data to learn from, your special case may call for a shallow learning model. But increasingly we're getting to this point where it's just like throw a neural network at the situation. It's this idea of sure you can use a PIST.
 or a rifle, but why not just use a bazooka? We've got the GPUs, we've got Google Cloud Platform, just use a Neural Network. Neural Networks also eliminated a very taxing hyper parameter called feature selection or feature engineering. In the shallow learning days, you need to be very selective about what features are going to input into your model. In certain circumstances, features that are very highly correlated can mess up certain models.
 performance. And so you want to hand remove any features that are highly correlated. Additionally, you may need to scale features down. And in case you have too many features, you can do one of two things. You can either hand remove specific features that you know aren't that important. Or you can pipe them through a dimensionality reduction model like principal component analysis PCA, before piping it into your linear regression model.
 decision tree. So we have two hyper parameters there. One is the feature engineering slash feature selection bit. And the other is the decision to use PCA. And this part of the equation is a very time intensive process. It's not just some dial you turn like the L1 regularization parameter. No, the stuff's going to take days and days to work with. Well, neural networks theoretically handle feature engineering for you in the early layers of a neural network, dimensionality reduction.
 automatically occurs as long as your layer width is less than the number of inputs. And neurons in a neural network learn to latch onto important bits of information and disregard less important bits of information. So neural networks effectively eliminated feature engineering and model selection. Now obviously that's a major oversimplification. I'm ruffling many feathers here. I'm sure. But you get the point that I'm getting at is over time these advanced
 in machine learning technology, they do exactly that. They remove hyper parameters and subsume them into the machine learning model as parameters. So in other words, over time, ideally, we won't have to be dealing with so many hyper parameters. And in fact, Google is so hot on this topic of automatic hyper parameter selection. They've created a project called AutoML and the concept of learning the ideal hyper parameters for your machine learning model.
 called meta-learning, right? Because you're learning how better to learn. You're learning the parameters that help you learn your parameters. Meta-learning and Google has a project called AutoML that they're working on very hard to solve this pain point. But in the meantime, you and me, we lowly developers, are just gonna have to suffer with hyper-parameter selection and tuning. All right, so we're gonna take a top-down approach. We're gonna start from the very top of model selection.
 And then once we get to neural networks, we're gonna start to design the layer architecture. And then within the layers, we're going to start to work on things like activation functions and regularization terms and stuff like this. Top down. We'll start with model selection. You'll recall from the shallow learning episodes in the resources section, I had a decision tree diagram that helped you pick your machine learning model based on your situation. Is it classification or is it regression? Is it unsupervised or is...
 it supervised. I'm going to leave it to you to go back to that diagram. That's kind of the very fine tuned model selection process. I'm going to paint with some very broad strokes here. Sort of the top dogs in the shallow learning machine learning models. So first off, are we dealing with unsupervised learning or supervised learning? If you're doing unsupervised, then you're going to go down a totally different path. That's a less common sort of machine learning scenario. And so I'm just going to say, hey, K means clustering. We'll just call it that for now. Moving on, assuming you're using supervised learning and...
 assuming that you actually have data labels to train on. Now we're going to ask ourselves some questions. Is the situation linear? That's the first and important question. Is your circumstance linear? Is the thing that you're trying to learn a linear equation? Well, how do you know if it's linear? You can sort of think about it. So for example, selecting hyperparameters, that is a non-linear situation. And here's why. Many parameters play with each other in a specific way. So for example, learning rate and...
 and epochs or optimization steps, the number of times you train on a specific batch. Learning rate and epochs, those two play together specifically. Generally, you want higher learning rate with lower epochs and vice versa. Okay, so they play together. Importantly, there's sort of a cutoff. There's a threshold at which lower learning rate or higher learning rate isn't going to help you no matter what the epochs. So there's kind of a cliff, or maybe it's not a...
 cliff, maybe it's something of a parabola curve. Anyway, the point being, this is not a linear situation. Linear situations are cases in which you can plot your data on a line. You know, all of your data points, throw them all on your graph, and they'll all kind of scatter generally around a line, or a hyperplane if you're dealing in more than two dimensions. If your situation is linear, you use linear regression or logistic regression. Linear regression for regression, it's going to output a number like the cost of a house, logistic regression for class.
 classification, deciding if it's a cat dog or tree. If you don't know if your situation is linear, the general rule thumb is give it a shot, try linear or logistic regression, give it a shot and see how it does. If it is non-linear, but you don't have a lot of data, okay, because if you have a lot of data, then you should probably just move on to the neural network. But generally speaking, lots of data you go to deep learning. If you don't have a lot of data, okay, we're going to work with naive bays, decision trees and all the decision tree off.
 We've got random forests, gradient boosting, extreme gradient boosting, XGD that's super popular these days. Which one do you use? Triumall. As you'll see with every hyperparameter, the name of the game is Triumall. And you compare the relative performance of one model to the next by way of something called cross-falledation, which we'll get to in the bit about grid search and random search. So, trial and error regression, trial and logistic regression, naive-based decision trees, random forests...
 gradient boosting, extreme gradient boosting. By the way, random forests is like decision trees plus plus. It's like a better decision tree. And gradient boosting is like decision tree plus plus plus. It's like an even better decision tree. An extreme gradient boosting or XGD is even more better than all that. So actually if you're gonna decide to go the decision tree route, you do yourself a favor by also trying random forests, gradient boosting and extreme gradient boosting because in theory, all those are just better.
 than decision trees anyway. My advice, if you're not gonna use Neural Network, use Gradient Boosting. That's my sane default I use out of the box. If I don't know what to do, I generally start with Gradient Boosting, which again is a spin off of decision tree, but with a lot of optimizations. All right, now let's assume you have lots of data, and you can enter the deep learning territory. Now we're gonna go into deep learning. We're gonna end this episode on Network Architecture and Design.
 network design. So the first very high level decision you're going to make in your neural network architecture is what types of layers are you going to use. In other words, what type of neural network is this in the first place? Then you really have the main three decisions to choose from LSTM RNNs, Convnet's and Multi-Layer perceptrons. There's plenty of other network architectures out there. There's a webpage called the Neural Network Zoo where you can look at images and
 descriptions of different types of neural networks like belief nets and auto encoders. Those two for example are actually very popular other neural network architectures besides the three that I just listed. And then there's a whole bunch more, but those are more advanced topics in deep learning. So the very common architectures are CNN, RNN, and MLP. And they have very obvious use cases. Are you doing vision stuff? Convolution neural network? CNN? Are you doing time stuff? LSTM, RNN. Are you doing...
 other MLP. So let's think of a few examples for these situations. You generally use a convolutional neural network if you're doing vision stuff. If you're looking at a picture, if you're playing a video game, if you're building a self-driving car, if vision is involved, you'll use a CNN. If time is involved, if you're doing stock trading, if you're doing weather prediction or natural language processing, you'll use an LSTM RNN, a recurrent neural network anyway. You don't have to necessarily use LSTM. You can use a vanilla recurrent neural network or you can...
 you can use a GRU recurrent neural network, but the most popular that same default is LSTM cells in an RNN for time series data. And for everything else, you'll use a multi-layer perceptron. That's basically like throwing all your inputs in a blender. If it doesn't have time and it doesn't have space, then it doesn't have anything, so you throw it in a blender. MLP. Okay then, how about our Bitcoin trading bot? MLDI just said stocks is a time series phenomenon, so LSTM.
 TMRNNs. Actually, we are using a convolutional neural network in our GitHub project, not an LSTM RNN. What? They said CNNs are for vision. And you said specifically that stock is time series and time series is LSTM. I did indeed, but this is one of those cases where you start with a default and I did indeed start with LSTMs and then you experiment with alternatives using grid search, random search, Bayesian Optimization.
 And I found through experimentation that CNN's worked better. And after reading a handful of other papers out there on archive.org and talking to colleagues who were doing algorithmic trading, it would appear that many people out there agree or have had the same experience that convnet outperform LSTMs for algorithmic trading. Why might that be? I actually don't know the reason. I'm as surprised as you are. I thought LSTMs would shine here. I think...
 the reason, and this is something I've read as well, is vanishing and exploding gradients. Remember from the NLP episodes, I said that the impetus for the invention of the LSTM cell was that recurrent neural networks had this thing called vanishing and exploding gradient problem. The idea goes that you've piped in time steps in a time series data set in order to predict something, maybe the next time step or some value. Well, if you have too many time steps you basically, or...
 overwhelm the neurons in your neural network. You're sort of piling on information after information after information. And you can cause this exploding gradient, they call it, where you saturate your neurons. It's kind of like yelling too loudly and then these neurons explode and there's blood everywhere. It's a horrible problem. Or the opposite can be the case where you don't have enough signal going back through your time steps in the training process because the signal gets dampened and dampened over time that's called the vanishing gradient problem. And the way we mitigate.
 This was we replaced the Rayloo activation function in a recurrent neural network We just plucked that Rayloo activation function out that neuron and we replaced it with a cell a LSTM cell Long short-term memory cell which has a whole bunch of complex architecture within it It's not just some activation function So it's like we plucked out this little marble of a neuron out of the network through it away And we popped in this and we pop in this complex watch looking cogs and
 years block and we snap it into place. That's the LSTM. And that thing has the capacity to learn to forget time sequences over time and latch on to specific time sequences over time, which allows it to mitigate the vanishing and exploding gradient problem. But, but you only really see that showcased in medium length time sequences like a sentence. I mean, what's the maximum number of words in a sentence, let's say 50, 100 words sentence. Okay, that's fine. It can hit
 handle it no problem. But can it handle an infinite sequence of steps? Well, Bitcoin price history is an infinite sequence of steps. I mean, we go all the way back to 2009 in second intervals, all the way till now. I mean, millions and millions of steps, and it keeps going. So it goes off into the future to infinity. So can an LSTM RNN really handle that many steps? I don't know, actually personally. And I think that's the running theory is to why LSTMs don't work on stocks.
 prices is that maybe they actually can't handle infinite sequences or very very very large sequences. There's ways that you can get around that. I've heard about stopping training per time step to a maximum window length in the past, but it gets pretty hairy. So what about a Convna? How do we make that work for stock prices or Bitcoin prices? Well, a human goes to a exchange terminal on their computer like gdax.com and they got a black screen and they have a price graph.
 that goes up and down and up, and it's made out of candlesticks that are colored red and green, and under the price graph is a histogram of volume. Well that's visual. You can imagine actually taking a screenshot of G-DACs once every second, saving it to your computer, and running that through your COMVNET to train on. Now that's an intuitive approach, but a cleaner approach is to actually use the same data you are using before for your LSTM, namely candlestick data, OHLCV. And what you'll do is...
 You'll construct a time window where the x-axis is your time, just like it would be in your exchange terminal. The x-axis is time steps and the z-axis actually, the channels or depth of your picture. It's basically like the RGB channels, what would be the RGB channels of a normal picture, you actually use your features there. So the depth of your picture is your candlestick. And then the height of your picture is nothing.
 it's just one. So it's like the machine learning model is visually looking at a time window of price data in order to determine whether to buy sell hold. Okay, so that's deciding on what type of layers to use. Now we're going to talk about the shape of your neural network layer width and number of layers and where you might place these LSTM or conv layers. By the way, most neural networks will have some amount of dense layers. Even if they are a convnet.
 most of their layers are conve layers. Maybe at the very end it'll still have one or two dense layers. And if you're an LSTM, you may still have some amount of dense layers before the LSTM layers and or after them. A dense layer, remember, is just a vanilla neural network layer. It's called dense because all neurons from the prior layer connect to all neurons of the current layer. Everything connects every which way, so it's dense. Okay.
 neural network shape. There is a same default that's recommended. You could find it out there on Stack Overflow. I've seen this quote that comes from a textbook. I don't know which textbook, but I've seen it over and over and it says a same default for network shape goes like this. You have your input layer. Okay, it's not really a layer, but they call it a layer. So you just you just have your inputs and you have your output layer and that's usually just going to be one neuron in the classification or a regression situation. If it's a multi-class class of a key.
 case in situation, your last layer is a softmax neuron, softmax, which we'll talk about in the activation functions bit. So you have your inputs and your outputs. Now what goes in between? What are the hidden layers? That's the thing that you really care about. Well, a sane default of number of layers is one. One hidden layer, maybe two. If you don't know anything else about your situation and you just had to guess, shot in the dark, one or two layers, start with one and give two a shot in your hyperparameter search. How about with? Well, the note.
 number of neurons in your one layer, a sane default would be the mean of the number of inputs and the number of outputs. So if you've got three inputs and one output, your one hidden layer should have two neurons, the mean of your inputs and your outputs. So it just gets smaller. It's basically performing dimensionality reduction on your inputs. That's a sane default. Some networks call for much larger structure. I mean, five, twelve neurons, more layers deep.
 It totally depends on the situation, but if you didn't know anything at all, go with the one or two layers and the width of your layers being the mean of your inputs and outputs. But a lot of times you can actually think about it. You can intuit the shape of your neural network. So remember in a prior episode I talked about using a neural network to detect a face. And I was actually describing a multi-layer perceptron to detect a face. What you'd usually use is a convent net. But let's go with the multi-layer.
 perceptron example just because it allows us to think a little clearer about this. You have a picture of a face, five pixels by five pixels. Okay, so 25 pixels total. It's an image on your hard drive. That's your input layer is your pixels. Now, you might reconstruct a face by combining black dots in your pixels into lines, lines and curves and edges and angles. How many of these types of things could you imagine? I don't know. Let's say six. These types of curves.
 and angles and lines. So your first layer in the neural network will be six neurons. Next we will combine those lines and edges and stuff into shapes. Eyes, ears, mouth, and nose. Four. Eyes, ears, mouth, nose. That's four neurons. It's going to detect four types of objects in the next layer. And finally, the last layer is one neuron to combine all the eyes, ears, mouth, and nose into face. That one...
 on being face detector. So 25 pixels, boil down into six angles and edges and curves and lines, those boil down into four objects, iZero's mouth and nose, and those boil down into one object, the face. So you can actually think your way through the design of a neural network architecture. It's not so black-box as people make it out to be necessarily. Sometimes it is. Sometimes you have no clue how things sort of combine.
 hierarchically like that. And so you go with the same default and use hyper parameter search to try to find better and better network architectures from there. And in this case with the face detector, you would indeed want to still use hyper parameter search to find a better architecture over time because you're almost always going to be wrong the first time. It's just something reasonable to start with. How about our Bitcoin trading bot? Well, we have OHLCV open high low closed volume. Cannel stick. That's our features per time.
 Now, like I said, I'm actually using a Convnet in the code, but I think it'll be easier and clearer to explain as an LSTM RNN. So I'm going to do it that way. Each time step takes in a candlestick. Five features. Those are your inputs. And the output we're going to say is the decision to buy or sell. That only exists in a reinforcement learning model. In a supervised learning model, it would basically be predicting the next price action. And then from there, you decide whether to buy or sell.
 So let's assume we're using the reinforcement learning scenario. We have inside of our reinforcement learning agent and LSTM model, where the inputs are five features, a candlestick, and the output is one neuron, the decision to buy or sell, which incidentally is going to be a 10-H activation function. We'll talk about that in the next episode. What goes in the middle? Well, we definitely want an LSTM layer, right? Because this is a time series. We want a layer that's building up information over time about how the...
 graph is acting in order that it can predict the price at the next time step. So theoretically we just need one LSTM layer. And according to the idea of the default width is the mean of the inputs and the output, the width of your LSTM layer is three. Three LSTM cells in a single layer. Bing-Bang-BOOM. Now let's play with this a little bit. We had five features coming from one exchange. What if we wanted to do that arbitrage thing, that risk arbitrage thing I mentioned the last episode? Now you'd have 10-
 features. A candle stick from G-Dacks and a candle stick from Kraken. Now you have 10 features coming in. Well that's nice. Usually more data is better, more features is better up into a certain point. There tends to be sort of this ceiling where you start to reach what's called the Curse of Dimensionality. You have too much information. Now I don't think 10 is too much information, but let's pretend that it is. What could you do? Well you could add a dense layer above your LSTM as the first layer, add a new layer.
 between the inputs and your LSTM layer and it's a dance layer. And what does it do? It boils down the inputs into their essence. Why don't we make the width of this dance layer four? We boil down 10 inputs to four. And then we send those off to the LSTM layer to start doing the history crunching. Maybe we don't want to overwhelm it. We want to send it to the essence. We'll let the dance layer boil the essence. Very nice, very nice. So we have a dance layer at the top, performing dimensionality reduction on the...
 inputs and passing it off to the LSTM layer, which is performing historical analysis on the time steps. Now, remember in the prior episode, I mentioned these things called indicators. We have these numbers that can summarize some amount of time steps in the past. Let's say 200 time steps in the past till now. You might want to take the simple moving average, SMA, or the exponential moving average, EMA, or the relative strength index, RSI. SMA, for example, represents...
 Since the moving average, the direction as a number from 200 time steps in the past till now, you can basically think of it like the angle from then till now. It's positive if we're going up, it's negative if we're going down. Now, we can use a third party library called technical analysis library, Tau Lib, TA, hyphen Lib, and we are in our project, actually. We could use that project to generate these numbers given the time window and pipe those in as inputs at the top.
 But there's a problem with that. The problem is we'll have to decide which technical indicators to use Which indicators do we want to use? There's tons of them tons and tons of them and for each of those indicators What is the time horizon 200 steps 100 steps a thousand steps? So those would just add additional hyper parameters onto your plate Nobody likes hyper parameters now normally more data is better and in fact We are experimenting with Talib in
 the project, but the reason I'm pooing it here is because we have an LSTM layer. An LSTM layer already learns aggregated information about your historical data. It's already doing this rolling process, this building up its own theory about what's going on through the time steps. In other words, it's kind of building its own technical indicators. So why don't we just let LSTM automatically learn technical indicators rather than piping them in our site?
 Then it can learn whatever sequence of time steps it likes, whether it be 200 or 400. And it can learn which specific indicators in its own little internals are valuable for its projections. In other words, it's going to automatically learn the SMA or the EMA, depending on what proves useful to the LSTM. So what would we do here? Well, we could add another neuron in the LSTM layer for every technical indicator we want to learn.
 the layer to learn. So let's just grab bag 10 indicators. I mean, that's a decent number of indicators that you might use in a typical Algo trading bot, 10 indicators. Let's have our LSTM layer learn 10 indicators all on its own. So we'll just add 10 neurons to the LSTM layer. Very cool. So that allows us to reduce the amount of hyper parameters we have to deal with. All right, we have a decent model here for trading. Now our trading bot, if we spin it as a reinforcement learning algorithm, which it is in code, is going to give us...
 a decision. It's going to give us what's called a signal. The amount it wants us to buy or the amount it wants us to sell or zero for hold. There's something missing from this equation though. It's telling us to buy and sell based on price history, but it doesn't know how much money we have to our name to buy and sell with. It doesn't know how much US dollars we have in GDACs that we can buy with, and it doesn't know how much bitcoin we have in GDACs to sell with.
 Now we could just in our code say buy only what we can afford based on our bots suggestion. But ideally the bot will suggest a price to buy and sell based on what you have. That way if you don't have as much it will say well you can still buy a little now and benefit. So ideally your balances would also be an input. Now you don't want to add your balances as an input at the top. Why? Because then your
 balances get mixed in with the history. It's like you threw your balances, your Bitcoin balance and your dollars balance in at the top in this LSTM blender. And now you have your balances splattered all over the graph, all over this price graph that it's built up for itself with technical indicators and stuff. You have your balances all over there. The history should be unaffected by your balance. The balance only pertains to now, the present moment. So what can we do? We can actually pipe in our balances.
 as inputs to the final neuron, the final layer. In a neural network, you can pipe in data at any point in the network. It doesn't actually have to happen at the very beginning. It can happen at any layer. You can pipe in additional features. And you'll see this a lot with image captioning where one part of the network is handling image recognition and another part of the network is handling natural language processing and then they connect with each other downstream. So in our code, what we do is...
 is we add the present data, the stationary here and now data, downstream in the network in a later layer. Past the time series aware layers, being LSTMs or conflators. Okay, so the point there was you can imagine building a network's shape and selecting the types of layers all by yourself in an intuitive fashion, even though a neural network is a theoretical black box. You can shape it, you can mold it with your hands.
 based on what you know about the situation, and then use hyper parameter search to search for a more optimal number of layers and width of layers. Now it looks like I have a little bit more time actually, so I'm gonna jump into activation functions, non-linearities. These I expected to get to in the next episode. Activation functions. Every neuron in the hidden layers of a neural network have applied to them an activation function, or a non,-
 linearity. These are things like sigmoid, tanh, raylu, and the like. Now an activation function is the function that you apply after the weighted sum. Every neuron inside of it is basically a linear regression unit. It's just a weighted sum of the prior neurons. So it learns these weights, it learns the theta parameters just like a linear regression to multiply coefficients by for every neuron in the prior layer. It's just...
 just a linear regression unit. And then you wrap that linear regression unit in an activation function in a non-linear function, something like a sigmoid or a tan H or a rey-loo. Now, why do we do that? Why do we have to wrap it in something? Why don't we just have a bunch of linear regression units all connected to each other and call that a neural network? Well, the reason is that linear functions, when combined, they make another linear function. In other words, if we did that, if every neuron was a linear regression unit,
 it, our result would be a linear machine learning model. So it could only learn linear situations like the price of a house. It couldn't learn something so complex as a face detector. Why is that? That seems strange. If you have a bunch of lines and you throw them all in a piece of paper, you don't have one big line. You have something kind of Zoro-looking, like Zoro-sliced every witch direction on a tapestry, and you'd be able to cut up the data into parts. And of course, the learning part of the machine learning process would learn how to slice
 how to partition things into their own cubbies. Right? Isn't that what you'd get by combining multiple linear functions? No. You would get a big line. You would actually get another line that's sort of the average of all these little lines. It's very interesting. It's not what you would expect. So you have to transform these neurons into non-linear versions of themselves so that you don't just end up with a big line. So how do we do that? We apply a...
 non-linear function to it. We pipe it through something that adds wiggles. Any wiggle, any wiggle at all. All that matters is that we get wiggles in our graph. Because when you shake up a bunch of wiggles and throw them onto a piece of paper, you do have wiggles that slice up the paper like Zorro. Zorro with curves, curvy Zorro. You don't get one giant wiggle. You get a bunch of criss-crossing wiggles. So the way we make our neural network non-linear, which is super, super important, that's the whole point of a neural network, is it's a...
 nonlinear function approximator is that we just add wiggles. We just take those lines that are output at every neuron and we bend them into curves. Now the classic curve that we've seen before is the sigmoid function, the S curve. It's an S between zero and one. So it's just an S on a graph between zero and one. An alternative version of that is a tan H, T-A-N-H, tan H function, which is an S just
 Just like a sigmoid function, just like a logistic regression unit, but goes between negative one and one. So it's a tall S. So we can apply either of these. We could either use a sigmoid activation function or we could use a tanH activation function. Which one should we use? Well, we should actually use tanH. And I actually don't know why sigmoid isn't used very commonly in neural networks. It must be something with the math, but tanH is much preferred as an activation function in a neural network. It kind of makes sense for our trading...
 because every neuron in the network is piping in with its own two cents as to whether to buy or sell. Right? A tan H goes from negative 1 to 1. In other words, buy some amount, buy some multiplication between 0 and positive 1, or sell some amount, sell some multiplication between 0 and negative 1. So each neuron is sending their own buy and sell signals. They all have their own little...
 opinion. And then the final neuron sort of collects all the votes, tallies all the votes, and says, you know what? I think the majority is raising their hand that we should buy. So tan H sometimes makes sense. I don't get exactly why you couldn't transform sigmoid into the equivalent of a bicell action. But for whatever reason, you just don't see a lot of sigmoids used within neural networks. What you see a lot more common than sigmoids or tan H is something called Raylou rectified linear unit, R-E-
 L U. Well, Raylou is a weird one. Raylou is a nonlinearity. It's not a line, and that's what's important. But it's almost a line. It's a line that y equals zero for x equals zero to negative infinity. And then it's an angle going to the top right. So it's kind of like a hockey stick or a V that you sort of tip over. Now it doesn't seem very useful intuitively. I mean, I can't think of how you sort of pull a bicell signal.
 a lot of that, or come up with any other sort of intuitive application of this activation function. I don't really understand why it works, but for whatever reason, it's computationally more efficient on your computer. It's faster to run. Something about the calculus works more efficient on this function. And importantly, it mitigates the vanishing and exploding gradient problem. It turns out the vanishing and exploding gradient problem is not only with RNNs.
 deep networks and conv nets specifically tend to be very, very, very deep. These things like ResNet and GoogleNet and stuff, they're very deep. Many convolutional layers. And so we use RayLoo activation functions in those layers to mitigate the vanishing and exploding gradient problem. And I don't understand why. I don't know why it mitigates that problem. But it's something about the math, something about the calculus. And therefore you'll see a RayLoo activation.
 function used much more commonly than anything else. It is the same default that comes out of the box with a neural network. One of those things where by default, your dense layers and your con layers use a ray-loo by default. And then you'll use hyper search to try tan H and see if it performs better than Ray-loo. There's a handful of other types of ray-loo. There's a Ray-loo family. There's something called the leaky ray-loo, the elu x.
 Sponential linear unit ELU. There's C-LU, S-E-L-U, there's C-R-E-L-U, Krayloo, all these different types of RaylUs They kind of they they look slightly different. They kind of had that hockey stick shape some of them curve The edge off of the hockey stick elbow and some of them don't stay at y equals zero Towards negative infinity, but they actually curve down a little bit to the bottom left towards
 negative infinity. They all add little different twists. They're kind of more modern versions of Rayloo that researchers are trying to hone in on the perfect Rayloo. This is another thing where you might want to try different versions of Rayloo with hyperparameter search. Try it, Elu and Celu and Krayloo and Leaky Rayloo and all those things. There's also a thing put out by Google in their grand quests to eliminate hyperparameters called the SWISH SWISH
 activation function that's actually a learnable activation function. It learns what's the best activation function for your neural network during the training process, which that would be so sweet. That would be really nice not to have to choose an activation function. I hate hyper parameters. So, sigmoid, softmax, Rayloo, and the Rayloo family. And generally, Rayloo is used by default. Try them all using hyper search, find what works best for your neural network.
 for R's in my experience in our Bitcoin trader, it's tan H. And those are the activation functions for the hidden layers. Those are the activation functions for the internal neurons communicating with each other. The last neuron or the last layer of your neural network is the output layer. It's the output function. It's the thing that's going to give you the human something of value. So if it's a classification scenario, you will use a sigmoid function actually.
 is a time when you can use the sigmoid, it's because it gives a classification to you, the human, that where that sigmoid function is valuable. You could use a tan H, we're using a tan H in our situation, because it gives a buy or a cell signal. You could use nothing, this is one case where you can not have an activation function, and what would that be? That's regression. Without an activation function wrapping your weighted sum, what you get is the weighted sum. That's the output. That's a number...
 and that might be the cost of a house, for example. So no activation function for regression scenarios. And then for multi-class classification, dog cat tree, you use a softmax, softmax. That's a multi-class version of a sigmoid function. And that would only exist as the output layer of your neural network. You wouldn't have a softmax inside of your neural networks hidden layers. Awesome guys, that was a long episode. I apologize for the length.
 All the resources I've listed up until now is where I got all this information from, so there's nothing new to put into the resources section. And next time we'll talk about regularization, optimizers, feature scaling and batch normalization, and finally, hyper search and how to do that with grid search such. Talk to you then.
